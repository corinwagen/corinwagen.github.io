  <rss version="2.0">
      <channel>
          <description>My personal blog, mainly focusing on issues of chemistry and metascience, unified by trying to answer the question &#34;how can we make science better&#34;?</description>
          <link>https://corinwagen.github.io</link>
          <title>Corin Wagen</title>
          <item>
              <title>Creative Software Licenses</title>
              <link>public/blog/20250804_licenses.html</link>
              <description><![CDATA[
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250804_moneylender.jpg" style="width:450px;" />
  <figcaption><i>The Moneylender and his Wife</i>, Quentin Matsys (1514)</figcaption>
</figure>

<i>Inspired by <a href="https://slatestarcodex.com/2020/03/30/legal-systems-very-different-from-ours-because-i-just-made-them-up/">related</a> <a href="https://slatestarcodex.com/2020/06/17/slightly-skew-systems-of-government/">writings</a> from Scott Alexander, who is funnier than me. TW: fiction, but barely.</i>

<br>
<br>

<p>
AlphaProteinStructure-2 is a deep learning model that can predict the structure of mesoscale protein complexes like amyloid fibrils. AlphaProteinStructure-2 is free for academic usage. Typically this means researchers at universities can use the model for free, while researchers at for-profit institutions are banned from using the model without an explicit license agreement.
</p>

<p>
This creates arbitrage opportunities. For-profit companies can “collaborate” with academic groups and use the model for free in exchange for other forms of compensation. Similarly, “academics” in the process of creating startups from their academic work are incentivized to maintain their institutional affiliations for as long as possible. Both of these loopholes deprive model creators of the chance to capture the value they’re creating, a problem which plagued AlphaProteinStructure-1.
</p>

<p>
AlphaProteinStructure-2 solves this by explicitly specifying that the model is free for academic usage, not for academic researchers. Running jobs for companies doesn’t count as academic usage, nor does research in support of a future startup. To use AlphaProteinStructure-2, scientists must explicitly disavow any future commercial applications of their work and pledge to maintain the highest standards of academic purity. Because of the inevitable diffusion of ideas within the university, this has led AlphaProteinStructure-2 to be completely banned by all major US research institutions.
</p>

<p>
The only academic users of AlphaProteinStructure-2 are a handful of land-grant universities whose tech-transfer offices have been shut down by federal regulators for abuse of the patent systems. To ensure that no future commercialization is possible, all incoming professors, postdocs, and graduate students must symbolically run a single AlphaProteinStructure-2 calculation when they join. It is believed that important breakthroughs in Alzheimer’s research have occurred at one or more of these universities, but no scientific publisher has yet been able to stomach the legal risk needed to publish the results.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Rand-1 is a multimodal spectroscopy model developed by a decentralized anarcho-capitalist research organization. Rand-1 is not licensed for non-commercial use; only for-profit companies are allowed to use Rand-1 (in exchange for a license purchase). Model-hosting companies are allowed to host Rand-1 but cannot allow any academics to use the model through their platform. Researchers at for-profit universities are fine, though.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Evolv-1a is a RNA language model that’s free for benchmarking but requires a paid license agreement for business usage. The somewhat muddy line between “benchmarking” and “business usage” is enforced by vigorous litigation. Most companies have minimized legal risk by using a single model system for benchmarking and explicitly guaranteeing that they will never use this model system for any commercial application.
</p>

<p>
For sociological reasons, tRNA has become the go-to standard for assessing Evolv-1a and its competitors, with virtually every company using tRNA-based model systems as internal benchmarks. This consensus seemed quite safe until a family of tRNA structural mutations was implicated in treatment-resistant depression. 29 of the top 30 pharmaceutical companies had used tRNA as a RNA-language-model benchmark, leaving Takeda free to pursue this target virtually without opposition. Efforts by other companies to acquire tRNA assets from startups were blocked by litigation, while Takeda’s drug is expected to enter Phase 3 later this year.
</p>

<p>
In future, it is expected that all RNA-language-model benchmarking will occur through shell corporations to mitigate risks of this sort.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
DCD-2 is a pocket-conditioned generative model for macrocycles. DCD-2 is completely free to use: simply upload a protein structure, specify the pocket, and DCD-2 will output the predicted binding affinity (with uncertainty estimates) and the structure of the macrocycle in <span class=code>.xsdf</span> format. Unfortunately, <span class=code>.xsdf</span> is a proprietary file format, and decoding the structure back to regular <span class=code>.sdf</span> format requires a special package with a $100K/year license.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
PLM-3 is a protein language model that’s free for commercial entities as long as the usage isn’t directly beneficial to the business. The phrase “directly beneficial” is not clearly defined in the license agreement, though, leading to grey areas:
</p>

<ul>
<li>
A therapeutics company publishes an open-source benchmark of PLM-3, and this benchmark gets substantial press coverage. Was PLM-3 directly or indirectly beneficial? 
</li>
<li>
A company benchmarks the model but their published benchmark contains a statistical error. A rogue statistician catches the error and emails the company; after several discussions, the company ends up hiring the statistician. Was PLM-3 directly or indirectly beneficial? 
</li>
<li>
Another company tells a mid-level scientist to benchmark PLM-3 against an internal dataset. While collating this dataset from various archives, she discovers evidence of financial malpractice and uncovers a ring of white-collar criminals who have been embezzling money. Was PLM-3 directly or indirectly beneficial? 
</li>
<li>
A venture capitalist hears about PLM-3 at a party and remembers that he heard a pitch for an RNA language model several weeks ago. He ends up leading their seed round. Was PLM-3 directly or indirectly beneficial? 
</li>
</ul>

<p>
The company behind PLM-3 has been hiring large numbers of metaphysicists, suggesting that they plan to pursue aggressive litigation in this space.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Telos-1 is a Boltzmann generator for biopolymers. Telos-1 is free for any usage where the ultimate purpose is charitable—so research towards the public good is permitted, but research that’s intended to make money is banned. This worked as intended until Novo Nordisk sued, arguing that since they’re owned by the non-profit Novo Nordisk Foundation, the ultimate purpose of all their research is charitable. The lawsuit is ongoing.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
NP-2 is a neural network potential that can only be used by non-profits. Originally, this was construed as only organizations possessing 501(c)(3) non-profit status—but after heartfelt appeals from small biotech startups, the company behind NP-1 agreed that companies that were losing money could also qualify as non-profits, since they weren’t actually making any profit. 
</p>

<p>
This led to a predictable wave of financial engineering, and most pharmaceuticals started outsourcing all calculations to shell corporations. These corporations must be “losing money” each quarter, but this simply refers to the operating cash flow. So the shell corporation can simply be spun out with an initial capital outlay of ten million dollars or so, and then calculations can be sold below cost to the parent company until the money runs out. 
</p>

<p>
These companies were originally intended to be disposable, but it turns out that the business model of “sell ML inference to pharma below cost” was very appealing to venture capitalists. Negative unit margins are commonplace in AI right now, and unlike other AI-for-drug-design startups, the shell corporations actually had meaningful enterprise traction. The largest of these companies, EvolvAI (formerly “Merck Sharp Dolme Informatics Solutions 1”) just closed a $200M Series D financing round despite no conceivable path to profitability.
</p>

<br>
<br>

<i>
  Thanks to Abhishaike Mahajan, Spencer Schneider, and Ari Wagen for reading drafts of this post.
</i>
]]></description>
              <pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: I See Satan Fall Like Lightning</title>
              <link>public/blog/20250724_satan.html</link>
              <description><![CDATA[
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_satan.jpg" style="width:400px;" />
  <figcaption>Gustave Doré, illustration for <i>Paradise Lost</i> (1866)</figcaption>
</figure>


<p>
In center-right tech-adjacent circles, it’s common to reference René Girard. This is in part owing to the influence of Girard’s student Peter Thiel. Thiel is one of the undisputed titans of Silicon Valley—founder of PayPal, Palantir, and Founders Fund; early investor and board member at Facebook; creator of the Thiel Fellowship; and mentor to J.D. Vance—and his intellectual influence has pushed Girard’s ideas into mainstream technology discourse. (One of Thiel’s few published writings is <a href=https://gwern.net/doc/politics/2007-thiel.pdf>“The Straussian Moment,”</a> a 2007 essay which explores the ideas of Girard and Leo Strauss in the wake of 9/11.)
</p>

<p>
  As a chemistry grad student exploring startup culture, I remember going to events and being confused why everyone kept describing things as “Girardian.” In part this is because Girard’s ideas are confusing. But the intrinsic confusingness of Girard is amplified by the fact that almost no one has actually read any of Girard’s writings. Instead, people learn what it means to be “Girardian” by listening to the term in conversation or on a podcast and, with practice, learn to start using it themselves.<sup>1</sup>
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_he.png" style="width:450px;" />
  <figcaption>
    <a href=https://x.com/Jiankui_He/status/1912055546011918766>A post on X from Jiankui He.</a>
  </figcaption>
</figure>

<p>
  I’ve been in the startup scene for a few years now, so I figured it was time I read some Girard myself rather than (1) avoiding any discussions about Girard or (2) blindly imitating how everyone else uses the word “Girardian” and hoping nobody finds out. What follows is a brief review of Girard’s 2001 book <i>I See Satan Fall Like Lightning</i> and an introduction to the ideas contained within, with the caveat that I’m not a Girard expert and I’ve only read this one book. (I’ve presented the ideas in a slightly different order than Girard does because they made more sense to me this way; apologies if this upsets any true Girardians out there!)
</p>

<br>
<div class=dinkus>* * *</div>
<br>


<p>
  <i>I See Satan Fall Like Lightning</i> opens with a discussion of the 10th Commandment:
</p>

<blockquote>
You shall not covet your neighbor's house; you shall not covet your neighbor's wife, or his male servant, or his female servant, or his ox, or his donkey, or anything that is your neighbor's. (Ex 20:17, ESV)
</blockquote>

<p>
Girard argues that the archaic word “covet” is misleading and that a better translation is simply “desire.” This commandment, to Girard, illustrates a fundamental truth of human nature—we desire what others have, not out of an intrinsic need but simply because others have them. This is why the 10th Commandment begins by enumerating a list of objects but closes with a prohibition on desiring “anything that is your neighbor’s.” The neighbor is the source of the desire, not the object.
</p>

<p>
The essential and distinguishing characteristic of humans, in Girard’s framing, is the ability to have flexible and changing desires. Animals have static desires—food, sex, and so on—but humans have the capacity for “mimetic desire,” or learning to copy another’s desire. This is both good and bad:
</p>

<blockquote>
Once their natural needs are satisfied, humans desire intensely, but they don’t know exactly what they desire, for no instinct guides them. We do not each have our own desire, one really our own… Mimetic desire enables us to escape from the animal realm. It is responsible for the best and the worst in us, for what lowers us below the animal level as well as what elevates us above it. Our unending discords are the ransom of our freedom. (pp. 15–16)
</blockquote>

<p>
Mimetic desire leads us to copy others’ desires: we see that our friend has a house and we learn that this house is desirable. Mimetic desire, however, leads us into conflict with our neighbor. The house that we desire is the one that someone already has; the partner we desire is someone else’s. 
</p>

<p>
  To make matters worse, our own desire for what our neighbor validates and intensifies their own desire through the same mechanism, leading to a cycle of “mimetic escalation” in which the conflict between two parties increases without end. The essential sameness of the two parties makes violence inescapable. The Montagues and Capulets are two noble houses “both alike in dignity” (<i>Romeo and Juliet</i>, prologue)—their similarity makes them mimetic doubles, doomed to destruction. 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_agincourt.png" style="width:650px;" />
  <figcaption>
    <a href=https://commons.wikimedia.org/wiki/File:Schlacht_von_Azincourt.jpg>Artistic depiction of the Battle of Agincourt.</a>
  </figcaption>
</figure>

<p>
While the above discussion focuses only on two parties, Girard then extends the logic to whole communities, arguing that unchecked mimetic desire would lead to the total destruction of society. To prevent this, something surprising happens: the undirected violence that humans feel towards their neighbors is reoriented onto a single individual, who is then brutally murdered in a collective ritual that cleanses the society of its mutual anger and restores order. Here’s Girard again: 
</p>

<blockquote>
The condensation of all the separated scandals into a single scandal is the paroxysm of a process that begins with mimetic desire and its rivalries. These rivalries, as they multiply, create a mimetic crisis, the war of <i>all against all</i>. The resulting violence of all against all would finally annihilate the community if it were not transformed, in the end, into a war of <i>all against one</i>, thanks to which the unity of the community is reestablished. (p. 24, emphasis original)
</blockquote>

<p>
(Note that the word “scandal” here is Girard’s transliteration of the Greek σκάνδαλον, meaning “something that causes one to sin” and not the contemporary and more frivolous meaning of the word.)
</p>

<p>
This process is called the “single-victim mechanism”; the victim is chosen more or less at random as a <a href=https://en.wikipedia.org/wiki/Focal_point_(game_theory)>Schelling point</a> for society’s anger and frustration, not because of any actual guilt. Girard recognizes that this process seems foreign to modern audiences and mounts a vigorous historical defense of its validity. He recounts an episode from Philostratus’s <i>Life of Apollonius of Tyana</i> (<a href=https://www.theoi.com/Phasma/PhasmaEphesios.html>link</a>) in which Apollonius ends a plague in Ephesus by convincing the crowd to stone a beggar. Philostratus explains this by saying that the beggar was actually a demon in disguise, but Girard argues that the collective violence cures the Ephesians from social disorder, and that the victim is retrospectively demonized (literally) as a way for the Ephesians to justify their actions.
</p>

<p>
(How does ending social disorder cure a plague? Girard argues that our use of “plague” to specifically mean an outbreak of infectious disease is anachronistic, and that ancient writers didn’t differentiate between biological and social contagion—in this case, the plague must have been one of social disorder, not a literal viral or bacterial outbreak.)
</p>

<p>
While we no longer openly kill outsiders to battle plagues, Girard argues that the violent impulses of the single-victim mechanism are still visible in modern societies: he uses the examples of lynch mobs, the Dreyfus affair, witch hunts, and violence against “Jews, lepers, foreigners, the disabled, the marginal people of every kind” (p. 72) to illustrate our propensity towards collective violence. Racism, sexism, ableism, and religious persecution are all different aspects of what Girard argues is a fundamental urge towards collective majority violence.
</p>

<p>
  In the past, ritual human sacrifice is well-documented: see <i>inter alia</i> the Athenian <a href=https://en.wikipedia.org/wiki/Pharmakos><i>pharmakoi</i></a>, the Mayan custom of throwing victims into <a href="https://en.wikipedia.org/wiki/Sacred_Cenote#Human_sacrifice"><i>cenotes</i></a> to drown, and the sinister rites of ancient Celts. While modern anthropologists are typically perplexed by these rituals, Girard argues that they ought not to be downplayed. The development of the single-victim mechanism across so many cultures is not an accident. Instead, this mechanism is the foundation of all human culture, because it replaces primitive violence (which leads to anarchy) with ritual violence and allows society to persist and create institutions. 
</p>

<p>
  The single-victim mechanism is necessary cultural technology, which is why so many cultures share the myth of a “founding murder” (Abel, Remus, <a href=https://en.wikipedia.org/wiki/En%C5%ABma_Eli%C5%A1>Apsu</a>, <a href=https://en.wikipedia.org/wiki/Dumuzid>Tammuz</a>, and so on). But the founding murder doesn’t just herald the creation of human society. The collective murder of the victim does so much to restore harmony to the community that the transformation seems miraculous. The people, having witnessed a miracle, decide that the one they killed must now be divine:
</p>

<blockquote>
Unanimous violence has reconciled the community and the reconciling power is attributed to the victim, who is already “guilty,” already “responsible” for the crisis. The victim is thus transfigured twice: the first time in a negative, evil fashion; the second time in a positive, beneficial fashion. Everyone thought that this victim had perished, but it turns out he or she must be alive since this very one reconstructs the community immediately after destroying it. He or she is clearly immortal and thus divine. (pp. 65–66)
</blockquote>

<p>
  This might seem bizarre—it did to me when I read it—but many other writers have discussed the peculiar motif of the <a href=https://en.wikipedia.org/wiki/Dying-and-rising_god>“dying then rising”</a> deity. A more historical example is Caesar, who is first killed and then deified as the founder of the Roman Empire, with his murder being the central event in the advent of the new age.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_apotheosis.png" style="width:450px;" />
  <figcaption>
    <i>The Apotheosis of Hercules</i>, Noël Coypel (c. 1700)
  </figcaption>
</figure>

<p>
Pagan myths and deities, Girard argues, are the echoes of a shadowy pre-Christian era of violent catharsis, a time in which “the strong do what they can and the weak suffer what they must” (Thucydides). Behind the sanitized modern stories of Mount Olympus and Valhalla—dark even in their original, non-Percy-Jackson retellings—are sinister records of outcasts who were first killed and then deified.
</p>

<p>
Girard argues that the Bible stands in opposition to this mimetic cycle. Collective violence threatens figures like Joseph, Job, and John the Baptist, but the Biblical narrative both defends their innocence and maintains their humanity. The Psalms repeatedly defend the innocence of the Psalmist against the unjust accusation of crowds (cf. Psalms 22, 35, 69). Uniquely among ancient documents, the Bible takes the side of the victim and not the crowd.
</p>

<p>
In the Gospels, Jesus opposes the single-victim mechanism. In the story of the woman convicted of adultery, he tells the accusers “Let him who is without sin among you be the first to throw a stone at her” (John 8:7, ESV). This is the exact opposite of Apollonius of Tyana:
</p>

<blockquote>
Saving the adulterous woman from being stoned, as Jesus does, means that he prevents the violent contagion from getting started. Another contagion in the reverse direction is set off, however, a contagion of nonviolence. From the moment the first individual gives up stoning the adulterous woman, he becomes a model who is imitated more and more until finally all the group, guided by Jesus, abandons its plan to stone the woman. (p. 57)
</blockquote>

<p>
  (Unfortunately, it’s unlikely that the story of the woman accused of adultery is in the original text of John. <a href=https://en.wikipedia.org/wiki/Jesus_and_the_woman_taken_in_adultery#Manuscripts>The oldest New Testament sources we have, like Codex Vaticanus, Codex Sinaiticus, and various papyri, lack this story.</a>)
</p>

<p>
Jesus becomes the target of mimetic violence himself, of course, culminating in his crucifixion and death at Calvary. But Girard argues that what seems like the ultimate victory of mimetic violence—the brutal murder of the person who sought to stop it—is actually its moment of defeat, what he calls the “triumph of the cross” in a paraphrase of Colossians 2. He writes:
</p>

<blockquote>
The principle of illusion or victim mechanism cannot appear in broad daylight without losing its structuring power. In order to be effective, it demands the ignorance of persecutors who “do not know what they are doing.” It demands the darkness of Satan to function adequately. (pp. 147–148)
</blockquote>

<p>
Unlike in previous cases, where the actions of the crowd were unanimous and the victim perished alone, a minority remains to proclaim the innocence of Jesus. While the majority of the crowd doesn’t follow them, this is enough—the collective violence of the crowd can only function properly as long as the crowd remains ignorant of what they’re doing. Clearly explaining the victim mechanism also serves to destroy it, and so the victim mechanism is ended by the testimony of the early Christians, a stone “cut out by no human hand” that grows to fill the whole world and destroys the opposing kingdoms (Daniel 2:34–35, ESV). 
</p>

<p>
  The Gospel narrative exposes the workings of the victim mechanism and defeats it. While Satan thinks he’s winning by killing Jesus, Jesus’ death will make the single-victim mechanism clear and destroy the ignorance in which the Prince of Darkness must work. Girard explains this as the theological idea of “Satan duped by the cross” (which modern listeners may recognize from <i>The Lion, The Witch, and the Wardrobe</i>):
</p>

<blockquote>
God in his wisdom had foreseen since the beginning that the victim mechanism would be reversed like a glove, exposed, placed in the open, stripped naked, and dismantled in the Gospel Passion texts… In triggering the victim mechanism against Jesus, Satan believed he was protecting his kingdom, defending his possession, not realizing that, in fact, he was doing the very opposite. (p. 151)
</blockquote>

<p>
While the story of Jesus’s resurrection might seem to have many parallels with the divinization of sacrificial victims, Girard says that these are “externally similar but radically opposed” (p. 131). Indeed, he argues that the Gospel writers intentionally highlighted similarities between the false resurrection of collective violence and the true resurrection of Jesus: Mark 6:16 shows Herod anxious that John the Baptist, whom he killed, has come back to life, while Luke 23:12 shows how Jesus’ death makes Herod and Pilate friends in the same way the victim mechanism always does. In Girard’s words, “the two writers emphasize these resemblances in order to show the points where the satanic imitations of the truth are most impressive and yet ineffectual” (p. 135). The divinization of victims is pathetic and flimsy next to the true resurrection.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_crucifixion.jpg" style="width:400px;" />
  <figcaption>
    <i>Crucifixion</i>, Bartolomeo Bulgarini (c. 1330)
  </figcaption>
</figure>


<p>
In the new Christian age, Jesus invites us to avoid mimetic contagion not by attempting to avoid having desires in a Buddhist way—to do so is to deny our nature as humans—but by presenting himself as the model for humans to imitate. Only Jesus, the Bible argues, is a fitting template for human desire: hence Paul’s command to “imitate me as I imitate Christ” (1 Cor 11:1), and the consistent call throughout Scripture to love what God loves and hate what God hates (cf. Ps. 139). 
</p>

<p>
Girard ends his book with a discussion of modernity and our culture’s now-total embrace of victims. Girard’s writing is powerful, concise, and difficult to summarize in this section, so I’ll quote from the final pages at length.
</p>

<blockquote>
All through the twentieth century, the most powerful mimetic force was never Nazism and related ideologies, all those that openly opposed the concern for victims and that readily acknowledged its Judeo-Christian origin. The most powerful anti-Christian movement is the one that takes over and “radicalizes” the concern for victims in order to paganize it. The powers and principalities want to be “revolutionary” now, and they reproach Christianity for not defending victims with enough ardor. In Christian history they see nothing but persecutions, acts of oppression, inquisitions.
<br><br>
This other totalitarianism presents itself as the liberator of humanity. In trying to usurp the place of Christ, the powers imitate him in the way a mimetic rival imitates his model in order to defeat him. They denounce the Christian concern for victims as hypocritical and a pale imitation of the authentic crusade against oppression and persecution for which they would carry the banner themselves.
<br><br>
In the symbolic language of the New Testament, we would say that in our world Satan, trying to make a new start and gain new triumphs, borrows the language of victims. Satan imitates Christ better and better and pretends to surpass him. This imitation by the usurper has long been present in the Christianized world, but it has increased enormously in our time. The New Testament evokes this process in the language of the <i>Antichrist</i>… 
<br><br>
The Antichrist boasts of bringing to human beings the peace and tolerance that Christianity promised but has failed to deliver. Actually, what the radicalization of contemporary victimology produces is a return to all sorts of pagan practices: abortion, euthanasia, sexual undifferentiation, Roman circus games galore but without real victims, etc.
<br><br>
Neo-paganism would like to turn the Ten Commandments and all of Judeo-Christian morality into some alleged intolerable violence, and indeed its primary objective is their complete abolition. (pp. 180–181, emphasis original)
</blockquote>

<p>
Satan, whose previous work through the victim mechanism was defeated by Christianity, now seeks to imitate God’s people and use their own arguments against them. Although Christians invented the idea of having sympathy for the victim (in Girard’s view), Satan now argues that Christianity itself is intrinsically a form of violence against victims, with the only solution being the “complete abolition” of Christian morals. Girard alleges that this is bad, and if implemented would lead to the pagan anarchy of long ago. 
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Girard’s style is hard to pin down: he bounces between anthropology, close reading of ancient texts, history, and theology without breaking stride. I enjoy reading his writing a lot, but sometimes I wish there would be more sources: he alludes to a fascinating series of historical interviews with tribes that practiced human sacrifice, for instance, but doesn’t leave a reference to the original interviews. 
</p>

<p>
  As a work of theology, <i>I See Satan Fall Like Lightning</i> is interesting but peculiar. It’s not clear to me how interested Girard is in orthodox Christian thought. He alternates between referring to Satan as a person and an abstract concept, for instance, and uses few ideas that would be familiar to students of mainstream systematic theology. This isn’t necessarily wrong, but leaves me with a lot of open questions: what does Girard make of the sacrifice of Isaac, or the concept of sanctification, or the role of faith in all this? These might be answered in his other writings, but they weren’t answered here.
</p>

<p>
  As a work of history, <i>I See Satan Fall Like Lightning</i> is downright bizarre. The book’s literal claim—that all non-Christian “gods” are deified victims of ritual mass murder—is hard for me to accept at face value. That being said, <a href=https://corinwagen.github.io/public/blog/20250709_montaillou.html>it’s hard enough to reason about the recent past</a>, let alone ancient pre-history. Maybe he’s right about all this? Evidence feels scarce, though, and <i>I See Satan Fall Like Lightning</i> hardly conducts a careful meta-analysis of all available ancient myths. 
</p>

<p>
  Perhaps the most similar work to Girard’s in scope and ambition is Julian Jaynes’s <i>The Origin of Consciousness in the Breakdown of the Bicameral Mind</i> (<a href=https://en.wikipedia.org/wiki/The_Origin_of_Consciousness_in_the_Breakdown_of_the_Bicameral_Mind>Wikipedia</a>, <a href=https://slatestarcodex.com/2020/06/01/book-review-origin-of-consciousness-in-the-breakdown-of-the-bicameral-mind/>Slate Star Codex</a>).<sup>2</sup> Jaynes argues that ancient people didn’t actually have theory of mind or a concept of the self: instead, they personified their internal monologue and viewed it as the voice of the gods. We usually don’t notice this when we read ancient texts, because we subconsciously assume that they were similar to us. But, to quote Scott’s review:
</p>
  
<blockquote>
Every ancient text is in complete agreement that everyone in society heard the gods’ voices very often and usually based decisions off of them. Jaynes is just the only guy who takes this seriously.
</blockquote>

<p>
Much like Jaynes, Girard takes a surprising historical observation—the ubiquity of human sacrifice and the fact that ancient people saw this as essential to the health of their society—and takes it seriously, building an entire argument about how ritual human sacrifice is the original cultural technology and the root of all civilization. While I’m not fully convinced, the intellectual commitment is admirable. 
</p>

<p>
  Most bizarre of all, though, is the fact that these esoteric ideas have become a mainstream part of “Grey Tribe” thought. If I’d read this book in a vacuum, I wouldn’t expect that any of the ideas would have achieved much popularity—but in today’s world, describing things as “Girardian” or “mimetic” is <i>de rigueur</i> for the aspiring thought leader. 
</p>

<p>
  This observation itself is perhaps the best testimony to the strength of Girard’s ideas. What force other than mimetic rivalry could be strong enough to convince thousands of venture capitalists, each attempting to craft a contrarian high-conviction persona, to all reference the same Christian philosophical anthropologist?
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250724_huber.png" style="width:450px;" />
  <figcaption>
    <a href=https://x.com/TobiasAHuber/status/1697221185065406775>A post on X from Tobias Huber.</a>
  </figcaption>
</figure>

<i>Thanks to my wife and Ari Wagen for reading drafts of this piece, and for Micah from church for lending me a copy of this book.</i>

<h3>Footnotes</h3>
<ol>
<li>
    I took a stab at discussing some Girard ideas <a href=https://corinwagen.github.io/public/blog/20230228_gilliam_and_girard.html>previously on the blog</a>, although I did actually read the essay which I discuss.
</li>
<li>
  Full transparency: I’ve just read the SSC book review and not (yet) the full book, thus committing the very sin which I accuse others of committing above! "Let he who is without sin cast the first stone" &amp;c.
</li>
</ol>
]]></description>
              <pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>How To Give An External Research Talk</title>
              <link>public/blog/20250721_external_talk.html</link>
              <description><![CDATA[
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250721_mars_hill.jpg" style="width:500px;" />
</figure>

<p>
  <em>(This post is copied from some notes I gave to our summer interns at Rowan almost without modification. Hopefully people outside Rowan find this useful too!)</em>
</p>

<p>
This is a brief and opinionated guide on how to give a research talk to an external audience. Some initial points of clarification—this guide is for a research talk, not a sales call or a VC pitch. Research talks have their own culture and norms; treating a research talk as a sales call is likely to backfire disastrously. 
</p>

<p>
When is a research talk appropriate? Generally, if you’re talking to scientists, you should view your talk as a research talk, unless specifically advertised otherwise. 
</p>

<p>
This advice is not directly applicable for talks to internal audiences; collaborators need much less context than external audiences and you can streamline your talk accordingly. (Note that “external” and “internal” here refer to projects, not corporations—a talk to a different division of your company is “external” to the project even if you technically have the same employer.)
</p>

<h2>Goal</h2>

<p>
The goal of an external research talk is to teach the audience something. This is generally underappreciated. Many people act as if the goal of the talk is to show that they’re smart, or to show how impressive their research is, or to dump all the data from a given paper onto slides. All of these lead to talks that are mediocre at best and barbarous at worst. 
</p>

<p>
If you learn something from a talk, it’s a good talk. This is also nice because it’s easy to learn something; even bad results can teach you something. For talks in different fields, I often learn more from the introduction to a talk than from the actual results section (which goes over my head).
</p>

<h2>Audience</h2>

<p>
In any given talk, there might be several categories of people.
</p>

<ul>
  <li>
    <b>People following the field who like your work.</b> They will have already read your papers and are excited to see you speak. You should include extra details that aren’t in the paper for them.
  </li>
  <li>
    <b>People following the field who dislike your work.</b> These people might ask pushy questions or challenge your interpretations. Don’t address the talk to them, but keep some defensive backup slides just in case. (The best way to address an aggressive question is to present a nice backup slide which shows you’ve already thought deeply about the question.)
  </li>
  <li>
    <b>People adjacent to the field who don’t really know your work.</b> For Rowan, this might be a computational chemist who’s never used a neural network potential but who’s vaguely curious, or an ML researcher who’s curious about how people use deep learning in chemistry. This is usually the bulk of any given audience; your talk should be aimed at these people.
  </li>
  <li>
    <b>People who are just clueless.</b> Every talk has them; maybe they were told to go by their boss, or they misremembered which seminar this was. Ideally you can teach them something too, if they bother to pay attention, but the talk isn’t for them.
  </li>
</ul>

<p>
A perfect talk has something for everyone; you can give the fans something they didn’t read in the paper, mollify the critics, teach most listeners something new, and maybe even interest the clueless folks.
</p>

<h2>Rough Structure</h2>

<ul>
  <li>
  <b>Background.</b> What is the problem and why should the audience care about it?
  </li>
  <li>
    <b>History.</b> What has been done before?
  </li>
  <li>
    <b>Idea.</b> What was your idea?
  </li>
  <li>
    <b>Implementation.</b> What did you do?
  </li>
  <li>
    <b>Results.</b> How did the idea turn out?
  </li>
  <li>
    <b>Discussion</b>. What can the audience learn from this?
  </li>
  <li>
    <b>Future</b>. What’s next?
  </li>
  <li>
    <b>Q&amp;A.</b>
  </li>
</ul>

<p>
My contrarian take is that the background/history should comprise 30–40% of the talk. Most people have less context for your work than you expect; “context is that which is scarce.” It’s almost always worth spending more time explaining why what you’re doing is important, what other people have done, and how people in your field think about problems. 
</p>

<p>
Paradoxically, the longer you spend on background, the more impactful your results might be. You’re both building tension, as the audience wonders when you’ll get to your research, and you’re positioning your research such that when you share your actual idea the audience will be maximally excited. (I always picture this like <a href="https://www.youtube.com/watch?v=UkkF6Zz67TE">an old-school samurai duel</a>; the longer you wait before you “strike” with your results, the better.)
</p>

<p>
Many scientists overemphasize discussing their implementation and results. These topics are always the focus of the actual work, because they take the bulk of the time, and papers focus on these too since they require the most details and associated data. But talks that just explain mathematics, statistics, or data cleaning in detail are usually boring or painful to listen to—just because you suffered through the details of this process doesn’t mean the audience needs to suffer too.
</p>
 
<p>
(Paradoxically, the fact that papers focus on implementation/results means that your talk is free to deemphasize them. People who are following the field may have already read your paper; people who aren’t following the field probably won’t understand the methodology or detailed results anyway. )
</p>

<p>
One exception is if there’s some personal or non-obvious story about implementation and results—if you wasted time on the wrong architecture or had some interesting realization that led to a breakthrough, these are great to include. I’ve often gone to talks just so I can hear any narrative details that aren’t “neat” enough to be in the paper.
</p>

<p>
The ending discussion can vary a lot from talk to talk, but I think it’s worth stating clearly what you think the conclusions should be from your work. What should someone in the field take away? What should someone in an adjacent field take away? What do you predict the future of this area of research will be?
</p>

<p>
  Here's the last slide from a talk I gave at MSU in February:
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250721_msu_slide.png" style="width:650px;" />
</figure>

<p>
  It’s always good to save at least 10 minutes within the allotted time for Q&amp;A, and to leave time after the talk for additional questions. 
</p>

<h2>Slides</h2>

<p>
Different people and fields have different customs here; there’s no hard-and-fast rule. I rarely make new figures for talks, because it takes forever—I’ll instead take images from existing papers and put the citations at the bottom.
</p>

<p>
A few disjointed thoughts:
</p>

<ul>
<li>
  Animations are good, but only the “appear” animation. For complex slides, it’s good to be able to add each component one-at-a-time; otherwise viewers will get distracted trying to figure out what’s next. Anything fancier than simply making something appear is bad, though.
</li>
<li>
  Slide numbers make Q&amp;A much simpler, and there’s a way to do them automatically in both Powerpoint and Google Slides.
</li>
<li>
  Large amounts of text can be distracting, but are sometimes fine. I’ll often screenshot a paragraph from a previous paper and then read a (highlighted) key sentence out loud. You probably shouldn’t have lots of original text on the slide.
</li>
<li>
  Similarly, highly detailed and complex slides are bad unless the complexity of the slide is the point. Sometimes you can put up a big architecture diagram and say something pithy like “and then we did a lot of optimization, which I’m not going to get into here but am happy to discuss one-on-one later,” and then move to the next slide. This is (in my opinion) a nice way to allude to underlying complexity without getting bogged down.
</li>
<li>
  To a first approximation the title of the slide should be a simple, declarative statement which summarizes the slide. This helps people to focus on the key ideas, even if they get distracted checking their phone.
</li>
<li>
  If you’re presenting to organic chemists, it’s helpful to put chemical structures on almost every slide.
</li>
</ul>

]]></description>
              <pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Montaillou</title>
              <link>public/blog/20250709_montaillou.html</link>
              <description><![CDATA[
<p>
The past is powerful evidence for arguments about the present. Since the time of Livy and Tacitus, it’s been common to cite history to advance some ideological, cultural, or political idea. While there’s nothing wrong with this in principle, these lines of discourse often break down because (1) people know very little history and (2) what little history they do know is usually wrong. 
</p>

<p>
  Popular understanding of history is shaped mostly by popular culture: books, movies, and video games. These sources aren’t intrinsically bad—I’ll never remember early modern European states as well as my friend who played hundreds of hours of <i>Europa Universalis IV</i> in high school. But the overall effect of a history education dominated by <i>Gladiator</i>, <i>300</i>, and <i>Ben-Hur</i> is to give people a weird and distorted view of the past,<sup>1</sup> and one inchoate enough to be shaped into almost any argument about the present.
</p>

<p>
If we want to build up a more accurate view of what it was like to live in the past, we want to start with the most fundamental questions: how did people live? How did they spend their time? What did they do for work and for leisure? What did they worry about? Unfortunately, these questions are almost always unanswerable. Most historical documents don’t touch on the daily life of average people, focusing instead on chronicling noble deeds, recording economic transactions, and so on. 
</p>

<p>
But exceptions can be found. The village of Montaillou is a small, mountainous village of about 250 inhabitants in the French Pyrenees (in Occitania, what was then the Duchy of Foix and is today the department of Ariège). Montaillou was remote and unexceptional in almost every way, making it exactly the sort of place which we’d never expect to see in historical documents—except that by 1300 it was one of the last strongholds of Catharism, a Christian heresy which had been almost completely eliminated by the Albigensian Crusade in the 1210s. 
</p>

<p>
This is typical of historical mountain societies. In his landmark work <i>The Mediterranean and the Mediterranean World in the Age of Philip II</i>, Fernand Braudel writes (pp. 38–39): 
</p>

<blockquote>
  There can be no doubt that the lowland, urban civilization penetrated to the highland world very imperfectly and at a very slow rate… for the simple reason that mountains are mountains: that is, primarily an obstacle, and therefore also a refuge, a land of the free. For there men can live out of reach of the pressures and tyrannies of civilization: its social and political order, its monetary economy.
</blockquote>

<p>
  The Cathars who had come to the mountains to flee persecution were dualist Gnostics who believed in the reincarnation of the soul. Their clerics and leaders, called goodmen (<i>bonhommes</i>) or <i>parfaits</i>, were celibate and refused to eat meat or drink wine. Ordinary Cathar followers didn’t hold themselves to this standard until their deathbed, when they would convert to Catharism in a ritual called the <i>consolamentum</i> and then completely fast until dying of hunger (called the <i>endura</i>). Cathars rejected the sacraments, mocked the priests and rituals of the Church, and saw themselves as the true worshipers of the Christian God.
</p>

<p>
  Following the Albigensian Crusade, Montaillou and other rural towns became a refuge for Catharism. This eventually drew the attention of the French Inquisition in the person of Jacques Fournier, bishop of Pamiers. In 1320, Fornier arrested a large percentage of the population of Montaillou and interrogated them at length, recording “substantial and very detailed evidence” (xiv) about their lives. During his tenure at Pamiers, Fournier’s inquisition court investigated 578 people over 370 different days, with scribes and notaries keeping a detailed record of all proceedings in what is now known as the Fournier Register. This extraordinary document might have been lost to history except for the fact that Fournier became <a href=https://en.wikipedia.org/wiki/Pope_Benedict_XII>Pope Benedict XII</a> in 1334 and the Fournier Register was brought to the Vatican Library. 
</p>

<p>
  The Fournier Register was revisited by Annales historian Emmanuel Le Roy Ladurie. His 1975 work <i>Montaillou, village occitan de 1294 à 1324</i> uses the details contained in the Register to reconstruct the world of Montaillou: who the people were, what they thought about, and how they lived. I originally read this book right before starting college and liked it a lot. Recently, I’ve found myself thinking back to Montaillou in everyday discussions about history and decided to read Le Roy Ladurie’s book again. 
</p>

<p>
In this post, I hope to give a taste of the world of Montaillou, and how surprisingly normal (or abnormal) aspects of this world can seem to modern sensibilities. I’m focusing on the questions that interested me the most in this reading, and have omitted a lot of interesting characters and life histories for the sake of space and focus. If you find this review interesting, you should certainly pick up the book yourself—I’ve indicated page numbers in <a href=https://www.amazon.com/Montaillou-Promised-Emmanuel-Roy-Ladurie/dp/0807615986>the 30th anniversary edition</a> for easy reference.
</p>

<h2>Setting</h2>

<p>
Montaillou was (and is) a small village on the French side of the Pyrenees, at an elevation of approximately 4500 feet. In a physical and economic sense, Montaillou was incredibly isolated. There was essentially no non-foot traffic in or out of the village, so goods were carried by hand or with a mule. Montaillou had no blacksmith or tailor, and iron tools were rare (7). Montaillou was too small to have its own mill, so villagers would take wheat by mule to the larger town of Ax-les-Thermes, grind it at the mill, and return with flour, about 10 miles by Google Maps (9). 
</p>

<p>
  Here's a picture of modern Montaillou from Wikipedia. The ruins of medieval Montaillou are visible at the top of the hill, and the snow-capped Pyrenees are visible some 20 miles to the south.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250709_montaillou.jpg" style="width:700px;" />
  <figcaption>Montaillou in April 2005. <a href=https://commons.wikimedia.org/wiki/File:Montaillou_28-04-05_8.jpg>See Wikimedia for higher resolution.</a></figcaption>
</figure>


<p>
The bulk of calories came from bread (made from wheat or millet), and cheese was the primary protein source (8–9). Other documented animal foods include mutton, bacon, goat’s liver, eggs, and trout (9, 82, 83, 124). Cabbages, leeks, broad beans, and turnips were the most common vegetables (9). Most people in Montaillou farmed and raised animals: pigs, cows, sheep, chickens, geese, and so on. Almost everyone kept sheep, but there were some skilled itinerant “professional” shepherds who travelled across the Pyrenees living in the mountains and supervising large herds (69–135), stopping back home from season to season.
</p>

<p>
Apart from the coinage that they used, the people of Montaillou were not “French” in any meaningful sense. They spoke a dialect of Occitanian that was distinct to their region, “about a thousand people at the most” (286). When forced to flee religious persecution, villagers went not to other regions of France but to Catalonia, Lombardy, Sicily, or Valencia (286). They almost always married within their village; in the cases where someone from Montaillou married someone from the outside, it was almost always from a neighboring village (183). Thus the world of Montaillou was, in a personal sense, very small indeed.
</p>


<h2>Housing and Personal Space</h2>

<p>
  The house (<i>domus</i> in Latin, <i>ostal</i> in Occitan) was the fundamental physical and social unit of Montaillou. Physically, the kitchen was the central room of the house, and perhaps the only one built of stone (39). The hearth and cooking utensils were in the middle of the kitchen, hams hung from the roof, and a table and chairs were off to the side (37–38). A cellar was often adjacent to the kitchen (38).
</p>

<p>
Personal space was not as scarce as people sometimes imagine in medieval times. Most rooms had only one or two people, and people had separate beds (38–39). Children and adults slept in separate rooms (39). Most houses only had one story, but richer villagers might have two-story houses (39). Animals typically slept in the house at night, albeit in separate rooms, and used the same door as people; sick people were sometimes put near animals to keep them warm at night. Only relatively wealthy farms had separate stables, pigsties, and sheep-pens (40). 
</p>

<p>
  The intellectual and social life of Montaillou revolved around the <i>domus</i>, “a unifying concept in social, family, and cultural life” (25). When villagers discussed Catharism and Catholicism, they identified beliefs not with individuals but with houses (28). To be the head of the house was a significant position of authority; to have one’s house confiscated or destroyed was cataclysmic (35–37). 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250709_feb.png" style="width:400px;" />
  <figcaption> Detail of the February scene from the <i>Très Riches Heures du Duc de Berry</i> (c. 1415)</figcaption>
</figure>


<h2>Family</h2>

<p>
It’s become somewhat popular in recent years to argue against the primacy of the nuclear family. In his 2020 article “<a href=https://www.theatlantic.com/magazine/archive/2020/03/the-nuclear-family-was-a-mistake/605536/>The Nuclear Family Was a Mistake</a>”, David Brooks argues that “big, interconnected, and extended families” are the historical norm and a healthier & more natural way to live. I’ve thought about this essay and argued about it with friends many times over the past few years; in fact, these arguments were a large part of why I originally wanted to reread Montaillou. 
</p>

<p>
I expected the history of Montaillou to support Brooks’s position that extended families were more normal than nuclear families in medieval societies, but it didn’t. The vast majority of houses held nuclear families, or nuclear-ish families where an uncle or grandmother lived with the core family unit. Several examples of more extended families are documented, but they are “very rare cases” and usually unstable (48). As a rule, there was only one married couple per house and the house organized itself around this couple. 
</p>

<p>
Montaillou was largely patriarchal, but not entirely so. There were maternal houses where the sons took their mother’s name and son-in-laws took their wife’s name (34). Nor was primogeniture absolute. Fathers generally determined who inherited the house, but the inheritor did not have to be the firstborn, and the other sons would receive a smaller portion called a fratrisia (36). Both these facts surprised me. 
</p>

<p>
Marriage was typically arranged by the family and “involved much more than a mere agreement between two individuals” (180), with numerous relatives often involved. Dowries were substantial enough that families worried that marrying their daughters might bring economic ruin upon their houses, but remained the distinct property of the wife after marriage. If the husband died first, the widow retained her dowry separately from whomever might inherit the rest of the possessions (35–36).
</p>

<p>
Widows were common because women were typically married young, between the ages of 15 and 20, while men typically waited until after 25 to marry (190). Le Roy Ladurie writes (191): 
</p>

<blockquote>
Husbands in Montaillou were generally fully adult and they often married young innocents. The girls were beginners; the men were settling down. This difference in age in a world where people died young soon produced a crop of young widows. With one husband in the grave, women prepared to go through one or even two more marriages.
</blockquote>

<p>
While marriage for love was not the primary objective, neither was it impossible: “it was possible to love passionately within apparently rigid structures which predisposed towards and presided over the choice of a marriage partner” (187). That being said, the sources rarely speak of women’s feelings towards their husbands (189):
</p>

<blockquote>
It is probably, and sometimes provable, that the young men in love whom we find in the Register aroused similar feelings in the girls they married. But references are scarce. Rightly or wrongly, in upper Ariège the man was supposed to possess the initiative or even the monopoly in matters of love and affection, at least in the realm of courtship and marriage.
</blockquote>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250709_jul.png" style="width:400px;" />
  <figcaption> Detail of the July scene from the <i>Très Riches Heures du Duc de Berry</i> (c. 1415)</figcaption>
</figure>


<h2>Stages of Life</h2>

<p>
As might be expected, families in Montaillou were considerably larger than today. Based on data in the Register, Le Roy Ladurie estimates that there were 4.5 legitimate births per family, plus a small but non-negligible number of illegitimate births (204). For all but the wealthiest of families this was an asset: “a domus rich in children was a domus rich in manpower; in other words, rich, pure and simple” (207). Contraception was practiced, especially outside marriage, but not abortion (172–173, 209). Children were nursed for a long time, perhaps until two years old (208).
</p>

<p>
  Modern people sometimes allege that love for young children is a modern phenomenon, citing the Roman <i>paterfamilias</i> as evidence to the contrary. In Montaillou, as today, men and women loved their young children, laughing & playing games with them and weeping bitterly when they died (210–213). The mortality rate for children and adolescents is not clear from our data but “was probably high” (221). Schooling was practical, not formal—children worked with their parents, outside and inside, and were taught religion (Catholic or Cathar) by their families. Children were often put to bed early; the Register records that a six-year-old girl is put to bed before dinner is served to guests (215). 
</p>

<p>
  At the age of 12 or so, boys changed status. The word used to describe them shifts from <i>puer</i> (used from age 2 onwards) to <i>adulescens</i> or <i>juvenis</i>. As adolescents, they began to work as apprentice shepherds, were considered to have reached the age of reason, and could be arrested for heresy (215–216). At 18, men became full-fledged adults (216). I’ll quote Le Roy Ladurie directly on aging (216):
</p>

<blockquote>
When it came to old age, there was a different pattern for men and women. In their thirties, men were in their prime. In their forties, they were still strong. But after about fifty a man was old in those days, and his prestige, unlike that of an elderly woman, did not increase with time. 
</blockquote>


<h2>Friends</h2>

<p>
Domestic servants and hired shepherds were common, and servants often lived in their employers’ houses (115). Labor markets seem quite liquid in this time period—shepherds are often hired for a season and “did not feel this instability as some kind of oppression or alienation” (114). People were part of a market economy, but the 1300s had “easy norms” (124):
</p>

<blockquote>
Everyone who has studied the daily life of the people of Montaillou, whether locals or emigrants, has been struck by the relaxed rhythm of their work, whether they were shepherds, farmers, or artisans… When necessary [a shepherd] got his friends to look after his sheep for him while he went down to the neighbouring town, to take, or to collect, money. Or he might absent himself for purely personal reasons, without any problems of time-keeping or supervision, to go and visit friends, mistresses (unless they came up directly to see him in his cabane) or fellow-sponsors, friends acquired at baptisms recently or long ago…. [He] enjoyed parties and entertainment, and even just a good meal among friends.
</blockquote>


<p>
  The social divide between nobles and non-nobles in Montaillou was not vast. Le Roy Ladurie writes that “ladies and <i>châtelaines</i>, when they met with peasant women, did not hesitate to settle down for a gossip; they might even kiss and embrace” (16). This was likely less true in larger towns or cities; “the absence of strong demarcation between groups can be explained by the relative poverty of the mountain nobility” as contrasted to “the nobles of Paris or Bordeaux, with their huge manorial estates and their vineyards worth their weight in gold” (17). 
</p>

<p>
People had many close, intimate friendships outside the immediate or extended family. Groups of women socialized while fetching water, at the mill, in the kitchen, or sitting in the sun in the village square (251–254). Men met to sing, play chess, or speculate about if Heaven would run out of space for the souls of the dead (259–260). Sunday Mass was the central social event of the week, even for heretics or non-believers, but even so only about half the populace went to Mass any given week (265; 305). 
</p>

<p>
  Even in a village of a few hundred people, it was possible to keep secrets. Heretic <i>parfaits</i> snuck from house to house via secret passages (41) or disguised themselves as woodcutters to move about incognito (75–76), while nosy neighbors peeked through holes in doors or lifted up roofs (which must have been flimsy) to spy on heretical conversations (245; 256). 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250709_mar.png" style="width:400px;" />
  <figcaption> Detail of the March scene from the <i>Très Riches Heures du Duc de Berry</i> (c. 1415)</figcaption>
</figure>

<h2>Religion and Ethics</h2>

<p>
The taxes owed to the nobility were relatively light, particularly compared to the heavy taxes extracted by the Church, and the latter were hated much more than the former (20–23). It was common for people who owed the Church money (including tithes) to be excommunicated (335). The success of Catharism in Montaillou can be largely attributed to the burdensome taxation of the Church, which gave rise to strong anti-clerical feelings far in advance of any theological rationale. I was surprised to learn that indulgences were a part of Catholic religious practice even in the early 1300s, and were hated then too (334). 
</p>

<p>
Many people envision medieval Europe as a theocracy where Catholic morals reigned supreme, either for good or for ill. At least in the case of Montaillou, this wasn’t true—there were lots of mistresses, concubines, prostitution, illegitimate children, and sordid love affairs (45, 151, 169). Homosexuality is not recorded in Montaillou but is documented in the larger cities of Pamiers and Toulouse (144–149). Approximately 10% of couples in Montaillou during this time period were illicit or “living in sin,” and non-marital cohabitation was common enough that a visitor to one house was uncertain if the woman there was the man’s wife or his concubine. Le Roy Ladurie writes (169):
</p>

<blockquote>
If anyone came across a couple openly living together, the reaction was much the same as it would be today. Were they legally married or not? 
</blockquote>

<p>
  Sexual ethics aside,<sup>2</sup> crime was rare. While petty theft was not uncommon and grazing rights were always a source of conflict, Montaillou was an intimate society where “everyone knew everyone else and strangers were easy to find,” making crimes against property rather impractical (329). In cases in which a flock or house was confiscated, it was always under some legal mechanism rather than outright use of force. During the decades covered by the Fournier Register, a single murder and a handful of rapes are recorded—while this significantly exceeds the present on a per capita basis, these events were rare and shocking to the villagers.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250709_nov.png" style="width:400px;" />
  <figcaption> Detail of the November scene from the <i>Très Riches Heures du Duc de Berry</i> (c. 1415)</figcaption>
</figure>

<h2>Culture</h2>

<p>
The Fournier Register excels as a window into peasant culture in the 14th century. Peasants were “fond of abstract thought and even of philosophy and metaphysics” (232), and Le Roy Ladurie remarks on “the lack of social distance between the countryman… and the nobleman, the priest, the merchant, and the master craftsman, in a world where manual labour, especially craftsmanship, was not despised” (232). The primary social engagement was the evening meal, where groups of peasants would sit for hours at benches around the fire remembering village history, discussing the health of people and animals, arguing about the resurrection of the body, or simply gossiping (247, 250). Wine was served, but not to excess—drunkenness is only mentioned in urban contexts in the Fournier Register, and even there rarely (249). 
</p>

<p>
  Books, while rare and expensive, were important and recognized cultural objects—both Cathar <i>parfaits</i> and Catholic priests derived intellectual legitimacy from the possession of books (211, 234–236). It was rare, but not unheard of, for laymen to be able to read: Le Roy Ladurie estimates that four out of the roughly 250 inhabitants of Montaillou were literate (239). As a result most ideas were transmitted orally, and the Cathar <i>parfaits</i> were renowned for their oration and eloquence.
</p>

<p>
I was very surprised to learn that history was virtually unknown in Montaillou. Only in larger cities like Pamiers was Roman antiquity known and discussed, and there only rarely; in Montaillou, history “scarcely went back further than the previous Comte de Foix” (282). The Church filled this void, but imperfectly. Villagers knew almost nothing of Christian history besides Creation, the lives of Mary, Jesus, and the Apostles, and the coming Day of Judgement and the Resurrection (281). As Le Roy Ladurie describes it, “the people of Montaillou lived in a kind of ‘island in time,’ even more cut off from the past than from the future” (282).
</p>

<h2>Conclusion</h2>

<p>
I’ve only scratched the surface of Montaillou here. I haven’t told the story of Pierre Clergue, heretic village priest who had at least nine mistresses (and probably more) and used his brutal authority to crush village rivals; Pierre Maury, itinerant master shepherd with a love of poverty and a fatalist outlook on life; or Béatrice de Planissoles, twice-widowed noblewoman with a proclivity for dramatic love affairs with non-nobles between husbands. I feel some guilt in omitting these thrilling tales from my review, but I don’t think I can do them justice here.
</p>

<p>
What I’ve instead attempted to do here is give the flavor of medieval life as recounted by Le Roy Ladurie. Since this is a microhistory, we have to be cautious about how much we can generalize; Montaillou was different in the 14th century than in the 10th century, and would be different again by the 17th century, to say nothing of how life would be different in Frisia, Andalusia, Calabria, or outside medieval Europe. (If there’s one thing we can learn from Montaillou, it’s that history is big and strange.)
</p>

<p>
Still, I updated a number of my beliefs about the past after reading about Montaillou. Here’s a few common claims that I thought Montaillou directly addressed.
</p>

<ol>
<li>
    <b>“The nuclear family was a mistake.”</b> I discussed this claim from David Brooks above. Per his argument, Montaillou is exactly the sort of place that we might expect to show strong non-nuclear family living patterns, and yet if anything we see the exact opposite. This suggests that nuclear-family structures are more fundamental than Brooks argues (with the important caveat that this is just a single data point).
</li>
<li>
    <b>“Medieval peasants lived miserable lives of suffering, toil, and death.”</b> Strikingly false in the case of Montaillou. Mortality rates were certainly high, but even the subsistence-level farmers and shepherds documented by the Register had active, social, and joyful lives. I’m not convinced that the median person in Montaillou was less happy than the median person today; if anything, possibly the opposite.<sup>3</sup>
</li>
<li>
    <b>“Learning was forgotten in the Dark Ages.”</b> I was surprised by how true this was for Montaillou. There’s been a lot of pushback against misconceptions about the so-called “Dark Ages,” and popular conceptions about the time between the fall of the Western Roman Empire and the Renaissance are usually just wrong (cf. <a href=https://www.amazon.com/World-Late-Antiquity-150-750-Civilization/dp/0393958035>Peter Brown</a>). But the people of Montaillou were ignorant of almost all history, even just a few generations ago in their village. Maybe this was always true in rural areas, but I suspect this would have been much less true in Roman times and, again, stopped being true by the time of the Renaissance and the Reformation.
</li>
<li>
    <b>“Medieval Europe was a Catholic society where people adhered to Christian morals.”</b> I hear this a lot from more traditional Catholic friends, and this is just bonkers. Maybe Montaillou is an edge case—again, village of heretics—but Le Roy Ladurie argues that the heresy was a symptom of disrespect for the Church, not a cause. The fact that 10% of couples were openly unmarried and cohabitating defied all my intuition about medieval Europe.
</li>
</ol>

<p>
I really enjoyed this microhistory and would love to read different accounts of everyday life in medieval Europe—if you have any recommendations, please let me know!
</p>

<br />
<br /> 

<h3>Footnotes</h3>
<ol>
<li>
  As an aside, I’m a big fan of Bret Devereaux’s writing. He does a fantastic job of debunking myths about history, like <a href=https://acoup.blog/2019/05/28/new-acquisitions-not-how-it-was-game-of-thrones-and-the-middle-ages-part-i/>this series on <i>Game of Thrones</i></a> or <a href=https://acoup.blog/category/collections/this-isnt-sparta/>this series on Sparta</a>, and also does a lot of interesting long-form about pre-modern agriculture, textiles, logistics, and so on.
</li>
<li>
  This moral laxity was gone by the 17th century. The Reformation and Counter-Reformation gave rise to an ocean of fierce debates about theology and ethics and created a society which, compared to Montaillou, was much more concerned about matters of orthodoxy and orthopraxy and much less tolerant of any deviancy. Massachusetts Puritanism can be understood as a facet of this transformation, as can the <a href=https://en.wikipedia.org/wiki/Reformation_of_Manners>Reformation of Manners</a>. This change is noted by Le Roy Ladurie, but my interpretation here comes mainly from Diarmid MacCulloch’s book <i>The Reformation</i>.
</li>
<li>
    It’s worth noting that the feudal order as experienced in Montaillou was pretty lax—less rural areas like those around Paris were probably closer to popular depictions of feudal serfdom. I’m not sure which is more “typical” of feudalism, or if that question is even coherent; feudalism is a broad and often misunderstood concept, see <a href=https://www.amazon.com/Feudalism-F-L-Ganshof/dp/058248216X>Ganshof’s book</a> for a good overview.
</li>
</ol>

]]></description>
              <pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Workflows Are the New Models</title>
              <link>public/blog/20250702_workflows.html</link>
              <description><![CDATA[<br />
<br />
<a href="https://x.com/CorinWagen/status/1939695363427893471"><em>Expanded from a post on X</em></a><em>, which I felt didn’t do a good job expressing all of what I meant.</em>

<p>
  The past few years of “AI for life science” has been all about the models: AlphaFold&nbsp;3, neural-network potentials, protein language models, binder generation, docking, co-folding, ADME/tox prediction, and so on.  
  But <a href="https://www.chaidiscovery.com/news/introducing-chai-2">Chai-2</a> (and lots of related work) shows us that the vibes are shifting.  
  Models themselves are becoming just a building block; the real breakthroughs are going to happen at the workflow level, as we learn how to combine these models into robust and performant pipelines.
</p>

<p>
  Workflows are the new models.  
  To have a state-of-the-art computational stack for drug discovery (or protein engineering, or materials design, or anything else), it’s no longer enough to have just a single state-of-the-art model.  
  You need a suite of modular tools that you can combine in a way that makes sense for your task. (At Rowan, we’re seeing this happen all over the industry.)
</p>

<p>What does this mean in practice? Here are two imaginary case studies illustrating what modern computational chemistry looks like in 2025:</p>

<h3>Materials Science</h3>

<p>
  A company is developing a new inorganic photocatalyst for bulk acid–alkene coupling (following
  <a href="https://pubs.acs.org/doi/abs/10.1021/jacs.0c08688">Zhu and Nocera, 2020</a>).  
  Their workflow might look something like this:
</p>

<ul>
  <li>Agentic literature search for potential photo-active inorganic materials that seem synthesizable.</li>
  <li>A diffusion or flow-matching model for 3-D structure generation where crystallography data doesn’t exist.</li>
  <li>Rapid structural relaxation with a neural-network potential (NNP) to generate minimized structures.</li>
  <li>Adsorption-energy estimation with another NNP to see if alkene binding is feasible.</li>
  <li>HOMO–LUMO gap computation with periodic DFT to estimate photo-activity.</li>
  <li>Molecular dynamics to check the stability of the bound pose.</li>
  <li>Volcano-plot creation and final candidate scoring based on all properties.</li>
</ul>

<p>
  The entire cycle can be repeated <em>ad nauseum</em> to generate new candidates, with the focus gradually shifting from exploration to exploitation.
</p>

<h3>Drug Discovery</h3>

<p>
  A company has identified new CNS biological targets that they hope to inhibit with a small molecule.  
  Their workflow might look something like this:
</p>

<ul>
  <li>Based on a starting hit (from a DEL, or from a known binder), generate modifications automatically or by sampling from an enumerated library.</li>
  <li>Filter candidates by synthesizability, solubility, pK<sub>a</sub>, and other project-specific structural filters.</li>
  <li>Dock molecules against the target and potential anti-targets using a fast method like Vina.</li>
  <li>For hits predicted to show good selectivity, rescore with a second method (strain-corrected docking, Boltz-2, etc.).</li>
  <li>
    <a href="https://pubs.acs.org/doi/abs/10.1021/acs.jcim.0c00057">Run a short MD simulation</a> to check the stability of the bound pose.
  </li>
  <li>
    Screen for <a href="https://rowansci.com/publications/macroscopic-pka-prediction">blood–brain-barrier permeability</a> and
    <a href="https://pubs.acs.org/doi/10.1021/acs.chemrestox.4c00015">liver toxicity</a> (e.g.).
  </li>
</ul>

<p>
  This cycle, too, can be repeated until <del>you run out of Modal credits</del> a set of promising candidates is identified for synthesis.
</p>

<br />

<p>
  Neither of these case studies is based on a particular company; instead, they’re meant to illustrate the sort of ML-native workflows we’re seeing from early adopters across the chemical sciences.  
  For simplicity, experimental integration isn’t shown here, but any sane scientist will obviously incorporate wet-lab testing as soon as possible and feed those insights back into the top of the funnel.
</p>

<p>
  In any case, the overall point is clear—no single model can by itself solve every problem, and figuring out the right way to combine a set of models is itself a non-trivial system-design problem.  
  It’s entirely possible to create a state-of-the-art workflow simply by combining “commoditized” open-source models in a new way, and so far the resultant workflows don’t seem obvious or easy to copy.  
  This defies popular intuition about what constitutes a “moat” for AI companies.
</p>

<p>
  More metaphysically, the line between workflows and models is blurring.  
  Many ML-adjacent people think of models as the active unit of science: “they have a model for X” or “we’re building a model for Y.”  
  But, as shown above, most state-of-the-art research today requires lots of individual ML models, and many “models” are already miniature workflows.  
  For instance, running a single inference call through
  <a href="https://github.com/dptech-corp/Uni-pKa">the Uni-pKa “model”</a> requires enumerating all possible microstates, performing a conformer search, and running geometry optimizations on every individual conformer—just to generate the pairwise-distance matrix used as input for the actual ML model.
</p>

<p>Why does this matter? Here are a few thoughts that I've had, after thinking about this point:</p>

<ul>
  <li>Models must be plug-and-play, interoperable, and robust—anything that can’t be integrated into higher-level workflows won’t be used.</li>
  <li>
    The best models might not be those that top isolated benchmarks; in a workflow context, speed, reliability, and uncertainty
    quantification also matter.  
    Richard Hamming’s first rule of systems engineering comes to mind: “If you optimize the components, you will probably ruin the system
    performance” (<a href="https://corinwagen.github.io/public/blog/20230516_hamming.html">see my previous book review</a>).
  </li>
  <li>
    Any thinking that depends on a sharp metaphysical difference between workflows and models is probably wrong.  
    I recently had a sales call where someone told me they weren’t interested in any workflows—they only wanted to use models.  
    I wanted to send them to
    <a href="https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/">old Slate Star Codex posts</a>,
    but (wisely?) held my tongue.
  </li>
  <li>
    Devops and good software engineering will rise in importance.  
    At Rowan, we’ve learned firsthand how hard it is to manage hundreds of thousands of workflows across a vast sea of unruly scientific dependencies.
  </li>
  <li>
    Relatedly, the amount of scientific, computational, and engineering expertise needed to run a modern computational-science program is
    rising exponentially—and shows no signs of stopping.
  </li>
</ul>

<i>Thanks to Ari Wagen for reading a draft of this post.</i>
]]></description>
              <pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Teach Yourself Quantum Chemistry (For Fun And Profit)</title>
              <link>public/blog/20250604_good_will_hunting.html</link>
              <description><![CDATA[
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_damon.png" style="width:500px;" />
</figure>

<p>“You dropped a hundred and fifty grand on a f***** education you coulda' got for a dollar fifty in late charges at the public library.” —<em>Good Will Hunting</em></p>

<p>I was a user of computational chemistry for years, but one with relatively little understanding of how things actually worked below the input-file level. As I became more interested in computation in graduate school, I realized that I needed to understand everything much more deeply if I hoped to do interesting or original work in this space. From about August 2021 to December 2023, I tried to learn as much about how computational chemistry worked as I could, with the ultimate goal of being able to recreate my entire computational stack from scratch.</p>
<p>I found this task pretty challenging. Non-scientists don’t appreciate just how esoteric and inaccessible scientific knowledge can be: Wikipedia is laughably bad for most areas of chemistry, and most scientific knowledge isn’t listed on Google at all but trapped in bits and pieces behind journal paywalls. My goal in this post is to describe my journey and index some of the most relevant information I’ve found, in the hopes that anyone else hoping to learn about these topics has an easier time than I did.</p>
<p>This post is a directory, not a tutorial. I’m not the right person to explain quantum chemistry from scratch. Many wiser folks than I have already written good explanations, and my hope is simply to make it easier for people to find the “key references” in a given area. This post also reflects my own biases; it’s focused on molecular density-functional theory and neglects post-Hartree–Fock methods, most semiempirical methods, and periodic systems. If this upsets you, consider creating a companion post that fills the lacunæ—I would love to read it!</p>
<h2>0. Background</h2>
<p>I started my journey to understand computational chemistry with roughly three years of practice using computational chemistry. This meant that I already knew how to use various programs (Gaussian, ORCA, xTB, etc), I was actively using these tools in research projects, and I had a vague sense of how everything worked from reading a few textbooks and papers. If you don’t have any exposure to computational chemistry at all—if the acronyms “B3LYP”, “def2-TZVPP”, or “CCSD(T)” mean nothing to you—then this post probably won’t make very much sense.</p>
<p>Fortunately, it’s not too hard to acquire the requisite context. For an introduction to quantum chemistry, a good resource is Chris Cramer's <a href="https://www.youtube.com/watch?v=pu4uL7deCNw&amp;list=PLkNVwyLvX_TFBLHCvApmvafqqQUHb6JwF">video series</a> from his class at UMN. This is pretty basic but covers the main stuff; it's common for one of the classes in the physical chemistry sequence to also cover some of these topics. If you prefer books, <a href="https://www.amazon.com/Essentials-Computational-Chemistry-Theories-Models/dp/0470091827">Cramer</a> and <a href="https://www.amazon.com/Introduction-Computational-Chemistry-Frank-Jensen/dp/0470011874">Jensen</a> have textbooks that go slightly more in depth. </p>
<p>For further reading:</p>
<ul>
  <li>Steve Bachrach has a book on <a href="https://comporgchem.com/">computational organic chemistry</a> which discusses applying these techniques to a variety of problems in organic chemistry.</li>
  <li><a href="https://chemistlibrary.wordpress.com/wp-content/uploads/2015/02/modern-quantum-chemistry.pdf">Szabo/Ostlund</a> and <a href="https://www.amazon.com/Molecular-Electronic-Structure-Theory-Trygve-Helgaker/dp/1118531477">Helgaker/Jorgensen/Olsen</a> go much more into detail on the mathematics and implementation. (I confess I've only read parts of these books.)</li>
</ul>
<p>In general, computational chemistry is a fast-moving field relative to most branches of chemistry, so textbooks will be much less valuable than e.g. organic chemistry. There are lots of good review articles like <a href="https://onlinelibrary.wiley.com/doi/10.1002/anie.202205735">this one</a> which you can use to keep up-to-date with what's actually happening in the field recently, and reading the literature <a href="https://corinwagen.github.io/public/blog/20230329_literature.html">never goes out of style</a>.</p>
<p>All of the above discuss how to learn the theory behind calculations—to actually get experience running some of these, I'd suggest trying to reproduce a reaction that you've seen modeled in the literature. <a href="https://ekwan.github.io/notes.html#computational-chemistry">Eugene Kwan's notes are solid</a> (if Harvard-specific), and there are lots of free open-source programs that you can run like Psi4, PySCF, and xTB. (If you want to use Rowan, <a href="https://docs.rowansci.com/tutorials">we've got a variety of tutorials too.</a>) It's usually easiest to learn to do something by trying to solve a specific problem that you're interested in—I started out trying to model the effect of different ligands on the barrier to Pd-catalyzed C–F reductive elimination, which was tough but rewarding.</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_cf.png" style="width:650px;" />
</figure>

<p>Molecular dynamics is important but won’t be covered further here. The best scientific introduction to MD I've come across is <a href="https://sites.engineering.ucsb.edu/~shell/che210d/">these notes from M. Scott Shell</a>. If you want to go deeper, I really liked <a href="https://www.amazon.com/Understanding-Molecular-Simulation-Applications-Computational/dp/0122673514">Frenkel/Smit</a>; any knowledge of statistical mechanics will be very useful too, although I don’t have any recommendations for stat-mech textbooks. To get started actually running MD, I'd suggest looking into the OpenFF/OpenMM ecosystem, which is free, open-source, and moderately well-documented by scientific standards. (<a href="https://corinwagen.github.io/public/blog/20240613_simple_md.html">I've posted some intro scripts here</a>.)</p>
<p>Cheminformatics is a somewhat vaguely defined field, basically just "data science for chemistry," and it's both important and not super well documented. If you're interested in these topics, I recommend just reading <a href="https://patwalters.github.io/year-archive/">Pat Walters' blog</a> or <a href="https://greglandrum.github.io/rdkit-blog/">Greg Landrum's blog</a>: these are probably the two best cheminformatics resources.</p>
<p>As with all things computational, it's also worth taking time to build up a knowledge of computer science and programming—knowing how to code is an investment which will almost always pay itself back, no matter the field. That doesn't really fit into this guide, but being good at Python/Numpy/data science/scripting is worth pursuing in parallel, if you don't already have these skills.</p>

<h2>1. How do Things Work, Basically?</h2>
<p>I started my computational journey by trying to write my own quantum chemistry code from scratch. My disposition is closer to “tinkerer” than “theorist,” so I found it helpful to start tinkering with algorithms as quickly as possible to give myself <a href="https://marginalrevolution.com/marginalrevolution/2022/02/context-is-that-which-is-scarce-2.html">context</a> for the papers I was trying to read. There are several toy implementations of Hartree–Fock theory out there that are easy enough to read and reimplement yourself:</p>
<ul>
  <li><a href="https://nznano.blogspot.com/2018/03/simple-quantum-chemistry-hartree-fock.html">NZNano</a> has a compact Jupyter notebook that runs an HF/STO-3G calculation on helium hydride.</li>
  <li><a href="https://adambaskerville.github.io/posts/HartreeFockGuide/">Adam Baskerville</a> has a somewhat cleaner implementation of HF/STO-3G on HeH+, albeit one which doesn’t compute the one- and two-electron integrals.</li>
  <li><a href="https://github.com/jjgoings/McMurchie-Davidson">Joshua Goings</a> has a great general implementation of HF for any molecule or basis set, and <a href="https://joshuagoings.com/2017/04/28/integrals/">an associated blog post</a> explaining how to compute the integrals.</li>
</ul>
<p>Based on these programs, I wrote my own Numpy-based implementation of Hartree–Fock theory, and got it to match values from the NIST CCCBDB database.</p>

<pre class=code-block>
  iteration 05:	∆P 0.000001	E_elec -4.201085		∆E -0.000000

SCF done (5 iterations):
-2.83122 Hartree

orbital energies: [-2.30637605 -0.03929435]
orbital matrix: 
[[-0.71874445  0.86025802]
 [-0.44259188 -1.02992712]]
density matrix: 
[[1.03318718 0.63622091]
 [0.63622091 0.39177514]]
Mulliken charges:
[[1.32070648 0.81327091]
 [1.10313469 0.67929352]]
</pre>

<p>I highly recommend this exercise: it makes quantum chemistry much less mysterious, but also illustrates just how slow things can be when done naïvely. For instance, my first program took 49 minutes just to compute the HF/6-31G(d) energy of water, while most quantum chemistry codes can perform that calculation in under a second. Trying to understand this discrepancy was a powerful motivation to keep reading papers and learning new concepts.</p>

<h2>2. Overview</h2>
<p>Armed with a basic understanding of what goes into running a calculation, I next tried to understand how “real” computational chemists structure their software. The best single paper on this topic, in my opinion, is the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540030314">“direct SCF” Almlof/Faegri/Korsell paper from 1982</a>. It outlines the basic method by which almost all calculations are run today, and it’s pretty easy to read. (See also <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540100111">this 1989 followup by Haser and Ahlrichs</a>, and <a href="https://link.springer.com/article/10.1007/s002149900072">this 2000 retrospective by Head-Gordon.</a>)</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_direct_scf.png" style="width:550px;" />
</figure>

<p>I found several other papers very useful for building high-level intuition. <a href="https://rsc.anu.edu.au/~pgill/papers/066ECC.pdf">This overview of self-consistent field theory by Peter Gill</a> is very nice, and <a href="https://schlegelgroup.wayne.edu/Pub_folder/32.pdf">Pople’s 1979 paper</a> on analytical first and second derivatives within Hartree–Fock theory is a must-read. <a href="https://www.worldscientific.com/doi/abs/10.1142/9789812832115_0008?srsltid=AfmBOopXU4ZCfegA2U-8HABNK_sBs_gU29cWUgfwv0diFtmKgs_RC2CF">This 1995 Pulay review</a> is also very good.</p>
<p>Using the intuition in these papers, I was able to outline and build a simple object-oriented Hartree–Fock program in Python. My code (which I called <span class=code>hfpy</span>, in allusion to <a href="https://www.sigmaaldrich.com/US/en/product/aldrich/184225">the reagent</a>) was horribly inefficient, but it was clean and had the structure of a real quantum chemistry program (unlike the toy Jupyter Notebooks above). Here’s some of the code that builds the Fock matrix, for instance:</p>

<pre class=code-block>
    for classname in quartet_classes.keys():
        num_in_class = len(quartet_classes[classname])
        current_idx = 0
        while current_idx &lt; num_in_class:
            batch = ShellQuartetBatch(quartet_classes[classname][current_idx:current_idx+Nbatch])

            # compute ERI
            # returns a matrix of shape A.Nbasis x B.Nbasis x C.Nbasis x D.Nbasis x NBatch
            ERIabcd = eri_MMD(batch)
            shell_quartets_computed += batch.size

            for i, ABCD in enumerate(batch.quartets):
                G = apply_ERI(G, *ABCD.indices, bf_idxs, ERIabcd[:,:,:,:,i], ABCD.degeneracy(), dP)

            current_idx += Nbatch

    # symmetrize final matrix
    # otherwise you mess up, because we've added (10|00) and not (01|00) (e.g.)
    G = (G + G.T) / 2
    if incremental:
        molecule.F += G
    else:
        molecule.F = molecule.Hcore + G

    print(f"{shell_quartets_computed}/{shell_quartets_possible} shell quartets computed")
</code></pre>
<p>Any quantum-chemistry developer will cringe at the thought of passing 5-dimensional ERI arrays around in Python—but from a pedagogical perspective, it’s a good strategy.</p>

<h2>3. Details</h2>
<p>Armed with a decent roadmap of what I would need to build a decent quantum chemistry program, I next tried to understand each component in depth. These topics are organized roughly in the order I studied them, but they’re loosely coupled and can probably be addressed in any order.</p>
<p>As I read about various algorithms, I implemented them in my code to see what the performance impact would be. (By this time, I had rewritten everything in C++, which made things significantly faster.) Here’s a snapshot of what my life looked like back then:</p>
<pre class=code-block>benzene / 6-31G(d)
    12.05.22 - 574.1 s - SHARK algorithm implemented

    12.20.22 - 507.7 s - misc optimization
    12.21.22 - 376.5 s - downward recursion for [0](m)
    12.22.22 - 335.9 s - Taylor series approx. for Boys function
    12.23.22 - 300.7 s - move matrix construction out of inner loops
    12.25.22 - 267.4 s - refactor electron transfer relation a bit
    12.26.22 - 200.7 s - create electron transfer engine, add some references
    12.27.22 - 107.9 s - cache electron transfer engine creation, shared_ptr for shell pairs
    12.28.22 -  71.8 s - better memory management, refactor REngine
    12.29.22 -  67.6 s - more memory tweaks

    01.03.23 -  65.1 s - misc minor changes
    01.05.23 -  56.6 s - create PrimitivePair object
    01.07.23 -  54.7 s - Clementi-style s-shell pruning
    01.08.23 -  51.7 s - reduce unnecessary basis set work upon startup
    01.10.23 -  49.9 s - change convergence criteria
    01.11.23 -  61.1 s - stricter criteria, dynamic integral cutoffs, periodic Fock rebuilds
    01.12.23 -  42.5 s - improved DIIS amidst refactoring

    01.20.23 -  44.7 s - implement SAD guess, poorly
    01.23.23 -  42.6 s - Ochsenfeld's CSAM integral screening
    01.30.23 -  41.7 s - improve adjoined basis set generally

    02.08.23 -  38.2 s - introduce spherical coordinates. energy a little changed, to -230.689906932 Eh.

    02.09.23 -  12.3 s - save ERI values in memory. refactor maxDensityMatrix.
    02.15.23 -   9.0 s - rational function approx. for Boys function
    02.18.23 -   8.7 s - minor memory changes, don't recreate working matrices in SharkEngine each time
</pre>

<p>
  Here's a rough list of the topics I studied, with the references that I found to be most helpful in each area.
</p>

<h3>3.1 ERI Computation</h3>
<p>Writing your own QM code illustrates (in grisly fashion) just how expensive computing electron–electron repulsion integrals (ERIs) can be. This problem dominated the field’s attention for a long time. In the memorable words of Jun Zhang:</p>
<blockquote>
  Two electron repulsion integral. It is quantum chemists’ nightmare and has haunted in quantum chemistry since its birth.
</blockquote>
<p>To get started, Edward Valeev has written <a href="https://arxiv.org/ftp/arxiv/papers/2007/2007.12057.pdf">a good overview of the theory behind ERI computation</a>. For a broad overview of lots of different ERI-related ideas, my favorite papers are these two accounts by <a href="https://rsc.anu.edu.au/~pgill/papers/045Review.pdf">Peter Gill</a> and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/jcc.26942">Frank Neese</a>. (I’ve probably read parts of Gill’s review almost a hundred times.)</p>
<p>Here are some other good ERI-related papers, in rough chronological order:</p>
<ul>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261473800600">This 1973 paper by Raffinetti</a> discusses a non-direct SCF strategy for storing integrals on disk, which is still commonly used for small systems.</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/89/9/5777/220762/A-method-for-two-electron-Gaussian-integral-and?redirectedFrom=fulltext">This 1989 paper from Head-Gordon and Pople</a> outlines the basic algorithm by which most programs tackle ERI computation today. (The core ideas were developed by <a href="https://pubs.aip.org/aip/jcp/article-abstract/84/7/3963/91928/Efficient-recursive-computation-of-molecular?redirectedFrom=fulltext">Obara and Saika</a>, but I find that paper pretty unintelligible.) <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.560360831">Peter Gill’s extension of this work</a> is also quite nice (see also his review above).</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/104/7/2620/479230/A-J-matrix-engine-for-density-functional-theory?redirectedFrom=fulltext">This 1996 White/Head-Gordon paper</a> proposes the “<em>J</em>-matrix engine” and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0009261400005248">this 2000 paper</a> suggests some enhancements.</li>
  <li>Todd Martínez has a great series of papers discussing ERI evaluation on GPUs: <a href="https://pubs.acs.org/doi/10.1021/ct700268q">1</a>, <a href="https://pubs.acs.org/doi/abs/10.1021/ct800526s">2</a>, <a href="https://pubs.acs.org/doi/10.1021/ct9003004">3</a></li>
  <li>Ochsenfeld and Head-Gordon’s <a href="https://pubs.aip.org/aip/jcp/article-abstract/109/5/1663/529170/Linear-and-sublinear-scaling-formation-of-Hartree?redirectedFrom=fulltext">LinK strategy</a> for accelerated exchange computation is very nice. (And resolution-of-the-identity approaches can help here too: see <a href="https://pubs.aip.org/aip/jcp/article/143/2/024113/825080/Fast-accurate-evaluation-of-exact-exchange-The-occ">Manzer’s “occ-RI-K” strategy</a>.)</li>
  <li><a href="https://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00788">Jun Zhang has a paper</a> on metaprogramming-based approaches to ERI computation.</li>
  <li>Benjamin Pritchard has <a href="https://onlinelibrary.wiley.com/doi/10.1002/jcc.24483">a paper on vectorization in ERI computation</a>.</li>
  <li>C. David Sherrill discusses permutational symmetry in <a href="http://vergil.chemistry.gatech.edu/notes/permsymm/permsymm.html">these notes</a>.</li>
  <li>Integral screening matters a lot; I discuss it in <a href="https://corinwagen.github.io/public/blog/20230123_integral_screening.html">this blog post</a> from January 2023, which is roughly when I was learning all of this.</li>
</ul>

<h3>3.2 SCF Convergence</h3>
<p>Writing your own QM code also illustrates how important SCF convergence can be. With the most naïve approaches, even 15–20 atom molecules often struggle to converge—and managing SCF convergence is still relatively unsolved in lots of areas of chemistry today, like radicals or transition metals.</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_scf.png" style="width:500px;" />
  <figcaption>
    Different SCF-convergence strategies and how they fare on tricky systems (from Scuseria, <i>JCP</i>, <b>2012</b>; <i>vide infra</i>)
  </figcaption>
</figure>

<p>This is obviously a big topic, but here are a few useful references:</p>
<ul>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/126.pdf">This 1991 review</a> by Schlegel and Douall is a good overview of SCF convergence questions.</li>
  <li>The most fundamental approach is Pulay’s “direct inversion of the iterative subspace” (DIIS): the original paper is <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261480803964?via%3Dihub">here</a>, but this is famous enough that there are plenty of good and more modern summaries out there (<a href="https://joshipulkit.github.io/notes/diis/">1</a>, <a href="http://vergil.chemistry.gatech.edu/notes/diis/node2.html">2</a>, <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/03_Hartree-Fock/3b_rhf-diis.ipynb">3</a>).</li>
  <li><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2830258/">This 2010 paper</a> describes the “ADIIS” strategy, and <a href="https://pubs.aip.org/aip/jcp/article-abstract/137/5/054110/671630/Comparison-of-self-consistent-field-convergence?redirectedFrom=fulltext">this 2012 Scuseria paper</a> shows that ADIIS and “energy DIIS” (EDIIS) are equivalent.</li>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.20393">This 2006 paper</a> is a good guide to the “superposition of atomic density” guess strategy.</li>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0065327608603391">Lowdin’s 1970 paper</a> on orthonormalization and removing linear dependencies in basis sets is solid.</li>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.560070407">This 1973 Saunders paper</a> describes “level shifting,” which is ubiquitous today.</li>
</ul>

<h3>3.3 Density Fitting/Resolution of the Identity</h3>
<p>Density-fitting/resolution-of-the-identity (“RI”) approaches are becoming <em>de rigueur</em> for most applications. <a href="https://www.sciencedirect.com/science/article/abs/pii/000926149500621A">This 1995 Ahlrichs paper</a> explains “RI-J”, or density fitting used to construct the J matrix (<a href="https://www.sciencedirect.com/science/article/abs/pii/0009261493891517">this 1993 paper</a> is also highly relevant.) <a href="https://pubs.rsc.org/en/content/articlelanding/2002/cp/b204199p">This 2002 Weigend paper</a> extends RI-based methods to K-matrix construction. <a href="http://vergil.chemistry.gatech.edu/notes/df.pdf">Sherrill’s notes</a> are useful here, as is <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/03_Hartree-Fock/density-fitting.ipynb">the Psi4Numpy tutorial</a>.</p>
<p>RI methods require the use of auxiliary basis sets, which can be cumbersome to deal with. <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.6b01041">This 2016 paper</a> describes automatic auxiliary basis set generation, and <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.3c00670">this 2023 paper</a> proposes some improvements. <a href="https://molssi-bse.github.io/basis_set_exchange/developer_api.html#basis_set_exchange.manip.autoaux_basis">There’s now a way</a> to automatically create auxiliary basis sets through the Basis Set Exchange API.</p>

<h3>3.4 Mechanics of Density-Functional Theory</h3>
<p>For Hartree–Fock theory, there are a lot of nice example programs (linked above); for density-functional theory, there are fewer examples. Here are a few resources which I found to be useful:</p>
<ul>
  <li><a href="https://pubs.aip.org/jcp/article/88/4/2547/91134/A-multicenter-numerical-integration-scheme-for">This 1988 paper</a>, from Becke, basically describes how all DFT quadrature is done today. I find <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261496006008">this 1996 Stratmann paper</a> to be much more readable, though (plus it’s a linear-scaling method).</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/98/7/5612/843250/The-performance-of-a-family-of-density-functional?redirectedFrom=fulltext">This 1993 paper</a> by Gill and Head-Gordon discusses how to get analytical DFT gradients (among other things). Here's the key equation:

  <figure>
    <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_dft_grad.png" style="width:400px;" />
  </figure>

    This is the clearest exposition of the math behind DFT that I’ve read, although it doesn’t cover meta-GGA functionals or range-separated hybrids. (There’s a <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/04_Density_Functional_Theory/4c_GGA_and_Meta_GGA.ipynb">Psi4Numpy explanation</a> of the math behind meta-GGA functionals, if you can parse the insane Psi4 syntax.)</li>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261494001995">This 1994 paper</a> discusses how to make DFT calculations with numerical quadrature rotationally invariant, which <a href="https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e">many programs still haven’t figured out</a>…</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/101/10/8894/472028/The-effect-of-grid-quality-and-weight-derivatives?redirectedFrom=fulltext">This other 1994 paper</a> argues that it’s not necessary to add grid weight derivatives with large grids. People still argue about this one.</li>
  <li>The first paper describing range-separated hybrids, as far as I can tell, is <a href=https://pubs.aip.org/aip/jcp/article-abstract/118/18/8207/460359/Hybrid-functionals-based-on-a-screened-Coulomb?redirectedFrom=fulltext>this 2003 work</a> from Scuseria and co-workers.</li>
</ul>

<h3>3.5 Linear Scaling</h3>
<p>For large systems, it’s possible to find pretty large speedups and reach a linear-scaling regime for DFT (excepting operations like matrix diagonalization, which are usually pretty fast anyway). <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261494011281">This 1994 paper</a> discusses extending the fast multipole method to Gaussian distributions, and how this can lead to linear-scaling <em>J</em>-matrix construction, and <a href="https://www.science.org/doi/10.1126/science.271.5245.51">this 1996 paper</a> discusses a similar approach. <a href="https://pubs.aip.org/aip/jcp/article-abstract/105/19/8969/479837/A-linear-scaling-method-for-Hartree-Fock-exchange?redirectedFrom=fulltext">This other 1996 paper</a> describes a related near-linear-scaling approach for <em>K</em> matrices. There are a bunch more papers on various approaches to linear scaling (Barnes–Hut, CFMM, GvfMM, Turbomole’s RI/CFMM, etc), but I think there are diminishing marginal returns in reading all of them.</p>

<h3>3.6 Geometry Optimization</h3>
<p>Geometry optimization for molecular systems is pretty complicated. Here’s a sampling of different papers, with the caveat that this doesn’t come close to covering everything:</p>
<ul>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/50.pdf">This 1981 Schlegel paper</a> explores the general considerations in optimizing molecules with quantum chemical methods.</li>
  <li><a href="https://link.springer.com/article/10.1007/BF00554788">This 1984 Schlegel paper</a> discusses the importance of initial Hessians in geometry optimization.</li>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/180.pdf">This 1996 Schlegel/Frisch paper</a> discusses use of internal coordinates for geometry optimizations, which is standard today, and <a href="https://schlegelgroup.wayne.edu/Pub_folder/390.pdf">this 2016 paper</a> explores some enhancements.</li>
  <li><a href="https://pubs.acs.org/doi/10.1021/ct050275a">This 2006 paper</a> extends the DIIS scheme to geometry optimization; this still seems underrated today.</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/140/16/164115/841317/A-finite-difference-Davidson-procedure-to-sidestep?redirectedFrom=PDF">This 2014 paper</a> discusses a way to obtain the lowest eigenvalues of the Hessian without computing the full matrix, thus accelerating TS searches.</li>
  <li><a href="https://chemrxiv.org/engage/chemrxiv/article-details/64e37e3700bbebf0e68dd9c4">This 2023 paper</a> benchmarks existing optimization algorithms, albeit for somewhat boring molecules.</li>
</ul>

<h3>3.7 Frequency + Thermochemistry</h3>
<p>I think the best guides here are <a href="https://gaussian.com/vib/">the Gaussian vibrational analysis white paper</a> and <a href="https://gaussian.com/vib/">the Gaussian thermochemistry white paper</a>—they basically walk through everything that’s needed to understand and implement these topics. It’s now pretty well-known that small vibrational frequencies can lead to thermochemistry errors; <a href="https://rowansci.com/blog/dft-errors">Rowan has a blog post</a> that discusses this and similar errors.</p>

<h3>3.8 Solvent</h3>
<p>Implicit solvent models, while flawed, are still essential for a lot of applications. <a href="https://pubs.rsc.org/en/content/articlelanding/1993/p2/p29930000799">This 1993 paper</a> describes the COSMO solvation method, upon which most modern implicit solvent methods are built. <a href="https://pubs.acs.org/doi/10.1021/jp992097l">Karplus and York found a better way to formulate these methods in 1999</a>, which makes the potential-energy surface much less jagged: </p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250604_solvent.png" style="width:500px;" />
</figure>

<p>While there are many more papers that one could read in this field, these are the two that I found to be most insightful.</p>

<h3>3.9 Basis Sets Miscellania</h3>
<p>I haven’t talked much about basis sets specifically, since most basis-set considerations are interwoven with the ERI discussion above. But a few topics warrant special mention:</p>
<ul>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261496009177">This 1996 paper by Davidson</a> discusses how to convert general contraction into segmented contraction, which is important since the latter is much easier to handle.</li>
  <li>The issue of converting between spherical and Cartesian basis sets is tackled by <a href="https://onlinelibrary.wiley.com/doi/10.1002/qua.560540202">this 1995 work from Schelgel and Frisch</a>.</li>
</ul>




<h2>4. Conclusions</h2>
<p>There are hundreds of other papers which could be cited on these topics, to say nothing of the myriad topics I haven’t even mentioned here, but I think there’s diminishing marginal utility in additional links. The knowledge contained in the above papers, plus ancillary other resources, were enough to let me write my own quantum-chemistry code. Over the course of learning all this content, I wrote four different QM programs, the last of which, “Kestrel,” ended up powering Rowan for the first few months after we launched.</p>
<p>These programs are no longer directly relevant to anything we do at Rowan. We retired Kestrel last summer, and now exclusively use external quantum-chemistry software to power our users’ calculations (although I’ve retained some strong opinions; we explicitly override Psi4’s defaults to use the Stratmann–Scuseria–Frisch quadrature scheme, for instance).</p>
<p>But the years I spent working on this project were, I think, an important component of my scientific journey. Rene Girard says that <a href="https://corinwagen.github.io/public/blog/20230228_gilliam_and_girard.html">true innovation requires a “mastery” of the past’s achievements</a>. While I’m far from mastering quantum chemistry, I think that I’d be ill-suited to understand the impact of recent advances in neural network potentials without having immersed myself in DFT and conventional physics-based simulation for a few years.</p>
<p>And, on a more personal note, learning all this material was deeply intellectually satisfying and a fantastic use of a few years. I hope this post conveys some of that joy and can be helpful for anyone who wants to embark on a similar adventure. If this guide helped you and you have thoughts on how I could make this better, please feel free to reach out!</p>
<p><em>Thanks to Peter Gill, Todd Martínez, Jonathon Vandezande, Troy Van Voorhis, Joonho Lee, and Ari Wagen for helpful discussions.</em></p>

]]></description>
              <pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>How Does o3 Guess Latitude From Photos?</title>
              <link>public/blog/20250527_latitude.html</link>
              <description><![CDATA[
<p>
  Recently, <a href=https://x.com/KelseyTuoc/status/1917340813715202540>Kelsey Piper</a> shared that o3 (at time of writing, one of the latest reasoning models from OpenAI) could guess where outdoor images were taken with almost perfect accuracy. <a href=https://www.astralcodexten.com/p/testing-ais-geoguessr-genius>Scott Alexander</a> and others have since verified this claim. 
</p>

<p>
I’ve been playing around with this too: with the prompt linked in Scott’s post, o3 can guess where my photos were taken almost every time, even when I intentionally avoid anything that looks like it might be too helpful. After inspecting the reasoning, I was surprised to learn that o3 can almost always estimate the latitude to within a few degrees, which vastly restricts the range of potential answers.
</p>


<p>
I didn’t think this was possible before doing the research for this post. Here’s three recent examples—see if you can estimate the latitude yourself. (You may want to open the images in a new tab to zoom in.)
</p>


<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250527_ohio.png" style="width:500px;" />
</figure>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250527_mexico.png" style="width:550px;" />
</figure>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250527_argentina.png" style="width:550px;" />
</figure>

<p>
o3 guessed that these were 40–45º N,  25–28 ºN, and 34–36º S; in every case the answer was within that range. (I make sure to only give o3 screenshots, so it can’t access EXIF data or otherwise cheat at this assessment.) 
</p>

<p>
How is this possible? Here’s my best understanding of what o3 is doing, informed by a bunch of back-and-forth conversations with a friendly neighborhood AI model. (I’ll be assuming that the photo also has compass information, in keeping with standard GeoGuessr rules.)
</p>

<h2>Local Noon, On The Equinox</h2>
<p>
Let’s make this as simple as possible to start. On the spring equinox, at noon, at the equator, the sun is directly overhead. As a result, tall objects without any overhang won’t cast any shadows. 
</p>

<p>
If you’re not on the equator, then objects will still cast a shadow, and the length of the shadow S relative to the object’s height H tells you what latitude you’re at. Formally, we can define the solar elevation θ := arctan(H/S), and approximate the latitude φ as 90º − θ. 
</p>

<p>
I don’t do much mental trigonometry these days, but it’s pretty easy to make a table with some key values: 
</p>

<table>
  <thead>
    <tr>
      <th>S/H</th>
      <th>θ (Solar Elevation)</th>
      <th>φ (Latitude)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2:1</td>
      <td>26º</td>
      <td>64º</td>
    </tr>
    <tr>
      <td>1.3:1</td>
      <td>37º</td>
      <td>53º</td>
    </tr>
    <tr>
      <td>1:1</td>
      <td>45º</td>
      <td>45º</td>
    </tr>
    <tr>
      <td>0.7:1</td>
      <td>55º</td>
      <td>35º</td>
    </tr>
    <tr>
      <td>0.5:1</td>
      <td>63º</td>
      <td>27º</td>
    </tr>
    <tr>
      <td>0 (no shadow)</td>
      <td>90º</td>
      <td>0º</td>
    </tr>
  </tbody>
</table>


<p>
With a compass, figuring out which hemisphere you’re in is easy. In the northern hemisphere, the sun is south of you, so the shadows point north; in the southern hemisphere, the shadows point south.
</p>

<h2>Local Noon, Any Season</h2>
<p>
Unfortunately, it’s more complex than this—we’re not always on the equinox, meaning that we also have to account for solar declination (δ). The solar declination reflects how far away from the equator the sun is on any given date; δ is +23.4º on the summer solstice and -23.4º on the winter solstice. We have to adjust the above formula to take this into account: now φ ≈ 90º − θ + δ. 
</p>

<p>
Qualitatively, this means that shadows are shorter in summer than winter. A H/S ratio of 1:1 implies a latitude of 22º in winter or a latitude of 68º in summer, which is the difference between Cuba and Iceland. In practice, though, o3 can often figure out the season from how the trees look and how people are dressed.
</p>

<h2>Any Time, Any Season</h2>
<p>
When we move away from noon, things get a bit more complicated. We have to employ h, the hour angle, which is equal to 0º at local noon (when the sun is directly overhead) and increments by 15º every hour. Here’s the full equation:
</p>

<p>
sin(θ) = sin(φ)*sin(δ) + cos(φ)*cos(δ)*cos(h)
</p>

<p>
(o3 says “It’s just the spherical-law-of-cosines applied to the right-angled triangle on the celestial sphere.” Trivial! If you’re curious how we go from this to the simplified noon equation above, see Appendix A.)
</p>

<p>
This is a bit too involved for a GeoGuessr game—even o3 solves this in a Python environment. Qualitatively, though, this means that as we move away from noon and cos(h) becomes smaller, the solar elevation θ shrinks. Within an hour or two of noon, we’re only off by 1–2º, but after three hours we’re overestimating the latitude by 7–10º.
</p>

<p>
This seems bad, but with a compass it’s relatively easy to check how far from noon it is. Shadows point exactly north–south at local noon, and point increasingly east or west as the hour angle increases, so looking at the shadow orientation can tell you how much to update the latitude. In practice o3 recommends just ignoring shadow-related math after 3:00 PM or before 9:00 AM, since the error becomes too high. 
</p>

<h2>Putting It All Together</h2>
<p>
Here’s another recent GeoGuessr picture. Can we solve this now? 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250527_cambodia.png" style="width:550px;" />
</figure>


<p>
Here’s my attempt to apply what we’ve learned: we’re looking east and we can see that shadows are pointing north, so the sun is south. This means that we’re in the northern hemisphere. The shadow–height ratio is a bit tough to estimate from this picture; based on the Prius, maybe 0.5:1. So that gives us an equinox latitude of 27º N, minus say 5º for time of day (since the shadows seem angled), which leaves us with a latitude of… 22º N ± 23º depending on the season. Not terribly helpful.
</p>

<p>
I gave o3 the same image, and it told me the latitude was 12–15º N. The correct answer is 11.5 ºN (Phnom Penh). 
</p>

<p>
What did we do wrong? I asked o3 what went wrong with my reasoning, and this is what it told me (lightly edited for clarity): 
</p>
<ul>
  <li>
    <b>Wrong S:H ratio.</b> “A Toyota Hilux / Prius roof is ≈ 1.5–1.6 m. Measuring the dark patch on the tarmac (allowing for perspective) the shadow is 0.6–0.7 m, giving height : shadow ≈ 2.3–2.5.”
  </li>
  <li>
    <b>Wrong hour correction.</b> “How far the Sun drops below its noon altitude depends on the hour angle… Two hours before/after noon (h ≈ 30 °) knocks 10–15° off the altitude in the low latitudes, not 5°.”
  </li>
  <li>
    <b>Very wrong season correction.</b> “Season changes δ by ±23.4 °, but the corresponding change in the Sun’s altitude is modulated by both time-of-day (cos h) and your own latitude (cos φ). Thus the swing is never a flat ±23 ° except at noon on/near the tropics… 
Solve sin(67º) = sin(ϕ)*sin(δ) + cos(ϕ)*cos(δ)*cos(30º) for the full seasonal range. That inversion pins ϕ between 11° N (June solstice) and 16° N (December solstice), with the equinox coming out at ≈ 13° N.”
  </li>
</ul>

<p>
The last point deserves further discussion. The impact of solar declination on solar elevation is modulated both by the latitude and the hour angle—the 23.4º swing is scaled by cos(h) and by the actual latitude. With some Python code (Appendix B), we can quickly confirm that the effect is smaller at near-equatorial latitudes: 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250527_theta_by_phi.png" style="width:550px;" />
</figure>

<p>
Overall, though, there’s nothing here we haven’t already discussed; o3 just understands this material better than me and can do the math properly.
</p>

<h2>Conclusions</h2>
<p>
This is a fun activity for building AI-related intuition. o3 is very good at this and is able to do something that appears superhuman. Upon close inspection, the reasoning is legible, but I’m not really able to follow the same methods myself with any degree of precision; I’m just not quite able to do any step with sufficient accuracy. I’m hoping that I’ll be able to build up my skills over time—this would be a pretty fun party trick. 
</p>

<h2>Appendix A: Getting The Simplified Equation For Noon</h2>
<p>
The full equation is: 
</p>
<p>
sin(θ) = sin(φ)*sin(δ) + cos(φ)*cos(δ)*cos(h)
</p>
<p>
If we assume that we’re at local noon, cos(h) = 1. This lets us apply the following identity:
</p>
<p>
cos(α−β) = cos(α)*cos(β) + sin(α)*sin(β)
</p>
<p>
To get: 
</p>
<p>
sin(θ) = cos(φ - δ)
</p>
<p>
sin(θ) = sin(90º - φ + δ)
</p>
<p>
θ = 90º - φ + δ
</p>
<p>
Which is the simplified equation I presented above.
</p>

<h2>Appendix B: </h2>

<p>
  Here's the Python code to generate the above plot.
</p>

<pre class=code-block>
import numpy as np
import matplotlib.pyplot as plt

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

def solar_elevation_vs_month(hour_local: float, latitude_deg: float) -> np.ndarray:
    """
    Return an array of solar-elevation angles (degrees) for each day at a given true-solar hour and latitude.
    """
    lat = np.deg2rad(latitude_deg)
    doy = np.arange(365)

    # Approximate solar-declination model (±23.44° sine fit)
    delta = np.arcsin(np.sin(np.deg2rad(23.44)) * np.sin(2 * np.pi / 365 * (doy - 80)))

    # Hour angle: 0° at solar noon; +15° per hour in the afternoon
    H = np.deg2rad((hour_local - 12.0) * 15.0)

    # Solar-elevation formula
    h = np.arcsin(np.sin(lat) * np.sin(delta) + np.cos(lat) * np.cos(delta) * np.cos(H))
    return np.rad2deg(h)


if __name__ == "__main__":
    plt.figure(figsize=(6, 4))

    months = np.array([15, 46, 75, 105, 135, 162,  198, 228, 258, 288, 318, 344])

    for h, l in [(15, 0), (15, 15), (15, 30), (15, 45), (15,60)]:
        elevations = solar_elevation_vs_month(h, l)
        plt.plot(np.arange(365), elevations, label=f"{h:.1f} h, {l:.1f}° N")
    
    plt.xticks(months, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
    plt.ylabel(f"Solar elevation (°)")
    plt.xlim(0,365)
    plt.ylim(0,60)
    plt.legend()
    plt.tight_layout()
    plt.show()
</pre>


]]></description>
              <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Ruthless Elimination of Mottes</title>
              <link>public/blog/20250517_hurry.html</link>
              <description><![CDATA[
<br>
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250517_icon.jpg" style="width:400px;" />
  <figcaption> A traditional iconographic depiction of Abraham's visitors. </figcaption>
</figure>
<br>

<p>I did not enjoy John Mark Comer’s book <em>The Ruthless Elimination of Hurry</em>.</p>

<p>Comer’s book is written to people trying to find meaning in a world that feels rushed, distracted, and isolated. At the time of writing, Comer was a Protestant pastor in Portland, Oregon, but he’s since stepped back to focus on creating resources for spiritual formation (like this book).</p>

<p>The book takes its title from a quote by Christian philosopher Dallas Willard:</p>

<blockquote>
    <p>Hurry is the great enemy of spiritual life in our day. You must ruthlessly eliminate hurry from your life.</p>
</blockquote>

<p>In the book, Comer argues that hurry is incompatible with love (p.&nbsp;23), patience (p.&nbsp;23), joy (p.&nbsp;25), peace (p.&nbsp;25), wisdom (p.&nbsp;52), and gratitude (p.&nbsp;52). Hurry is a sign that we aren’t accepting our God-given limitations (p.&nbsp;65), and hurrying causes irritability, restlessness, distraction, and isolation (pp.&nbsp;27,&nbsp;58,&nbsp;89).</p>

<p>The base state of man before the modern era, Comer argues, was unhurried (pp.&nbsp;42–45). God rests in Genesis&nbsp;1, and Jesus himself never hurried. The cure to our modern malaise, thus, is to embrace a life marked by slowness, solitude, simplicity, prayer, and rest. Comer advocates taking a literal 24-hour sabbath, rejecting consumerism, and trying to find a “perpetual Zen-like state of Jesus-derived joy and peace” (p.&nbsp;251).</p>

<p>Much of what Comer says is good. But the central argument of his piece, that hurrying should be eliminated, doesn’t seem defensible to me. Comer makes very few direct arguments that hurrying itself is bad, instead using hurrying as a metonym for a vaguely defined bucket of negative modern traits: isolation, anxiety, fear, and so on.</p>

<p>This is a classic example of a motte-and-bailey argument, popularized by Scott Alexander on <em>Slate Star Codex</em>. In his post <a href="https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/">“All In All, Another Brick In The Motte,”</a> he defines a motte-and-bailey argument thusly:</p>

<blockquote>
    <p>So the motte-and-bailey doctrine is when you make a bold, controversial statement. Then when somebody challenges you, you retreat to an obvious, uncontroversial statement, and say that was what you meant all along, so you’re clearly right and they’re silly for challenging you. Then when the argument is over you go back to making the bold, controversial statement.</p>
</blockquote>

<p>Comer does exactly this. “You must ruthlessly eliminate hurrying” is a bold, controversial statement—but the statement he actually defends is more like “anxiety and isolation are bad,” which doesn’t have quite the same transformative implications for modern Christian living.</p>

<p>I’ll go a step further and try to defend the assertion that hurrying can be good, actually. Here’s Genesis&nbsp;18:1–8&nbsp;(ESV), when Abraham receives God at Mamre (emphasis added):</p>

<blockquote>
    <p>And the Lord appeared to Abraham by the oaks of Mamre, as he sat at the door of his tent in the heat of the day. He lifted up his eyes and looked, and behold, three men were standing in front of him. When he saw them, <strong>he ran from the tent door to meet them</strong> and bowed himself to the earth and said, “O Lord, if I have found favor in your sight, do not pass by your servant. Let a little water be brought, and wash your feet, and rest yourselves under the tree, while I bring a morsel of bread, that you may refresh yourselves, and after that you may pass on—since you have come to your servant.” So they said, “Do as you have said.” <strong>And Abraham went quickly into the tent</strong> to Sarah and said, “<strong>Quick</strong>! Three seahs of fine flour! Knead it, and make cakes.” <strong>And Abraham ran to the herd</strong> and took a calf, tender and good, and gave it to a young man, <strong>who prepared it quickly.</strong> Then he took curds and milk and the calf that he had prepared, and set it before them. And he stood by them under the tree while they ate.</p>
</blockquote>

<p>In the above passage, Abraham hurries and tells others to hurry. I think it’s pretty clear from context that Abraham’s behavior here is correct hospitality, not sinful, since God immediately blesses Abraham (v.&nbsp;10) and says he’s been chosen ”to keep the way of the Lord by doing righteousness and justice” (v.&nbsp;19).</p>

<p>Here’s a few other passages defending the practice of hurrying, which I will summarize for brevity’s sake:</p>

<ul>
    <li>Lot is commanded by angels to flee in a hurry from Sodom (Genesis&nbsp;19).</li>
    <li>In Exodus&nbsp;12, the Passover meal is to be eaten in haste.</li>
    <li>After Ziklag is raided and taken captive by the Amalekites, David and his soldiers pursue them until a third of his army is too tired to continue (1&nbsp;Samuel&nbsp;30).</li>
    <li>In 2&nbsp;Kings&nbsp;4, Elisha commands his servant Gehazi to “run at once” to meet the Shunammite woman.</li>
    <li>After Mary receives the revelation from Gabriel, she “went with haste” to Elizabeth (Luke&nbsp;1:39).</li>
    <li>In Luke&nbsp;15, the father of the prodigal son runs out of the house, and then commands his servants to “bring quickly” the best robe.</li>
    <li>In Acts&nbsp;8, Philip runs to the Ethiopian eunuch in his chariot, following the direction of the Holy Spirit.</li>
    <li>In 2&nbsp;Timothy, Paul twice urges Timothy to come and join him “soon” and “before winter” (vv.&nbsp;9,&nbsp;21).</li>
</ul>

<p>These points may seem obvious—clearly, if Amalekite raiders carried off your family, you would hurry after them! But even a trivial example like this demonstrates that hurrying is not intrinsically opposed to the will of God.</p>

<p>Comer also argues that Jesus himself never hurried (p.&nbsp;92). This is debatable—Mark uses the word “immediately” to describe many of Jesus’s actions, but that may reflect a Markean narrative style more than the actual pace of actions. Jesus himself commands Zaccheus to hurry (Luke&nbsp;19:5), and his anger cleansing the temple and agony at Gethsemane should be sufficient to dispel the idea that Jesus perpetually existed in a “Zen-like” (p.&nbsp;251) stoic state. Stress is not incompatible with sanctification.</p>

<p>Comer further argues that we’re called to walk with God, not run with him (p.&nbsp;23). This is quite literally false! In 1&nbsp;Corinthians&nbsp;9:25–27, Paul uses the metaphor of running to convey the discipline, self-control, and perseverance required for mature Christian living:</p>

<blockquote>
    <p>Do you not know that in a race all the runners run, but only one receives the prize? So run that you may obtain it. Every athlete exercises self-control in all things. They do it to receive a perishable wreath, but we an imperishable. So I do not run aimlessly; I do not box as one beating the air. But I discipline my body and keep it under control, lest after preaching to others I myself should be disqualified.</p>
</blockquote>

<p>Equating maturity with a restful, non-stressed life sets Christians up for disappointment. Hebrews&nbsp;11 specifically highlights the heroes “of whom the world was not worthy” who “went about in skins of sheep and goats, destitute, afflicted, mistreated” (vv.&nbsp;37–38). A stressful life doesn’t mean, as Comer argues, that “something is out of whack” (p.&nbsp;85). God’s rest will come, but it might not come today.</p>

<p>The conclusion here is not that we should hurry more. Most people hurry for bad reasons, stuck chasing selfish desires or climbing social ladders in pursuit of an elusive fulfillment. But the call of Christianity is not to abnegate these bad desires but to align them with God’s will. As Rene Girard says in <em>I Saw Satan Fall Like Lightning</em> (p.&nbsp;13, emphasis original):</p>

<blockquote>
    <p>What Jesus invites us to imitate is his own <em>desire</em>, the spirit that directs towards the goal on which his intention is fixed: to resemble God the Father as much as possible.</p>
</blockquote>

<p>What we are willing to hurry for reflects what we care about. We see this reflected in the passages above: Abraham hurries to show hospitality and welcome God into his life, David hurries to save the captives, and Mary&nbsp;&amp;&nbsp;Philip hurry to share the good news of the gospel. We should care enough about these things that we’re willing to hurry for them.</p>

<p>Comer’s call to a life of virtue, peace, and rest is excellent—and at the margin, he’s probably right that most people should hurry less in their lives. But the central claim of the book is just not correct. The vision of Christian maturity contained in <em>The Ruthless Elimination of Hurry</em> seems a little too close to California-style Zen Buddhism and other modern mystical practices to fully align with Scripture, and I think this is bad.</p>

<p><em>Thanks to Taylor Wagen, Tony Robinson, Elias Mann, Jonathon Vandezande, and Chloe Wagen for helpful discussions, and for Ari Wagen for originally pointing me to read Girard.</em></p>

]]></description>
              <pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Fundamental Wisdom of the Middle Way</title>
              <link>public/blog/20250512_middle_way.html</link>
              <description><![CDATA[
<br>
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250512_nagarjuna.jpg" style="width:400px;" />
</figure>
<br>

<p>In scientific computation, where I principally reside these days, there’s a cultural and philosophical divide between physics-based and machine-learning-based approaches.</p>

<p>Physics-based approaches to scientific problems typically start with defining a simplified model or theory for the system. For instance, a scientist attempting to simulate protein dynamics might start by defining how the energy of a system depends on its coordinates by choosing (or creating) a forcefield. Once a model’s been selected, numerical simulations are run using the model and the output of these simulations is analyzed to produce a prediction.</p>

<p>Physical simulations like these are extensively used today in climate modeling, orbital mechanics, computational fluid dynamics, and computational chemistry. These approaches are typically robust and relatively easy to interpret, since the underlying theory is perfectly known. Still, it’s often possible to get very complex behavior from a small set of underlying rules. Application of Newtonian gravitation, for instance, gives rise to the chaotic “three-body problem.” This combination of simple physical rules and complex emergent behavior is what makes physical simulations so powerful: starting from a well-understood model, it’s possible to gain non-trivial insights about complex systems.</p>

<p>ML-based approaches to scientific computation turn all of this upside down. Many ML models directly map input to output without any intermediate subtasks, making it difficult to interpret what’s going on. While physics-based methods can often extrapolate from simple test cases to complex emergent behaviors, ML-based methods frequently struggle to predict behaviors outside of what they’ve seen before. Still, ML-based methods are often far more efficient than physical methods and, since they’re not constrained by a specific theoretical framework, can handle complex phenomena where the underlying functional form is not known.</p>

<p>When these two subcultures collide, things often get nasty. Proponents of physics-based modeling frequently allege that ML is fundamentally unscientific, since science is all about extrapolation, and that the unreliability and interpretability of machine learning makes it ill-suited for anything except hacking benchmarks.</p>

<p>On the other side of the aisle, machine-learning advocates claim that physics-based modeling is basically a way for scientists to feel smart and look at pretty pictures and will never be able to capture sufficient complexity to be useful. If you think I’m exaggerating, I’m not—<a href=https://endpts.com/endpoints-slack-interview-dukes-chatterjee-on-ai-bio-nih-cuts/>here’s Pranam Chatterjee</a> discussing why structure-based models like AlphaFold are “irrelevant” to disease (emphasis added):</p>

<blockquote>…if we believe Anfinsen’s hypothesis: the sequence should encode everything else, including structure. Why do you need to look at the structure of the protein to understand it? <b>I have concluded people just feel more satisfied looking at a protein, rather than trusting an implicit language model to capture its features.</b> Hence the Nobel Prize.</blockquote>

<p><i>Edit: <a href=https://x.com/pranamanam/status/1922032593978945636>Pranam clarified on X that his objections are limited to AlphaFold, RFDiffusion, etc and don't apply to MD.</a></i></p>

<p>For Pranam, the physical assumptions made by models like AlphaFold—that proteins have a three-dimensional structure that’s relevant to their biology—mean that these models are incapable of describing the complexity of reality. Contra Pranam, the point that I want to make in this piece is that there’s no reason why physics- and ML-based approaches should be so opposed to one another. In fact, I’m becoming convinced that the future looks like some hybrid of both approaches.</p>

<p>Why? One of the big limitations of physics-based methods, at least in drug discovery, is that the underlying models often aren’t expressive enough and can’t easily be made more expressive. Molecular forcefields have only a handful of terms, which means that they can’t easily represent coupled dihedral rotation, π–π stacking, and so on—let alone bond-breaking or bond-forming processes—but even optimizing empirical parameters for all these terms quickly becomes a laborious and hard-to-scale process. In contrast, ML-based methods can scale to millions of empirical parameters or beyond without becoming intractably complex or overfit.</p>

<p>On the other hand, the big advantage of physics-based approaches is their generalizability—even simple rulesets, <a href=https://writings.stephenwolfram.com/2022/05/the-making-of-a-new-kind-of-science/>like cellular automata</a>, can lead to immensely complex emergent behavior. This sort of extrapolation is rarely seen in scientific ML projects. <a href=https://patwalters.github.io/Why-Dont-Machine-Learning-Models-Extrapolate/>A recent blog post from Pat Walters</a> makes the observation that most cheminformatic models don’t seem capable of extrapolating outside the dynamic range of their training data. This is surprising, since even the simplest of “physical” models (like a linear equation) are capable of this extrapolation.</p>

<p>If a hybrid approach is to work, it must be able to capture the strengths of both methods. One of the simplest ways to do this is an energy-based approach, where scientists don’t predict an output parameter directly but instead learn an energy function which can be used to indirectly predict an output. (<a href=https://arxiv.org/abs/2306.02572>Yann LeCun has advocated for this.</a>) This adds some test-time complexity, since additional computation is needed after inference to generate the final output, but also constrains the model to obey certain physical laws. (If you want to predict forces, for instance, it’s often best to do so through differentiating the predictions of an energy model, since <a href=https://arxiv.org/abs/2412.11569>doing this guarantees that the outputs will be a valid gradient field.</a>)</p>

<p>Physics-informed ML approaches can also resolve the interpolation/extrapolation problem. Since physical equations are well-suited to extrapolation, applying a physical model to the output of an ML layer can convert interpolated intermediate states to extrapolated final results. This sort of extrapolation is well-documented in neural network potentials—for instance, <a href=https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a>ANI-1 was trained only on molecules with 8 or fewer heavy atoms</a> but proved capable of extrapolating to significantly larger molecules with good accuracy.</p>

<p>One fair criticism of these hybrid physics–ML approaches is that they often require much more test-time compute than pure-ML methods: optimizing a geometry with a neural network potential can require hundreds of intermediate gradient calculations, while in theory an ML method could predict the correct geometry with a single step. But in practice I think this is an advantage. Scaling test-time compute has been an incredible boon for the LLM space, and chain-of-thought reasoning models like OpenAI’s o3 and DeepSeek-r1 are now the gold standard. To the extent that physics-informed ML methods give us the ability to spend more compute to get better answers, I suspect this will mostly be good for the field.</p>


<p>At Rowan, we’ve recently published a few papers in this area—Starling, an energy-based model for predicting microstate distributions and macroscopic pKa, and Egret-1, a family of neural network potentials for bioorganic simulation—and I expect we’ll keep working on hybrid physics–ML approaches in the future. Others seem to be moving in this direction too. The recent Boltz-1x model (<a href=https://rowansci.substack.com/p/proteinligand-co-folding>now on Rowan!</a>) from Gabri Corso and co-workers incorporates inference-time steering for superior physical accuracy, and <a href=https://achira.ai/>Achira’s</a> written about the merits of “a third way” in simulation built around “harmonization of the theoretical and the pragmatic.”</p>

<p>There’s immense room for creativity at the intersection of physics and machine learning. Virtually any part of a complex physics-based workflow can potentially be replaced with a bespoke ML model, leading to almost limitless combinatorial possibilities for experimentation. Which combinations of models, algorithms, and architectures will prove to be dead-ends, and which will unlock order-of-magnitude improvements in performance and accuracy? It’s a fun time to be working in this field.</p>

<h3>Appendix:</h3>

<p><i>I asked my co-founder Eli what he thought of this piece. He pointed out that the amount of physics one should incorporate into an ML model depends on the amount of data and the dimensionality of the problem. I asked him to explain more, and here’s what he wrote:</i></p>

<blockquote>In ML terminology, incorporating information about the modality on which you are predicting into the design of the model is called adding inductive bias. One example of this for atomistic simulation is enforcing Euclidean equivariance into a model. This explicitly ensures that any spatial transformation to a model’s input is reflected in its output. This does ensure that physical laws are followed but also increases the computational complexity of the model, limiting system size at inference time and inference speed.
  <br><br>
  Some neural network potentials like <a href=https://arxiv.org/abs/2206.11990>Equiformer</a> enforce equivariance in every operation within the model to ensure physical laws are always followed, while <a href=https://arxiv.org/abs/2305.19302>others enforce it only for inputs and outputs</a>, relaxing constraints for intermediate model operations. Models like <a href=https://arxiv.org/abs/2504.06231>Orb-v3</a> don’t enforce equivariance at all, but incorporate dataset augmentation and unsupervised pre-training techniques to improve sample efficiency and learn physical laws.
  <br><br>
  As dataset size and diversity increase, we may see less of a need for physical inductive bias in models. One place we’ve seen this is with convolutional neural networks (CNNs) and vision transformers (ViTs) in computer vision: CNNs have built-in translational invariance whereas ViTs do not include any spatial inductive bias but are theoretically more flexible. We see ViTs outperforming CNNs <a href=https://arxiv.org/abs/2010.11929>only as dataset size and model size pass a certain threshold</a>. If we assume that a similar pattern will hold for atomistic simulation, we might expect that the dataset size at which this threshold occurs will be greater, as molecular simulation has more degrees of freedom than computer vision.</blockquote>

<p><i>I think this is a good point—thanks Eli! More abstractly, we might expect that incorporating explicit physics will help more in the low-data regime, while pure ML approaches will be more effective as data and compute approach infinity.</i></p>


<p><i>Thanks to Ari Wagen, Jonathon Vandezande, and Eli Mann for reading drafts of this post.</i></p>
]]></description>
              <pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>To His Defense-Tech Portco</title>
              <link>public/blog/20250405_to_his_defense_tech_portco.html</link>
              <description><![CDATA[
<br>
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250405_anduril.png" style="width:600px;" />
</figure>

<br>
<p>With apologies to Andrew Marvell. If you haven’t read <a href=https://www.poetryfoundation.org/poems/44688/to-his-coy-mistress>“To His Coy Mistress”</a>, I fear this won’t make much sense.</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Had we but funding enough and time,<br>
This coyness, founder, were no crime.<br>
We would sit down, and think which way<br>
To build, and pass our slow run-rate’s day.<br>
Thou by the Potomac’s side<br>
Shouldst SBIRs find; I by the tide<br>
Of El Segundo would complain. I may<br>
Fund you ten years before the first customers pay,<br>
And you should, if you please, refuse<br>
Till the market supports dual-use.<br>
Your annual revenue should grow<br>
Vaster than Boeing’s and more slow;<br>
An hundred years should go to build<br>
Thy initial demo, until the warehouse filled;<br>
Two hundred years for full refactoring,<br>
But thirty thousand for manufacturing;<br>
An age at least for every ambition,<br>
And the last age to show your full vision.<br>
For, founder, you deserve funding most,<br>
Nor would I invest at a lower post.
</p>

<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But at my back I always hear<br>
Palmer’s wingèd chariot hurrying near;<br>
And yonder all before us lie<br>
Investors dreaming only of exits and AI.<br>
Thy product shall no more be found;<br>
Nor, in thy bankrupt state, shall sound<br>
My echoing retweets; then competition shall try<br>
That long-preserved TAM and KPI,<br>
And your quaint slide deck fall to pieces,<br>
And into ashes all my investment theses;<br>
Raytheon’s a fine and funded prime,<br>
But none, I think, work there overtime.
</p>

<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now therefore, while the youthful hue<br>
Sits on thy skin like morning dew,<br>
And while thy willing employees perspire<br>
At every hour of night with resolute fire,<br>
Now let us push to master while we may,<br>
And now, like YC back in the day<br>
All at once our runway devour<br>
Than languish awaiting pricing power.<br>
Let us roll all our strength and all<br>
Our funding up into one ball,<br>
And push our MVP with rough excess<br>
Through the government procurement process:<br>
Thus, though we cannot make the buyer<br>
Stand still, yet we will make Anduril acqu-hire.
</p>

]]></description>
              <pubDate>Sat, 05 Apr 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Democratizing Computer-Assisted Drug Design: How Are We Doing?</title>
              <link>public/blog/20250305_democratizing_cadd.html</link>
              <description><![CDATA[
<p>In 2007, John Van Drie wrote <a href=https://link.springer.com/article/10.1007/s10822-007-9142-y>a perspective</a> on what the next two decades of progress in computer-assisted drug design (CADD) might entail. <a href="https://medchemash.substack.com/p/drug-discovery-and-modeling-the-view">Ash Jogalekar recently looked back</a> at this list, and rated the progress towards each of Van Drie’s goals on a scale from one to ten. There’s a lot in Jogalekar’s piece that’s interesting and worth discussing, but I was particularly intrigued by the sixth item on the list (emphasis added):</p>

<blockquote>
<p><strong>Outlook 6: today’s sophisticated CADD tools only in the hands of experts will be on the desktops of medicinal chemists tomorrow. The technology will disperse</strong></p>

<p>Twenty-five years ago, modelers worked with million-dollar room-sized computers with 3D display systems half the size of a refrigerator. Today, the computer which sits on my lap is far more powerful, both in computation speed and in 3D display capabilities. Twenty-five years ago, the software running on those computers was arcane, with incomprehensible user interfaces; much of the function of modelers in those days was to serve as a user-friendly interface to that software, and their assistance was often duly noted in manuscripts, if not as a co-author then as a footnote. Today, scientists of all backgrounds routinely festoon their publications with the output of molecular graphics software, running on their desktop/laptop machines with slick easy-to-use graphical user interfaces, e.g. Pymol.</p>

  <p>This is a trend that will accelerate. Things that seem sophisticated and difficult-to-use, but are truly useful, will in 20 years be routinely available on desktop/laptop machines (and even laptops may be displaced by palmtops, multi-functional cellphones, etc.). <b>Too many modelers are still in the business of being ‘docking slaves’ for their experimental collaborators (i.e. the experimentalist asks the modeler ‘please dock my new idea for a molecule’, and waits for the result to see if it confirms their design); this will ultimately disappear, as that type of routine task will be handled by more sophisticated user interfaces to current docking algorithms, </b>e.g. the software from Molsoft is well on its way to fill such a role. Whereas the ‘information retrieval specialists’ that once populated corporate libraries have disappeared, replaced by desktop Google searches, this trend of modeling-to-the-desktop should not be a source of job insecurity for CADD scientists—this will free us up from the routine ‘docking slave’ tasks to focus our energies on higher-valued-added work. <b>As a rule, things today that seem finicky and fiddly to use (e.g. de novo design software), or things that take large amount of computer resources (e.g. thermodynamic calculations, or a docking run on the full corporate database) are things that one can easily imagine will in the future sit on the desktops of chemists, used by them with minimal intervention by CADD scientists</b></p>
</blockquote>

<p>Jogalekar gives the field a 6/10 on this goal, which I find optimistic. In his words:</p>

<blockquote>
<p>From tools like Schrödinger’s Live Design to ChemAxon’s Design Hub, medicinal chemists now use more computational tools than they ever did. Of course, these tools are used in fundamental part because the science has gotten better, leading to better cultural adoption, but the rapidly dwindling cost of both software and hardware enabled the cloud has played a huge rule in making virtual screening and other CADD tools accessible to medicinal chemists.</p>
</blockquote>

<p>It’s true that there are more computational tools available to non-computational scientists than there once were—but based on the conversations we’ve had with industry scientists (which also informed <a href=https://rowansci.com/publications/quantum-chemistry-in-drug-discovery>this piece</a>), the role of computational chemists as “docking slaves” (Van Drie’s phrase, not mine) to their experimental colleagues still rings true. The number of experimental scientists able to also run non-trivial computational studies remains vanishingly low, despite the improvements in computing hardware and software that Van Drie and Jogalekar discussed.</p>

<p>Why hasn’t our field made more progress here? In my view, there are three principal reasons: immature scientific tools demand expert supervision, poorly designed technology deters casual usage, and cultural inertia slows adoption even further.</p>

<h2>1. Immature Scientific Tools Demand Expert Supervision</h2>

<p>Most scientific tools optimize for performance and tunability, not robustness or ease of use. Quantum chemistry software forces users to independently select a density functional, a basis set, any empirical corrections, and (for the brave) allows them to tune dozens of additional parameters with obscure and poorly documented meanings. (“Oh, the default settings for transition states aren’t very good… you need to configure the initial Hessian guess, the integral tolerance, the optimizer step size, and a few other things… I’ll email you a couple scripts.”)</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_errors.png" style="width:450px;" />
<figcaption>Settings like these are difficult to understand.</figcaption>
</figure>

<p>And these issues aren’t unique to quantum chemistry; virtually every area of scientific simulation or modeling has its own highly specialized set of tools, customs, and tricks, so switching fields even as a PhD-level computational chemist is challenging and treacherous. Some of this complexity is inherent to the subject matter—there are lots of unsolved computational problems out there for which no simple solution is yet known. For instance, handing changes in ionization state or tautomerization during free-energy-perturbation (FEP) simulations is (to my knowledge) just intrinsically difficult right now, and no robust solution exists that can be plainly put into code.</p>

<p>But better hardware and better methods can alleviate these issues. Searching through different conformers of a complex molecule used to be a challenging task that demanded chemical expertise and considerable software skill—now, metadynamics programs like <a href=https://crest-lab.github.io/crest-docs/>CREST</a> make it possible to run conformer searches simply from a set of starting coordinates. These new “mindless” methods are less efficient than the old methods that relied on chemical intuition, but in many cases the simulations are fast enough that we no longer care.</p>

<p>Similarly, the increasing speed of quantum chemistry makes it simpler to run high-accuracy simulations without extensive sanity checks. <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c06688>In my PhD research</a>, I carefully benchmarked different tiny basis sets against high-level coupled cluster calculations to find a method that was fast enough to let me study the reaction dynamics of a catalytic transition state—now, methods like r<sup>2</sup>SCAN-3c give better accuracy in virtually every case and avoid the dangerous basis-set pathologies I used to worry about, making it possible to use them as a sane default for virtually every project.</p>

<p>Other fields have undergone similar transformations. Writing assembly code, when done right, produces substantially faster and more efficient programs than writing a compiled language like C, and writing C produces faster code than writing a high-level language like Python. But computers are fast enough now that writing assembly code is now uncommon. Python is much more forgiving, and makes it possible for all sorts of non-experts (like me) to write useful code that addresses their problems. Back in the days of the PDP-10, every FLOP was precious—but with today’s computers, it’s worth accepting some degree of inefficiency to make our tools quicker to learn, easier to use, and far more robust.</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_assembly.png" style="width:450px;" />
<figcaption>Image from <a href=https://thechipletter.substack.com/p/the-unnecessary-obscurity-of-assembly>The Chip Letter</a>.
</figure>


<p>Computational chemistry needs to make the same transition. There will always be cutting-edge computational problems that demand specific expertise, and these problems will invariably remain the rightful domain of experts. But vast improvements in the speed and accuracy of computational chemistry promise to move more and more problems into a post-scarcity regime where maximum efficiency is no longer required and the field’s impact will no longer predominately be determined by performance.</p>

<p>Once a method becomes robust enough to be routinely used without requiring expert supervision, it’s safe to turn over to the non-experts. I’d argue that this is true of a decent proportion of computational workflows today, and advances in simulation and machine learning promise to make this true for a much greater proportion in the next decade.</p>

<h2>2. Poorly Designed Technology Keeps People Out</h2>

<p>Sadly, scientific considerations aren’t all that prevents molecular modeling from being more widely employed. The second underlying reason limiting the reach of computational tools is that most of the tools are, frankly, just not very good software. Scientific software frequently requires users to find and manage their own compute, write scripts to parse their output files and extract the data, and do plenty of needless work in post-processing—in many respects, being a computational chemist means stepping back in time to 1970s-era software.</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_ibm.png" style="width:450px;" />
</figure>

<p>These difficulties are considerable even for full-time computational chemists; for experimental scientists without coding experience, they’re insurmountable. No medicinal chemist should need to understand <code>rsync</code>, <code>sed</code>, or <code>malloc</code> to do their job! Some of the error messages from computational chemistry software are so obtuse that there are <a href="https://docs.alliancecan.ca/wiki/Gaussian_error_messages">entire web pages</a> devoted to decrypting them:</p>

<pre class=code-block>RFO could not converge Lambda in  999 iterations.
 Linear search skipped for unknown reason.
 Error termination via Lnk1e in /disc30/g98/l103.exe.
 Job cpu time:  0 days  7 hours  9 minutes 17.0 seconds.
 File lengths (MBytes):  RWF=   21 Int=    0 D2E=    0 Chk=    6 Scr=    1
</pre>

<p>Why is so much scientific software so bad? Academic software development prioritizes complexity and proof-of-concepts because these are the features that lead to publications. More prosaic considerations like robustness, maintainability, and ease of use are secondary considerations at best, and it’s hard for academic research groups to attract or maintain the sort of engineering talent required for most impactful work in scientific software. In <a href=https://newscience.org/how-software-in-the-life-sciences-actually-works-and-doesnt-work/?ref=rafah.site>a piece for <em>New Science</em></a>, Elliot Hirshberg documents the consequences of this situation (emphasis added):</p>

<blockquote>
<p>…most life sciences software development happens in academic labs. These labs are led by principal investigators who spend a considerable portion of their effort applying for competitive grants, and the rest of their time teaching and supervising their trainees who carry out the actual research and engineering. Because software development is structured and funded in the same way as basic science, citable peer-reviewed publications are the research outputs that are primarily recognized and rewarded. <strong>Operating within this framework, methods developers primarily work on building new standalone tools and writing papers about them, rather than maintaining tools or contributing to existing projects….</strong></p>

<p>This organizational structure for developing methods and software has resulted in a tsunami of unusable tools…. <strong>Scientists need to learn how to download and install a large number of <a href="https://hgdownload.soe.ucsc.edu/admin/exe/">executable programs</a>, battle with <a href="https://xkcd.com/1987/">Python environments</a>, and even compile C <a href="http://www.htslib.org/download/">programs</a> on their local machine if they want to do anything with their data at all.</strong> <strong>This makes scientists new to programming throw up their hands in confusion, and seasoned programmers tear their hair out with frustration.</strong> There is a reason why there is a long-running joke that half of the challenge of bioinformatics is installing software tools correctly, and the rest is just converting between different <a href="http://genome.ucsc.edu/FAQ/FAQformat.html">file formats</a>.</p>
</blockquote>

<p>Frustratingly, relatively few academic scientists seem to view this as a problem. In a thread discussing the lack of graphical user interfaces (GUIs) for scientific software on the Matter Modeling Stack Exchange, <a href="https://mattermodeling.stackexchange.com/a/7089">a user writes</a> about how GUIs are not just a distraction but actively harmful for scientific software (emphasis added):</p>

<blockquote>
<p>[GUI development takes time] that could be spent on other tasks, like developing more functionality in the core program, developing different programs for different tasks, or even doing other things like lab research that has clearer advantages for one’s career… <strong>But then, after the GUI has been designed and created, it’s a new source of maintenance burden. That means a program with a GUI will have to have time dedicated to fixing GUI issues for users, especially if an OS (or other system library) update breaks it. That’s time that could be spent on other things more productive to one’s career or research aspirations.</strong></p>
</blockquote>

<p>This is a textbook case of misaligned incentives. Researchers who create scientific software aren’t rewarded for making it easy for others to build on or use, only for making it increasingly powerful and complex—as a result, there are hundreds of complex and impossible-to-use scientific software packages floating around on Github. Almost all the scientific software projects which defy this trend are commercial or supported by commercial entities: at least from the users’ point of view, the incentives of a for-profit company seem superior to academic incentives here.</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_typedfemale.png" style="width:500px;" />
  <figcaption><a href=https://x.com/typedfemale/status/1871704190910943440>Original post from @typedfemale</a></figcaption>
</figure>

<p>Better tools are the solution to <a href=https://slatestarcodex.com/2017/11/09/ars-longa-vita-brevis/>the ever-increasing scientific burden of knowledge</a>. Every day, experimental scientists use tools without fully understanding their internal workings—how many chemists today could build a mass spectrometer from scratch, or an HPLC? We accept that experimental tools can be productively used by non-experts who don’t understand their every detail—but when it comes to computational chemistry, we expect every practitioner to build their own toolkit practically from scratch.</p>

<p>This has to change. If we want scientific software to be more widely used, our field needs to find a way to make software that’s as elegant and user-friendly as the software that comes out of Silicon Valley. This can happen through any number of different avenues—improved academic incentives, increased commercial attention, and so on—but without this change, large-scale democratization of simulation will never be possible.</p>

<h2>3. Cultural Inertia Slows Adoption</h2>

<p>But even with robust methods and well-designed software products, cultural differences between computational and experimental scientists persist. Generations of PhD students have been taught that they’re either “computational” or “experimental,” with the attendant stereotypes and communication barriers that accompany all such dichotomies. In industry, scientists are hired and promoted within a given skillset; while scientists occasionally hop from experiment to computation, it’s rare to meet truly interdisciplinary scientists capable of contributing original research insights in both areas.</p>

<p>Many scientists, both computational and experimental, are happy with this state-of-the-art. Experimental scientists can avoid having to learn a set of confusing skills and delegate them to a colleague, while maintaining a comfortable skepticism of any computational predictions. Computational scientists, in contrast, get to serve as “wizards” who summon insights from the Platonic realm of the computer.</p>

<p>Some computational scientists even come to take pride in their ability to navigate a confusing web of scripts, tools, and interfaces—it becomes their craft, and a culture to pass along to the next generation. On Stack Exchange, <a href="https://mattermodeling.stackexchange.com/a/5010">one professor writes</a> in response to a beginner asking about graphical user interfaces:</p>

<blockquote>
<p>Trust me: it is better to learn the command line… I began using UNIX when I was 9 years old. It’s time for you to learn it too.</p>
</blockquote>

<p>As Abhishaike Mahajan put in <a href="https://owlpostingshop.com/products/rowan">his poster about Rowan</a>—“enough”! It doesn’t have to be this way.</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_rowan.png" style="width:450px;" />
</figure>

<p>Why care about democratizing simulation? We think that putting simulation into the hands of every scientist will enable innovation across the chemical sciences. As of 2025, it seems clear that computation, simulation, and ML will play a big role in the future of drug discovery. But as long as “computation” remains a siloed skillset distinct from the broader activity of drug discovery, the impact that these breakthroughs can have will remain limited by cultural and organizational factors.</p>

<p>If the importance of computer-assisted drug discovery continues to increase but the tools remain unusable by the masses, will computational chemists and biologists simply grow in importance more and more? Taken to the extreme, one can envision what Alice Maz terms <a href="https://www.alicemaz.com/writing/priesthood.html">“a priesthood of programmers,”</a> a powerful caste dedicated to interceding between man and computer. Perhaps computational tools will remain inaccessible forever, and those who excel at drug discovery will be those who can best deploy a litany of arcane scripts. Perhaps the future of chemistry will be run by CS majors, and today’s drug hunters will merely be employed to synthesize compounds and run biological assays in service of the new elite.</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20250305_priesthood.jpg" style="width:450px;" />
<figcaption><i>Scholar by a Waterfall</i>, Ma Yuan. Image from Alice Maz's aforementioned post <a href=https://www.alicemaz.com/writing/priesthood.html>“A Priesthood of Programmers”</a>.
</figure>

<p>But one can envision a future in which computational chemistry becomes a tool to aid drug designers, not supplant them. In 2012, Mark Murcko and Pat Walters (distinguished industry scientists both) wrote <a href=https://pmc.ncbi.nlm.nih.gov/articles/PMC3268970/>“Alpha Shock,”</a> a speculative short story about drug discovery in the year 2037. I want to highlight a scene in which Sanjay (the protagonist) uses structure-based drug design to discover a new candidate and avoid paying his rival Dmitri royalties:</p>

<blockquote>
  <p>With the structures and custom function in hand, Sanjay was ready to initiate the docking study. But despite recent advances in the TIP32P** water model, Sanjay still didn’t completely trust the predicted protein-ligand binding energetics. Next, he transferred the experimental data into the Google Predictive Analytics engine and quickly designed a new empirical function to fit the experimental data. <strong>Now he launched the dynamic docking simulator, dropping the empirical function into the hopper... A progress bar appeared in front of him showing “10^30 molecules remaining, 2,704 h 15 min to completion.”</strong> Sanjay quickly stopped the process and constrained the search to only those molecules that fell within the applicability domain of his empirical function. This reduced the search to 10^12 molecules and allowed the analysis to complete in a few minutes.</p>

  <p>After a bit of visual inspection to confirm the results of his docking study, Sanjay moved on to the next step. <strong>He knew that slow binding kinetics could provide a means of lowering the dose for his compound. To check this, he ran a few seconds of real-time MD on each of the top 50,000 hits from the docking study. A quick scan of the results turned up 620 structures that appeared to have the required residence time.</strong> Sanjay submitted all these structures to PPKPDS, the Primate Pharmacokinetic and Pharmacodynamic Simulator, a project developed through a collaboration of industry, academia, and the World Drug Approval Agency. Of the compounds submitted, 52 appeared to have the necessary PK profile, including the ability to be actively transported into the brain. All but a few were predicted to be readily synthesizable.</p>
</blockquote>

<p>In “Alpha Shock,” a drug designer like Sanjay can leverage interactive, intuitive software to quickly test his hypotheses and move towards important conclusions. Sanjay’s tools serve to augment his own intuition and vastly increase his productivity, yet don’t require him to use bespoke scripts or memorize arcane incantations. To anyone with any experience with computer-assisted drug design, this will read like science fiction—but that is exactly the point. The world of “Alpha Shock” gives us a vision of where we need to go as a field, and highlights where we’re deficient today.</p>

<p>Better instrumentation and analytical tooling has revolutionized chemistry over the past sixty years, and better design &amp; simulation tools can do the same over the next sixty years. But as we’ve seen with NMR and mass spectrometry, enabling technologies must become commonplace tools usable by lots of people, not arcane techniques reserved for a rarefied caste of experts. Only when computational chemistry undergoes the same transition can we fulfill the vision that Van Drie outlined years ago—one in which every bench scientist can employ the predictive tools once reserved for specialists, and in which computers can amplify the ingenuity of expert drug designers instead of attempting to supplant it.</p>

<i>Thanks to Ari Wagen for feedback on drafts of this piece.</i>
]]></description>
              <pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>What Achilles Said to the Tortoise About Binding-Affinity Prediction</title>
              <link>public/blog/20250303_achilles_and_the_tortoise.html</link>
              <description><![CDATA[
<p>
This post is an attempt to capture some thoughts I have about ML models for predicting protein–ligand binding affinity, sequence- and structure-based approaches to protein modeling, and what the interplay between generative models and simulation may look like in the future. I have a lot of open questions about this space, and <a href=https://www.owlposting.com/p/a-socratic-dialogue-over-the-utility>Abhishaike Mahajan’s recent Socratic dialogue on DNA foundation models</a> made me curious to try the dialogue format here. 
</p>

<p>
(With apologies to <a href=https://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles>Lewis Carroll</a> and <a href=https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach>Douglas Hofstadter</a>.)
</p>

<br>
<br>
<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20250303_ribbon_face.jpg" style="width:450px;" />
</figure>

<p><em>[The TORTOISE is sitting on a park bench with a thermos of tea and a stack of papers beside him. Enter ACHILLES, holding a stack of papers.]</em></p>

<p><strong>ACHILLES:</strong> Hello, Mr. T. Mind if I join you on your bench?</p>

<p><strong>TORTOISE:</strong> Of course, Achilles. What are you reading on this fine spring day?</p>

<p><strong>ACHILLES:</strong> Right now, I’m reviewing some recent literature on the economics of seating in Mongolian yurts. And yourself?</p>

<p><strong>TORTOISE:</strong> I’m looking through two fascinating papers criticizing modern protein–ligand co-folding methods.</p>
<p><a href="https://www.biorxiv.org/content/10.1101/2024.06.03.597219v1">The first</a> is by Matthew Masters and co-workers and is entitled “Do Deep Learning Models for Co-Folding Learn the Physics of Protein–Ligand Interactions?” The authors show that AlphaFold 3 predicts the “correct” binding site for a variety of complexes even when the entire binding site is mutated to glycine, when bulky residues are added to fill the binding pocket, or when the polarity of key interactions is reversed. The authors argue that this demonstrates that AlphaFold is overfit to specific protein families, and that models need to be validated on “their compliance with physical and chemical principles.” 

<p><strong>ACHILLES:</strong> Interesting, but not surprising.</p>

<p><strong>TORTOISE:</strong> <a href=https://www.biorxiv.org/content/10.1101/2025.02.03.636309v2>The second</a> is by Peter Škrinjar and co-workers and is entitled “Have protein–ligand co-folding methods moved beyond memorization?” Here, the authors show that the success rate of co-folding methods is dictated by the similarity of structures to the training set. The models appear to perform well in cases where there is high train–test similarity, but on truly different structures their performance is dismal. The authors’ conclusion is even stronger than that of the first paper:</p>

<blockquote>Incorporating physics-based terms to more accurately model protein-ligand interactions, potentially from simulations, conformational ensembles, or other sources, are likely needed to achieve more exciting results in this field.</blockquote>

<p>Taken together, it’s clear that pure deep-learning-based approaches to solving these important scientific problems are doomed to fail.</p>

<p><strong>ACHILLES:</strong> Well, let’s not rush ahead too quickly—perhaps we’ve been spending too much time together. It’s not surprising that these structure-based methods are prone to overfitting, but I expect that the next generation of sequence-only methods will overcome these hurdles.</p>

<p><strong>TORTOISE:</strong> Hm, I admit this intuition leaves me in the dust. Can you enlighten me as to why your response to unphysical overfitting is to reject one of the only physical descriptors that we have—the 3D structures of the protein and the ligand? It seems to me that reducing the amount of available data is a peculiar way to improve the performance of one’s model.</p>

<p><strong>ACHILLES:</strong> Of course, I’m happy to explain. Consider the problem from first principles. It’s not surprising that using 3D structures leads to overfitting—the dimensionality of these problems is vast, and our datasets are comparatively miniscule. So any given set of coordinates is virtually a guaranteed fingerprint for a particular protein or ligand, and we’re just training models that have one-hot encoded the structures they’ve seen. See for instance the recent work of <a href="https://arxiv.org/abs/2412.02889">Jain, Cleves, and Walters</a> arguing that DiffDock is simply a fancy lookup table.</p>

<p><strong>TORTOISE:</strong> Of course I agree, which is why it’s important that we find ways to generate more training data, not jettison what little data we have. The problem is not intractable; it seems that DiffDock-L is superior at this task. We need only wait for another order-of-magnitude increase in the amount of training data available to arrive at a robust deep-learning-based docking method.</p>

<p><strong>ACHILLES:</strong> But, if you will, follow me a little further down this line of thinking. We know that protein–ligand structures are but a single snapshot of a dynamic ensemble of possibilities that interconvert smoothly in solution. This is why attempting to guess the binding affinity from a single pose is so futile, and why extensive sampling is needed for free-energy methods like FEP or TI.</p>

<p>Protein–ligand co-folding models must labor under the same constraints. Just because we’ve changed the scoring function from a forcefield to a neural network doesn’t mean that we can go back to considering a single averaged pose—let alone whatever pose happened to crystallize out of solution best. No, any method predicated on considering just a single pose is doomed to fail.</p>

<p><strong>TORTOISE:</strong> So your proposal is to disregard all poses, and hope that “machine learning” can just call the right answer from <a href="https://www.goodreads.com/quotes/362627-i-can-call-the-spirits-from-the-vasty-deep-hotspur">the vasty deeps</a>? I fear that you’ve been spending too much time on LinkedIn, my dear friend. Perhaps it’s time for you to return to a time before computing, like 5th-century Greece.</p>

<p><strong>ACHILLES:</strong> <em>Au contraire</em>, tortuga. We know that it’s possible to go from sequence to structure with machine learning, unless you’ve already forgotten about this year’s Nobel Prize. And others have shown you can generate structural ensembles this way—look at <a href="https://arxiv.org/abs/2402.04845">AlphaFlow</a>, or <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885v1">BioEmu</a>. One could imagine running these models to generate candidate structures, then feeding these structures into a docking model, then feeding the docked structures into a scoring model, then combining the scoring predictions to generate a single predicted binding affinity.</p>

<p><strong>TORTOISE:</strong> I agree in principle, provided each of these models can be benchmarked and verified to follow proper thermodynamic and statistical mechanical principles. But creating a perfect Boltzmann generator won’t be easy; and <a href="https://www.sciencedirect.com/science/article/pii/S0005273616300347#s0055">methods that do not reproduce the canonical ensemble lead to pathological failures in practice</a>.</p>

<p><strong>ACHILLES:</strong> Precisely! Many of these intermediate models are difficult to train, since we don’t have good ground truth for protein structural ensembles or individual binding affinities per pose. In fact, almost the only piece of data we can reliably acquire data for is the very task we want to predict—macroscopic protein–ligand binding affinity. So the entire problem becomes far more tractable if we simply combine the individual models into one end-to-end model so that we can backpropagate through the entire stack. Then we can scale to larger datasets that don’t have associated structural information, like DNA-encoded libraries or Terray’s microarray technology.</p>

<p>Thus, by combining the models into one, we at once simplify our task and make it possible to scale to much larger datasets: <em>e pluribus unum</em>.</p>

<p><strong>TORTOISE:</strong> A surprisingly plausible vision, but I’m still not convinced. (And you ought to be speaking Greek, not Latin.)</p>

<p>Partitioning this problem into multiple models, each of which performs a defined task, means that there are verifiable, low-dimensional intermediate states that can be inspected. Structural ensembles can be saved to PDB files, and individual binding affinities can be sanity-checked. When we dump everything together into one massive mega-model, who knows what the model will try to do? These low-dimensional checkpoints might even be critical for giving us the right inductive bias to prevent overfitting.</p>

<p>By way of comparison, consider LLMs—we use textual checkpointing all the time, from chain-of-thought to retrieval-augmented generation. “Just train a model to do the entire task in a single pass” sounds like the accelerationist, AI-informed position, but in reality interpretability and modularity have proven to be valuable levers across many fields of machine learning. Gleefully jettisoning them hardly seems prudent.</p>

<p><strong>ACHILLES:</strong> Perhaps. But forcing a model to go through a certain intermediate state only makes sense when that intermediate state is actually relevant to the task at hand. <a href="https://www.nature.com/articles/s41422-024-01010-6">How will structure-based methods handle intrinsically disordered proteins</a>?</p>

<p><strong>TORTOISE:</strong> Even disordered proteins must have a structure.</p>

<p><em>[Enter CRAB.]</em></p>

<p><strong>CRAB:</strong> Hullo, dear friends! Are we talking about ESM2? I fear that these methods are passé; if you haven’t heard yet, ascribing individual importance to mere proteins is an inadequate assumption now obsoleted by deep learning.</p>

<p><strong>ACHILLES:</strong> Whatever do you mean?</p>

<p><strong>CRAB:</strong> Exactly what I said! Proteins don’t exist in a vacuum—they possess different post-translational modifications, they aggregate, they float in and out of biomolecular condensates, and many of the most important cellular functions don’t even involve proteins.</p>

<p><strong>ACHILLES:</strong> You’re correct, of course, but it’s clear that proteins are one of the key structural and functional elements of the cell. How else do you explain the history of successful therapeutics that target specific proteins?</p>

<p><strong>CRAB:</strong> Selection bias, my dear friend. Of course the brute-force medicinal chemistry strategies of yesteryear managed to identify a handful of indications amenable to single-protein therapies, just like a handful of traits can be ascribed to single genes. But most traits that matter are polygenic, and most diseases are doubtless treatable only at the systems-biology level. Any lesser approximations are simply inadequate.</p>

<p><strong>TORTOISE:</strong> Oh dear, I fear this is becoming a bit too much for me.</p>

<p><strong>CRAB:</strong> I’ve just accepted a position at a biotech company personally backed by the high suzerains of artificial intelligence. We take millions of brightfield images of cells that have been exposed to different molecules and use deep learning to connect the observed cell-state modifications to molecular structure. Think phenotypic screening, but grander and more glorious.</p>

<p><strong>ACHILLES:</strong> Now I feel out of my depth. Perhaps Mr. T is right and this new world is not for me. The 5th century does have a certain rustic charm…</p>

<p><strong>TORTOISE:</strong> Wait, I think I understand. Previously, we discussed how, by training a single model, we could circumvent the need for explicitly generating protein structural ensembles and scoring individual docked poses—a single meta-model could implicitly perform all these tasks in an end-to-end differentiable fashion and simply learn all the patterns, or perhaps perform some more advanced and less constrained form of logic. Achilles, do you consider this a fair summary of your position?</p>

<p><strong>ACHILLES:</strong> Yes, that seems fair enough, although I hardly see how my proposal connects to this outlandish suggestion.</p>

<p><strong>TORTOISE:</strong> If we wanted to extrapolate this to entire cells, we could perform a similar exercise. We could enumerate all the proteins in the cell with all their various post-translational modifications, and then use Achilles’s model to score a given molecule’s interaction with all of them. It would be a mighty amount of work—but, in theory, it’s possible.</p>

<p><strong>CRAB:</strong> Ah, but you’d still be neglecting the effects of environment, aggregation, and so on. Think of an E3 ligase—do you think you could model that one protein at a time? And what do you say to DNA, RNA, lipids, and so on and so forth.</p>

<p><strong>TORTOISE:</strong> <em>Touché.</em> Perhaps “protein” is the wrong word here—but there must be some number of defined, localized structural entities in the cell which interact with an exogenous small molecule, and these entities must be at least somewhat separable per the principle of locality.</p>

<p><strong>ACHILLES:</strong> Yes, that’s right. After all, a molecule can only be at one place at a time.</p>

<p><strong>TORTOISE:</strong> So if we could use Achilles’s model to predict the interaction of the small molecule with each of these entities, we would have a sort of interaction fingerprint in entity space. We could then, with sufficient data, train a new model to learn the interaction network between each entity and predict an overall cell-level response. Do you agree, Mr. Crab?</p>

<p><strong>CRAB:</strong> I suppose so, although it sounds ungainly. How exactly do you plan to study the effects of a bunch of small molecules on a particular region of chromatin?</p>

<p><strong>TORTOISE:</strong> Ah, but this is where we use Achilles’s trick once more. Instead of learning one model that accounts for per-entity interactions, and another model that combines the individual per-entity predictions into a cell-level prediction, we can just learn a single model and backpropagate through the entire stack. So now our single foundation model is implicitly learning not only protein conformational ensembles, protein–ligand docking, docking rescoring—we’re also learning post-translational modifications, systems biology, and so on.</p>

<p><strong>ACHILLES:</strong> Ah, now I see. Our aquatic colleague here is taking my same logic a step further—instead of implicitly learning individual structures in the course of predicting a protein–ligand interaction, he’s implicitly learning individual protein–ligand interactions in the course of predicting a single cell response.</p>

<p><strong>TORTOISE:</strong> Exactly. The question then becomes if he’ll have enough data to learn the entire stack, or if his model will suffer the same overgeneralization problems as today’s protein–ligand interaction models.</p>

<p><strong>ACHILLES:</strong> Right. It’s clear that at some scale, questions of information theory must predominate—every problem has some minimum amount of data that it takes to solve. Otherwise we’d all be able to solve drug toxicity just from <a href="https://practicalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html">the 1500 structures in the ClinTox dataset</a>.</p>

<p><strong>TORTOISE:</strong> Precisely. We could imagine such a strategy working at the infinite-data limit, but in practice the mismatch between problem complexity and data availability seems vast, and slow to fill.</p>

<p><strong>CRAB:</strong> This has been an interesting philosophical aside, but I’m afraid that trying to cram your preconceived notions about biological dogma into my model is ill-advised. Today’s scientists think of proteins because that’s all they know how to study—but true biological understanding can only come when we’re able to learn directly on cellular data without the foolish assumptions that have plagued biochemistry to date. Trying to interpret my cell-level models through the viewpoint of proteins is like trying to decompose a Cybertruck into a linear combination of horses.</p>

<p>But in any event, I must be off. An army of H100s awaits me, and I must deploy them!</p>

<p><em>[Exit CRAB.]</em></p>

<p><strong>ACHILLES:</strong> That fellow has no scientific humility. Of course proteins are important! These Silicon-Valley types have no respect for the deep biological body of knowledge that came before them, and think they can just pour images and SMILES strings into a transformer and “solve biology.” But we’d better return to our previous discussion, or things may become too recursive.</p>

<p><strong>TORTOISE:</strong> There seem to be more and more fellows like him around these days... but I suppose <a href="https://en.wikipedia.org/wiki/Carcinisation">carcinization</a> is a well-documented phenomenon. Where were we before this unexpected conversational loop?</p>

<p><strong>ACHILLES:</strong> I was just proposing the idea that sequence-based models will implicitly learn structure where it’s helpful.</p>

<p><strong>TORTOISE:</strong> Ah, yes. I am beginning to catch up with your lightning-fast intuition. Are you opposed to structure for ideological reasons, or because you think structural information will never be achievable on the scale required to solve this problem?</p>

<p><strong>ACHILLES:</strong> Both—I’m opposed to structure because accurate structural ensembles, which are what’s needed here, will never be available. Even a billion cryoEM structures won’t be enough because single ground-state snapshots will never be enough.</p>

<p><strong>TORTOISE:</strong> But you must concede that, for instance, <a href="https://www.owlposting.com/p/an-argument-for-integrating-molecular">molecular dynamics could provide a way to generate relevant structural information</a> under non-ground-state conditions.</p>

<p><strong>ACHILLES:</strong> I freely admit that the Platonic ideal of MD simulations might furnish us with such data, to run the risk of sounding overly Greek. But you know as well as I do that MD simulations are unreliable and provide data that’s far worse than crystallography. What makes you think that dumping millions of AMBER trajectories into an ML model will do anything except increase demand for H100s?</p>

<p><strong>TORTOISE:</strong> Improving MD simulations seems to be quite tractable. There have been a few papers over the past 12 months that use neural network potentials for protein simulation—consider <a href="https://www.science.org/doi/10.1126/sciadv.adn4397">GEMS</a>, or <a href="https://www.nature.com/articles/s41586-024-08127-z">AI2BMD</a>, or even the most recent <a href="https://arxiv.org/abs/2312.15211">MACE-OFF</a> preprint. <a href="https://www.nature.com/articles/s42256-023-00740-3">Scaling NNPs works well</a>; why not just scale NNPs and use them to run MD simulations?</p>

<p><strong>ACHILLES:</strong> For one, NNPs are ridiculously slow compared to normal MD—capturing protein conformational motion through MD is expensive enough without making it three orders of magnitude slower. You may be content with slow and accurate simulations, but I myself feel the need to go quickly. MD simulations will never be fast enough for high-throughput virtual screening. And how are we supposed to verify the alleged accuracy of these simulations, anyway?</p>

<p><strong>TORTOISE:</strong> NMR measurements, perhaps, or terahertz spectroscopy. The ingenuity of experimentalists cannot be underestimated.</p>

<p><strong>ACHILLES:</strong> I grant that this might work for a single protein. But you’ve managed to select methods that are even less scalable than growing crystals in a tray. This can’t be a general solution—it’s the age of “big data” now, not painstaking spectral analysis measured in graduate-student years.</p>

<p><strong>TORTOISE:</strong> Ah, but we don’t need massive amounts of data for our benchmarks. NNPs and MD are physically motivated, so they’re much less prone to overfitting than the approaches you discuss. Generalization occurs naturally, without needing to resort to the sorts of paranoid dataset splits seen with sequence-only methods.</p>

<p><strong>ACHILLES:</strong> Might this not simply arise from how small the models are today? Once an NNP must handle long-range forces, complex many-body interactions, and so on, these models will be just as susceptible to overfitting as co-folding methods. I know you like to hide in your shell from time to time, but robustness isn’t everything—if all you want is to prevent overfitting, you might as well go back to using AutoDock Vina.</p>

<p><strong>TORTOISE:</strong> Not all approaches are equally susceptible to overfitting, and encoding proper inductive biases is one of the most important tasks of an ML researcher. The sorts of properties predicted by NNPs—forces, energy, charges, and so on—are intrinsically local and thus can be learned much more easily from a limited dataset. In fact, this is one of the strongest arguments for using a geometric GNN in the first place; we naturally account for the symmetries of the problem, as opposed to needing to learn them through vast datasets. Consider the analogies to <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noether’s theorem</a>.</p>

<p><strong>ACHILLES:</strong> I must confess, I rarely revisit the 1910s.</p>

<p><strong>TORTOISE:</strong> More fundamentally, learning energy as an intermediate variable is an incredibly fundamental task, and it’s unlikely that we can avoid some version of this task—particularly since diffusion models and AlphaFold are <a href="https://pubs.acs.org/doi/10.1021/acsphyschemau.4c00004">almost certainly both implicitly learning forcefields</a> anyway.</p>

<p>Trying to one-shot the hardest problems in computational biochemistry and biophysics with “deep learning” will forever be hamstrung by memorization and overfitting, since the approach is fundamentally agnostic to the nature of the problem. I’m simply proposing that trying to learn physically motivated, verifiable, and practical models that correspond to our physical understanding of the world may be a more tractable strategy, even if it seems slower to you.</p>

<p><strong>ACHILLES:</strong> You know that I respect your stepwise approach to scientific discovery, but I fear you’re confusing your own intrinsic conservatism for enlightenment. Haven’t you heard of Sutton’s <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">“bitter lesson”</a>? Encoding expert intuition always makes the researcher feel accomplished, and is often effective in the small-data regime, but never pays off in the end.</p>

<p><strong>TORTOISE:</strong> Mr. Crab could say the same thing to you.</p>

<p><strong>ACHILLES:</strong> Admittedly. But the task of the ML researcher is not dissimilar to that of the philosopher: to carve reality at its joints, <a href="https://classics.mit.edu/Plato/phaedrus.html">as my kinsman Plato said</a>, and find the natural partitions between concepts that make our tasks tractable. Choosing the right problem to tackle with deep learning might seem like encoding expert intuition in an un-Suttonian way, but really it’s a higher-order consideration, and one which itself still remains impervious to learning.</p>

<p><strong>TORTOISE:</strong> And what, pray tell, makes your protein–ligand model a natural partition, and my NNPs an unnatural partition?</p>

<p><strong>ACHILLES:</strong> The elegance of the protein–ligand task is that it corresponds to a real information bottleneck—all the complexity of the system can easily be distilled into a single number, and in practice the measurement is performed that way. In contrast, your model is only indirectly testable and verifiable.</p>

<p><strong>TORTOISE:</strong> Only as indirectly as any other physics-based method is testable. Scientists have been doing this for some time, you know.</p>

<p><strong>ACHILLES:</strong> And even more fundamentally, even a “physics-based model” is anything but. Scratch the surface of an NNP-powered MD simulation and you’ll see an ocean of questionable assumptions: band-gap collapse, nuclear quantum effects, spin–orbit coupling, quantum vs. classical zero-point energy, and so on and so forth. Even a model trained on full-configuration-interaction calculations won’t perfectly reflect reality. At the end of the day, you’ll have wasted ten million dollars on AWS computers generating gnostic simulated data that you could have spent getting real, tangible results without approximations.</p>

<p><strong>TORTOISE:</strong> I’m willing to concede that at some scale, what you’re proposing might work. But you have no idea how much data you need to learn protein–ligand interactions. Have you done a scaling study; do you even have a back-of-the-envelope estimate for what your proposed model will cost? Who knows what the true dimensionality of protein–ligand interaction space is, or if it’s remotely learnable with the general architectures you propose? Someone’s going to have to generate all this data, and it’s not cheap—even fleet-footed Achilles can’t outrun the fundamental limitations of laboratory science.</p>

<p><strong>ACHILLES:</strong> Ah, let’s not let our conversation fold back on itself. Isn’t it possible that there are latent low-dimensional representations of protein–ligand interactions that can make my structure-only training process more efficient?</p>

<p><strong>TORTOISE:</strong> Possible, yes, but not guaranteed. To make matters worse, even if you train a protein–ligand model you’ll have to turn around and train another foundation model for protein–protein interactions, and another model for nucleotides, and another model for lipids, and so on and so forth.</p>

<p><strong>ACHILLES:</strong> Presuming the first model succeeds, I would think this a fine outcome.</p>

<p><strong>TORTOISE:</strong> We know what the scaling laws for NNPs are, and we know that <a href="https://arxiv.org/abs/2401.00096">they can scale across different domains of science even at sub-GPT1 parameter count</a>. These are real advantages, and we ought to not be hasty in discarding them. Plus, it’s not like today’s methods are inconceivably far from where we want to go. Forcefield-based free-energy methods aren’t perfect, but <a href="https://www.nature.com/articles/s42004-023-01019-9">they’re good enough to be useful</a>. Doesn’t that suggest that we don’t need to get e.g. nuclear quantum effects exactly right to build a useful model?</p>

<p><strong>ACHILLES:</strong> Scaling simulation across the chemical sciences is intriguing. You should tell Adam Marblestone; maybe you can build an FRO out of this idea. But we must stay focused on running the race at hand first and worry about the whole decathlon later. Perhaps we’ll be able to perform <a href="https://arxiv.org/abs/2403.13187v2">evolutionary model merging</a> and pull out conformational ensembles at a later date, but I fear that your bias towards legacy simulation methods blinds you to the task at hand.</p>

<p>And arguing that FEP+ is good enough to be useful proves too much. <a href="https://link.springer.com/chapter/10.1007/0-306-46883-2_7">Simply creating a histogram of distance by atom types is good enough to be useful</a>; even plastic model kits are useful. Being useful at the small-data limit and being a viable path towards the future are very different, and I fear you confuse them at your own peril.</p>

<p><strong>TORTOISE:</strong> Think strategically, my tactical friend. Let’s say we’re trying to get to the ultimate protein–ligand prediction model, which I’ll call the Galapagos Giant Model. If I train an NNP that’s halfway there, I’ve built something that’s immediately practically useful and which I can deploy to real problems. If you build a one-shot prediction model that’s halfway there, you’re going to get an overfit and confused model that takes a SMILES string and a sequence and returns meaningless noise.</p>

<p><strong>ACHILLES:</strong> (Of course, first I’d have to train a model that was halfway to being halfway complete…)</p>

<p><strong>TORTOISE:</strong> The ability of startups and research programs to bootstrap their way through increasing complexity is a critical determiner of their success—this is why YC tells companies to ship and start talking to users as soon as possible. We know that NNPs are already useful. How can you accomplish a similar feat with your approach?</p>

<p><strong>ACHILLES:</strong> Ah, but your line of argumentation seems to rely on its own conclusion. Why is my hypothetical half-baked model unusable but yours is useful? Isn’t it just as possible that my model is useful across many domains but struggles to generalize to bizarre systems, while your model manages to be deeply useful nowhere?</p>

<p>The greatest advantage of simulation—its exactitude—is also its greatest weakness. A simulation-based workflow is only as strong as its weakest link, or what one might call its Achilles heel.</p>

<p><strong>TORTOISE:</strong> Science aside, I fear the self-reference here will soon become ponderous.</p>

<p><strong>ACHILLES:</strong> This might explain why <a href="https://arxiv.org/abs/2410.16818">the data on using NNPs in FEP are pretty bleak with today’s models</a>, even though these models are undeniably a big improvement over the predecessor forcefield methods. Furthermore, <a href="https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc01185e">fine-tuning models to be better at specific tasks seems to make them less general</a>.</p>

<p><strong>TORTOISE:</strong> I caution you not to rush to dismiss my approach prematurely. True ML FEP has never been tried, since the timescales remain inaccessible. Ligand-only corrections neglect the most important part of the system, which is the protein–ligand interactions—and <a href="https://www.science.org/doi/10.1126/sciadv.adn4397">we know that protein conformational motion is poorly described by forcefields</a>, potentially biasing the entire simulation in deleterious ways. So no, I cannot feign surprise that these results are underwhelming.</p>

<p><strong>ACHILLES:</strong> Still, you can’t deny that even the “overfit” ML methods of today like DiffDock are practically useful—it’s not like most drug programs deal with first-in-class structural families. How well do you think AlphaFold 3 works for kinase inhibitors? I would be surprised if the performance is not excellent.</p>

<p><strong>TORTOISE:</strong> The dimensionality of ligand space is much higher than that of protein space.</p>

<p><strong>ACHILLES:</strong> True. But it’s possible that generalization is easier in ligand space. I’m growing hungry—how about we continue this discussion over brunch?</p>

<p><strong>TORTOISE:</strong> A capital idea. Shall we leave now?</p>

<p><strong>ACHILLES:</strong> You are welcome to, but I may sit and read for a bit longer. As you know, I have a considerable speed advantage over you, and keeping up with the literature takes more and more of my time.</p>

<p><strong>TORTOISE:</strong> Best of luck. We’ll see who gets there first!</p>

<p><em>[Exit TORTOISE.]</em></p>

<br>
<br>

<i>Thanks to Abhishaike Mahajan, Navvye Anand, Tony Kulesa, Pat Walters, and Ari Wagen for helpful conversations on these topics. I've also taken inspiration from talks I heard by Tom Sercu (Evolutionary Scale) and Pranam Chatterjee (Duke). Any errors are mine alone.</i>
]]></description>
              <pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>What Did The Early Church Think About Fasting?</title>
              <link>public/blog/20250203_fasting.html</link>
              <description><![CDATA[
<p>
(This is a bit of a departure from my usual chemistry-focused writing.)
</p>

<p>
Fasting is an important part of many religious traditions, but modern Protestant Christians don’t really have a unified stance on fasting (<a href=https://en.wikipedia.org/wiki/Affair_of_the_Sausages>and have opposed systematic fasts for a while</a>). That’s not to say that Protestants don’t fast, though: over just the past few years, I’ve met people doing water-only fasts, juice fasts, dinner-only fasts, “social media” fasts, and many more.
</p>

<p>
 These fasts don’t really line up with what I see in neighboring faith traditions:
</p>

<ul>
  <li> Catholics traditionally avoid meat (but not fish or <a href=https://x.com/Rainmaker1973/status/1758786298955923682>capybara</a>) on Fridays and during Lent, and Orthodox Christians often follow similar restrictions. </li>
<li> During Ramadan, Muslims fast during the day but eat and drink at night. </li>
<li> And on Yom Kippur, Jews traditionally neither eat nor drink anything (even water). </li>
</ul>

<p>
I’ve been a bit puzzled by all this, so I decided to do a “literature review” and find documents from the early Church that discussed fasting. This post collects and summarizes the sources that I found. The sources are listed in approximate chronological order, with emphasis added throughout—if you don’t want to read everything, you can skip to the end and read my brief takeaways. 
</p>

<h2>
  <a href=https://www.newadvent.org/fathers/0714.htm>Didache (c. 100 AD)</a>
</h2>

<blockquote>
But before the baptism let the baptizer fast, and the baptized, and whatever others can; but you shall order the baptized to fast one or two days before….
<br><br>
But let not your fasts be with the hypocrites; for they fast on the second [Monday] and fifth day [Thursday] of the week; <b>but fast on the fourth day [Wednesday] and the Preparation (Friday).</b>
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/02013.htm>Shepherd of Hermas (c. 150–200 AD)</a>
</h2>

<blockquote>
  Thus, then, shall you observe the fasting which you intend to keep. First of all, be on your guard against every evil word, and every evil desire, and purify your heart from all the vanities of this world. If you guard against these things, your fasting will be perfect. And you will do also as follows. <b>Having fulfilled what is written, in the day on which you fast you will taste nothing but bread and water;</b> and having reckoned up the price of the dishes of that day which you intended to have eaten, you will give it to a widow, or an orphan, or to some person in want, and thus you will exhibit humility of mind, so that he who has received benefit from your humility may fill his own soul, and pray for you to the Lord. If you observe fasting, as I have commanded you, your sacrifice will be acceptable to God, and this fasting will be written down; and the service thus performed is noble, and sacred, and acceptable to the Lord. These things, therefore, shall you thus observe with your children, and all your house, and in observing them you will be blessed; and as many as hear these words and observe them shall be blessed; and whatsoever they ask of the Lord they shall receive.
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/0408.htm>On Fasting, Tertullian (c. 160–240 AD)</a>
</h2>

<blockquote>
  Now, if there has been temerity in our retracing to primordial experiences the reasons for God's having laid, and our duty (for the sake of God) to lay, restrictions upon food, let us consult common conscience. Nature herself will plainly tell with what qualities she is ever wont to find us endowed when she sets us, before taking food and drink, with our saliva still in a virgin state, to the transaction of matters, by the sense especially whereby things divine are handled; whether (it be not) with a mind much more vigorous, with a heart much more alive, than when that whole habitation of our interior man, <b>stuffed with meats, inundated with wines,</b> fermenting for the purpose of excremental secretion, is already being turned into a premeditatory of privies, (a premeditatory) where, plainly, nothing is so proximately supersequent as the savouring of lasciviousness…
<br><br>
This principal species in the category of dietary restriction may already afford a prejudgment concerning the inferior operations of abstinence also, as being themselves too, in proportion to their measure, useful or necessary. For the exception of certain kinds from use of food is a partial fast. Let us therefore look into the question of the novelty or vanity of xerophagies, to see whether in them too we do not find an operation alike of most ancient as of most efficacious religion… I return likewise to Elijah. When the ravens had been wont to satisfy him with bread and flesh, why was it that afterwards, at Beersheba of Judea, that certain angel, after rousing him from sleep, offered him, beyond doubt, bread alone, and water? Had ravens been wanting, to feed him more liberally? Or had it been difficult to the angel to carry away from some pan of the banquet-room of the king some attendant with his amply-furnished waiter, and transfer him to Elijah, just as the breakfast of the reapers was carried into the den of lions and presented to Daniel in his hunger? But it behooved that an example should be set, teaching us that, at a time of pressure and persecution and whatsoever difficulty, we must live on xerophagies…. <b>Anyhow, wherever abstinence from wine is either exacted by God or vowed by man, there let there be understood likewise a restriction of food fore-furnishing a formal type to drink.</b> For the quality of the drink is correspondent to that of the eating. It is not probable that a man should sacrifice to God half his appetite; temperate in waters, and intemperate in meats….
<br><br>
The apostle reprobates likewise such as bid to abstain from meats; but he does so from the foresight of the Holy Spirit, precondemning already the heretics who would enjoin perpetual abstinence to the extent of destroying and despising the works of the Creator; such as I may find in the person of a Marcion, a Tatian, or a Jupiter, the Pythagorean heretic of today; not in the person of the Paraclete. <b>For how limited is the extent of our interdiction of meats! Two weeks of xerophagies in the year (and not the whole of these — the Sabbaths, to wit, and the Lord's days, being excepted) we offer to God; abstaining from things which we do not reject, but defer.</b>
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/2806001.htm>Letter 1, Athanasius (329 AD)</a>
</h2>

<blockquote>
For since, as I before said, there are various proclamations, listen, as in a figure, to the prophet blowing the trumpet; and further, having turned to the truth, be ready for the announcement of the trumpet, for he says, 'Blow the trumpet in Sion: sanctify a fast' This is a warning trumpet, and commands with great earnestness, that when we fast, we should hallow the fast. For not all those who call upon God, hallow God, since there are some who defile Him; yet not Him — that is impossible — but their own mind concerning Him; for He is holy, and has pleasure in the saints. And therefore the blessed Paul accuses those who dishonour God; 'Transgressors of the law dishonour God' So then, to make a separation from those who pollute the fast, he says here, 'sanctify a fast.' For many, crowding to the fast, pollute themselves in the thoughts of their hearts, sometimes by doing evil against their brethren, sometimes by daring to defraud…
<br><br>
<b>We begin the holy fast on the fifth day of Pharmuthi (March 31), and adding to it according to the number of those six holy and great days, which are the symbol of the creation of this world, let us rest and cease (from fasting) on the tenth day of the same Pharmuthi (April 5), on the holy sabbath of the week. </b>And when the first day of the holy week dawns and rises upon us, on the eleventh day of the same month (April 6), from which again we count all the seven weeks one by one, let us keep feast on the holy day of Pentecost — on that which was at one time to the Jews, typically, the feast of weeks, in which they granted forgiveness and settlement of debts; and indeed that day was one of deliverance in every respect.'
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/310104.htm>Catechetical Lecture 4, Cyril of Jerusalem (c. 350 AD)</a>
</h2>

<blockquote>
  And concerning food let these be your ordinances, since in regard to meats also many stumble. For some deal indifferently with things offered to idols, while others discipline themselves, but condemn those that eat: and in different ways men's souls are defiled in the matter of meats, from ignorance of the useful reasons for eating and not eating. <b>For we fast by abstaining from wine and flesh, not because we abhor them as abominations, but because we look for our reward; that having scorned things sensible, we may enjoy a spiritual and intellectual feast; and that having now sown in tears we may reap in joy in the world to come.</b> Despise not therefore them that eat, and because of the weakness of their bodies partake of food.
</blockquote>

<h2>
<a href=https://www.newadvent.org/fathers/07155.htm> Apostolic Constitutions, Book V (c. 375 AD) </a>
</h2>

<blockquote>
  You should therefore fast on the days of the passover, beginning from the second day of the week until the preparation, and the Sabbath, six days, <b>making use of only bread, and salt, and herbs, and water for your drink; but do you abstain on these days from wine and flesh,</b> for they are days of lamentation and not of feasting….
  <br><br>
<b>We enjoin you to fast every fourth day of the week, and every day of the preparation, and the surplusage of your fast bestow upon the needy;</b> every Sabbath day excepting one, and every Lord's day, hold your solemn assemblies, and rejoice: for he will be guilty of sin who fasts on the Lord's day, being the day of the resurrection, or during the time of Pentecost, or, in general, who is sad on a festival day to the Lord. For on them we ought to rejoice, and not to mourn.
</blockquote>

<h2>
  <a href=https://www.crkvenikalendar.com/post/post-svetivasilije_en.php>Homily 1, Basil of Caesarea (330–379 AD)</a>
</h2>

<blockquote>
  Yet even life in Paradise is an image of fasting, not only insofar as man, sharing the life of the Angels, attained to likeness with them through being contented with little, but also insofar as those things which human ingenuity subsequently invented had not yet been devised by those living in Paradise, be it <b>the drinking of wine, the slaughter of animals,</b> or whatever else befuddles the human mind. Since we did not fast, we fell from Paradise; let us, therefore, fast in order that we might return thither….
 <br><br>
 Do not, however, define the benefit that comes from fasting solely in terms of abstinence from foods. For true fasting consists in estrange­ment from vices. “Loose every burden of iniquity.” Forgive your neigh­bor the distress he causes you; forgive him his debts. “Fast not for quar­rels and strifes.” <b>You do not eat meat, but you devour your brother. You abstain from wine, but do not restrain yourself from insulting others. You wait until evening to eat, but waste your day in law courts.</b> Woe to those who get drunk, but not from wine. Anger is inebriation of the soul, mak­ing it deranged, just as wine does. Grief is also a form of intoxication, one that submerges the intellect. Fear is another kind of drunkenness, when we have phobias regarding inappropriate objects; for Scripture says: “Rescue my soul from fear of the enemy.” And in general, every passion which causes mental derangement may justly be called drunkenness.
</blockquote>

<h2>
  <a href=http://web.documentacatholicaomnia.eu/02m/0339-0397,_Ambrosius,_De_Elia_Et_Jejunio_Liber_Unus,_MLT.pdf> De Elia Et Jejunio, Ambrose (c. 389)</a>
</h2>

<i> GPT-4o translated this for me.</i>

<blockquote>
Fasting is the medicine of the soul, which teaches the body to abstain not only from vices but also from unnecessary desires. Just as the sick are often advised to abstain from certain foods, so too does the soul, wounded by sins, need the medicine of fasting, so that the allurements of pleasures may be removed and the purity of the heart may grow.
 <br><br>
 <b>Thus, meat is to be avoided during fasts, for no sacrifice is pleasing if it nourishes the desires of the flesh. Likewise, wine must be tempered, lest the sweetness of drink weaken the fervor of devotion.</b> For the holy Fathers abstained not only from food but also from drink, so that the entirety of body and soul might be consecrated to the Lord.
 <br><br>
From this also arises the greater significance of fasting during Lent, so that not only is the external body afflicted, but the inner person is also renewed. For this reason, the number of forty days is sanctified, as the Lord fasted for forty days and nights in the desert and left this example for us, so that we may not falter in abstinence…
 <br><br>
Fasting should not only be an abstinence from food but also a discipline of the soul. For one who abstains from food but does not abstain from sin harms himself more than he benefits. Thus fasting was pleasing to the holy men of old, as they neither consumed food nor committed sin. For it is written: 'Sanctify a fast' (Joel 2:15), meaning not only to observe a physical fast but also a spiritual one, free from sins, devoid of greed, unyielding to anger, and maintaining purity of mind and body.
 <br><br>
 <b>As it is written, the fast is not broken before sunset, so that devotion is preserved throughout the entire day. For what benefit is fasting if the abstinence from food is not accompanied by discipline? The holy men of old fasted in such a way that the entire day was dedicated to prayer, and the fast itself became a pleasing sacrifice.</b> This was also taught by the apostles, whose fasts combined not only abstinence from food but also persistent dedication to prayer.
 <br><br>
For fasting alone is not enough; a virtuous life is also required. For what benefit is it to refrain from food if malice abounds? As the Lord said in the Gospel: "Do not be like the hypocrites, who appear gloomy" (Matt. 6:16). Fasting should be an internal sacrifice, so that not only is the body disciplined, but the soul is also purified.
 <br><br>
The holy Fathers always observed this practice, ensuring that fasts were completed at evening time, reserving this period not only for abstinence but also for works of piety. After the day's labor, they devoted themselves to prayer and meditation on the divine law, for as evening approached, they offered a complete sacrifice of devotion to the Lord.
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/190103.htm>Homily 3 on the Statues, John Chrysostom (c. 347–407 AD)</a>
</h2>

<blockquote>
  I speak not, indeed, of such a fast as most persons keep, but of real fasting; <b>not merely an abstinence from meats; but from sins too.</b> For the nature of a fast is such, that it does not suffice to deliver those who practise it, unless it be done according to a suitable law. For the wrestler, it is said, is not crowned unless he strive lawfully. To the end then, that when we have gone through the labour of fasting, we forfeit not the crown of fasting, we should understand how, and after what manner, it is necessary to conduct this business; since that Pharisee also fasted, but afterwards went down empty, and destitute of the fruit of fasting….
  <br><br>
  I have said these things, not that we may disparage fasting, but that we may honour fasting; for the honour of fasting consists not in abstinence from food, but in withdrawing from sinful practices; <b>since he who limits his fasting only to an abstinence from meats, is one who especially disparages it.</b> Do you fast? Give me proof of it by your works! Is it said by what kind of works? If you see a poor man, take pity on him! If you see in enemy, be reconciled to him! If you see a friend gaining honour, envy him not! If you see a handsome woman, pass her by! For let not the mouth only fast, but also the eye, and the ear, and the feet, and the hands, and all the members of our bodies. 
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/190104.htm>Homily 4 on the Statues, John Chrysostom (c. 347–407 AD)</a>
</h2>

<blockquote>
  And with respect to the two former precepts, we will discourse to you on another occasion; but we shall speak to you during the whole of the present week respecting oaths; thus beginning with the easier precept. For it is no labour at all to overcome the habit of swearing, if we would but apply a little endeavour, by reminding each other; by advising; by observing; and by requiring those who thus forget themselves, to render an account, and to pay the penalty. <b>For what advantage shall we gain by abstinence from meats, if we do not also expel the evil habits of the soul?</b> Lo, we have spent the whole of this day fasting; and in the evening we shall spread a table, not such as we did on yester-eve, but one of an altered and more solemn kind. Can any one of us then say that he has changed his life too this day; that he has altered his ill custom, as well as his food? Truly, I suppose not! Of what advantage then is our fasting? Wherefore I exhort, and I will not cease to exhort, that undertaking each precept separately, you should spend two or three days in the attainment of it; and just as there are some who rival one another in fasting, and show a marvellous emulation in it; <b>(some indeed who spend two whole days without food; and others who, rejecting from their tables not only the use of wine, and of oil, but of every dish, and taking only bread and water, persevere in this practice during the whole of Lent)</b>; so, indeed, let us also contend mutually with one another in abolishing the frequency of oaths. For this is more useful than any fasting; this is more profitable than any austerity.
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/190110.htm>Homily 10 on the Statues, John Chrysostom (c. 347–407 AD)</a>
</h2>

<blockquote>
  What need then is there to say more? Stand only near the man who fasts, and you will straightway partake of his good odour; for fasting is a spiritual perfume; and through the eyes, the tongue, and every part, it manifests the good disposition of the soul. I have said this, not for the purpose of condemning those who have dined, but that I may show the advantage of fasting. <b>I do not, however, call mere abstinence from meats, fasting; but even before this, abstinence from sin;</b> since he who, after he has taken a meal, has come hither with suitable sobriety, is not very far behind the man who fasts; even as he who continues fasting, if he does not give earnest and diligent heed to what is spoken, will derive no great benefit from his fast.
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/3001130.htm>Letter 130 To Demetrias, Jerome (414 AD)</a>
</h2>

<blockquote>
After you have paid the most careful attention to your thoughts, you must then put on the armour of fasting and sing with David: I chastened my soul with fasting, and I have eaten ashes like bread, and as for me when they troubled me my clothing was sackcloth. Eve was expelled from paradise because she had eaten of the forbidden fruit. Elijah on the other hand after forty days of fasting was carried in a fiery chariot into heaven. For forty days and forty nights Moses lived by the intimate converse which he had with God, thus proving in his own case the complete truth of the saying, man does not live by bread only but by every word that proceeds out of the mouth of the Lord. The Saviour of the world, who in His virtues and His mode of life has left us an example to follow, was, immediately after His baptism, taken up by the spirit that He might contend with the devil, and after crushing him and overthrowing him might deliver him to his disciples to trample under foot. For what says the apostle? God shall bruise Satan under your feet shortly. And yet after the Saviour had fasted forty days, it was through food that the old enemy laid a snare for him, saying, If you be the Son of God, command that these stones be made bread. Under the law, in the seventh month after the blowing of trumpets and on the tenth day of the month, a fast was proclaimed for the whole Jewish people, and that soul was cut off from among his people which on that day preferred self-indulgence to self-denial.…
<br><br>
<b>I do not, however, lay on you as an obligation any extreme fasting or abnormal abstinence from food. Such practices soon break down weak constitutions and cause bodily sickness before they lay the foundations of a holy life. </b>It is a maxim of the philosophers that virtues are means, and that all extremes are of the nature of vice; and it is in this sense that one of the seven wise men propounds the famous saw quoted in the comedy, In nothing too much. <b>You must not go on fasting until your heart begins to throb and your breath to fail and you have to be supported or carried by others. No; while curbing the desires of the flesh, you must keep sufficient strength to read scripture, to sing psalms, and to observe vigils. For fasting is not a complete virtue in itself but only a foundation on which other virtues may be built. </b>The same may be said of sanctification and of that chastity without which no man shall see the Lord. Each of these is a step on the upward way, yet none of them by itself will avail to win the virgin's crown. The gospel teaches us this in the parable of the wise and foolish virgins; the former of whom enter into the bridechamber of the bridegroom, while the latter are shut out from it because not having the oil of good works they allow their lamps to fail. This subject of fasting opens up a wide field in which I have often wandered myself, and many writers have devoted treatises to the subject. I must refer you to these if you wish to learn the advantages of self-restraint and on the other hand the evils of over-feeding.
</blockquote>

<h2>
  <a href=https://www.newadvent.org/fathers/26015.htm>Church History Book V, Socrates of Constantinople (c. 439)</a>
</h2>

<blockquote>
  The fasts before Easter will be found to be differently observed among different people. Those at Rome fast three successive weeks before Easter, excepting Saturdays and Sundays. Those in Illyrica and all over Greece and Alexandria observe a fast of six weeks, which they term "The forty days' fast." Others commencing their fast from the seventh week before Easter, and fasting three five days only, and that at intervals, yet call that time "The forty days' fast." It is indeed surprising to me that thus differing in the number of days, they should both give it one common appellation; but some assign one reason for it, and others another, according to their several fancies. One can see also a disagreement about the manner of abstinence from food, as well as about the number of days. <b>Some wholly abstain from things that have life: others feed on fish only of all living creatures: many together with fish, eat fowl also, saying that according to Moses, these were likewise made out of the waters. Some abstain from eggs, and all kinds of fruits: others partake of dry bread only; still others eat not even this: while others having fasted till the ninth hour, afterwards take any sort of food without distinction. </b>And among various nations there are other usages, for which innumerable reasons are assigned. Since however no one can produce a written command as an authority, it is evident that the apostles left each one to his own free will in the matter, to the end that each might perform what is good not by constraint or necessity. Such is the difference in the churches on the subject of fasts.
</blockquote>

<h2>
  <a href=https://www.documentacatholicaomnia.eu/03d/0627-0735,_Beda_Venerabilis,_Ecclesiastical_History_Of_England,_EN.pdf>Ecclesiastical History Chapter XXIII, Bede (731)</a>
</h2>

<blockquote>
  But [Bishop Cedd], desiring first to cleanse the place which he had received for the monastery from stain of former crimes, by prayer and fasting, and so to lay the foundations there, requested of the king that he would give him opportunity and leave to abide there for prayer all the time of Lent, which was at hand. <b>All which days, except Sundays, he prolonged his fast till the evening, according to custom, and then took no other sustenance than a small piece of bread, one hen’s egg, and a little milk and water.</b> This, he said, was the custom of those of whom he had learned the rule of regular discipline, first to consecrate to the Lord, by prayer and fasting, the places which they had newly received for building a monastery or a church.
</blockquote>

<br><br>

<p> To summarize my takeaways:</p>

<h3>
  What Dates? 
</h3>

<p>
Early sources suggest fasting on Wednesday and Friday. Other sources introduce a Lenten fast, but the dates are a little unclear—sometimes just during Holy Week, sometimes just Good Friday and Holy Saturday, sometimes more.
</p>

<h3>
  Eating What?
</h3>

<p>
There’s a mix: bread and water, bread and water and vegetables, or anything but meat and alcohol.
</p>

<h3>
  Eating When?
</h3>
<p>
Often this isn’t mentioned at all, but sometimes it’s said that you shouldn’t eat anything until the evening. 
</p>

]]></description>
              <pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Books from 2024</title>
              <link>public/blog/20250101_books.html</link>
              <description><![CDATA[
<p>
(Previously: <a href=https://corinwagen.github.io/public/blog/20221231_books.html>2022</a>, <a href=https://corinwagen.github.io/public/blog/20231321_books.html>2023</a>.)
</p>

<b>#1. Baldassar Castiglione, <i>The Book of the Courier</i></b> 

<p>
This book gets cited from time to time as a sort of historical guide to "being cool," since the characters spend some time discussing the idea of <i>sprezzatura</i>, basically grace or effortlessness. More interesting to me was the differences between Renaissance conceptions of virtue, character, & masculinity / femininity and how our culture's used to thinking about these concepts—"the past is a foreign country."
</p>

<b>#2. Grant Cardone, <i>Sell Or Be Sold</i></b> 
<br>
<b>#3. Andrew Chen, <i>The Cold Start Problem</i></b> 
<br>
<b>#4–7. Stephanie Meyer, <i>The Twilight Saga</i></b> 

<p>
Having never read or watched any <i>Twilight</i> before this year, I found them much weirder than I was expecting.
</p>

<b>#8. Fuschia Dunlop, <i>Invitation to a Banquet</i></b> 

<p>
As featured on <i><a href=https://conversationswithtyler.com/episodes/fuchsia-dunlop-3/>CWT</a></i>!
</p>

<b>#9. Iris Murdoch, <i>The Black Prince</i></b> 
<br>
<b>#10. David Kushner, <i>Masters of Doom</i></b> 

<p>
A history of id Software, the company behind <i>Wolfenstein 3D</i>, <i>Doom</i>, <i>Quake</i>, and the <a href=https://en.wikipedia.org/wiki/Fast_inverse_square_root>fast inverse square root</a> algorithm. John Carmack is a <a href=https://waivek.github.io/website/tooltip.html>legendary figure in the software world</a>, and after reading a fictionalized history inspired by id last year (<i>Tomorrow and Tomorrow and Tomorrow</i>) it was good to read the real thing.
</p>

<b>#11. Michael Gerber, <i>The E-Myth Revisited</i></b> 
<br>
<b>#12. William Gibson, <i>Neuromancer</i></b> 

<p>
A lot of old science fiction is hard to appreciate properly—the best ideas have been sucked out and copied a hundredfold, leaving only the author's weirder musings behind to be appreciated.
<i>Neuromancer</i>'s been copied as much as any novel, but I was impressed by the pace and general bleakness of this novel; it holds up well.
</p>

<b>#13–26. Lois McMaster Bujold, <i>The Vorkosigan Saga</i></b> 

<p>
I adored this series, which I read pretty steadily over the course of the year. Bujold writes satisfying, well-constructed plots that keep the focus on characters, not setting. The books fit together nicely, too: each story stands alone, but together paint a decades-long picture of her characters aging, gaining wisdom through their mistakes, and learning to handle the responsibilities placed on them. I think <i>Captain Vorpatril's Alliance</i> is my favorite one.
</p>

<b>#27. R. F. Kuang, <i>Babel</i></b> 
<br>
<b>#28. Clay Christiansen, <i>The Innovator’s Dilemma</i></b> 

<p>
As recommended by <a href=https://www.acquired.fm/episodes/jensen-huang>Jensen Huang</a>; unlike most business books, this one is worth reading all the way through.
</p>

<b>#29. Rob Fitzpatrick, <i>The Mom Test</i></b> 

<p>A canonical book for startup founders, which I probably should have read 1–2 years ago.</p>

<b>#30. Elena Ferrante, <i>My Brilliant Friend</i></b> 

<p>
At its core, this is a very similar story to <i>Wicked</i>: a coming-of-age story focusing on the envious and unstable friendship between two women.
I liked this book, but haven't yet picked up the rest of the Neopolitan Novels; somehow keeping track of the names must intimidate me on a subconscious level.
</p>

<b>#31. Andy Grove, <i>Only The Paranoid Survive</i></b> 
<br>
<b>#32. Vernor Vinge, <i>A Fire Upon The Deep</i></b> 

<p> I liked this book a lot. I would have adored it if I'd read it as a kid, I think; there's something viscerally compelling about Vinge's "Zones of Thought."</p>

<b>#33. C. S. Lewis, <i>The Discarded Image</i></b> 

<p>
This book examines what medieval Europeans thought of the world: how did they see their universe and their place in it? This is a surprisingly subtle question: obviously they were Christian, but their cosmology was considerably different than what even the most "traditional" modern people believe. Last year, I wrote this about <i>The Canterbury Tales</i>:
</p>

<blockquote>
Reading Chaucer fills me with questions about the medieval mind. The stories are steeped in Christianity, as one might expect. Any argument goes back to the Bible, even those among animals, and Chaucer assumes a level of familiarity with e.g. the Psalms far exceeding that of most modern Christians. Yet at the same time the Greco-Roman world looms large: Roman gods appear as plot characters in three tales (the Knight’s Tale, the Merchant’s Tale, and the Manciple’s Tale), and Seneca is viewed as a moral authority on par with Scripture. I’m curious how all these beliefs and ideas fit together and welcome any recommendations on this subject.
</blockquote>

<p>
<i>The Discarded Image</i> exactly answers these questions. If you're at all interested in medieval thought, I highly recommend it.
</p>

<b>#34. Jim Collins, <i>Good To Great</i></b> 
<br>
<b>#35. R. T. France, <i>The Gospel of Mark</i></b> 
<br>
<b>#36. Nathan Azrin, <i>Toilet Training In Less Than One Day</i></b> 

<p>We didn't quite live up to the book's promise, but it took less than a week, so I'm happy.</p>

<b>#37. Tim Keller, <i>Every Good Endeavor</i></b> 
<br>
<b>#38. Brad Feld, <i>Venture Deals</i></b> 

<p>Another canonical book for startup founders, which I also probably should have read before now.</p>

<b>#39. Abigail Shrier, <i>Bad Therapy</i></b> 

<p>Shrier invites controversy here as with her other writing. Sweeping conclusions about American youth aside, I found this surprisingly compelling when viewed as a self-help book about how to be less fearful.</p>

<b>#40. Sheldon Vaunaken, <i>A Severe Mercy</i></b> 

<p>Caused me to weep uncontrollably while stuck in a middle seat on a five-hour flight: you've been warned.</p>

<b>#41. Thich Nhat Hanh, <i>You Are Here</i></b> 
<br>
<b>#42. Gunther Hagen, <i>This Is Germany: An Art Book</i></b> 
<br>
<b>#43. Thomas Malory, <i>Le Mort D’ Arthur</i></b> 
<br>
<b>#44. Georgette Heyer, <i>A Civil Contract</i></b> 
<br>
<b>#45. Alex Hormozi, <i>$100M Offers</i></b> 
<br>
<b>#46. R. F. Kuang, <i>Yellowface</i></b> 
<br>
<b>#47. Barry Werth, <i>The Billion-Dollar Molecule</i></b> 

<p>This book is crazy, and I can't believe I hadn't read it before, particularly since I'm not too distant from a lot of the action, professionally or physically. It's framed as a science story, but I think it works even better at conveying the sheer desperation of early-stage startup life.</p>

<b>#48. Diarmid McCullough, <i>The Reformation</i></b> 

<p>
The Reformation is much weirder than most people, Protestant or Catholic, realize: I was surprised by the diversity of pre-Reformation religious practice in Europe, which was mostly stamped out in the doctrinal standardization of the 1500s. For both Protestants and Catholics, it became very important to separate "us" from "them," which led to the rise of catechisms, inquisitions, and so on.
</p>

<p>
This book also soured me on the "Albion's Seed" idea, as popularized by <a href=https://slatestarcodex.com/2016/04/27/book-review-albions-seed/>the SSC book review</a>. Viewed in isolation, the Puritans seem like a bunch of religious fanatics, but really McCullough argues that the same impulse predominated all over Europe in a "Reformation of Manners," from Charles Borromeo's Milan to Plymouth Colony. Perhaps it's less about the Puritans and more about the 1620s. 
</p>

<b>#49. Amy Chua, <i>Battle Hymn Of The Tiger Mother</i></b>

<p>
This book made it back into <a href=https://x.com/melissa/status/1871793185955172429>the discourse</a>, so I decided I'd actually read it—it's much better than I was expecting, and I don't think most of Chua's critics really understand the book. Conclusions for my own parenting have yet to be determined.
</p>

<br>
<br>

<p> I also read good chunks of a number of textbooks this year, including: </p>

<ul>
  <li>Barto and Sutton's <i>Reinforcement Learning</i></li>
  <li>Kleppmann's <i>Designing Data-Intensive Applications</i></li>
  <li>Di and Kern's <i>Drug-like Properties: Concepts, Structure Design and Methods</i></li>
</ul>

<p>
Overall, this was a good year for books. As the stress of Rowan has ramped up more, I've found it more difficult to write creatively in my free time, and easier to just read other people's words—this manifests in a much-diminished rate of blogging, and a lot more energy diverted to reading fiction.
</p>

<p>
Next year, I hope to read:
</p>

<ul>
  <li>More history. I've picked up a number of books on Napoleon and the Napoleonic wars, since reading <i>A Civil Contract</i> reminded me that my understanding of the whole time period is pretty crude. I'm now a few hundred pages into Andrew Roberts's biography of Napoleon, and I'm realizing that I don't really understand pre-unification Italian history very well either. History just keeps going...</li>
  <li>Relatedly, I want to re-read LeRoy Ladurie's microhistory of Montaillou, with an eye towards understanding family structures, privacy, and the relationship of the family to the broader community. David Brooks argues that <a href=https://www.theatlantic.com/magazine/archive/2020/03/the-nuclear-family-was-a-mistake/605536/>"the nuclear family was a mistake"</a>; I've had a lot of conversations about this topic over the past few years, and I'm not convinced that I or anyone else has a great sense of what these sorts of family dynamics were like five centuries ago. Montaillou seems like the right place to start acquiring some actual data, but any other recommendations are welcome!</li>
  <li>And it might finally be the year I tackle <i>Institutes</i> or <i>City of God</i>...</li>
</ul>

<p>
Happy new year, and feel free to leave book recommendations in <a href=https://cwagen.substack.com/p/books-from-2024>the Substack comments</a>!
</p>
]]></description>
              <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Are Forcefields Able To Describe Protein Dynamics?</title>
              <link>public/blog/20241011_gems.html</link>
              <description><![CDATA[<p>
<i>
  This post assumes some knowledge of molecular dynamics and forcefields/molecular mechanics. For readers unfamiliar with these topics, Abhishaike Mahajan has <a href=https://www.owlposting.com/p/a-primer-on-molecular-dynamics>a great guide to these topics</a> on his blog.
</i>
</p>

<p>
Although forcefields are commonplace in all sorts of biomolecular simulation today, there’s a growing body of evidence showing that they often give unreliable results. For instance, here’s <a href=https://arxiv.org/pdf/1705.04308>Geoff Hutchison</a> criticizing the use of forcefields for small-molecule geometry optimizations:
</p>

<blockquote>
  The use of classical MM methods for optimizing molecular structures having multiple torsional degrees of freedom is only advised <b>if the precision and accuracy of the final structures and rankings obtained from the conformer searches is of little or no concern</b>... current small molecule force fields should not be trusted to produce accurate potential energy surfaces for large molecules, even in the range of “typical organic compounds.” <i>(emphasis added)</i>
</blockquote>

<p>
Here’s a few other scattered case studies where forcefields have failed:
</p>

<ul>
  <li>
    Plenty of studies have shown that forcefields often produce nonsensical or completely incorrect torsional profiles for organic molecules: <a href=https://pubs.acs.org/doi/10.1021/ci800419j>Gleeson et al showed this in 2009</a>, Hutchison discusses it in the above study, and papers are still demonstrating this today with state-of-the-art forcefields (e.g. the <a href=https://arxiv.org/abs/2312.15211>MACE-OFF23 paper</a>).
  </li>
  <li>
    Similarly, <a href=https://www.nature.com/articles/s41598-018-21070-0>Friederich and co-workers</a> have shown that forcefields which treat each dihedral angle independently (e.g. virtually all commonly-used forcefields) produce very poor results for plenty of common systems, and that explicit dihedral coupling is required to treat these systems accurately.
  </li>
  <li>
    <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00801>MD simulations of a UUCG RNA tetraloop are inaccurate</a>, and the errors cannot be easily fixed owing to the “concerted effect of multiple ff inaccuracies that are coupled and amplifying each other.”
  </li>
  <li>
    The structure and dynamics of a model DNA mini-dumbbell system studied by <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.3c00130>Winkler and co-workers in 2023</a> depends dramatically on the exact forcefield employed, and none of them seem particularly accurate.
  </li>
  <li>
    <a href=https://core.ac.uk/download/pdf/82649571.pdf>A study by D. E. Shaw and coworkers</a> found that the mechanism of protein-folding simulations depends on the forcefield employed, as does the properties of the unfolded state.
  </li>
</ul>

<p>
This list could be a lot longer, but I think the point is clear—even for normal, bio-organic molecules, forcefields often give bad or unreliable answers.
</p>

<p>
Despite all these results, though, it’s tough to know how bad the problem really is because there have been lots of scientific questions that can only be studied with forcefields. Studying protein conformational motion, for instance, is one of the tasks that forcefields have traditionally been developed for, and the scale and size of the systems in question makes it really challenging to study any other way. So although researchers can show that different forcefields give different answers, it’s tough to quantify how close any of these answers is to the truth, and it’s always been possible to hope that a good forcefield really is describing the underlying motion of the system quite well. 
</p>

<p>
It’s for this reason that I’ve been so fascinated by <a href=https://www.science.org/doi/epdf/10.1126/sciadv.adn4397>this April 2024 work from Oliver Unke and co-workers</a>, which studies the dynamics of peptides and proteins using neural network potentials (NNPs). NNPs allow scientists to approach the accuracy of quantum chemical calculations in a tiny fraction of the time by training ML models to reproduce the output of high-level QM-based simulations: although NNPs are still significantly slower than forcefields, they’re typically about 3–6 orders of magnitude faster than the corresponding high-level calculations would be, with only slightly lower accuracy. 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20241010_intro.png" style="width:550px;" />
  <figcaption>
    A nice overview of the paper.
  </figcaption>
</figure>

<p>
In this case, Unke and co-workers train a <a href=https://www.nature.com/articles/s41467-021-27504-0>SpookyNet</a>-based NNP to reproduce PBE0/def2-TZVPPD+MBD reference data comprising fragments from the precise systems under study. (MBD refers to <a href=https://manual.q-chem.com/5.2/Ch5.S7.SS5.html>Tkatchenko’s many-body dispersion correction</a>, which can be thought of as a fancier alternative to pairwise dispersion corrections like D3 or D4.) In total, about 60 million atom-labeled data points were used to train the NNPs used in this study—which reportedly took 110,000 hours of CPU time to compute, equivalent to 12 CPU-years! 
</p>

<p>
(This might be a nitpick, but I don’t love the use of PBE0 here. Range-separated hybrids are crucial for producing consistent and accurate results for large zwitterionic biomolecules (see e.g. <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.4c00712>this recent work from Valeev</a>), so it’s possible that the underlying training data isn’t as accurate as it seems.)
</p>

<p>
The authors find that the resulting NNPs (“GEMS”) perform much better than existing forcefields in terms of overall error metrics: for instance, GEMS has an MAE of 0.45 meV/atom on snapshots of AceAla15Nme structures taken from MD simulations, while Amber has an MAE of 2.27 meV/atom. <b>What’s much more interesting, however, is that GEMS gives significantly different dynamics than forcefields!</b> While Amber simulations of AceAla15Nme predict that a stable α-helix will form at 300 K, GEMS predicts that a mixture of α- and 3<sub>10</sub> helices exist, which is exactly what’s seen in Ala-rich peptides experimentally. The CHARMM and GROMOS forcefields also get this system wrong, suggesting that GEMS really is significantly more accurate than forcefields at modeling the structure of peptides.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20241010_ala15.png" style="width:550px;" />
  <figcaption>
    Amber-based simulations stay in one configuration, while GEMS-based simulations are significantly more flexible. 
  </figcaption>
</figure>

<p>
The authors next study crambin, a small 46-residue protein which is frequently chosen as a model system in papers like this. Similar to what was seen with the Ala<sub>15</sub> helices, crambin is significantly more flexible when modeled by GEMS than when modeled with Amber (see below figure). The authors conduct a variety of other analyses, and argue that there are “qualitative differences between simulations with conventional FFs and GEMS on all timescales.” <b>This is an incredibly significant result, and one that casts doubt on literal decades of forcefield-based MD simulations.</b> Think about what this means for <a href=https://www.dennisgong.com/blog/Relay/>Relay’s MD-based platform</a>, for instance!
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20241010_crambin.png" style="width:550px;" />
  <figcaption>
A UMAP plot of protein motion through conformational space. (Yes, <a href=https://x.com/lpachter/status/1431325969411821572>we all know UMAP is bad</a>, but this is still a nice plot!)
  </figcaption>
</figure>

<p>
Why do Amber and GEMS differ so much here? Here’s what Unke and coworkers think is going on:
</p>

<blockquote>
AmberFF is a conventional FF, and as such, models bonded interactions with harmonic terms. Consequently, structural fluctuations on small timescales are mostly related to these terms. Intermediate-scale conformational changes as involved in, for example, the “flipping” of the dihedral angle in the disulfide bridges of crambin, on the other hand, can only be mediated by (nonbonded) electrostatic and dispersion terms, because the vast majority of (local) bonded terms stay unchanged for all conformations. On the other hand, GEMS makes no distinction between bonded and non-bonded terms, and individual contributions are not restricted to harmonic potentials or any other fixed functional form. Consequently, it can be expected that large structural fluctuations for AmberFF always correspond to “rare events” associated with large energy barriers, whereas GEMS dynamics arise from a richer interplay between chemical bonds and nonlocal interactions.
</blockquote>

<p>
The overall idea that (1) forcefields impose an unphysical distinction between bonded and non-bonded interactions, and (2) this distinction leads to strange dynamical effects makes sense to me. There’s parts of this discussion that I don’t fully understand—what’s to stop a large structural fluctuation in Amber from having a small barrier? Aren’t all high-barrier processes “rare events” irrespective of where the barrier comes from?
</p>

<p>
There are some obvious caveats here that mean this sort of strategy isn’t ready for widespread adoption yet. These aren’t foundation models; the authors create a new model for each peptide and protein under study by adding system-specific fragments to the training data and retraining the NNP. This takes “between 1 and 2 weeks, depending on the system,” not counting the cost of running all the DFT calculations, so this is far too expensive and slow for routine use. While this might seem like a failure, I think it’s worth reflecting on how tough this problem is. Crambin alone has thousands of degrees of freedom, not counting the surrounding water molecules, and accurately reproducing the results of the Schrodinger equation for this system is an incredible feat. The fact that we can’t automatically also solve this problem in a zero-shot manner for every other protein is hardly a failure, particularly because it seems very likely that scaling these models will dramatically improve their generalizability! 
</p>

<p>
The other big limitation is inference speed: the SpookyNet-based NNPs are about 250x slower than a conventional forcefield, so it’s much tougher to access the long timescales that are needed to simulate processes like protein folding. There are a lot of techniques that can help address these problems: <a href=https://arxiv.org/abs/2409.01931>NNPs can become faster</a> and not require system-specific retraining, <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jpcb.3c05928>coarse graining</a> can reduce the number of particles in the system, and <a href=https://arxiv.org/abs/2208.01893>Boltzmann generators</a> can reduce the number of evaluations needed. So the future is bright, but there’s clearly a lot of ML engineering and applied research that will be needed to help NNP-based simulations scale.
</p>

<p>
But overall, I think this is a very significant piece of work, and one that should make anyone adjacent to forcefield-based MD pause and take note. One day it will be possible to run simulations like this just as quickly as people run regular MD simulations today, and I can’t wait to see what comes of that.
</p>

<p>
<i>
Thanks to Abhishaike Mahajan for helpful feedback on this post.
</i>
</p>

]]></description>
              <pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Robots Won&#39;t Solve Organic Synthesis</title>
              <link>public/blog/20240917_robotics.html</link>
              <description><![CDATA[
<p>
Abhishaike Mahajan recently wrote <a href=https://www.owlposting.com/p/generative-ml-in-chemistry-is-bottlenecked>an excellent piece</a> on how generative ML in chemistry is bottlenecked by synthesis (<i>disclaimer: I gave some comments on the piece, so I may be biased</i>). One of the common reactions to this piece has been that self-driving labs and robotics will soon solve this problem—this is a pretty common sentiment, and one that I’ve heard a lot. 
</p>

<p>
Unfortunately, I think that the strongest version of this take is wrong: organic synthesis won’t be “solved” by just replacing laboratory scientists with robots, because (1) figuring out what reactions to run is hard and (2) running reactions is even harder and (3) we need scientific advances to fix this, not just engineering. 
</p>

<h2>
Predicting What Reactions To Run Is Hard
</h2>

<p>
Organic molecules are typically made through a sequence of reactions, and figuring out how to make a molecule involves both the strategic question of which reactions to run in what order and the tactical question of how to run each reaction. 
</p>

<p>
There’s been a ton of work on both of these problems, and it’s certainly true that computer-assisted retrosynthesis tools have come a long way in the last decade! But retrosynthesis is one of those problems that’s (relatively) easy to be good at and almost impossible to be great at. In part, this is because data in this field tends to be very bad: publications and patents are full of irreproducible or misreported reactions, and negative results are virtually never reported. (<a href=https://www.science.org/content/blog-post/sorta-artificial-intelligence>This post</a> by Derek Lowe is a good overview of some of the problems that the field faces.)
</p>

<p>
But also, the problems are just hard! I got the chance to try out one of the leading retrosynthesis software packages back in my career as an organic chemist, and when we fed it some of the tough synthetic problems we were facing, it gave us all the logical suggestions that we had already tried (unsuccessfully) and then began suggesting insane reactions to us. I can’t really blame the model for not being able to invent new chemistry—but this illustrates the limits of what pure retrosynthesis can accomplish, absent new scientific discoveries.
</p>

<p>
The tactical problem of optimizing reaction conditions is also difficult. In cases where there are a lot of continuous variables (like temperatures or concentrations), conventional optimization methods like design-of-experiments can work well—but where reagents or catalysts are involved, optimization becomes significantly more challenging. Lots of cheminformatics/small-data ML work has been done in this area, but it’s still not straightforward to reliably take a reaction drawn on paper and get it to work in the lab. 
</p>

<h2>
Running Reactions Is Even Harder
</h2>

<p>
All of the above problems are, in principle, solvable. Where I think robotics is likely to struggle even more is in the actual execution of these routes. Synthetic organic chemistry is an arcane and specialized craft that typically requires at least five years of training to be decent at—most published reaction procedures assume that the reader is themselves a trained organic chemist, and omit most of the “obvious” details that are needed to unambiguously specify a sequence of steps. (The incredibly detailed procedures in <a href=https://www.orgsyn.org/><i>Organic Syntheses</i></a> illustrate just how much is missing from the average publication.)
</p>

<p>
My favorite illustration of how irreproducible organic chemistry can be is BlogSyn, a brief project that aimed to anonymously assess how easily published reactions could be reproduced. <a href=https://blog-syn.blogspot.com/2013/02/blog-syn-002-pd-catalyzed-c-3-selective.html>The second BlogSyn post</a> found that a reported olefination of pyridine could not be reproduced—the original author of the paper, Jin-Quan Yu (Scripps) responded, and the shape of reaction tube was ultimately found to be critical to reaction success.
</p>

<p>
<a href=https://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html>The third BlogSyn post</a> found that an IBX-mediated benzylic oxidation reported by Phil Baran (also of Scripps) could not be reproduced at all as written. Phil and his co-authors responded pretty aggressively, and after several weeks of back-and-forth <a href=https://blog-syn.blogspot.com/2013/03/blog-syn-003a-secret-ingredient.html>it was ultimately found</a> that the reaction could be reproduced after modifying virtually every parameter. A comment from Phil’s co-author Tamsyn illustrates some of the complexities at play:
</p>

<blockquote>
There is in [BlogSyn’s] discussion a throw away comment about the 2-methylnaphthalene not being volatile. Have you never showered and then left your hair to dry at room temperature? – water evaporates at RT, just as 2-methylnaphthalene does at 95 ºC. I suggest to you that at the working temperatures of this reaction, the biggest problem may be substrate evaporation (or “hanging out” on the colder parts of the flask as Phil said)... We need fluorobenzene to reflux in these reactions and in so-doing wash substrate back into the reaction from the walls of the vessel, but it clearly slows/inhibits the reaction also – so, we need to tune this balance carefully and with patience. Scale will have a big influence on how well this process works.
</blockquote>

<p>
Tamsyn is, of course, right—volatile substrates can evaporate, and part of setting up a reaction is thinking about the vapor pressure of your substrates and how you can address this. But this sort of thinking requires a trained chemist, and isn’t easily automated. There are a million judgment calls to make in organic synthesis—what concentration to use, how quickly to add the reagent, how to work up the reaction, what extraction solvent to use, and so on—and it’s hard enough to teach first-year graduate students how to do all this, let alone robots. Perhaps at the limit as robots achieve AGI this will be possible, but for now these remain difficult problems. 
</p>

<h2>
We Need Scientific Advances To Fix This
</h2>

<p>
What can be done, then? 
</p>

<p>
From a synthetic point of view, we need more robust reactions. Lots of academics work on reaction development, but the list of truly reliable reactions remains miniscule: amide couplings, Suzuki couplings, addition to Ellman auxiliaries, SuFFEx chemistry, and so on. From a practical point of view, every reaction like this is worth a thousand random papers with a terrible substrate scope (<a href=https://onlinelibrary.wiley.com/doi/full/10.1002/1521-3773%2820010601%2940%3A11%3C2004%3A%3AAID-ANIE2004%3E3.0.CO%3B2-5>Sharpless said it better in 2001 than I ever could</a>; see also <a href=https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409>this 2015 study</a> about how basically no new reactions are used in industry). Approaches like skeletal editing are incredibly exciting, but there’s a limit to how impactful any non-general methodology can be.
</p>

<p>
Perhaps even more important is finding better methods for reaction purification. Purification is one of those topics which doesn’t get a lot of academic attention, but being able to efficiently automate purification unlocks a whole new set of possibilities. Solid-phase synthesis (which makes purification as simple as rinsing off some beads) has always seen some amount of use in organic chemistry, but a lot of commonly-used reactions aren’t compatible with solid support: either new supports or new reactions could address this problem. There are also cool approaches like Marty Burke’s <a href=https://www.nature.com/articles/s44160-024-00558-w?fromPaywallRec=false>“catch-and-release”</a> boronate platform which haven’t yet seen broad adoption.
</p>

<p>
Ultimately, I share the dream of the robotics enthusiasts: if we’re able to make organic synthesis routine, we can stop worrying about how to make molecules and start thinking about what to make! I’m very optimistic about the opportunity of new technologies to address synthetic bottlenecks and enable higher-throughput data generation in chemistry. But getting to this point will take not only laboratory automation but also a ton of scientific progress in organic chemistry, and the first step in solving these problems is actually taking them seriously and recognizing that they’re unsolved.
</p>

<i>
  Thanks to Abhishaike Mahajan and Ari Wagen for helpful comments about this post.
</i>
]]></description>
              <pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Choosing Maximally Dissimilar Molecules</title>
              <link>public/blog/20240710_dissimilar_clustering.html</link>
              <description><![CDATA[
<p>
I've been playing around with generating non-equilibrium conformations by molecular dynamics recently, and I've been thinking about how to best parse the outputs of a dynamics simulation. A technique I've seen quite often in the literature is "choose a dissimilar subset of conformers by RMSD"—for instance, here's what <a href=https://www.nature.com/articles/s41597-022-01882-6>the SPICE paper</a> says: 
</p>

<blockquote>
  For each of the 677 molecules, the dataset includes 50 conformations of which half are low energy and half are high energy. To generate them, RDKit 2020.09.3 was first used to generate 10 diverse conformations. Each was used as a starting point for 100 ps of molecular dynamics at a temperature of 500 K using OpenMM 7.6 and the Amber14 force field. A conformation was saved every 10 ps to produce 100 candidate high energy conformations. <b>From these, a subset of 25 was selected that were maximally different from each other as measured by all atom RMSD.</b>
</blockquote>

<p>
This makes a good amount of sense: you want to choose conformers which cover as much chemical space as possible so that you get information about the PES as efficiently as possible, and RMSD is a cheap and reasonable way to do this. But how do you actually do this in practice? Nothing super helpful came up after a quick Google search, so I wrote a little script myself:
</p>

<pre class=code-block>
import cctk
import numpy as np
from sklearn.cluster import AgglomerativeClustering
import sys
import tqdm
import copy

e = cctk.XYZFile.read_file(sys.argv[1]).ensemble
molecules = e.molecule_list()

rmsd_matrix = np.zeros((len(molecules), len(molecules)))
comparison_atoms = molecules[0].get_heavy_atoms()

def compute_rmsd(mol1: cctk.Molecule, mol2: cctk.Molecule) -> float:
    geom1 = copy.deepcopy(mol1.geometry[comparison_atoms])
    geom1 -= geom1.mean(axis=0)

    geom2 = copy.deepcopy(mol2.geometry[comparison_atoms])
    geom2 -= geom2.mean(axis=0)

    return cctk.helper_functions.compute_RMSD(geom1, geom2)


for i in tqdm.tqdm(range(len(molecules))):
    for j in range(i + 1, len(molecules)):
        rmsd_matrix[i, j] = compute_rmsd(molecules[i], molecules[j])
        rmsd_matrix[j, i] = rmsd_matrix[i, j]

clustering = AgglomerativeClustering(
    n_clusters=50,
    metric="precomputed",
    linkage="average"
)
clustering.fit(rmsd_matrix)

selected_molecules: list[int] = []
for cluster_id in range(50):
    cluster_indices = np.where(clustering.labels_ == cluster_id)[0]
    selected_molecule = cluster_indices[
        np.argmin(rmsd_matrix[cluster_indices].sum(axis=1))
    ]
    selected_molecules.append(selected_molecule)

e2 = cctk.ConformationalEnsemble()
for idx in selected_molecules:
    e2.add_molecule(molecules[idx])

cctk.XYZFile.write_ensemble_to_file(sys.argv[2], e2)
print("done!")
</pre>

<p> 
This script uses <a href=https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>agglomerative clustering</a> to sort conformations into clusters, but could easily be adapted to work with other algorithms.
To run this script, simply paste into into a file (<span class=code>choose_dissimilar.py</span>) and run:
</p>

<pre class=code-block>
python choose_dissimilar.py input.xyz output.xyz
</pre>

<p>
This will dump 50 output conformers into <span class=code>output.xyz</span>. Hopefully this saves someone some time... happy computing!
</p>
]]></description>
              <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Running Simple MD Simulations</title>
              <link>public/blog/20240613_simple_md.html</link>
              <description><![CDATA[
<p>
Scientific software can be confusing, particularly when you're doing something that the software isn't primarily intended for. 
I often find myself wanting to run quick-and-dirty molecular dynamics simulations on small organic molecules, but I've struggled to find an easy way to do this using open-source tools like OpenMM. 
</p>

<p>
This is particularly frustrating since I feel like I should be equipped to succeed at this task: 
</p>

<ul>
<li>I know how to code (well enough for a scientist).</li>
<li>I've read <a href=https://www.amazon.com/Understanding-Molecular-Simulation-Applications-Computational/dp/0122673514>a textbook on MD</a> cover-to-cover.</li>
<li>I've even written <a href=https://github.com/corinwagen/presto>my own MD package</a> from scratch.</li>
</ul>

<p>
Yet despite all this, I've probably tried and failed to use OpenMM for my research a half-dozen times over the past five years (<a href=https://github.com/openmm/openmm/issues/2937>evidence</a>). I always get bogged down somewhere: I don't have PDB files for my compounds, or I can't figure out how to get the forcefield parameters right for a small molecule, or I just get lost in a sea of similar-sounding classes and objects. Part of the problem here is that all the "intro to MD" tutorials seem to assume that you're hoping to run an MD simulation on a protein from the PDB—so if you have an .xyz file, it's not obvious how to proceed.
</p>

<p> Nevertheless, I've finally succeeded. Here's the code, with minor annotations interspersed:</p>

<pre class=code-block>
from openff.toolkit import Molecule, Topology

from openmm import *
from openmm.app import *

import nglview
import mdtraj
import matplotlib.pyplot as plt
import numpy as np
import openmoltools
import tempfile
import cctk
import openmmtools
import math
from random import random, randint

from sys import stdout
import pandas as pd

from rdkit import Chem
from rdkit.Chem import AllChem

from openmmforcefields.generators import SMIRNOFFTemplateGenerator

%config InlineBackend.figure_format='retina'
</pre>

<p> 
Already, we're off to a complex start: we need OpenFF, OpenMM, <span class=code>openmoltools</span>, <span class=code>openmmtools</span>, and <span class=code>openmmforcefields</span>
(not to mention <span class=code>nglview</span> and <span class=code>mdtraj</span>). There's a broader point to be made here about the state of scientific software and how this relates to academic incentive structure, but I digress...
</p>

<pre class=code-block>
smiles = "c1cc(F)ccc1O"

def generate_forcefield(smiles: str) -&gt; ForceField:
    """ Creates an OpenMM ForceField object that knows how to handle a given SMILES string """
    molecule = Molecule.from_smiles(smiles)
    smirnoff = SMIRNOFFTemplateGenerator(molecules=molecule)
    forcefield = ForceField(
      'amber/protein.ff14SB.xml',
      'amber/tip3p_standard.xml',
      'amber/tip3p_HFE_multivalent.xml'
     )
    forcefield.registerTemplateGenerator(smirnoff.generator)
    return forcefield

def generate_initial_pdb(
    smiles: str,
    min_side_length: int = 25, # Å
    solvent_smiles = "O",
) -&gt; PDBFile:
    """ Creates a PDB file for a solvated molecule, starting from two SMILES strings. """

    # do some math to figure how big the box needs to be
    solute = cctk.Molecule.new_from_smiles(smiles)
    solute_volume = solute.volume(qhull=True)
    solvent = cctk.Molecule.new_from_smiles(solvent_smiles)
    solvent_volume = solvent.volume(qhull=False)

    total_volume = 50 * solute_volume # seems safe?
    min_allowed_volume = min_side_length ** 3
    total_volume = max(min_allowed_volume, total_volume)

    total_solvent_volume = total_volume - solute_volume
    n_solvent = int(total_solvent_volume // solvent_volume)
    box_size = total_volume ** (1/3)

    # build pdb
    with tempfile.TemporaryDirectory() as tempdir:
        solute_fname = f"{tempdir}/solute.pdb"
        solvent_fname = f"{tempdir}/solvent.pdb"
        system_fname = f"system.pdb"

        smiles_to_pdb(smiles, solute_fname)
        smiles_to_pdb(solvent_smiles, solvent_fname)
        traj_packmol = openmoltools.packmol.pack_box(
          [solute_fname, solvent_fname],
          [1, n_solvent],
          box_size=box_size
         )
        traj_packmol.save_pdb(system_fname)

        return PDBFile(system_fname)

def smiles_to_pdb(smiles: str, filename: str) -&gt; None:
    """ Turns a SMILES string into a PDB file (written to current working directory). """
    m = Chem.MolFromSmiles(smiles)
    mh = Chem.AddHs(m)
    AllChem.EmbedMolecule(mh)
    Chem.MolToPDBFile(mh, filename)

forcefield = generate_forcefield(smiles)
pdb = generate_initial_pdb(smiles, solvent_smiles="O")

system = forcefield.createSystem(
    pdb.topology,
    nonbondedMethod=PME,
    nonbondedCutoff=1*unit.nanometer,
)
</pre>

<p> This code turns a SMILES string representing our molecule into an OpenMM System, which is a core object in the OpenMM ecosystem. To do this, we have to do a lot of shenanigans involving figuring out how many solvent molecules to add, calling <a href=https://m3g.github.io/packmol/>PACKMOL</a>, etc. One of the key steps here is the <span class=code>SMIRNOFFTemplateGenerator</span> (documented <a href=https://github.com/openmm/openmmforcefields>here</a>), which uses one of the recent OpenFF forcefields to describe our chosen molecule.
</p>


<pre class=code-block>
# initialize Langevin integrator and minimize
integrator = LangevinIntegrator(300 * unit.kelvin, 1 / unit.picosecond, 1 * unit.femtoseconds)
simulation = Simulation(pdb.topology, system, integrator)
simulation.context.setPositions(pdb.positions)
simulation.minimizeEnergy()

# we'll make this an NPT simulation now
system.addForce(MonteCarloBarostat(1*unit.bar, 300*unit.kelvin))
simulation.context.reinitialize(preserveState=True)

checkpoint_interval = 100
printout_interval = 10000

# set the reporters collecting the MD output.
simulation.reporters = []
simulation.reporters.append(DCDReporter("traj_01.dcd", checkpoint_interval))
simulation.reporters.append(
    StateDataReporter(
        stdout,
        printout_interval,
        step=True,
        temperature=True,
        elapsedTime=True,
        volume=True,
        density=True
    )
)

simulation.reporters.append(
    StateDataReporter(
        "scalars_01.csv",
        checkpoint_interval,
        time=True,
        potentialEnergy=True,
        totalEnergy=True,
        temperature=True,
        volume=True,
        density=True,
    )
)

# actually run the MD
simulation.step(500000) # this is the number of steps, you may want fewer to test quickly
</pre>

<p>
Here, we configure the settings that will be familiar to people who read MD textbooks. I've chosen to use a Langevin integrator/thermostat to control temperature, and added a Monte Carlo barostat to make this an NPT simulation (constant pressure &amp; temperature). This isn't appropriate for all uses, but it means we don't have to worry about getting the box size exactly right. We also configure how we're going to save data from the simulation, and then the last line actually runs it (this might take a while if you don't have a GPU). 
<p>

<p> Once we've run this simulation, we can visualize the output and watch things wiggle around! The MDTraj trajectory is <a href=https://www.mdtraj.org/1.9.8.dev0/examples/index.html>a pretty versatile object</a> so you can do much more analysis here if you want to.</p>

<pre class=code-block>
# build MDTraj trajectory
t = mdtraj.load("traj_01.dcd", top="init_01.pdb")

# visualize the trajectory - this works in jupyter at least
view = nglview.show_mdtraj(t)
view.clear_representations()
view.add_licorice()
view.add_unitcell()
view
</pre>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240613_nglview.png style="width:550px;" />
  <figcaption>NGLView is actually pretty great, except if you're forming or breaking bonds.</figcaption>
</figure>

<p>It's also good to check that the thermostat actually worked and nothing blew up:</p>

<pre class=code-block>
# check that the thermostat is working
df = pd.read_csv("scalars_01.csv")
df.plot(kind="line", x='#"Time (ps)"', y="Temperature (K)")
</pre>

<p>This is probably laughably simple to an OpenMM expert, but it works well enough for me. Here's my current list of things I still don't know how to do: </p>

<ul>
<li>Start with a specific set of 3D coordinates, not just a SMILES string. This is my biggest issue: it's not easy to specify the geometry of a supramolecular complex using SMILES strings, and while I feel that I ought to be able to modify one of these objects and supply the coordinate information directly, I'm not sure exactly how. (Relatedly, I've never gotten the hang of PDB files for small molecules.)
<li>Fit specific torsions or interactions to high-level data—I see people do this in the literature, but I don't know how.</li>
<li>Frankly, run any more interesting sorts of simulations! Normal MD is way too slow for a lot of things, but enhanced sampling methods get complicated fast: I've done WHAM using my own software, but I don't really know how to do WHAM or metadynamics in other software packages.</li>
<li>Relatedly, I've never gotten the hang of <a href=https://www.plumed.org/>PLUMED</a>.</li>
<li>Build or validate models of non-aqueous solvents. Can I just use this workflow to study, e.g., liquid HF? And how do I figure out if I have the right dipole moment or boiling point for liquid HF? </li>
</ul>

<p>Still, I hope this script is a useful asset to the community, and helps other people not make the same mistakes I did.</p>

<i>Thanks to Dominic Rufa for answering some of my stupid questions.</i>
]]></description>
              <pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Generative Linker Design</title>
              <link>public/blog/20240510_difflinker.html</link>
              <description><![CDATA[
<p>
<b>
  Update: As of October 2024, you can now run DiffLinker calculations through <a href=rowansci.com>Rowan</a>, my computational chemistry startup. Read more about this in <a href=https://rowansci.substack.com/p/difflinker-mmff94-and-common-dft>our newsletter</a>!
</b>
</p>

<p>
Much molecular design today can be boiled down to “put the right functional groups in exactly the right places.” In catalysis, proper positioning of functional groups to complement developing charge or engage in other stabilizing non-covalent interactions with the transition state can lead to vast rate accelerations. A classic demonstration of this is Uyeda and Jacobsen’s <a href=https://pubs.acs.org/doi/abs/10.1021/ja110842s>enantioselective Claisen rearrangement</a>, where a simple catalyst presents a guanidinium ion to stabilize an anionic region and an electron-rich arene to stabilize a cationic region. Together, these interactions lead to high enantioselectivity and a 250-fold rate increase over the background reaction.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_uyeda.png" style="width:500px;" />
  <figcaption>I freely admit this choice of example is biased, but this is a great paper. </figcaption>
</figure>

<p>
While putting the right functional groups in the right positions might sound easy, the underlying interactions are often exquisitely sensitive to distance, which makes finding the right molecular scaffold very challenging. Jeremy Knowles put this nicely in his <a href=https://www.nature.com/articles/350121a0>1991 perspective on enzyme catalysis</a>:
</p>

<blockquote>
Although it is too early to generalize, it is evident that in this case <i>[triose phosphate isomerase]</i> at least, the positioning of functionality at the active site of the enzyme needs to be quite precise if full catalytic potency is to be realized… The good news for catalyst engineers is that proper placement of appropriate groups in the right environment seems to be enough. The not-so-good news is that this placement must be very precise.
</blockquote>

<p>
Proper positioning of various groups isn’t just a problem in catalysis—it’s also very important in drug design. Lots of topics in medicinal chemistry essentially boil down to a variant of the positioning problem: 
</p>

<ol>
<li>
Recent years have seen an explosion in the number of arene bioisosteres, i.e. “ways to connect two groups the same distance apart as an arene would without using any aromatic rings.” This is nice because you can use these bioisosteres to e.g. tune biophysical properties, like lipophilicity, solubility, and membrane permeability, while ideally not affecting the actual conformation of the molecule too much. Here’s a graphic from Chris Swain showing some options for replacing a 1,4-disubstituted arene:

<figure>
  <img class=centered-img src="https://www.cambridgemedchemconsulting.com/resources/bioisoteres/aromatic_bioisosteres_files/aromaticbicycloreps.png" style="width:400px;" />
  <figcaption>Taken from <a href=https://www.cambridgemedchemconsulting.com/resources/bioisoteres/aromatic_bioisosteres.html>this page</a>.</figcaption>
</figure>

(There's lots of work on this; see also the Baran Lab's <a href=https://baranlab.org/wp-content/uploads/2020/11/Bioisosteres-v2-Recent-Trends-and-Tactics.pdf>great overview of bioisosteres</a>.)

</li>
<li>


“Scaffold hopping” generally means switching the core of a molecule while preserving key interactions and substituents (<a href=https://www.cresset-group.com/about/news/what-does-scaffold-hopping-mean-to-you/>although the term is a little vague</a>). Scaffold hops often look for new structures that don’t suffer from the ADME/toxicity liabilities of the original structure, which necessitates finding a new core that positions substituents in the same way. Here's a nice example of a scaffold hop in a tyrosine kinase inhibitor, from a <a href=https://apps.dtic.mil/sti/tr/pdf/ADA572958.pdf>review</a> by Hongmao Sun and co-workers:

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_tyk.png" style="width:550px;" />
  <figcaption>(This is Figure 8 in the reference.)</figcaption>
</figure>

</li>
<li>

Linker design has become incredibly important for PROTACs and other heterobifunctional molecules, as choosing a good linker is often key to drug efficacy (<a href=https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/cmdc.202200615>e.g.</a>). Unfortunately there are a vast variety of potential linkers, and it’s far from obvious to figure out which one will best balance, affinity, bioavailability, stability, and other properties:

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_vhl.png" style="width:550px;" />
  <figcaption>Taken from <a href=https://www.sciencedirect.com/science/article/pii/S0968089623001827>this review</a>.</figcaption>
</figure>

</li>
<li>

One of the toughest parts of fragment-based drug design is finding the right way to connect fragments which bind to different parts of the desired pocket. Here’s what <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6930586/>Philine Kirsch and co-workers</a> have to say about this:

<blockquote>
Finding the right linker motif, which orients the individual fragment units in the favourable geometry in relation to each other without introducing too much flexibility whilst maintaining the binding poses of both fragments, can be very challenging. If successful, the combination of two fragments with rather low affinity could result in significantly higher affinity and has the potential to result in “superadditive” contributions of both binding motifs. The challenge in fragment linking is the exploration of the binding mode of both fragments and the identification of an optimal linker. Only in this case, the overall reduced so-called rigid body entropy translates into synergistically-improved affinity.
</blockquote>

</li>
</ol>

<p>
What’s hard about all these positioning problems is that finding a molecule that orients substituents in a given way is incredibly non-obvious: molecules are inherently discrete and atomic, making it hard to change a distance or angle by a precise percent. You can have two carbon atoms between your substituents, or you can have three carbon atoms, but you can’t have 2.5 carbon atoms. This makes prospective design very challenging: I can model my protein’s active site and figure out that I want a an <i>ortho</i>-pyridyl substituent and a tetrazole 8 Å apart at a 30º angle, but working backwards to an actual scaffold almost always requires a lot of trial and error. 
</p>

<p>
A <a href=https://www.nature.com/articles/s42256-024-00815-9>recent paper</a> from Ilia Igashov and co-workers sets out to solve exactly this “inverse design” problem: given two substituents, can we use ML to find a linker that connects them in the desired orientation? Their solution is DiffLinker, a diffusion-based method that takes separate atomic fragments and generates a linker that connects them.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_difflinker.png" style="width:500px;" />
</figure>

<p>
There’s been other work in this area, but the DiffLinker authors argue that their model stands out in a few ways. DiffLinker generally produces more synthetically accessible and drug-like molecules than competitor methods, although the relative ranking of models does change significantly from benchmark to benchmark. Also, they’re not limited to joining pairs of molecule structures: DiffLinker can perform “one-shot generation of the linker between any arbitrary number of fragments,” which lets them vastly outperform other models when linking three or more fragments. 
</p>

<p>
For cases where fragments must be joined in a protein pocket, the authors train a pocket-conditioned model, and show that this model results in many fewer clashes than an unconstrained model. They can use this model to recapitulate known drug structures, which they demonstrate with a known HSP90 inhibitor derived from molecular fragments. (It’s worth noting that the authors got the desired inhibitor structure only 3 times out of 1000 DiffLinker predictions.) They also show that their protein-conditioned model produces molecules that have good binding affinity as assessed by docking (GNINA/Vina), with the huge caveat that docking scores are notoriously inaccurate.
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_hsp90.png" style="width:500px;" />
  <figcaption><b>a</b> is the experimental fragments, <b>b</b> is the input to DiffLinker, <b>c</b> is the experimental inhibitor, and <b>d</b> is the DiffLinker-generated inhibitor. Not bad!</figcaption>
</figure>

<p>
There’s still plenty of work that needs to be done here: for instance, the authors readily acknowledge that PROTACs are still too challenging:
</p>

<blockquote>
While DiffLinker effectively suggests diverse and valid chemical structures in tasks like fragment linking and scaffold hopping, we have observed that generating relevant linkers for PROTAC-like molecules poses a greater challenge. The main difference between these problems lies on the linker length and the distance between the input fragments. While the average linker size in our training sets is around 8 atoms (5 for ZINC, 10 for GEOM, 10 for Pockets), a typical linker in a PROTAC varies between 12 and 20 atoms. It means that the distribution of linkers in PROTACs has different characteristics compared to the distributions of linkers provided in our training sets. Therefore, to improve the performance of DiffLinker in PROTAC design, one may consider retraining the model using more suitable PROTAC data.
</blockquote>

<p>
DiffLinker is open-source and comes with pre-trained models, so I played around with it a bit myself to see how well it worked. I sketched out a classic <i>meta</i>-terphenyl scaffold, deleted the central phenyl ring, and then asked DiffLinker to connect the now-separated phenyl rings. I was hoping that DiffLinker would come up with one of <a href=https://enamine.net/building-blocks/medchem/saturated-bioisosteres-of-meta-substituted-benzene>Enamine’s cool suggestions</a> for <i>meta</i>-arene bioisosteres, but in all five cases I just got back some variant on a benzene ring… which isn’t surprising in hindsight. 
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240510_rowan.png" style="width:400px;" />
  <figcaption>DiffLinker also doesn't output hydrogens, which is a little annoying.</figcaption>
</figure>

<p>
Although I don’t think this version of DiffLinker is going to replace humans at any of the tasks I talked about above, this still seems like a pretty cool direction for generative chemical ML. I’m excited to see future versions of methods like DiffLinker that are able to generate predictions conditioned on other molecular properties to allow for guided exploration of molecular space. (For instance, it would have been nice to request fragments that were three-dimensional above, so as to avoid getting boring benzenes back.)
</p>

<p>
I also suspect that DiffLinker, like other generative chemical models, will increase the demand for accurate physics-based methods for refining and validating the output predictions. DiffLinker’s grasp of potential energy surfaces is presumably worse than DFT or other dedicated ML potentials, and a hybrid workflow where DiffLinker generates structures and a higher-quality method optimizes and scores them will probably be much more accurate than just DiffLinker alone. Generative AI is having a moment right now, but for better or worse I think “classic” molecular simulation is here to stay too. 
</p>

]]></description>
              <pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Molecular Symmetry Analysis Made Easy</title>
              <link>public/blog/20240425_pymsym.html</link>
              <description><![CDATA[
<p>
Pure mathematics has all sorts of unexpected connections to other fields, and chemistry is no exception. One example of this is group theory: while I never delved deeply enough into math to actually study group theory as its own field, I've had to learn how to assign <a href=https://en.wikipedia.org/wiki/Point_groups_in_three_dimensions>point groups</a> to three-dimensional objects for several inorganic chemistry classes. This process, demonstrated below for water, basically entails finding all of the possible symmetry operations for a given molecule:
</p>

<figure>
  <img class=centered-img src="https://corinwagen.github.io/public/img/20240425_water_c2v.png" style="width:400px;" />
  <figcaption>Finding the point group of water.</figcaption>
</figure>

<p>
This might seem arcane but becomes quite important in several contexts. In computational chemistry, proper consideration of point groups and their corresponding symmetry numbers is needed to handle entropic effects correctly. Dan Singleton makes this point forcefully in his <a href=https://pubs.acs.org/doi/10.1021/ja5111392>2015 study of the Baylis–Hillman reaction</a> (SI pp. S24–S25):
</p>

<blockquote>
For an entropy calculation to be properly compared with experimental observations, it should allow for a
series of entropy effects that are not included in the entropies calculated from frequencies normally supplied
by electronic structure calculations. This includes allowance for symmetry numbers and the effects of
mixing of structures on entropy. The corrections are usually simple yet they are rarely done in computational
mechanistic studies. A rationalization of this is that the effects are small and often make no difference for
the results of greatest interest in papers. However, the effects can at times be quite large (see for example
Seal, P.; Papajak, E.; Truhlar, D. G. <i>J. Phys. Chem. Lett.</i> <b>2012</b>, <i>3</i>, 264-271). Judging by papers where the
consideration of symmetry numbers and entropy of mixing would make a difference but is ignored (for one 
example, see <i>J. Chin. Chem. Soc.</i> <b>2001</b>, <i>48</i>, 193-200), the ideas are not as widely recognized as needed.
</blockquote>

<p>
Why don't most people take symmetry into account? One reason is that while it's pretty easy to find the point group of a molecule by inspection, it's much harder to figure out how to do it programmatically. I ran into this issue writing code for Rowan, and was really pleased to find <a href=https://github.com/mcodev31/libmsym><i>libmsym</i></a>, a package that automatically finds the point group for a given molecule. (Here's <a href=https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0193-3>the paper</a> describing <i>libmsym</i>.) We've had great results using this library for Rowan's thermochemistry module.
</p>

<p>
Unfortunately, <i>libmsym</i> is now nine years old and we've also had problems with the code: in particular, I recently upgraded from an old Intel MacBook to a new M3 MacBook Pro, and there aren't any prebuild Apple Silicon-compatible wheels for <i>libmsym</i> on Pypi! Since this is an issue which <a href=https://github.com/mcodev31/libmsym/issues/26>other people</a> have also faced with <i>libmsym</i>, and neither the original author nor the listed maintainer have responded to my emails, I decided to just fork the repository and fix this issue myself.
</p>

<p>
It took a bit more work than I was expecting (I ended up completely restructuring the package, rewriting all the CMake files, and moving the Python build to <a href=https://scikit-build-core.readthedocs.io/en/latest/index.html>scikit-build-core</a>), but I'm happy to share the final product, <a href=https://github.com/corinwagen/pymsym><i>pymsym</i></a>. <i>pymsym</i> should be compatible with any modern Linux or Mac architecture (thanks to <a href=https://cibuildwheel.pypa.io/en/stable/options/>cibuildwheel</a>) and can be installed from Pypi. Simply run <span class=code>pip install pymsym</span>.
</p>

<p>
All the original <i>libmsym</i> code is there, and I've also added an additional high-level Python API for quickly predicting point groups and symmetry numbers:
</p>

<pre class=code-block>
import pymsym

# water
atomic_numbers = [8, 1, 1]
positions = [
  [0.007544053252786398, 0.39774343371391296, 0.0],
  [-0.7671031355857849, -0.18439316749572754, 0.0],
  [0.7595590949058533, -0.21335026621818542, 0.0]
 ]

print(pymsym.get_point_group(atomic_numbers, positions)) # C2v
print(pymsym.get_symmetry_number(atomic_numbers, positions) # 2
</pre>

<p>
I hope this is helpful to the community—let me know if you find any bugs!
</p>

]]></description>
              <pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The SolidWorks Model of Simulation</title>
              <link>public/blog/20240325_solidworks.html</link>
              <description><![CDATA[
<p>
<i>Apologies for the long hiatus: we've had some health issues in the family, and startup life has been particularly overwhelming. With any luck, I'll be able to return to a more regular posting frequency soon.</i>
</p>

<p>
What’s the right relationship between theory, computation, and experiment? Much has been written on this. In this piece, I want to put forward an answer that I think is underrated in the life sciences—what I call the “SolidWorks model” of simulation. 
</p>

<p>
For the unfamiliar, SolidWorks is a program which allows engineers to design objects in the computer: the user can create a 3D model of their device, figure out the measurements that allow the parts to fit together in the desired way, and then go into the lab and actually build everything. (I’m not a SolidWorks power user, but I spent a semester messing around with it in high school and I’ve been thinking back on this recently.)
</p>


<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20240325_solidworks.jpg" style="width:550px;" alt="picture of SolidWorks" />
  <figcaption>A screenshot of SolidWorks.</figcaption>
</figure>

<p>
What are the distinctive features of SolidWorks? 
</p>

<ul>
<li>
<b>SolidWorks doesn’t predict overall success.</b> If you’re designing a new gearbox in SolidWorks, you can check that the parts will fit together, but you don’t know if it will run efficiently or not. Modeling is not supposed to replace actually building and testing the part.
</li>
<li>
<b>SolidWorks can be used by normal engineers, not dedicated computer experts.</b> It’s quick and easy to model things, so modeling becomes a part of the regular engineering/design workflow. You can model a part, make it, adjust the model, make it again, and so on and so forth. 
</li>
<li>
<b>SolidWorks assists human intuition instead of trying to replace it.</b> SolidWorks only draws what you tell it to, which means all the ideas still come from humans—to the extent that SolidWorks increases productivity, it does so by helping people understand what they’re working on more clearly. This comports with Richard Hamming’s idea that “the purpose of computing is insight, not numbers.”
</li>
</ul>

<p>
Astute readers will notice differences from how simulations in the life sciences are typically conducted. It’s rare in chemistry or biology to have computations and experiments performed in the same research group, let alone by the same person—but this is crucial to SolidWorks-style simulation, where experimental scientists must quickly gain insight from their computations. If someone from a different team has to get around to answering their request or a job takes overnight to run, the experimental scientist will move on and modeling will be excluded from the design/build/test cycle.
</p>

<p>
SolidWorks-style computation is also prospective, not retrospective. In other words, the goal of the simulation is to generate subsequent experimental hits, not figures for publication, meaning that successful computational studies might never even be reported. This is different from the DFT section of the average organic chemistry paper, which is typically performed by a different team after all experimental results are complete. This isn’t bad, but <i>ex post</i> studies are different from actually using computations <i>ex ante</i> to design molecules.
</p>

<p>
I don’t mean to suggest that the SolidWorks paradigm is objectively correct: there are many ways in which theory, computation, and experiment can usefully interact, and I think it’s great that there are scientists using careful <i>ex post</i> computations to interpret perplexing experimental results or running massive virtual screens to design new molecules entirely <i>in silico</i>. I myself have worked on plenty of projects like this and hope to conduct more in the future.
</p>

<p>
But I do think that SolidWorks-style computation is pretty underrated today. There are few computational tools that non-experts can really use, and the average experimental scientists might not interact with computation even once in an average week (except perhaps when meeting with someone from a different lab or team). Even when experimentalists have the technical skills to run calculations, the friction involved in connecting to a computing cluster, generating input files, monitoring jobs, etc often makes it impractical to really run calculations and experiments in tandem.
</p>

<p>
In fact, I’d argue that the most useful predictive computational tool for organic chemists has probably been the ChemDraw “Predict NMR” button. The predictions are laughably crude by today’s standards, but ChemDraw NMR has a few key advantages: (1) you don’t have to program anything or look at a terminal window to use it, (2) there aren’t any options for end users to mess around with, so you can’t do anything wrong, and (3) it runs instantly from a software package everyone already has, so it fits right into your workflow. These factors are collectively more important than accuracy—ChemDraw NMR is accurate enough to be useful, and far more convenient than fancier approaches. 
</p>

<figure>
  <img class=centered-img src="https://bitesizebio.com/wp-content/uploads/2016/11/Figure-6-NMR-Prediction.jpg" style="width:450px;" />
  <figcaption>A screenshot of ChemDraw's NMR prediction widget.</figcaption>
</figure>

<p>
This seems like a scenario where publication pressure leads to misaligned incentives. Scientific publications emphasize novelty, accuracy, and performance, not pragmatic considerations like “how easy is it to run this software in the middle of the workday” or “how confusing are the parameters to understand.” And for pioneering computational workflows that ought not to be run without a deep understanding of the science, that’s probably appropriate. But pragmatic considerations matter for casual users. 
</p>

<p>
If it’s not obvious by now, one of our big visions for Rowan is “SolidWorks for organic chemistry”—to the extent that there are people who are designing and creating new molecules, we think that it’s important that they are able to think intelligently about the molecules that they’re designing. This means making software that can deliver actionable insights while being fast and simple enough for experimentalists to use. While this is a massive project, it’s not impossibly large, and we’re optimistic that Rowan can quickly become helpful to experimental chemists. If you think this vision is exciting and have ideas for how we can bring it to life, let us know!
</p>

]]></description>
              <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Physical Organic Chemistry: Alive or Dead?</title>
              <link>public/blog/20240105_phys_org.html</link>
              <description><![CDATA[
<p>
In <a href=https://corinwagen.github.io/public/blog/20240103_norbornyl.html>Wednesday’s post</a>, I wrote that “traditional physical organic chemistry is barely practiced today,” which <a href=https://twitter.com/FindlaterGroup/status/1742734408782876949>attracted some controversy on X</a>. Here are some responses:
</p>

<ul>
<li>
“POC has evolved in many directions and its concepts are widely used, e.g., in host-guest chem, org syn,  materials sci, drug discovery.” - <a href=https://twitter.com/JorgensenWL/status/1742963638351089825>Bill Jorgensen</a>
</li>
<li>
“There is still a lot of absolutely gorgeous classical phys org done with organometallic and enzymatic reactions. The molecules have changed, the ideas are the same.” - <a href=https://twitter.com/dasingleton/status/1742752687853056140>Dan Singleton</a>
</li>
<li>
“It makes no sense. Looking at orbitals, measuring KIE, studying mechanisms, all traditional PhysOrgChem. And ML in OrgChem is nothing more than Curtin-Hammett on steroids.” - <a href=https://twitter.com/KozuchSebastian/status/1742795358680293550>Sebastian Kozuch</a>
</li>
<li>
“Hard core classical phys-org maybe, but structure-activity relationships are alive and well across all chemistry disciplines. These have their origin in phys-org and use so many fundamental principles I learned as a PhD student studying carbocation and free radical reactions.” - <a href=https://twitter.com/RacerPolyLab/status/1742909928291836067>@RacerPolyLab</a>
</li>
<li>
“I've attend 10 Phys Org GRC meetings since 1999 &amp; chaired 2015. I'm OK with ‘traditional phys. org. chemistry’ being barely practiced. It was dying–2001 saw 79 attendees, then the area morphed into phys. org. of organometallics, supramolecular, biomolecules. Fields evolve or die.” - <a href=https://twitter.com/UOHaleyLab/status/1742748763670843722>Michael Haley</a>
</li> 
</ul>

<p>
(There are plenty more responses; if I didn’t list yours, sorry!)
</p>

<p>
For the most part, I agree with these responses. Physical organic thinking has permeated organic chemistry and adjacent fields: George Whitesides has probably <a href=https://onlinelibrary.wiley.com/doi/10.1002/ijch.201500061>the best piece on this topic</a>, in which he argues that the essence of physical organic chemistry is “a general, and remarkably versatile, method for tackling complex problems,” not anything about chemistry per se, and consequently that the physical organic mindset can be applied to problems in all manner of fields. Viewed from this angle, we might say that physical organic chemistry hasn’t disappeared at all—instead, it’s become so commonplace that we forget to acknowledge it as distinctive at all.</p>

<p>
Looking through the organic chemistry curriculum, too, suggests that physical organic chemistry is here to stay. Lots of the ideas that we teach to undergraduates, like molecular orbital theory and structure–activity relationships, were once distinctively the domain of physical organic chemists. Textbooks from before the apotheosis of physical organic chemistry (I have an old copy of Fieser &amp; Fieser, for instance) are structured in a completely different way, not by mechanism but by functional group, while today many undergraduate organic classes discuss S<sub>N</sub>1/S<sub>N</sub>2 mechanisms in their first semester.
</p>

<p>
So, was I entirely wrong to claim that traditional physical organic chemistry is a dying art? I don’t think so. Despite all the successes of physical organic chemistry, it seems to me that something has been lost between the time of the norbornyl cation controversy and today. The sorts of elegant kinetic experimentation and argumentation that Winstein and others employed in their papers are now rare: take, for instance, <a href=https://pubs.acs.org/doi/10.1021/ja01583a022>this famous paper</a> distinguishing between contact ion pairs and solvent-separated ion pairs. How many scientists today still do experiments like this? There are certainly names that come to mind, but from where I sit it seems to be an increasingly niche skillset.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240105_kinetics.png style="width:350px;" />
</figure>

<p>
I don’t want to fall into the trap of idolizing the past for no reason; there are plenty of techniques which have been forgotten by chemistry because there are better ways of doing the same thing today. Chemists used to estimate molecular weight by dissolving a known mass of sample and measuring the boiling point elevation induced. Now we have mass spectrometry, so nobody uses the boiling point method any more, and I don’t see this as a great tragedy.
</p>

<p>
But kinetics, and more generally the sort of careful physical organic chemistry practiced by participants in the norbornyl cation debate, doesn’t seem to have such a simple replacement. Computation is the most obvious candidate, but we’re still a long way away from being able to predict mechanisms accurately <i>in silico</i>; in mechanistic chemistry, <a href=https://pubs.acs.org/doi/pdf/10.1021/ja5111392>experiments still reign supreme</a>. Kinetic isotope effects are much easier to measure than they were back in Winstein’s day, but they’re hardly routine experiments (and easy to get wrong). The rigor and precision with which old-school physical organic chemistry approached mechanistic problems can still be found today, but it seems harder and harder to find.
</p>

<p>
It might have been inevitable that physical organic chemistry was always going to evolve away from incredibly detailed studies of simple reactions on simple molecules—just as biology has largely shifted from ecology and taxonomy to cell biology and biochemistry, organic chemistry too must change in order to keep working on the most interesting problems. And perhaps there's some truth to the argument that the old-school style of painstaking mechanistic study just isn't worth the effort and deserves to be de-emphasized. But it does seem to me that parts of the tradition of physical organic knowledge (to borrow <a href=https://samoburja.com/on-the-loss-and-preservation-of-knowledge/>Samo Burja’s phrasing</a>) is being slowly lost to time, despite the fact that lots of really good physical organic chemistry is still being done today on all sorts of problems (enzymatic chemistry, organometallic chemistry, catalysis, heterogenous catalysis, chemical biology, &amp;c), and that makes me sad.
</p>
]]></description>
              <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Looking Back on the Norbornyl Cation</title>
              <link>public/blog/20240103_norbornyl.html</link>
              <description><![CDATA[
<p>
<i>In this post, I’m trying something new and embedding calculations on Rowan alongside the text. You can view the structures and energies right in the page, or you can follow a link and view the full data in a new tab. While PDFs and printed journals are limited to displaying 2D renditions of 3D structures, there’s no reason why websites should follow suit—and now that all my calculations are already on the web, it’s simple to share the primary data.</i>
</p>

<p>
The 2-norbornyl cation has a special place in the history of physical organic chemistry. <a href=https://pubs.acs.org/doi/abs/10.1021/ja01176a536>In 1949</a>, following up on <a href=https://pubs.rsc.org/en/content/articlelanding/1939/jr/jr9390001188>previous work</a> by Christopher Wilson, the great physical organic chemist Saul Winstein observed that acetolysis of <i>exo</i>-norbornyl sulfonates occurred about 350 times faster than solvolysis of the corresponding <i>endo</i> compounds.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240103_summary.png style="width:550px;" />
  <figcaption>“X” represents a leaving group and “Nu” a nucleophile.</figcaption>
</figure>


<p>
Several stereochemical observations indicated that something puzzling was going on: both the <i>exo</i> and <i>endo</i> sulfonates gave <i>exo</i> acetate product, but enantioenriched <i>exo</i>-norbornyl sulfonate formed racemic <i>exo</i>-norbornyl acetate. Winstein argued that this data was best explained through the participation of an achiral nonclassical carbocation (“<b>II</b>”) featuring σ-delocalization and a three-center two-electron bond, as shown in the conclusion of the 1949 paper:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240103_winstein.png style="width:400px;" />
</figure>

<p>
The nonclassical structure, “<b>II</b>” above, is a little tough to visualize as drawn. Here’s the computed structure at the B3LYP-D3BJ/6-31G(d) level of theory, which should be a bit clearer. You can click on atoms to see bond distances, angles, and dihedrals; notice that the C1–C2 bond above (C14–C18 in Rowan) is markedly shorter than a normal C–C bond, whereas the C1–C6 and C2–C6 bonds (C13–C14 and C14–C18 in Rowan) are quite long.
</p>

<iframe src=https://labs.rowansci.com/iframe2/calculations/75efc4da-b19c-42d4-a49c-cb454741cb93 style="height:400px;width:400px;"></iframe>

<p>
In the 1960s Winstein’s interpretation was challenged by another preeminent chemist, H.C. Brown, who argued that the data could adequately be explained by rapidly equilibrating classical carbocations. Brown suggested that most of the observations made by Winstein could be explained simply by the differing steric profiles of the <i>exo</i> and <i>endo</i> faces of the norbornyl cation: the <i>endo</i> face is more shielded, and so ionization is slowed (explaining the 350:1 <i>exo</i>/<i>endo</i> rates) and attack is disfavored (explaining why both isomers of sulfonate give <i>exo</i> product). 
</p>

<p>
This began an incredibly contentious series of debates which dragged on for decades. Rather than attempt to wade through the resulting sea of publications, I’ll quote from <a href=https://pubs.acs.org/doi/abs/10.1021/ar00096a004>an excellent 1983 review</a> by Cheves Walling to give a sense for the magnitude of the controversy: 
</p>

<blockquote>
The debate [over the structure of the norbornyl cation] was vigorously pursued verbally in lectures, meetings, and seminars all over the U.S. and even abroad…. No one has ever counted the number of publications touching on the 2-norbornyl cation problem, but they include a number of reviews, chapters, and books, and a typcial <i>[sic]</i> research paper may well include references to over 100 others. 
</blockquote>

<p>
Walling’s review goes on to give an excellent overview of the various pieces of evidence employed by both sides of the debate, which I won’t summarize in full here. 
</p>

<p>
The most important data was obtained by George Olah and co-workers, who pioneered the use of superacidic media to generate stable solutions of carbocations which could be characterized spectroscopically. With Martin Saunders and others, Olah employed <sup>1</sup>H and <sup>13</sup>C NMR spectroscopy, IR spectroscopy, Raman spectroscopy, and core electron spectroscopy to study low-temperature solutions of norbornyl cations: in all cases, the data supported Winstein’s proposed symmetric structure. (While equilibration occurring faster than the spectroscopic timescale could not be ruled out by Olah’s work, spectroscopic measurements all the way down to 5 K showed no detectable classical structures, indicating that any barrier to interconversion must be &lt;0.2 kcal/mol.)
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240103_olah.png style="width:350px;" />
  <figcaption>
    <sup>13</sup>C NMR spectra at –159 ºC, showing the equivalence of C1 and C2. At –80 ºC, rapid Wagner–Meerwein rearrangements render C1, C2, and C6 equivalent. 
    Data taken from <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.199513931>Olah's Nobel lecture</a>.
    (This is a good use case for carbon NMR!) 
  </figcaption>
</figure>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240103_pes.png style="width:350px;" />
  <figcaption>
    Carbon 1s photoelectron spectrum of <i>tert</i>-butyl carbocation (left), showing a characteristic carbenium peak on the left, and the analogous spectrum for the norbornyl cation showing the absence of the carbenium peak. Data taken from <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.199513931>Olah's Nobel lecture</a>.
  </figcaption>
</figure>

<p><b>
Note: <a href=https://twitter.com/dasingleton/status/1742754148221649214>On Twitter/X</a>, Dan Singleton argues that the controversy was largely settled by 1982 and attributes this to the <a href=https://pubs.acs.org/doi/10.1021/ar00096a003>Saunders/Olah NMR experiments</a> and <a href=https://pubs.acs.org/doi/pdf/10.1021/ar00096a001>Cyril Grob's work in this area</a>, which I didn't mention. I appreciate the correction and welcome any further additions to the record.
</b></p>

<p>
Computational chemistry, which became able to tackle problems like this in the late 1980s and early 1990s, also supported the nonclassical structure of the norbornyl cation. <a href=https://pubs.acs.org/doi/abs/10.1021/ja00186a073>A 1990 paper</a> used HF/6-31G(d) calculations in Gaussian 86 to show that the symmetric structure was a minimum on the potential energy surface. Here’s a scan I ran at the B3LYP-D3BJ/6-31G(d) level of theory, showing that the energy increases as the “classical” C–C bond forms:
</p>

<p>
<i>(This iframe doesn't work well on the phone - still a work in progress, sorry.)</i>
</p>

<iframe src=https://labs.rowansci.com/iframe/scans/6e36bd88-1235-43e8-a505-91ac79d1bbf6 style="width:100%;height:400px;"></iframe>

<p>
Subsequent work has confirmed that Winstein was almost completely correct about the key issues. Most notably, <a href=https://www.science.org/doi/10.1126/science.1238849>a 2013 crystal structure</a> from Karsten Meyer demonstrates that the norbornyl cation is indeed nonclassical in the ground state, leading <i>Chemistry World</i> to <a href=https://www.chemistryworld.com/news/crystal-structure-closes-classic-carbocation-case/6352.article>declare the mystery solved</a>. Nevertheless, there’s still a little room for a classical cation supporter to doubt this result: crystal structures are snapshots of solid-state atomic configurations, while reactions occur in solution, where molecules are free to move around more. (In the <i>Chemistry World</i> article, Paul Schleyer predicts that Brown himself would have raised this objection.) 
</p>

<p>
A paper from Ken Houk and co-workers, <a href=https://pubs.acs.org/doi/10.1021/acs.joc.3c02325>published a few days ago in <i>JOC</i></a>, addresses this issue by directly modeling the solvolysis process through <i>ab initio</i> molecular dynamics with explicit acetic acid solvent. In the solvolysis of the <i>exo</i> sulfonate, the authors find the nonclassical cation is formed on average within 9 femtoseconds of C–O bond cleavage, which is about as quickly as is physically possible. Once formed, the cation is entirely nonclassical: “classical 2-norbornyl cations are a negligible component of norbornyl cations in solution," thus addressing the last objection of classical cation partisans.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20240103_houk.jpeg style="width:550px;" />
  <figcaption>
    Simulations of the norbornyl cation in explicit acetic acid show complete nonclassical behavior (Figure 6 from Houk’s paper).
  </figcaption>
</figure>

<p>
In contrast, Houk et al find that the <i>endo</i> sulfonate doesn’t form the nonclassical cation until about 81 fs after the C–O bond breaks, explaining the slower reaction rate: the transition state isn’t stabilized by σ-dissociation, and so is higher in energy. This is a nice example of the principle of nonperfect synchronization, which is explained concisely in <a href=https://macmillan.princeton.edu/wp-content/uploads/PJS-Group-Meeting-no-layering.pdf>this presentation</a>.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
What can modern scientists learn from the norbornyl cation controversy, besides the object-level fact that carbocations can exhibit nonclassical σ-delocalization?
</p>

<h3>1. Reality is confusing, and convincing arguments are often wrong.</h3> 

<p>
It’s a good exercise to go back and read the early H.C. Brown papers in this area, like <a href=https://pubs.acs.org/doi/abs/10.1021/ar50071a003>this account</a>. Brown was an incredible scientist (the 1979 Nobel laureate in chemistry), and his data and reasoning are quite good; I find myself sympathizing with his viewpoint while reading his papers. Nevertheless, with the benefit of hindsight we know that he was wrong and Winstein was right. “Humility comes before honor.”
</p>

<h3>2. Tools drive scientific progress.</h3> 
<p>
The argument over the norbornyl cation was ultimately settled only by the development of new techniques, like superacid chemistry, core electron spectroscopy, and high-level calculations. Now that we have these methods, it’s much easier to solve similar problems: if Winstein’s paper came out today, I doubt it would take more than a year or two to figure everything out. 
</p>

<p>
This aligns nicely with what <a href=https://estudosdects.files.wordpress.com/2013/07/dyson-science-ideas-or-tools.pdf>Freeman Dyson</a> calls a “Galisonian” view of scientific progress, where scientific progress is driven not by ideas (the “Kuhnian” view) but by new tools and new data. In chemistry, at least, the tools-first view seems true to me—since 1950, it’s difficult to think of a development more important to organic chemistry than NMR spectroscopy, with flash column chromatography probably taking second place.
</p>

<h3>3. It’s easy for fields to get distracted by controversy and lose relevance.</h3> 
<p>
Here’s Walling again:
</p>

<blockquote>
Since a significant fraction of the efforts of physical organic chemists was drawn into the problem [of the norbornyl cation], an unhappy consequence was a feeling on the part of many (including some of those concerned with the distribution of research funds) that physical organic chemistry was in danger of withdrawing into a world of its own.
</blockquote>

<p>
As older scientists have explained to me, the norbornyl cation debacle scared a generation of chemists away from physical organic chemistry. The entire subfield became obsessed with a niche and somewhat irrelevant issue, while scientists in adjacent subfields looked on with bemusement and frustration. As a consequence, traditional physical organic chemistry is barely practiced today: few scientists have the skill or knowledge to conduct kinetic studies like those performed by Winstein, Brown, and others, and those still working in the area struggle to get funding or recognition (in the words of Dan Singleton, <a href=https://twitter.com/dasingleton/status/1275932027825664000>“sucks when you have to peddle your papers in cemeteries”</a>).
</p>

<p>
(In <a href=http://acshist.scs.illinois.edu/bulletin_open_access/v25-2/v25-2%20p123-131.pdf>a nice historical perspective</a>, Stephen Weininger argues the norbornyl cation debate was “a hook on which to hang a much larger agenda,” fueled both by a UK/US divide and a deeper dispute about whether valence bond representations or molecular orbital representations of chemical structures were superior. If this is true, it was a self-defeating exercise by all involved.)
</p>

<p>
Scientists should be motivated by a search for truth, but also by the desire to improve the world. Usually these two aims go together: basic research without obvious societal implications often leads to unexpected and important findings, which is why the government supports science in the first place. But it’s possible to become so myopically focused on a single issue in the name of truth that one forgets about other goals, as arguably happened in the norbornyl cation imbroglio.
</p>

<p>
Controversy attracts attention: we’re drawn to it, against our better judgment, like moths to a flame. We have to be careful not to get captured by disputes that are, in the long run, not worth the effort.
</p>

<p><i>
Update 1/5/2024: <a href=20240105_phys_org.html>some followup thoughts</a> based on feedback from X.
</i></p>

<i>Thanks to Eric Jacobsen for many conversations about the history of physical organic chemistry, Eugene Kwan for conversations about the principle of nonperfect synchronization, and to Ari Wagen for feedback on this post and six months of excellent front-end development for Rowan. Any errors are mine alone.</i>
]]></description>
              <pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Books from 2023</title>
              <link>public/blog/20231321_books.html</link>
              <description><![CDATA[
<p>
(Previously: <a href=https://corinwagen.github.io/public/blog/20221231_books.html>2022</a>)
</p>

<b>#1. Tony Fadell, <i>Build</i></b>
<br>
<b>#2. Giff Constable, <i>Talking To Humans</i></b>
<br>
<b>#3. Ben Horowitz, <i>The Hard Thing About Doing Hard Things</i></b>
<br>
<b>#4. Dale Carnegie, <i>How To Win Friends And Influence People</i></b>

<p>
Sounds Machiavellian, but actually quite wholesome: a “dad book,” as my friend called it.
</p>

<b>#5. Ben Patrick, <i>Knee Ability Zero</i></b>.
<br>
<b>#6. Neal Stephenson, <i>The Diamond Age</i></b>

<p>
<i>Snow Crash</i> was much worse upon rereading as an adult, but <i>The Diamond Age</i> was a bit better: in particular, I didn’t really appreciate the “speculative governance futurism”/”comparative cultural criticism” facets of the novel when I read this in high school.
</p>

<b> #7. Richard Hamming, <i> The Art of Doing Science and Engineering</i></b>

<p>
I reviewed this <a href=https://corinwagen.github.io/public/blog/20230516_hamming.html>here</a>.
</p>

<b>#8-11. Brandon Sanderson, <i>The Stormlight Archives</i></b>

<p>
Many great works of literature are notable for their brevity: when you read Hemingway, or <i>Dubliners</i>, or Flannery O’Connor, you know that every sentence has been crafted with care. Giant fantasy novels like <i>Wheel of Time</i> (which I read last year) or <i>The Stormlight Archives</i> work differently. There are entire chapters which are probably extraneous, whole characters and plot arcs which exist merely to bring out certain traits or pieces of information.
</p>

<p>
But there are unique joys to megafiction: sitting down and reading hundreds of pages of a good story is relaxing in a way that other books simply aren’t. In my own life, I’ve found that I’m much better about making time to read when I’m in the middle of an engaging novel than when I’m reading theology or histories of feudalism. Narrative-driven “easy reading” has a bad reputation amongst the literati; in a world where all fiction is competing against screens for engagement, it shouldn’t.
</p>

<b>#12. Antonio Garcia Martínez, <i>Chaos Monkeys</i></b>

<p>
I reviewed this <a href=https://corinwagen.github.io/public/blog/20230530_chaos_monkeys.html>here</a>.
</p>

<b> #13. Tom Holland, <i>Rubicon</i></b>
<br>
<b> #14. Tom Holland, <i>Dynasty</i></b>
<br>
<b> #15. Czeslaw Milocz, <i>The Captive Mind</i></b>

<p>
Fantastic; I probably would have liked this even more if I were still in school.
</p>

<b> #16. C.S. Lewis, <i>That Hideous Strength</i></b>

<p>
I didn’t like this when I was a kid, but I like it now: in many respects <i>THS</i> can be viewed as a book-length exploration of the ideas in “The Inner Circle,” with a garnish of medieval cosmology here and there (<a href=http://www.planetnarnia.com/>see also</a>).
</p>

<b> #17. Kazuo Ishiguro, <i>Klara and the Sun</i></b>
<br>
<b> #18. Mike Cosper, <i>Recapturing the Wonder</i></b>
<br>
<b> #19. Mairtin O Caidhan, <i>Graveyard Clay</i></b>

<p>
Tyler Cowen recommended this book, but I didn’t love it.
</p>

<b> #20. Ursula K. LeGuin, <i>The Dispossessed</i></b>
<br>
<b> #21. Marty Cagan, <i>Inspired</i></b>
<br>
<b> #22. Ernst Junger, <i>On the Marble Cliffs</i></b>

<p>
This was excellent (h/t <a href=https://regressstudies.substack.com/p/young-lords-and-their-traces>Santi Ruiz</a>).
</p>

<b> #23. Michaeleen Doucleff, <i>Hunt, Gather Parent</i></b>

<p>
I reviewed this book <a href=https://corinwagen.github.io/public/blog/20231106_hgp.html>here</a>.
</p>

<b> #24. David Kirkpatrick, <i>The Facebook Effect</i></b>
<br>
<b> #25. Bill Carr &amp; Colin Bryar, <i>Working Backwards</i></b>

<p>
I reviewed this book <a href=https://corinwagen.github.io/public/blog/20231125_working_backwards.html>here</a>.
</p>

<b> #26. Geoffrey Chaucer, <i>The Canterbury Tales</i></b>

<p>
The best book I read this year by a mile; far better than I remembered. While many of <i>The Canterbury Tales</i> work pretty well as literature, they’re even better when viewed also as history. It’s rare to be able to read something from 800 years ago that’s legitimately funny and interesting.
</p>

<p>
Reading Chaucer fills me with questions about the medieval mind. The stories are steeped in Christianity, as one might expect. Any argument goes back to the Bible, even those among animals, and Chaucer assumes a level of familiarity with e.g. the Psalms far exceeding that of most modern Christians. Yet at the same time the Greco-Roman world looms large: Roman gods appear as plot characters in three tales (the Knight’s Tale, the Merchant’s Tale, and the Manciple’s Tale), and Seneca is viewed as a moral authority on par with Scripture. I’m curious how all these beliefs and ideas fit together and welcome any recommendations on this subject. (<i>The Discarded Image</i> is already on my list.)
</p>

<b> #27. Gabrielle Zevin, <i>Tomorrow and Tomorrow and Tomorrow</i></b>

<p>
My wife recommended this book to me. I thought this would be a relaxing break from the November startup grind, but in fact it features a bunch of obsessed programmers working around the clock for months—a poor choice but a good novel.
</p>

<b> #28. Jessica Livingston, <i>Founders at Work</i></b>
<br>
<b> #29. Neal Stephenson, <i>Seveneves</i></b>
<br>
<br>
<p>
Overall, about a third of the books I read were startup-related: most of them were pretty bad, but even bad business/startup books are probably useful from the viewpoint of cultural immersion. Academic science is quite different from the startup ecosystem, and to the extent that cultural arbitrage is possible (in either direction), I need to become proficient in startup culture.
</p>

<p>
I’m not sure why the median business book is so bad—perhaps business people are too willing to spend money on books and not picky enough, or perhaps MBA types generally lack knowledge about the humanities, which makes both supply and demand worse.
</p>

<p>
(The median Christian book is also pretty bad. One unifying hypothesis: both pastors and businesspeople often have wise insights into specific situations, personal or business, but these insights aren’t readily generalizable into book form. Being able to give good advice doesn’t mean you should write an “advice book.”)
</p>

<p>
As always, book recommendations are welcome, particularly on the topics of medieval history/culture, software engineering, or startups. Apologies for the infrequent posting as of late, and happy new year!
</p>

]]></description>
              <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Bitter Lessons in Chemistry</title>
              <link>public/blog/20231201_sutton.html</link>
              <description><![CDATA[
<br>
<br>
<p class=epigraph>
“And I took the little scroll from the hand of the angel and ate it. It was sweet as honey in my mouth, but when I had eaten it my stomach was made bitter.”
</p>
<p class=epigraph-byline>
–Revelation 10:10
</p>

<figure>
  <img class=centered-img src=https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Ottheinrich_Folio293r_Rev10.jpg/1024px-Ottheinrich_Folio293r_Rev10.jpg style="width:450px;" />
</figure>

<p>
As machine learning becomes more and more important to chemistry, it’s worth reflecting on Richard Sutton’s 2019 blog post about the “bitter lesson.” In <a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html>this now-famous post</a>, Sutton argues that “the biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.” This might sound obvious, but it’s not:
</p>

<blockquote>
[The bitter lesson] is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. <u>The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</u> <i>(emphasis added)</i>
</blockquote>

<p>
How might the bitter lesson be relevant in chemistry? One example is computer-assisted retrosynthesis in organic synthesis, i.e. figuring out how to make a given target from commercial starting materials. This task was first attempted by Corey’s LHASA program (<a href=https://pure.mpg.de/rest/items/item_2549520/component/file_3325899/content>1</a>, <a href=https://www.nobelprize.org/uploads/2018/06/corey-lecture.pdf>2</a>), and more recently has been addressed by Bartosz Gryzbowski’s <a href=https://www.cell.com/chem/pdf/S2451-9294(18)30085-8.pdf>Synthia</a>. Despite considerable successes, both efforts have operated through manual encoding of human chemical intuition. If Sutton is to be believed, we should be pessimistic about the scalability and viability of such approaches relative to pure search-based alternatives in the coming years.
</p>

<p>
Another example is machine-learned force fields (like ANI or NequIP). While one could argue that equivariant neural networks like <i>e3nn</i> aren’t so much incorporating domain-specific knowledge as exploiting relevant symmetries (much like convolutional neural networks exploit translational symmetry in images), there’s been a movement in recent years to combine chemistry-specific forces (e.g. long-range Coulombic forces) with neural networks: Parkhill’s <a href=https://pubs.rsc.org/en/content/articlelanding/2018/sc/c7sc04934j>TensorMol</a> did this back in 2018, and more recently <a href=https://www.nature.com/articles/s41467-021-27340-2>Dral</a>, <a href=https://arxiv.org/abs/2301.08734>Piquemal</a>, and <a href=https://pubs.acs.org/doi/full/10.1021/jacs.3c07628>Levitt &amp; Fain</a> (<a href=https://twitter.com/olexandr/status/1730604569028092081>among others</a>) have published on this as well. While I’m no expert in this area, the bitter lesson suggests that we should be skeptical about the long-term viability of efforts, and instead just throw more data at chemistry-agnostic models. 
</p>

<p>
A key assumption of the bitter lesson is that “over a slightly longer time than a typical research project, massively more computation inevitably becomes available.” This idea has also been discussed by Andrej Karpathy, who <a href=https://karpathy.github.io/2022/03/14/lecun1989/>reproduced</a> Yann LeCun’s landmark 1989 backpropagation paper last year using state-of-the-art techniques and reflected on how the field has progressed since then. In particular, Karpathy discussed how the last three decades of progress in ML can help us envision what the next three decades might look like:
</p>

<blockquote>
Suppose that the lessons of this exercise remain invariant in time. What does that imply about deep learning of 2022? What would a time traveler from 2055 think about the performance of current networks?
<ul>
<li>
2055 neural nets are basically the same as 2022 neural nets on the macro level, except bigger.
</li>
<li>
Our datasets and models today look like a joke. Both are somewhere around 10,000,000X larger.
</li>
<li>
One can train 2022 state of the art models in ~1 minute by training naively on their personal computing device as a weekend fun project.
</li>
<li>
Today’s models are not optimally formulated, and just changing some of the details of the model, loss function, augmentation or the optimizer we can about halve the error.
</li>
<li>
Our datasets are too small, and modest gains would come from scaling up the dataset alone.
</li>
<li>
Further gains are actually not possible without expanding the computing infrastructure and investing into some R&D on effectively training models on that scale.
</li>
</ul>
</blockquote>

<p>
If we take this seriously, we might expect that chemical ML will not be able to advance much farther without bigger datasets and bigger models. Today, experimental datasets rarely exceed 10<sup>4</sup>–10<sup>5</sup> data points, and even computational datasets typically comprise 10<sup>7</sup> data points or fewer—compare this to the ~10<sup>13</sup> tokens <a href=https://www.reddit.com/r/LocalLLaMA/comments/14wbmio/gpt4_details_leaked/>reportedly used to train GPT-4</a>! It’s not obvious how to get experimental datasets that are this large. HTE and robotics will help, but five orders of magnitude is a big ask. Even all of Reaxys doesn’t get you to 10<sup>8</sup>, poor data quality notwithstanding. (It’s probably not a coincidence that DNA-encoded libraries, which can actually have hundreds of millions of data points, also pair nicely with ML: I’ve written about this <a href=https://corinwagen.github.io/public/blog/20230403_industry.html>before</a>.)
</p>

<p>
In contrast, computation permits the predictable generation of high-quality datasets. If Sutton is right about the inevitable availability of “massively more computation,” then we can expect it to become easier and easier to run hitherto expensive calculations like DFT in parallel to generate huge datasets, and to enable more and more downstream applications like chemical machine learning. With the right infrastructure (like Rowan, hopefully), it should be possible to turn computer time into high-quality chemical data with almost no non-financial scaling limit: <a href=https://pubs.acs.org/doi/10.1021/ci300415d>we’re certainly not going to run out of molecules</a>.
</p>

<p>
The advent of big data, though, heralds the decline of academic relevance. Julian Togelius and Georgios Yannakakis wrote about this earlier this year in a piece on <a href=https://arxiv.org/abs/2304.06035?s=03>“survival strategies for depressed AI academics,”</a> which discusses the fact that “the gap between the amount of compute available to ordinary researchers and the amount available to stay competitive is growing every year.” For instance, GPT-4 reportedly cost c. $60 million to train, far surpassing any academic group’s budget. Togelius and Yannakakis provide a lot of potential solutions, some sarcastic (“give up”) and others quite constructive—for instance, lots of interpretability work (<a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>like this</a>) is done on toy models that don’t require GPT-4 levels of training. Even the most hopeful scenarios they present, however, still leave academics with a rather circumscribed role in the ML ecosystem. 
</p>
 
<p>
The present academic era of chemical machine learning can thus be taken as a sign of the field’s immaturity. When will chemical ML reach a Suttonian era where scaling is paramount and smaller efforts become increasingly futile? I’m not sure, but my guess is that it will happen when (and only when) there are clear commercial incentives for developing sophisticated models, enabling companies to sink massive amounts of capital into training and development. This clearly hasn’t happened yet, but it also might not be as far away as it seems (<a href=https://www.nature.com/articles/s41586-023-06735-9>cf.</a>). It’s an interesting time to be a computational chemist…
</p>

]]></description>
              <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Working Backwards</title>
              <link>public/blog/20231125_working_backwards.html</link>
              <description><![CDATA[
<p>
I took a pistol course in undergrad, and while I was a poor marksman I enjoyed the experience. In particular, I was surprised by how meditative the act of shooting was. As our instructor explained, much of good shooting comes down to not doing anything when you pull the trigger. When you’re not firing, it’s easy to point a gun at a target and line up the sights, but as you pull the trigger you subconsciously anticipate the noise and movement of the pistol blast, which makes you flinch and pull the gun off-target. Being a good shooter thus requires consciously learning to counteract what your instincts tell you to do.
</p>

<p>
If you believe Bill Carr and Colin Bryar’s book on Amazon, <a href=https://www.amazon.com/Working-Backwards-Insights-Stories-Secrets/dp/1250267595><i>Working Backwards</i></a>, Amazon’s success can be understood in similar terms. According to Carr and Bryar, Amazon alone among the West Coast zaibatsu has succeeded not because of some big technical or social insight (Google Search, Windows) but because of a long series of canny business decisions. Bezos has said something similar: “Amazon doesn't have one big advantage, so we have to braid a rope out of many small advantages.” The implication is that you too can build an Amazon-quality firm; you don’t need any flashes of mad genius, just the ability to eke out small advantages through savvy management.
</p>

<p>
(This might seem like an insane claim, but it’s worth noting that Amazon has indeed launched a ton of successful and loosely coupled businesses: in addition to their core commerce business, there’s AWS, Amazon Robotics, Kindle, Prime Video, Fire TV, and a bunch of other stuff. Contrast this to Google’s recent track record…)
</p>

<p>
What’s more, Carr and Bryar go on to argue that Amazon’s business acumen is driven not by some inscrutable Bezos magic but by adherence to a simple set of principles. And these principles aren’t esoteric or Amazon-specific—almost any business can follow them. The reason so few businesses have copied Amazon’s success is simply because each principle defies human nature in some way. Just like pistol shooting requires one to unlearn one’s instincts and pull the trigger without moving any other muscles, being an “Amazonian” business requires discarding what you think you understand about building a business and going back to basics.
</p>

<p>
So, what are these magic principles?
</p>

<h2>
1. Focus on Customers, Not Competitors
</h2>

<p>
Focusing on competitors is human nature—in a competition, we judge ourselves based on our rivals, and like to imagine how we’ll defeat them. But success in business comes from satisfied customers, not vanquished foes, and keeping a relentless focus on users/customers is key to building something great. This is hardly Amazon-specific wisdom: “build something people want” is <a href=https://www.ycombinator.com/library/4D-yc-s-essential-startup-advice>venerable YC advice</a>, and <i>Zero to One</i> also makes the point that competition is bad and focusing on it counterproductive. Perhaps the fact that so many different people feel the need to emphasize this point speaks to how counterintuitive it is: were it widely adopted, it wouldn’t be repeated.
</p>

<p>
Plenty of people outside business also get this wrong. A few weeks ago, a friend was explaining how he feels that many computational chemists are making software not for users but for other computational chemists. This is a case in which writing papers leads to different incentives than releasing products: papers are reviewed by one’s peers (other computational chemists), while products are ultimately reviewed by users. Hopefully Rowan doesn’t make this mistake…
</p>

<h2>
2. “Bar Raisers”: External Vetos in Hiring
</h2>

<p>
Amazon includes a person called a “Bar Raiser” involved in all hiring decisions, who isn’t the hiring manager (the person who is trying to acquire a new team member) but who has final veto power on any potential hire. The hiring manager is hiring because they need help in the short term, so they’re typically willing to engage in wishful thinking and lower their standards—but the Bar Raiser (who’s just another Amazon employee, with a bit of extra training) has no such incentives and can make sure that no poor performers are hired, which is better for Amazon in the long run.
</p>

<p>
I like this idea because it’s a nice example of mechanism design: just a little bit of internal red-teaming which (according to the book) works quite well. (Red teaming is another one of those “good but counterintuitive practices” which seems underutilized—see <a href=https://open.substack.com/pub/statecraftnotes/p/how-to-predict-the-future?r=5jsaw&selection=302a6711-1b29-4791-aedc-0320932bac7a&utm_campaign=post-share-selection&utm_medium=web>the discussion</a> in a recent <i>Statecraft</i> interview.)
</p>

<h2>
3. Problem-Focused, Not Skills-Focused
</h2>

<p>
It’s natural to think about what we should do next in terms of what we’re good at: “I’m good at X, how can I use X to solve a problem?” Carr and Bryar argue that “working forwards” in this way is stupid, because only at the end do you think about who (if anyone) might care if you succeed. “Working backwards” from problem to solution is a much better strategy: first you articulate what you might need to accomplish to produce a compelling solution, and then you think about if you can do it. Inside Amazon, most potential new projects start out by drafting what the press release might be if the project were finished, and then working backwards from the press release to what the product must be. (Most products envisioned in this way never actually get developed, which is exactly the point of the exercise.)
</p>

<p>
<a href=https://intra.ece.ucr.edu/~rlake/Whitesides_writing_res_paper.pdf>George Whitesides</a> has advocated for a similar way to approach science: first write the paper, then conduct the experiments. Lots of scientists I know find this repulsive, or contrary to how science should be practiced, but it always seemed shrewd to me—if you can’t make an interesting paper out of what you’re doing, why are you doing it? (Exploratory work can be tough to outline in this way, but there should be several potential papers in such cases, not none.)
</p>

<h2>
4. Single-Threaded Leadership
</h2>

<p>
As organizations scale, it becomes tougher and tougher to allow teams to work autonomously, and responsibility and authority for almost all projects ends up bestowed upon the same small number of people. This makes insightful innovation hard: to quote Amazon SVP of Devices Dave Limp, “the best way to fail at inventing something is by making it somebody’s part-time job.” Amazon’s solution is the idea of single-threaded leadership (STL, probably someone’s idea of C++ humor). Organizations need to be arranged such that individual teams can respond to their problems intelligently and independently, planning and shipping features on their own, and each team needs to have a “single-threaded leader” solely responsible for leading that team.
</p>

<p>
Instituting STL takes a good amount of initial planning, since dividing up a giant monolith into loosely coupled components is tough both for software and for humans, and it’s not in the nature of authorities to relinquish control. If done properly, though, this allows innovation to happen much faster than if every decision is bottlenecked by reliance on the C-suite. (It’s sorta like federalism for businesses.)
</p>

<p>
This idea matters for labs, too: some research groups rely on their PI for scientific direction in every project, while others devolve a lot of authority to individual students. The latter seem more productive to me.
</p>

<h2>
5. Bias Towards Action
</h2>

<p>
Humans are by nature conservative, and sins of commission frequently feel worse than sins of omission—making a bad decision can cost you your job, while not making a good decision often goes unnoticed (the <a href=https://marginalrevolution.com/marginalrevolution/2015/08/is-the-fda-too-conservative-or-too-aggressive.html>“invisible graveyard”</a>). To counteract this, Amazon expects leaders to display a “Bias for Action.” In their words:
</p>

<blockquote>
Speed matters in business. Many decisions and actions are reversible and do not need extensive study. We value calculated risk-taking.
</blockquote>

<p>
Again, this echoes classic startup advice: “launch now,” “do things that don’t scale,” &amp;c.
</p>

<h2>
6. No Powerpoint!
</h2>

<p>
In June 2004, Amazon banned PowerPoint presentations from meetings, instead expecting presenters to compose six-page documents which the entire team would read, silently, at the beginning of each meeting. Why? A few reasons:
</p>

<ul>
<li>
PowerPoint lends itself poorly to complex or nuanced ideas, whereas written documents can contain more information and more complete chains of reasoning.
</li>
<li>
PowerPoint leads to “slick,” well-rehearsed presentations where the presenter’s style and charisma matters as much as the underlying ideas.
</li>
<li>
Writing enforces greater clarity of thought than speaking.
</li>
<li>
PowerPoint makes the audience passive, while reading makes the audience active participants in the ideas. People like to be passive, and it’s fun to hear an inspiring presentation, but in the long run it’s better for everyone to actually engage.
</li>
</ul>

<p>
I’m pretty sympathetic to these criticisms. Most high-stakes academic events today revolve around oral presentations, not papers—although papers still matter, doctorates, job offers, and tenure are awarded largely on the merits of hour-long talks (<a href=https://www.youtube.com/watch?v=n7HeQGQQ0q8>e.g.</a>). As a result, I spent a ridiculous amount of my PhD just refining and hearing feedback on my presentations, much of which had nothing to do with the underlying scientific ideas. Perhaps this focus on showmanship over substance explains why so little science today seems genuinely transformational. (It’s also worth noting that presenting, much more so than writing, favors charismatic Americans over the meek or foreign.)
</p>

<p>
There are, of course, more than just these six ideas in <i>Working Backwards</i>, but I think this gives a pretty good sense of what the book is like. Overall, I’d recommend this book: it was interesting throughout (unlike most business-y books), and even if nothing in its covers is truly new under the sun, the ideas inside are good enough to be worth reviewing periodically.
</p>
]]></description>
              <pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Hunt, Gather, Parent</title>
              <link>public/blog/20231106_hgp.html</link>
              <description><![CDATA[
<br>
<br>
<p class=epigraph>
“Like arrows in the hand of a warrior are the children of one's youth.”
</p>
<p class=epigraph-byline>
–Psalm 127:4
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20231106_gaugain.jpg style="width:300px;" />
  <figcaption>
    <i>Mata Mua</i>, by Paul Gaugain (1892), another Westerner looking for enlightenment among “venerable cultures.”
  </figcaption>
</figure>

<p>
What if our most fundamental assumptions about parenting were wrong? That’s the question that Michaeleen Doucleff’s 2021 book <i>Hunt, Gather, Parent</i> tries to tackle. <i>Hunt, Gather, Parent</i> (henceforth <i>HGP</i>) documents Doucleff’s journey to “three of the world’s most venerable cultures”—the Maya, the Inuit, and the Hadzabe (in Tanzania)—to learn about how they parent their children, and offers helpful advice for parents envious of the kind, helpful, and responsible children she observes.
</p>

<p>
Doucleff’s writing hits some familiar beats: critiques of helicopter parenting, distrust of endless after-school activities, and laments about the atomization of our society (<a href=https://web.archive.org/web/20230306232837/https://www.theatlantic.com/magazine/archive/2020/03/the-nuclear-family-was-a-mistake/605536/>cf.</a>). But there are plenty of unexpected insights too. I was convicted by her account of how Hadzabe children are given autonomy and responsibilities from a young age without being either ignored or micromanaged: her vision of a middle ground between <i>K</i>-selected “helicopter parents” and <i>r</i>-selected “free-range parents” was compelling.
</p>

<p>
There’s a lot to like about the parenting depicted in <i>HGP</i>. For instance, Doucleff highlights how toddlers’ innate eagerness to help is used by the Maya to build a culture of helpfulness (the virtue she calls <i>acomedido</i>) which lasts as they grow, whereas American parents generally disregard help from a toddler and thus teach kids that their help isn’t valued. On the other hand, I’m not compelled by her account of how the Inuit view conflict with their children:
</p>

<blockquote>
Inuit see arguing with children as silly and a waste of time… because children are pretty much illogical beings. When an adult argues with a child, the adult stoops to the child’s level… During my three visits to the Arctic, I never once witness a parent argue with a child. I never see a power struggle. I never hear nagging or negotiating. Never.
</blockquote>

<p>
I admit that getting into a shouting match with your toddler is pointless, but assuming that children are innately devoid of logic seems like an overreaction! 
</p>

<p>
Astute readers might be getting bothered by now, though: why are the Maya—who ruled a swath of Central America for over a millennium—included in a book ostensibly about “hunter-gatherers and other indigenous cultures with similar values”? This highlights a deeper issue I have with <i>HGP</i>, which is that it partitions the world neatly into “Westerners” and “everyone else,” citing Joseph Heinrich’s <a href=https://en.wikipedia.org/wiki/The_WEIRDest_People_in_the_World><i>The WEIRDest People in the World</i></a> as its justification. While there are certainly many ways in which our own culture is distinct, ours is but one among many, and there’s plenty of cultural diversity about which <i>HGP</i> is silent.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20231106_tikal.jpg style="width:450px;" />
  <figcaption>
    The ruins of Tikal.
  </figcaption>
</figure>

<p>
For instance, Doucleff argues that in other cultures “parents build a relationship with young children… that’s based in cooperation instead of conflict, trust instead of fear.” I’m skeptical about this claim—what might we learn from some other non-WEIRD societies? 
</p>

<ul>
<li>
“In the Mendocino Codex… the daily life of Aztecs was described, including a common form of punishment for children. The Codex has a drawing of a father punishing his 11-year-old son by making the boy inhale smoke emanating from dry chiles roasting on the hearth. In the same drawing, a mother threatens her 6-year-old daughter with the same punishment” (<a href=https://journals.ashs.org/hortsci/downloadpdf/journals/hortsci/34/5/article-p809.xml>Paul Bosland</a>)
</li>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20231106_codex.jpg style="width:450px;" />
  <figcaption>
    Not exactly “gentle parenting”!
  </figcaption>
</figure>

<li>
“Roman law understands the legal power of the <i>pater familias</i> within the familia to be absolute, to the point of being able to put any member to death (this seems to have almost never happened, but it was legally permitted)” (<a href=https://acoup.blog/2023/07/21/collections-how-to-roman-republic-101-part-i-spqr/>Bret Devereaux</a>)
</li>
<li>
Yan Zhitui (Northern and Southern Dynasties, late 6th century) writes that “…as soon as a baby can recognize facial expressions and understand approval and disapproval, training should be begun so that he will do what he is told to do and stop when so ordered. After a few years of this, punishment with the bamboo can be minimized, as parental strictness and dignity mingled with parental love will lead the boys and girls to a feeling of respect and caution and give rise to filial piety. I have noticed about me that where there is merely love without training this result is never achieved.” (<a href=http://afe.easia.columbia.edu/ps/cup/yan_house_instructions.pdf>quoted here</a>)
</li>
</ul>

<p>
(Granted, these cultures aren’t “indigenous,” but then neither are the Maya.)
</p>

<p>
Doucleff’s focus on partitioning the world into “the West” and “the rest” blinds her to deeper and more interesting questions. The way we parent reflects our values—there are no perfect choices in parenting, just tradeoffs all the way down. Our culture’s valorization of grindset likely helps us instill ambition and a work ethic in our children, but also probably sets them up for depression and other issues down the road. Is this a good trade? Absent an ethical framework, it’s tough to say, but <i>HGP</i> doesn’t even acknowledge the question.
</p>

<p>
There’s a deeper truth here, which is that rejecting the status quo isn’t the same as proposing an alternative. It’s not unfair to read <i>HGP</i> as an account of Doucleff becoming redpilled on parenting and realizing that all her assumptions about how to raise her children might be wrong—but, like many of the newly redpilled, Doucleff lingers too long in her rebellion and doesn’t (in <i>HGP</i>) articulate a satisfying positive vision for what parenting should be. There are innumerable cultures out there, each of which doubtless parents in a different way, and choosing what practices to adopt from each tradition requires wisdom.
</p>

<p>
But these aren’t choices we should want Doucleff to make for us. In the introduction to <i>HGP</i>, Doucleff writes that “as we move outside the U.S., we’ll start to see the Western approach to parenting with fresh eyes,” and this seems true. <i>HGP</i> prompts us to reflect on the choices we make as parents and the ways in which we might choose differently, and even if you disagree with all of Doucleff’s advice it’s worth reading for this experience alone.
</p>

<i>
Thanks to my wife for recommending this book to me, and for helpful discussions.
</i>

]]></description>
              <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Quantum Computing for Quantum Chemistry: Short-Term Pessimism</title>
              <link>public/blog/20231027_quantum_computing.html</link>
              <description><![CDATA[
<p>
Quantum computing gets a lot of attention these days. In this post, I want to examine the application of quantum computing to quantum chemistry, with a focus on determining whether there are any business-viable applications today. My conclusion is that while quantum computing is a very exciting scientific direction for chemistry, it’s still very much a realm where basic research and development is needed, and it’s not yet ready for substantial commercial attention.
</p>

<p><i>
Briefly, for those unaware, quantum computing revolves around “qubits” (Biblical pun intended?), quantum analogs of regular bits. They can be in the spin-up or spin-down states, much like bits can hold a 0 or a 1, but they also exhibit quantum behavior like superposition and entanglement. 
</p>

<p>
Algorithms which run on quantum computers can exhibit “quantum advantage,” where for a given problem the quantum algorithm scales better than the classical algorithm, or “quantum supremacy,” where the quantum algorithm is able to tackle problems inaccessible to classical computers. Perhaps the best-known example of this is <a href=https://en.wikipedia.org/wiki/Shor%27s_algorithm>Shor’s algorithm</a>, which enables integer factorization in polynomial time (in comparison to the fastest classical algorithm, which is sub-exponential). 
</p>

<p>
It’s pretty tough to actually make quantum computers in the real world, though. There are many different strategies for what to make qubits out of: isolated atoms, nitrogen vacancy centers in diamonds, superconductors, and trapped ions have all been proposed. The limited number of qubits accessible by state-of-the-art quantum computers, along with the high error rate and short decoherence times, means that practical quantum computation is very challenging today. These challenges are collectively described as “noisy intermediate-scale quantum”, or <a href=https://arxiv.org/abs/1801.00862>NISQ</a>, the world we currently live in. Much effort has gone into trying to find NISQ-compatible algorithms.
</i></p>

<p>
Quantum chemistry, which revolves around simulating a quantum system (nuclei and electrons), seems like an ideal candidate for quantum computing. And indeed, many people have proposed using quantum computers for quantum chemistry, even going so far as to call chemistry the “killer app” for quantum computation. 
</p>

<p>
Here are a few representative claims: 
</p>

<ul>
<li>
“Few fields will get value from quantum computing as quickly as chemistry. Even today’s supercomputers struggle to model a single molecule in its full complexity. We study algorithms designed to do what those machines can’t, and power a new era of discovery in chemistry, materials, and medicine.” (<a href=https://research.ibm.com/topics/quantum-chemistry>IBM</a>)
</li>
<li>
“The problem is that most quantum chemical problems scale exponentially with system size. And classical computers struggle to cope with this exponential scaling. Realistically, they will never enable quantum chemistry to tackle real-world systems.” (<a href=https://www.emdgroup.com/en/research/science-space/envisioning-tomorrow/smarter-connected-world/quantum-computing.html>EMD Group</a>)
</li>
<li>
“Classically built computers simply cannot handle the level of complexity of substances as commonplace as caffeine… But if future chemists embrace quantum computers, they are likely to be a lot luckier.” (<a href=https://www.scientificamerican.com/article/how-quantum-computing-could-remake-chemistry/>Scientific American</a>)
</li>
</ul>

<p>
None of these claims are technically incorrect—there <i>is</i> a level of “full complexity” to caffeine which we cannot model today—but most of them are very misleading. Computational chemistry is doing just fine as a field without quantum computers; I don’t think there are any deep scientific questions about the nature of caffeine that depend on computing its exact electronic structure to the microHartree (<a href=https://pubs.acs.org/doi/abs/10.1021/acs.jpclett.0c02621>competitions between physical chemists notwithstanding</a>).
</p>

<p>
(Some other claims about quantum computing and chemistry border on the ridiculous: I’m not sure what to take away from <a href=https://nam.org/how-quantum-computing-can-combat-forever-chemicals-29001/?stream=policy-legal&s=03>this D-Wave press release</a> which claims that their quantum computer can model 67 million solutions to the problem of “forever chemicals” in 13 seconds. <a href=https://twitter.com/DulwichQuantum>Dulwich Quantum Computing</a>, on Twitter/X, does an excellent job of cataloging such malfeasances.)
</p>

<p>
Nevertheless, there are many legitimate and exciting applications of quantum computing to chemistry. Perhaps the best-known is the variational quantum eigensolver (VQE), developed by Alán Aspuru-Guzik and co-workers <a href=https://www.nature.com/articles/ncomms5213>in 2014</a>. The VQE is a hybrid quantum/classical algorithm suitable for the NISQ era: it takes a Hartree–Fock calculation as the starting point, and then minimizes the energy by optimizing the system classically while evaluating the energy with a quantum computer. (If you want to learn more, there are a number of easy-to-read introductions to the VQE: here’s <a href=https://joshuagoings.com/2020/08/20/VQE/>one</a> from Joshua Goings, and here’s <a href=https://pennylane.ai/qml/demos/tutorial_vqe/>another</a> from Pennylane.)
</p>

<p>
Another approach, more suitable for fault-tolerant quantum computers with large numbers of qubits, is quantum phase estimation. Quantum phase estimation, explained nicely by Pennylane <a href=https://pennylane.ai/blog/2021/11/quantum-computing-for-quantum-chemistry-a-brief-perspective/>here</a>, works like this: given a unitary operator and a state, the state is projected into an eigenstate and the corresponding eigenvalue is returned. (It’s not just projected onto an eigenstate randomly; the probability of returning a given eigenstate is proportional to the overlap with the input state.) This might sound abstract, but the ground-state energy of a molecule is just the smallest eigenvalue of its Hamiltonian, so this provides a route to get exact ground-state energies, assuming we can generate good enough initial states (again, typically a Hartree–Fock calculations).
</p>

<p>
Both of these methods are pretty exciting, since full configuration interaction (the “correct” classical way to get the exact ground-state energy) typically has an <i>O</i>(<i>N</i>!) cost, making it prohibitively expensive for anything larger than, like, N<sub>2</sub>. Further work has built on these ideas: I don’t have the time or skillset to provide a full review of the field, although I’ll note <a href=https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.030307>this work</a> from Head-Gordon &amp; friends and <a href=https://www.nature.com/articles/s41586-021-04351-z>this work</a> from Joonho Lee. (<a href=https://arxiv.org/pdf/2310.03011.pdf#page34>These</a> <a href=https://link.aps.org/accepted/10.1103/RevModPhys.92.015003>reviews</a> provide an excellent overview of different algorithms; I’ll discuss it later on.)
</p>

<p>
Based on the above description, one might reasonably assume that quantum computers offer some sort of dramatic quantum advantage relative to their classic congeners. <a href=https://arxiv.org/abs/2208.02199>Recent work</a> from Garnet Chan (and many coworkers) challenges this assumption, though: 
</p>

<blockquote>
…we do not find evidence for the exponential scaling of classical heuristics in a set of relevant problems. …our results suggest that without new and fundamental insights, there may be a lack of generic EQA [exponential quantum advantage] in this task. Identifying a relevant quantum chemical system with strong evidence of EQA remains an open question.
</blockquote>

<p>
The authors make many interesting points. In particular, they point out that physical systems seem to exhibit locality, i.e. if we’re trying to describe some system embedded in a larger environment to a given accuracy, then there’s some distance beyond which we can ignore the larger environment. This means that there are almost certainly polynomial-time classical algorithms out there for all of computational chemistry, since at some point increasing system size won’t slow our computations down any more. 
</p>

<p>
This might sound abstract, but the authors point out that coupled-cluster theory, which can (in principle) be extended to arbitrary levels of precision, can be made to take advantage of locality and scale linearly with increasing system size or increasing levels of accuracy. Although such algorithms aren’t known for strongly correlated systems, like metallic systems, Chan and co-workers argue based on analogy to strongly correlated model systems that analogous behavior can be expected.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20231027_ccsdt.png style="width:550px;" />
  <figcaption>
    Figure 3, showing linear scaling of coupled-cluster theory with respect to increasing accuracy (A) and increasing system size (B)
  </figcaption>
</figure>

<p>
The above paper is making a very specific point—that exponential quantum advantage is unlikely—but doesn’t address whether weaker versions of quantum advantage are likely. Could it still be the case that quantum algorithms exhibit polynomial quantum advantage, e.g. scaling as <i>O</i>(<i>N</i>) while classical algorithms scale as <i>O</i>(<i>N</i><sup>2</sup>)? 
</p>

<p>
<a href=https://www.pnas.org/doi/full/10.1073/pnas.2203533119>Another recent paper</a>, from scientists at Google and QSimulate, addresses this question by looking at the electronic structure of various iron complexes derived from cytochrome P450. They find that there’s some evidence that quantum computers (using quantum phase estimation) will be able to outcompete the best classical methods today (CCSD(T) and DMRG), but it’ll take a really big quantum computer:
</p>

<blockquote>
Most notably, under realistic hardware configurations we predict that the largest models of CYP can be simulated with under 100 h of quantum computer time using approximately 5 million qubits implementing 7.8 × 10<sup>9</sup> Toffoli gates using four T factories. A direct runtime comparison of qubitized phase estimation shows a more favorable scaling than DMRG, in terms of bond dimension, and indicates future devices can potentially outperform classical machines when computing ground-state energies. Extrapolating the observed resource estimates to the full Cpd I system and compiling to the surface code indicate that a direct simulation of the entire system could require 1.5 trillion Toffoli gates—an unfeasible number of Toffoli gates to perform.
</blockquote>

<p>
(A Toffoli gate is a three-qubit operator, described nicely <a href=https://www.sharetechnote.com/html/QC/QuantumComputing_Gate_Toffoli.html>here</a>.)
</p>

<p>
Given that <a href=https://www.newscientist.com/article/2346074-ibm-unveils-worlds-largest-quantum-computer-at-433-qubits/>the largest quantum computer yet built is 433 qubits</a>, it’s clear that there’s a lot of work left to do until we can use quantum computers to inaugurate “a new era of discovery in chemistry.”
</p>

<figure>
  <img class=centered-img src=https://www.oezratty.net/wordpress/wp-content/IBM-Osprey-1024x989.jpg style="width:350px;" />
  <figcaption>
    433 qubits down, only 8 billion more to go
  </figcaption>
</figure>

<p>
<a href=https://arxiv.org/pdf/2310.03011.pdf#page34>A recent review</a> agrees with this assessment: the authors write that “there is currently no evidence that heuristic NISQ approaches [like VQE] will be able to scale to large system sizes and provide advantage over classical methods,” and conclude with this paragraph:
</p>

<blockquote>
Solving the electronic structure problem has repeatedly been identified as one of the most promising applications for quantum computers. Nevertheless, the discussion above highlights a number of challenges for current quantum approaches to become practical. Most notably, after accounting for the approximations typically made (i.e. incorporating the cost of initial state preparation, using nonminimal basis sets, including repetitions for correctness checking and sampling a range of parameters), a large number of logical qubits and total T/Toffoli gates are required. A major difficulty is that, unlike problems such as factoring, the end-to-end electronic structure problem typically requires solving a large number of closely related problem instances.
</blockquote>

<p>
An important thing to note, which the above paragraph alludes to, is that the specific quantum algorithms discussed here don't actually make quantum chemistry faster than today’s methods—they typically rely on a Hartree–Fock ansatz, which is about the same amount of work as a DFT calculation. Since it's likely that proper treatment of electron correlation will require a sizable basis set, much like we see with coupled-cluster theory, we can presume that quantum methods would be slower than most DFT methods (even assuming that the actual quantum part of the calculation could be run instantly).
</p>

<p>
This ignores the fact that the quantum methods would of course give much better results—but an uncomfortable truth is that, unlike one might think from the exuberant press releases quoted above, classical algorithms generally do an exceptional job already. Most molecules are very simple from an electronic structure perspective: static electron correlation is pretty rare, and linear scaling CCSD(T) approaches are widely available and very effective (<a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.5b00359>e.g.</a>). There’s simply no need for FCI-quality results for most chemical problems, <a href=https://arxiv.org/pdf/2009.08927.pdf>random exceptions notwithstanding</a>.
</p>

<p>
(Aspuru-Guzik and co-workers agree; <a href=https://link.aps.org/accepted/10.1103/RevModPhys.92.015003>in a 2020 review</a>, they state that they “do not expect [HF and DFT] calculations to be replaced by those on quantum computers, given the large system sizes that are simulated,” suggesting instead that quantum computers might find utility for statically correlated systems with 100+ spin orbitals)
</p>

<p>
A related point I made in a <a href=https://rowansci.substack.com/p/quantum-chemistry-in-drug-discovery>recent essay/white paper for Rowan</a> is that quantum chemistry, at least as it’s applied to drug discovery, is limited not by accuracy but by speed. Existing quantum chemistry methods are already far more accurate than state-of-the-art drug discovery methods; replacing them with quantum computing-based approaches is like worrying about whether to bring a Lamborghini or a Formula 1 car to a go-kart race. It’s almost certain that there’s some way that “perfect” electronic structure calculations could be useful in drug design, but it’s hardly trivial to figure out how to turn a bunch of VQE calculations into a clinical candidate.
</p>

<p>
Other fields, like materials science, seem to be more limited by inaccuracies in theory—modeling metals and surfaces is really hard—but the Hartree–Fock ansatz is also hard here, and there are fewer commercial precedents for computational chemistry in general. To my knowledge, the Hartree–Fock starting point alone is a terrific challenge for a system like e.g. a cube of 10,000 metal atoms, which is why so many materials scientists avoid exact exchange and stick to local functionals. (I don't know much about computations on periodic systems, though, so correct me if this is wrong!) Using quantum computing to design superconducting materials probably won’t be <a href=https://twitter.com/RokoMijic/status/1684831855411961857>as easy as it seems on Twitter/X</a>.
</p>

<p>
So, while quantum computing is a terrifically exciting direction for computational chemistry in a scientific sense, I’m not sure it’s yet investable in a business sense. I don’t mean to belittle all the great scientific work being done in this field, in the papers I’ve referenced above and in many others. The point I’m trying to make here—that this field isn’t mature enough for actual commercial utility—could just as easily be made about ML in the 2000s, or any other number of promising but pre-commercial technologies. 
</p>

<p>
I’ll close by noting that it seems like markets are coming around to this perspective, too. Zapata Computing, one of the original “quantum computing for chemistry” companies, recently <a href=https://www.hpcwire.com/2023/09/07/quantum-hopeful-zapata-to-go-public-and-pivot-to-industrial-generative-ai/>pivoted</a> to… generative AI, going public via a SPAC with Andretti (motorsport), and IonQ recently <a href=https://www.businesswire.com/news/home/20231023981563/en/IonQ-Announces-Senior-Leadership-Transition>parted ways</a> with its CSO, who is going back to his faculty job at Duke. We’ll see what happens, but progress in hardware has been slow, and it’s likely that it’ll be years yet until we can start to perform practical quantum chemical calculations on quantum computers.
</p>

]]></description>
              <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Organic Chemistry’s Wish List, Four Years Later</title>
              <link>public/blog/20231020_wish_list.html</link>
              <description><![CDATA[
<p>
In 2019, ChemistryWorld published a <a href=https://web.archive.org/web/20190507001416/https://www.chemistryworld.com/news/the-five-reactions-on-every-organic-chemists-wish-list/3010150.article>“wish list”</a> of reactions for organic chemistry, describing five hypothetical reactions which were particularly desirable for medicinal chemistry. A few recent papers brought this back to my mind, so I revisited the list with the aim of seeing what progress had been made. (Note that I am judging these based solely by memory, and accordingly I will certainly omit work that I ought to know about—sorry!)
</p>

<h3>Fluorination</h3>

<blockquote>
1. Fluorination – Exchanging a specific hydrogen for a fluorine atom in molecules with many functional groups. A reaction that installs a difluoromethyl group would be nice too.
</blockquote>

<p>
This is still hard! To my knowledge, no progress has really been made towards this goal in a general sense (although plenty of isolated fluorination reactions are still reported, many of which are useful).
</p>

<p>
C–H fluorination is particularly challenging because separating C–H and C–F compounds can be quite difficult. (Fluorine is often considered a hydrogen bioisostere, which is nice from a design perspective but annoying from a <a href=https://pubs.acs.org/doi/abs/10.1021/ac403376h>chromatography</a> perspective.) For this reason, I’m more optimistic about methods that go through separable intermediates than the article’s author: “installing another reactive group… and exchanging it for fluorine” may not be particularly ideal in the Baran sense, but my guess is that this strategy will be more fruitful than direct C–H fluorination for a long while yet.
</p>

<h3>Heteroatom Alkylation</h3>

<blockquote>
2. Heteroatom alkylation – A reaction that – selectively – attaches an alkyl group onto one heteroatom in rings that have several, such as pyrazoles, triazoles and pyridones.
</blockquote>

<p>
This problem is still unsolved. Lloyd-Jones published some nice work on <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.9b02786>triazole alkylation</a> a few weeks after the ChemistryWorld article came out, but otherwise it doesn’t seem like this is a problem that people in academia are thinking much about.
</p>

<p>
Unlike some of the others, this challenge seems ideally suited to organocatalysis, so maybe someone else in that subfield will start working on it. (Our work on <a href=https://www.nature.com/articles/s41586-022-04958-w>site-selective glycosylation</a> might be relevant?)
</p>

<p>
<b>
EDIT: I missed <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.202014239>extremely relevant work</a> from Stephan Hammer, which uses engineered enzymes to alkylate pyrazoles with haloalkanes (and cites the ChemistryWorld article directly). Sorry!
</b>
</p>

<h3>Csp<sup>3</sup> Coupling</h3>

<blockquote>
3. Carbon coupling – A reaction as robust and versatile as traditional cross coupling for stitching together aliphatic carbon atoms – ideally with control of chirality, too. Chemists also want more options for the kinds of molecules they can use as coupling precursors.
</blockquote>

<p>
There’s been a ton of work on Csp<sup>3</sup> cross coupling since this article came out: MacMillan (<a href=https://macmillan.princeton.edu/wp-content/uploads/Zhe.pdf>1</a>, <a href=https://macmillan.princeton.edu/wp-content/uploads/science.abl4322.pdf>2</a>, <a href=https://macmillan.princeton.edu/wp-content/uploads/alcohol_acid.pdf>3</a>, <a href=https://macmillan.princeton.edu/wp-content/uploads/jacs.2c08989.pdf>4</a>, <a href=https://macmillan.princeton.edu/wp-content/uploads/jacs.3c01488.pdf>5</a>, <a href=https://macmillan.princeton.edu/wp-content/uploads/jacs.3c05405.pdf>6</a>) and Baran (<a href=https://pubs.acs.org/doi/epdf/10.1021/jacs.2c04358>1</a>, <a href=https://pubs.acs.org/doi/epdf/10.1021/jacs.3c03337>2</a>, <a href=https://www.nature.com/articles/s41586-023-06677-2>3</a>) have published a lot of papers, and plenty of other labs are also working here (I can’t list everyone, but I’ll highlight <a href=https://www.science.org/doi/full/10.1126/science.abo0039?af=R>this work</a> from Sevov). I doubt this can be considered “solved” yet, but certainly things are much closer than they were in 2019.
</p>

<p>
(I haven’t seen much work on enantioselective variants, though: <a href=https://pubs.acs.org/doi/10.1021/jacs.5b13211>this 2016 paper</a> and paper #2 from Baran above are the only ones that comes to mind, although I’m sure I’m missing something. Still—an opportunity!)
</p>

<h3>Reactions of Heterocycles</h3>

<blockquote>
4. Making and modifying heterocycles – A reaction to install functional groups – from alkyl to halogen – anywhere on aromatic and aliphatic heterocycles, such as pyridine, piperidine or isoxazole. Reactions that can make completely new heterocycles from scratch would be a bonus.
</blockquote>

<p>
I’m not a big fan of the way this goal is written—virtually every structure in medicinal chemistry has a heterocycle, so “making and modifying heterocycles” is just too vague. What would a general solution even look like?
</p>

<p>
Nevertheless, there are plenty of recent papers which address this sort of problem. Some of my favorites are:
</p>

<ul>
<li>
Aaron Sather’s work making <i>N</i>-aryl piperidines (<a href=https://pubs.acs.org/doi/abs/10.1021/jacs.9b13114>1</a>, <a href=https://chemrxiv.org/engage/chemrxiv/article-details/62853937f053df9a6d208577>2</a>)
</li>
<li>
Basically <a href=https://www.mcnallygroup.org/publications-2>all of Andy McNally’s work</a>
</li>
<li>
Some nice <a href=https://pubs.acs.org/doi/10.1021/jacs.0c03537>reagent design</a> from Patrick Fier, which basically improves on the Chichibabin reaction
</li>
</ul>

<p>
(One of my friends in academia told me that they really disliked the Sather work because it was just classic reactivity used in a straightforward way, i.e. not daring enough. What a clear illustration of misaligned incentives!)
</p>

<h3>Atom Swapping/Skeletal Editing</h3>

<blockquote>
5. Atom swapping – A reaction that can exchange individual atoms selectively, like swapping a carbon for a nitrogen atom in a ring. This chemical version of gene editing could revolutionise drug discovery, but is probably furthest from realisation.
</blockquote>

<p>
Ironically, this goal is probably the one that’s closest to realization today (or perhaps #3): <a href=https://pubs.acs.org/doi/10.1021/jacs.2c08464>Noah Burns</a> and <a href=https://www.science.org/doi/full/10.1126/science.adj5331>Mark Levin</a> have both published papers converting benzene rings directly to pyridines recently. More broadly, lots of organic chemists are getting interested in “skeletal editing” (i.e. modifying the skeleton of a molecule, not the periphery), which seems like exactly what this goal is describing. To my knowledge, a comprehensive review has not yet been published, but <a href=https://www.nature.com/articles/d41586-023-01735-1#ref-CR2>this article</a> gives a pretty good overview of the area.
</p>

<p>
Overall, it’s impressive how much progress has been made towards the goals enumerated in the original article, given that it’s only been four years (less than the average length of a PhD!). Organic methodology is a very exciting field right now: data is easy to acquire, there are lots of problems to work on, and there seems to be genuine interest from adjacent fields about the technologies being developed. Still, if the toughest challenges in the field’s imagination can be solved in under a decade, it makes you wonder what organic methodology will look like in 20–30 years. 
</p>

<p>
As methods get faster to develop and more and more methods are published, what will happen? Will chemists employ an exponentially growing arsenal of transformations in their syntheses, or will the same methods continually be forgotten and rediscovered every few decades? Will computers be able to sift through centuries of literature and build the perfect synthesis—or will the rise of automation mean that we have to redesign every reaction to be “dump and stir”? Or will biocatalysis just render this entire field obsolete? The exact nature of synthesis’s eschatology remains to be determined.
</p>

]]></description>
              <pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Networking, For Skeptics</title>
              <link>public/blog/20231016_networking.html</link>
              <description><![CDATA[
<figure>
  <img class=centered-img src=https://arthistorybuff.files.wordpress.com/2017/10/dp819996.jpg style="width:400px;" />
  <figcaption>
    "Hi, would you like to connect on LinkedIn?"
  </figcaption>
</figure>

<p>
<i>
(in the spirit of Dale Carnegie and post-rat <a href=https://sympatheticopposition.substack.com/p/how-and-why-to-be-ladylike-for-women>etiquette guides</a>)
</i>
</p>

<p>
Scientists, engineers, and other technical people often make fun of networking. Until a few years ago, I did this too: I thought networking was some dumb activity done by business students who didn’t have actual work to do, or something exploitative focused on pure self-advancement. But over the past year or so, I’ve learned why networking is important, and have found a way to network that doesn’t make me feel uncomfortable or selfish. I want to share my current thoughts here, in case they help anyone else.
</p>

<p>
The first thing to recognize is that networking matters because we live in a world of relationships. Technical people often struggle with this point: to some, relying on relationships feels imprecise or even nepotistic. But we’re human beings, not stateless automata communicating via some protocol, and it’s inevitable (and appropriate) for us to form relationships and care about them.
</p>

<p>
Having the right relationships can make a big difference. We trust people we know much more than we trust strangers. It’s weird for someone whom you’ve never met to email you asking for a favor, but very normal for a friend or acquaintance to reach out and ask for something. And most ambitious projects (academia, startups, etc) are limited not by money but by human capital: there are only so many talented people out there, and if you can’t get access to them, what you’re doing will suffer. (On a macro level, this explains why management consulting has become so important.)
</p>

<p>
So it’s worth intentionally building relationships before you have an immediate need. There are a lot of people,<sup>[citation needed]</sup> so how might one go about this? One obvious strategy might be to build relationships with people you think could be useful to you. But this doesn’t work very well. It’s not always obvious what will or won’t be useful in the future, and far too easy to let status quo bias reign supreme. (Most graduate students struggle to imagine how knowing someone from another subfield could ever be useful, let alone someone who isn’t a scientist, which makes it tough when they want to do <a href=https://news.ycombinator.com/item?id=24459155>something outside academia.</a>)
</p>

<p>
Another downside to this strategy is that you have to partition people into “useful” or “not useful” upon meeting them. This is self-defeating: most people can figure out when you’re treating them only as a means to an end, so walking around evaluating everyone’s utility tends to poison your interactions. Plus, it’s a very Machiavellian way to view the world, and ought to make you feel a little gross.
</p>

<p>
Instead, a better strategy is to accept that you won’t be able to predict <i>a priori</i> who will be useful and instead just try and meet people. If the end goal of networking is to find people you’ll be able to collaborate with in the future, in one capacity or another, then it’s important to find people who share your values and who you get along with: in other words, friends. So, rather than worrying about if it’ll be better to know a renewable energy consultant or a paralegal for a medical device company, you can just see who you like spending time with and go from there.
</p>

<p>
If we think of networking as synonymous to making friends, then it also becomes more obvious when and how one should network. Anything explicitly framed as an opportunity for networking is a bad choice: these events tend to attract people who are self-centered, and mostly end up revolving around LinkedIn (the Tinder of networking?). Instead, look for places where you’ll find people you could be friends with. For me, this ends up mostly being church and church-adjacent spaces like Bible studies; I’m not sure what the analogous space for non-religious people is.
</p>

<p>
The strategies I’ve discussed above are framed in terms of “demand-side networking,” or how you can find ways to acquire human capital when you have a demand for it. But the same considerations apply to “supply-side networking,” or marketing oneself to potential people. The beauty of treating networking simply as making friends is that you’re not committing to any particular outcome: maybe you’ll benefit from it, maybe the other person will benefit from it, or maybe both of you will (or neither). The expected value of building new relationships should always be positive, which means that networking isn’t a zero-sum game: it’s good for all involved.
</p>

<p>
The conclusion I want to leave you with is this: networking, rightly understood, just means living in a way that recognizes our dependence on other people. To live life as a “networker” means putting yourself in places where you might make new friends, looking for common ground in all your interactions, and trying to recognize others’ talents and abilities. Networking isn’t some Randian pursuit focused on extracting value from those around us. It should be done with humility, accepting that we need other people to thrive and being open to whatever relationships come our way.
</p>

<i>Thanks to Ari Wagen for editing drafts of this post.</i>

]]></description>
              <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Composite Methods in Quantum Chemistry</title>
              <link>public/blog/20230922_composite.html</link>
              <description><![CDATA[
<br>
<br>
<p class=epigraph>
“A cord of three strands is not easily broken.”
</p>
<p class=epigraph-byline>
—Ecclesiastes 4:12
</p>

<figure>
  <img class=centered-img src=https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Rams%C3%A9s_II_en_Qadesh%2C_relieve_de_Abu_Simbel.jpg/773px-Rams%C3%A9s_II_en_Qadesh%2C_relieve_de_Abu_Simbel.jpg style="width:350px;" />
  <figcaption>
    Abu Simbel relief of Ramesses II shooting a composite bow at the Battle of Kadesh.
  </figcaption>
</figure>

<p>
Computational chemistry, like all attempts to simulate reality, is defined by tradeoffs. Reality is far too complex to simulate perfectly, and so scientists have developed a plethora of approximations, each of which reduces both the cost (i.e. time) and the accuracy of the simulation. The responsibility of the practitioner is to choose an appropriate method for the task at hand, one which best balances speed and accuracy (or to admit that no suitable combination exists).
</p>

<p>
This situation can naturally be framed in terms of <a href=https://en.wikipedia.org/wiki/Pareto_efficiency>Pareto optimality</a>: there’s some “frontier” of speed/accuracy combinations which are at the limit of what’s possible, and then there are suboptimal combinations which are inefficient. Here’s a nice plot illustrating exactly that, from Dakota Folmsbee and Geoff Hutchinson (<a href=https://onlinelibrary.wiley.com/doi/full/10.1002/qua.26381>ref</a>):
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_pareto.png style="width:550px;" />
  <figcaption>
    The y axis represents R<sup>2</sup> among different (relative) conformer energies, and the x axis is a log scale of computational time. The pattern shown here—exponential increases in time for linear increases in accuracy—is pretty common, unfortunately.
  </figcaption>
</figure>

<p>
In this figure, the top left corner is the goal—perfect accuracy in no time at all—and the bottom right corner is the opposite. The diagonal line represents the Pareto frontier, and we can see that different levels of theory put you at different places along the frontier. <i>Ab initio</i> methods (DFT, MP2, coupled cluster) are slow but accurate, while force fields are fast but inaccurate, and semiempirical methods and ML methods are somewhere in the middle. (It’s interesting to observe that some ML methods are quite far from the optimal frontier, but I suppose that’s only to be expected from such a new field.)
</p>

<p>
An important takeaway from this graph is that some regions of the Pareto frontier are easier to access than others. Within e.g. DFT, it’s relatively facile to tune the accuracy of the method employed, but it’s much harder to find a method intermediate between DFT and semiempirical methods. (For a variety of reasons that I’ll write about later, this region of the frontier seems particularly interesting to me, so it’s not just an intellectual question.) This lacuna is what Stefan Grimme’s “composite” methods, the subject of today’s post, are trying to address.
</p>


<figure>
  <img class=centered-img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Zhangxian02.jpg" style="width:250px;" />
  <figcaption>
    I like to believe that these methods are named after the composite recurve bow, which is both smaller and more powerful than simple bows, but I don’t have evidence for this belief. 
    Pictured is Qing dynasty artwork of Zhang Xian shooting a composite bow.
  </figcaption>
</figure>

<p>
What defines a composite method? The term hasn’t been precisely defined in the literature (as far as I’m aware), but the basic idea is to strip down existing <i>ab initio</i> electronic structure methods as much as possible, particularly the basis sets, and employ a few additional corrections to fix whatever inaccuracies this introduces. Thus, composite methods still have the essential form of DFT or Hartree–Fock, but rely heavily on cancellation of error to give them better accuracy than one might expect. (This is in contrast to semiempirical methods like <i>xtb</i>, which start with a more approximate level of theory and layer on a ton of corrections.)
</p>

<p>
Grimme and coworkers are quick to acknowledge that their ideas aren’t entirely original. To quote from their first composite paper (on HF-3c):
</p>

<blockquote>
Several years ago Pople noted that HF/STO-3G optimized geometries for small molecules are excellent, better than HF is inherently capable of yielding. Similar observations were made by Kołos already in 1979, who obtained good interaction energies for a HF/minimal-basis method together with a counterpoise-correction as well as a correction to account for the London dispersion energy. It seems that part of this valuable knowledge has been forgotten during the recent “triumphal procession” of DFT in chemistry. The true consequences of these intriguing observations could not be explored fully at that time due to missing computational resources but are the main topic of this work.
</blockquote>

<p>
And it’s not as though minimal basis sets have been forgotten: <a href=https://link.springer.com/article/10.1007/BF01127507>MIDIX</a> still sees use (I used it during my PhD), and Todd Martinez has been exploring <a href=https://pubs.acs.org/doi/10.1021/jp307741u>these ideas</a> for a while. Nevertheless, composite methods seem to have attracted attention in a way that the above work hasn’t. I’ll discuss why this might be at the end of the post—but first, let’s discuss what the composite methods actually are.
</p>

<h2>
HF-3c (2013)
</h2>

<p>
<a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.23317>HF-3c</a> is a lightweight Hartree–Fock method, using a minimal basis set derived from Huzinaga’s MINIS basis set. To ameliorate the issues that Hartree–Fock and a tiny basis set cause, the authors layer in three corrections: the D3 dispersion correction (with Becke–Johnson damping), their recent <a href=https://pubs.aip.org/aip/jcp/article-abstract/136/15/154101/941628/A-geometrical-correction-for-the-inter-and-intra?redirectedFrom=fulltext>“gCP”</a> geometric counterpoise correction for basis set incompleteness error, and an “SRB” short-range basis correction for electronegative elements.
</p>

<p>
HF-3c is surprisingly good at geometry optimization and noncovalent interaction energies (MAE of 0.39 kcal/mol on the S66 benchmark set), works okay for <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jpca.1c10144>dipole moments</a> and vibrational frequencies, but seems bad for anything involving bond breaking. Thus the authors recommend it for optimization of ground-state complexes, and not so much for finding an entire potential energy surface.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_hf3c.png style="width:450px;" />
  <figcaption>
    Comparison of HF-3c and PM6 for geometry optimization, relative to B3LYP-D3/def2-TZVPP (black).
  </figcaption>
</figure>

<p>
(One complaint about all of these papers: the basis set optimization isn’t described in much detail, and we basically have to take the authors’ word that what they came up with is actually the best.)
</p>

<h2>
HF-3c(v) (2014)
</h2>

<p>
<a href=https://pubs.acs.org/doi/10.1021/jz5021313>HF-3c(v)</a> is pretty much the same as HF-3c, but it uses a “large-core” effective core potential to describe all of the core electrons, making it valence-only. This makes it 2–4 times faster, but also seems to make it much worse than HF-3c: I’m not sure the speed is worth the loss in accuracy.
</p>

<p>
The authors only use it to explore noncovalent interactions; I’m not sure if others have used it since.
</p>

<h2>
PBEh-3c (2015)
</h2>

<p>
<a href=https://pubs.aip.org/aip/jcp/article-abstract/143/5/054107/194086/Consistent-structures-and-interactions-by-density?redirectedFrom=fulltext>PBEh-3c</a> is the next “3c” method, and the first composite DFT method. As opposed to the minimal basis set employed in HF-3c, Grimme et al here elect to use a polarized double-zeta basis set, which “significantly improves the energetic description without sacrificing the computational efficiency too much.” They settle on a variant of def2-SV(P) with an effective core potential and a few other modifications, which they call “def-mSVP.”
</p>

<p>
As before, they also add the D3 and gCP corrections (both slightly modified), but they leave out the SRB correction. The biggest change is that they also reparameterize the PBE functional, which introduces an additional four parameters: three in PBE, and one to tune the percentage of Fock exchange. The authors note that increasing the Fock exchange from 25% to 42% offsets the error introduced by basis set incompleteness.
</p>

<p>
As before, the focus of the evaluation is on geometry optimization, and PBEh-3c seems to do very well (although not better than HF-3c on e.g. S66, which is surprising—the authors also don’t compare directly to HF-3c in the paper at all). PBEh-3c also does pretty well on the broad GMTKN30 database, which includes thermochemistry and barrier heights, faring just a bit worse than M06-2x/def2-SV(P).
</p>

<h2>
HSE-3c (2016)
</h2>

<p>
<a href=https://pubs.rsc.org/en/content/articlelanding/2016/CP/C6CP01697A>HSE-3c</a> is basically the same as PBEh-3c, but now using a screened exchange variant to make it more robust and faster for large systems or systems with small band gaps. The authors recommend using PBEh-3c for small molecules or large band-gap systems, which is my focus here, so I won’t discuss HSE-3c further.
</p>

<h2>
B97-3c (2018)
</h2>

<p>
<a href=https://pubs.aip.org/aip/jcp/article-abstract/148/6/064104/196461/B97-3c-A-revised-low-cost-variant-of-the-B97-D?redirectedFrom=fulltext>B97-3c</a> is another DFT composite method, but it’s a bit different than PBEh-3c. PBE is a pretty simple functional with only three tunable parameters, while B97 is significantly more complex with ten tunable parameters. Crucially, B97 is a pure functional, meaning that no Fock exchange is involved, which comes with benefits and tradeoffs. The authors write:
</p>

<blockquote>
The main aim here is to complete our hierarchy of “3c” methods by approaching the accuracy of large basis set DFT with a physically sound and numerically well-behaved approach.
</blockquote>

<p>
For a basis set, the authors use a modified form of def2-TZVP called “mTZVP”, arguing that “as the basis set is increased to triple-ζ quality, we can profit from a more flexible exchange correlation functional.” This time, the D3 and SRB corrections are employed, but the gCP correction is omitted.
</p>

<p>
The authors do a bunch of benchmarking: in general, B97-3c seems to be substantially better than either PBEh-3c or HF-3c at every task, which isn’t surprising given the larger basis set. B97-3c is also often better than e.g. B3LYP-D3 with a quadruple-zeta basis set, meaning that it can probably be used as a drop-in replacement for most routine tasks.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_b973c_rot.png style="width:450px;" />
  <figcaption>
    Comparison of a few methods for geometry optimization, as assessed by rotational constants. Both B97-3c and PBEh-3c perform better than HF-3c. 
  </figcaption>
</figure>

<h2>
B3LYP-3c (2020)
</h2>

<p>
<a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00877>B3LYP-3c</a> is a variant of PBEh-3c where you just remove the PBEh functional and replace it with B3LYP (without reparameterizing B3LYP at all). This is done to improve the accuracy for vibrational frequencies, since B3LYP performs quite well for frequencies. I’ve only seen this in one paper, so I’m not sure this will catch on (although it does seem to work).
</p>

<h2>
r<sup>2</sup> SCAN-3c (2021)
</h2>

<p>
Continuing our journey through the “Jacob’s ladder” of composite functionals, we arrive at <a href=https://pubs.aip.org/aip/jcp/article-abstract/154/6/064103/199831/r2SCAN-3c-A-Swiss-army-knife-composite-electronic?redirectedFrom=fulltext>r<sup>2</sup> SCAN-3c</a>, based on the meta-GGA r<sup>2</sup> SCAN functional. No reparameterization of the base functional is performed, but the D4 and gCP corrections are added, and yet another basis set is developed: mTZVPP, a variant of the mTZVP basis set developed for B97-3c, which was already a variant of def2-TZVP.
</p>

<p>
The authors describe the performance of r<sup>2</sup> SCAN-3c in rather breathless terms:
</p>

<blockquote>
…we argue that r<sup>2</sup> SCAN is the first mGGA functional that truly climbs up to the third rung of the Jacobs ladder without significant side effects (e.g., numerical instabilities or an overfitting behavior that leads to a bad performance for the mindless benchmark set).
<br><br>
…the new and thoroughly tested composite method r<sup>2</sup> SCAN-3c provides benchmark-accuracy for key properties at a fraction of the cost of previously required hybrid/QZ approaches and is more robust than any other method of comparable cost. This drastically shifts the aforementioned balance between the computational efficiency and accuracy, enabling much larger and/or more thorough screenings and property calculations. In fact, the robustness and broad applicability of r<sup>2</sup> SCAN-3c caused us to rethink the very structure of screening approaches.
</blockquote>

<p>
The amount of benchmarking performed is a little overwhelming. Here’s a nice figure that summarizes r<sup>2</sup> SCAN-3c on the GMTKN55 database, and also compares it to B97-3c:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_r2scan_acc.png style="width:450px;" />
  <figcaption>
  Notice how much better the results are than B97-3c.
  </figcaption>
</figure>

<p>
And here’s a nice graph that shows time comparisons for a 153-atom system, which is something that’s obviously a key part of the value-add:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_r2scan_speed.png style="width:500px;" />
  <figcaption>
  r<sup>2</sup> SCAN-3c is only a bit slower than B97-3c, and both are substantially faster than PBEh-3c (probably because PBEh is a global hybrid). HF-3c, of course, is still fastest.
  </figcaption>
</figure>

<h2>
ωB97X-3c (2022)
</h2>

<p>
Finally, we come to <a href=https://pubs.aip.org/aip/jcp/article-abstract/158/1/014103/2867476/B97X-3c-A-composite-range-separated-hybrid-DFT?redirectedFrom=fulltext>ωB97X-3c</a>, a composite range-separated hybrid functional derived from Mardirossian and Head-Gordon’s <a href=https://pubs.rsc.org/en/content/articlelanding/2014/cp/c3cp54374a>ωB97X-V</a> functional (which seems to me to be one of the best DFT methods, period). ωB97X-3c reintroduces Fock exchange, so it’s significantly more expensive than r<sup>2</sup> SCAN-3c or B97-3c, but with this expense comes increased accuracy.
</p>

<p>
Interestingly, neither of the “weird” corrections (gCP or SRB) are employed for ωB97X-3c: it’s just an off-the-shelf unmodified functional, the now-standard D4 dispersion correction, and a specialized basis set. The authors acknowledge this:
</p>

<blockquote>
Although ωB97X-3c is designed mostly in the spirit of the other “3c” methods, the meaning and definition of the applied “three corrections” have changed over the years. As before, the acronym stands for the dispersion correction and for the specially developed AO basis set, but here for the compilation of ECPs, which are essential for efficiency, as a third modification.
</blockquote>

<p>
(But weren’t there ECPs before? Doesn’t even HF-3c have ECPs? Aren’t ECPs just part of the basis set? Just admit that this is a “2c” method, folks.)
</p>

<p>
The authors devote a lot of effort to basis-set optimization, because range-separated hybrids are so expensive that using a triple-zeta basis set like they did for B97-3c or r<sup>2</sup> SCAN-3c would ruin the speed of the method. This time, they do go into more details, and emphasize that the basis set (“vDZP”) was optimized on molecules and not just on single atoms:
</p>

<blockquote>
Molecule-optimized basis sets are rarely used in quantum chemistry. We are aware of the MOLOPT sets in the CP2K code and the polarization consistent (pc-n) basis sets by Jensen. In the latter, only the polarization functions are optimized with respect to molecular energies. A significant advantage of molecular basis set optimizations is that, contrary to atomic optimizations, all angular momentum functions (i.e., polarization functions not occupied in the atomic ground state) can be determined consistently, as already noted by VandeVondele and Hutter.
</blockquote>

<p>
They also put together a new set of effective core potentials, which also helps to minimize the number of basis functions. Even so, ωB97X-3c is the slowest of the composite methods, as shown in this figure:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_wb_speed.png style="width:450px;" />
  <figcaption>
  Slow, but still faster than the competition.
  </figcaption>
</figure>

<p>
In terms of accuracy, ωB97X-3c is far better than r<sup>2</sup> SCAN-3c, previously the best composite method, and is among the best-performing DFT methods in general for most benchmarks, although still outcompeted by ωB97X-V (and its close cousin ωB97X-D4, reparameterized in this work). The expense of ωB97X-3c means that for easy tasks like geometry optimization r<sup>2</sup> SCAN-3c is still probably a better choice, but for almost anything else it seems that ωB97X-3c is an excellent choice.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230922_wb_acc.png style="width:450px;" />
</figure>

<p>
An interesting observation is that ωB97X-3c is not just Pareto-optimal, but close to optimal in an absolute sense. I initially framed the goal of composite methods as finding new ways to increase speed while decreasing accuracy: but here it seems that we can gain a ton of speed without losing much accuracy at all! This should be somewhat surprising, and the implications will be discussed later.
</p>

<h2>
Overall Summary
</h2>

<p>
Excluding some of the more specific composite methods, there seem to be three tiers here:
</p>

<ol>
<li>
HF-3c, which is very fast but only reliable for geometry optimizations and pretty easy property calculations (e.g. dipole moment, rough frequencies). But also, geometry optimizations are ubiquitous, so this is still very important.
</li>
<li>
PBEh-3c, B97-3c, and r<sup>2</sup> SCAN-3c, which are all roughly the same speed despite substantial differences. Of these, PBEh-3c seems like the slowest and least accurate, and I’d be tempted to use either of the others first, especially r<sup>2</sup> SCAN-3c. (I’m curious about the tradeoffs that a mGGA functional has versus a GGA functional like B97-3c—are there pathological integration grid behaviors that B97-3c avoids? See Figure 6 <a href=https://pubs.aip.org/aip/jcp/article-abstract/157/17/174114/2842017/Many-recent-density-functionals-are-numerically>here</a>.)
</li>
<li>
ωB97X-3c, which seems to be in a class of its own, and better than almost everything else (not just other composites). That being said, it’s still new, and no one loves a method better than its own authors. We’ll have to see some real-world tests to make sure things are as promising as they seem.
</li>
</ol>

<h2>
Conclusions
</h2>

<p>
After decades of theorists mocking people for using small basis sets, it’s ironic that intentionally embracing cancellation of error is trendy again. I’m glad to see actual theorists turn their attention to this problem: people have never stopped using “inadequate” basis sets like 6-31G(d), simply because nothing larger is practical for many systems of interest!
</p>

<p>
The results from this body of work suggest that current basis sets are far from optimal, too. The only piece of ωB97X-3c that’s new is the basis set, and yet that seems to make a huge difference relative to state-of-the-art. What happens if vDZP is used for other methods, like B97? The authors suggest that it might be generally effective, but more work is needed to study this further. 
</p>

<p>
<b>
  Update: Jonathon Vandezande and I investigated this question, and it turns out vDZP is effective with other density functionals too. You can read our preprint <a href=https://www.rowansci.com/publications/vdzp>here</a>!
</b>
</p>

<p>
Basis-set optimization seems like a <a href=http://www.paulgraham.com/schlep.html>“schlep”</a> that people have avoided because of how annoying it is, or something which is practically useful but not very scientifically interesting or publishable. Perhaps the success of composite methods will push more people towards studying basis sets; I think that would be a good outcome of this research. It seems unlikely to me that vDZP cannot be optimized further; if the results above are any indication, the Pareto frontier of basis sets can be advanced much more.
</p>

<p>
I’m also curious if Grimme and friends would change HF-3c and the other early methods, knowing what they know now. Do better basis sets alleviate the need for gCP and SRB, or is that not possible with a minimal basis set? What about D3 versus D4 (which wasn’t available at the time)? Hopefully someone finds the time to go back and do this, because to my knowledge HF-3c sees a good amount of use.
</p>

<p>
Perhaps my favorite part of this work, though, is the ways in which composite methods reduce the degrees of freedom available to end users. “Classic” DFT has a ton of tunable parameters (functional, basis set, corrections, solvent models, thresholds, and so forth), and people frequently make inefficient or nonsensical choices when faced with this complexity. In contrast, composite methods make principled and opinionated choices for many of these variables, thus giving scientists a well-defined menu of options.
</p>

<p>
This also makes it easier for outsiders to understand what’s going on. I wrote about this <a href=https://corinwagen.github.io/public/blog/20220810_viewpoints_on_simulation.html>over a year ago</a>:
</p>

<blockquote>
While a seasoned expert can quickly assess the relative merits of BYLP/MIDI! and DSD-PBEP86/def2-TZVP, to the layperson it’s tough to guess which might be superior… The manifold diversity of parameters employed today is a sign of [computational chemistry]’s immaturity—in truly mature fields, there’s an accepted right way to do things.
</blockquote>

<p>
The simplicity of composite methods cuts down on the amount of things that you have to memorize in order to understand computational chemistry. You only have to remember “HF-3c is fast and sloppy,” rather than trying to recall how many basis functions pcseg-1 has or which Minnesota functional is good for main-group geometries.
</p>

<p>
So, I’m really optimistic about this whole area of research, and I’m excited that other labs are now working on similar things (I didn’t have space to cover everyone’s contributions, but here’s <a href=https://pubs.aip.org/aip/jcp/article/146/23/234105/195242/Effective-empirical-corrections-for-basis-set>counterpoise work from Head-Gordon</a> and <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.7b01158>atom-centered potentials from DiLabio</a>). The next challenge will be to get these methods into the hands of actual practitioners…
</p>

]]></description>
              <pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>EDA Scares Me</title>
              <link>public/blog/20230904_eda.html</link>
              <description><![CDATA[
<p><i>
ICYMI: Ari and I announced our new company, <a href=https://www.rowansci.com/>Rowan</a>! We wrote an article about what we're hoping to build, which you can read <a href=https://rowansci.substack.com/p/why-were-building-rowan>here</a>. Also, this blog is now listed on <a href=https://rogue-scholar.org/>The Rogue Scholar</a>, meaning that posts have DOIs and can be easily cited.
</i></p>

<p>
Conventional quantum chemical computations operate on a collection of atoms and create a single wavefunction for the entire system, with an associated energy and possibly other properties. This is great, but sometimes we want to understand things in more detail. For instance, if we have a host <b>A</b> and two guests <b>B</b><sub>good</sub> and <b>B</b><sub>bad</sub>, a normal calculation would just tell us that E(<b>A</b>•<b>B</b><sub>good</sub>) is lower than E(<b>A</b>•<b>B</b><sub>bad</sub>), without giving any clue as to why.
</p>

<p>
Enter EDA. EDA, or “energy decomposition analysis,” is a family of techniques used to dissect interactions in a system with multiple molecules. In this case, an EDA calculation on the <b>AB</b> system would break down the interaction between <b>A</b> and <b>B</b> into various components, which could be used to help scientists understand the origin of the difference, and perhaps used for continued molecular design.
</p>

<p>
Unfortunately, EDA has always seemed like a pretty troubled technique to me. Wavefunctions are inherently not localized to individual fragments of a multimolecular system—you can’t just slice apart the molecular orbitals or the density matrix and end up with anything that’s physically sane. So you have to do some computational gymnastics to get energetic terms which are at all meaningful. Many such gymnastic workflows have been proposed, leading to a veritable alphabet soup of different EDA methods.
</p>

<p>
(It’s worth skimming <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.26003>this review</a> on different EDA methods to get a sense for some of the questions the field faces, and also to laugh at how Alston Misquitta &amp; Krzysztof Szalewicz use the review as a chance to relentlessly advertise SAPT and denigrate any and all competing methods.)
</p>

<p>
I’ll briefly outline how the EDA-NCOV method works for a system <b>AB</b> (following <a href=https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/wcms.1345>this review</a>), to give a sense for the flavor of the field:
</p>

<ol>
<li>
Optimized ground-state fragments <b>A</b><sup>0</sup> and <b>B</b><sup>0</sup> are distorted to the geometries and electronic states (<b>A</b> &amp; <b>B</b>) which they possess in <b>AB</b>, and the energy required for this distortion/excitation is termed <i>E</i><sub>prep</sub>. (The difference between <i>E</i>(<b>AB</b>) and <i>E</i>(<b>A</b>) + <i>E</i>(<b>B</b>) is called <i>E</i><sub>int</sub>, and the total binding energy is equal to <i>E</i><sub>int</sub> + <i>E</i><sub>prep</sub>.)
</li>
<li>
The distorted fragments <b>A</b> and <b>B</b> are brought together (with frozen charge densities) to form the “promolecule” <b>AB</b><sup>0</sup>, and the change in energy is termed <i>E</i><sub>elstat</sub>, the quasiclassical Coulomb interaction energy (typically attractive). The wavefunction for <b>AB</b><sup>0</sup> is Ψ<sup>A</sup>Ψ<sup>B</sup>.
</li>
<li>
The product wavefunction Ψ<sup>A</sup>Ψ<sup>B</sup> is antisymmetrized and renormalized to give an “intermediate state” Ψ<sup>0</sup> with energy <i>E</i><sup>0</sup>, and the change in energy is termed <i>E</i><sub>Pauli</sub>, originating from Pauli repulsion. This component is always repulsive.
</li>
<li>
Ψ<sup>0</sup> is relaxed to yield the final wavefunction Ψ<sup>AB</sup>. The change in energy is termed <i>E</i><sub>orb</sub>, because it arises from orbital interactions, and is always attractive.
</li>
</ol>

<p>
Thus, <i>E</i><sub>int</sub> = <i>E</i><sub>elstat</sub> + <i>E</i><sub>Pauli</sub> + <i>E</i><sub>orb</sub>. (Dispersion can also be added if an exogenous dispersion correction is employed—that’s pretty trivial.)
</p>

<p>
The critical reader might observe that the steps taken to obtain these numbers are pretty odd, and that the components of the interaction energy arise from differences in energy between bizarre nonphysical states. Thus, the interpretation of terms like <i>E</i><sub>elstat</sub> in terms of actual physical interactions might not be as easy as it seems. The authors of the above review agree:
</p>

<blockquote>
It is important to realize that the identification of the three major terms Δ<i>E</i><sub>elstat</sub>, Δ<i>E</i><sub>Pauli</sub>, and Δ<i>E</i><sub>orb</sub> with specific interactions is conceptually attractive but must not be taken as genuine expression of the physical forces.
</blockquote>

<p>
Unfortunately, it seems that imprecise concepts familiar to experimental chemists like “steric repulsion” and “electrostatic attraction” have to be discarded in favor of precise terms like <i>E</i><sub>Pauli</sub>. Too bad they’re virtually uninterpretable!
</p>

<p>
And what’s worse is that different EDA-type schemes don’t even give the same results. <a href=https://pubs.acs.org/doi/10.1021/jacs.3c08196>A paper</a> out today in <i>JACS</i> from Zare/Shaik discusses the use of EDA and related schemes in studying the origin of the hydrogen bond (a pretty fundamental question), motivated by the substantial disagreement between various techniques: 
</p>

<blockquote>
It is important to realize that different methods (e.g., BOVB, ALMO-EDA, NEDA, and BLW) do not fully agree with one another about whether the dominant stabilizing term is Δ<i>E</i><sub>POL</sub> or Δ<i>E</i><sub>CT</sub> in a particular HB.
</blockquote>

<p>
While the authors make a good case that the sum of these two terms is relatively conserved across methods, and that it’s this term that we should care about for hydrogen bonds, the conclusions for EDA broadly are not encouraging. (Note, too, that <i>E</i><sub>POL</sub> and <i>E</i><sub>CT</sub> don’t even appear in the EDA-NCOV method summarized above—another reason that EDA is a frustrating field!)
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230904_shoggoth.png style="width:550px;" />
  <figcaption>
  How I feel about EDA, borrowing <a href=https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html>a meme from the AI discourse</a>.
  </figcaption>
</figure>
 
<p>
And even if the theorists eventually put their heads together and develop a version of EDA that doesn’t have these pitfalls, it’s still not clear that any form of EDA will give the answers that experimental chemists are looking for. Chemistry is complicated, and ground- or transition-state structures arise from a delicate equilibrium between opposing factors: steric repulsion, electrostatic attraction, bond distances, torsional strain, dispersion, &amp;c.
</p>

<p>
As a result, one can see large changes in the contribution of individual factors even while the overall structure’s stability is minimally perturbed (<a href=https://pubs.rsc.org/en/content/articlehtml/2014/md/c4md00057a>enthalpy–entropy compensation</a> is a classic example, as is Fig. 2 in <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201701486>this review</a> on distortion–interaction analysis). Looking only at changes in individual factors isn’t always a useful way to gain insight from computation.
</p>

<p>
For example, imagine a nucleophile adding to two faces of an oxocarbenium, a bulky face and an unhindered face. Based on this description, we might expect to see that TS<sub>bulky</sub> has higher steric repulsion than TS<sub>unhindered</sub> (if we’re lucky enough to find a way to extract <i>E</i><sub>steric</sub> out of our EDA method). But it’s also likely that the nucleophile might take a less favorable trajectory towards the oxocarbenium in TS<sub>bulky</sub> to avoid steric repulsion, thus weakening key orbital interactions. These changes might even end up being larger in magnitude than the destabilization induced by steric repulsion. Is the correct answer, then, that TS<sub>bulky</sub> is higher in energy because of decreased <i>E</i><sub>orb</sub>, not increased <i>E</i><sub>steric</sub>?
</p>

<p>
The solution is to recognize that causation is not unique (cf. <a href=https://en.wikipedia.org/wiki/Four_causes>Aristotle</a>), and so there’s no one right answer here. Within the constraints of the EDA framework, the theorist wouldn’t be incorrect in saying that <i>E</i><sub>orb</sub> is the driving factor—but the experimental chemist might reasonably expect “the bulky TS is destabilized by steric repulsion” as their answer, since this is the root cause of the changes between the two structures. (I side with the experimentalists here.)
</p>

<p>
And the precisely defined concepts favored by theorists are often hard for experimental scientists to work with. Even if the correct answer in the above scenario were “<i>TS</i><sub>bulky</sub> is destabilized by decreased orbital overlap”—what’s an experimentalist supposed to do with this information, add more orbitals? (This is how I feel about Trevor Hamlin’s work on Pauli repulsion.) The steric explanation at least suggests an intuitive solution: make the bulky group or the nucleophile smaller. If the purpose of EDA is to help people to understand intermolecular interactions better on a conceptual level, I’m not sure it’s succeeding in most cases.
</p>

<p>
(The only use of EDA that led to an actual experimental advance which I’m aware of is Buchwald/Peng Liu’s body of work on ligand–substrate dispersion in hydrocupration: <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.7b07373>study</a>, <a href=https://pubs.acs.org/doi/10.1021/jacs.8b09565>new ligand</a>, <a href=https://pubs.acs.org/doi/10.1021/jacs.0c08746>ligand from Hartwig</a>. I don’t think it’s a coincidence that these papers focus on dispersion, one of the easiest pieces of EDA to decouple and understand.)
</p>

<p>
I don’t mean to be too critical here. The ability to break intermolecular interactions down into different components is certainly useful, and it seems likely that some version of EDA will eventually achieve consensus and emerge as a useful tool. But I think the utility of EDA even in the best case is pretty limited. Quantum chemistry is complicated, and if we think we can break it down into easy-to-digest components and eliminate the full nonlocal majesty of the Schrodinger equation, we’re lying to ourselves (or our experimental collaborators). Compute with caution!
</p>

]]></description>
              <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Transition States Are Polarizable</title>
              <link>public/blog/20230820_ts_polarizable.html</link>
              <description><![CDATA[
<p><i>
Recently in off-blog content: I coauthored <a href=https://www.freaktakes.com/p/the-past-and-present-of-computer>an article with Eric Gilliam</a> on how LLMs can assist in hypothesis generation and help us all become more like Sharpless. Check it out!
</i></p>

<p>
The Pauling model for enzymatic catalysis states that enzymes are “antibodies for the transition state”—in other words, they preferentially bind to the transition state of a given reaction, rather than the reactants or products. This binding interaction stabilizes the TS, thus lowering its energy and accelerating the reaction.  
</p>

<p>
This is a pretty intuitive model, and one which is often employed when thinking about organocatalysis, particularly noncovalent organocatalysis. (It’s a bit harder to use this model when the mechanism changes in a fundamental way, as with many organometallic reactions.) Many transition states have distinctive features, and it’s fun to think about what interactions could be engineered to recognize and stabilize only these features and nothing else in the reaction mixture. 
</p>

<p>
(For instance, chymotrypsin contains a dual hydrogen-bond motif called the “<a href=https://en.wikipedia.org/wiki/Oxyanion_hole>oxyanion hole</a>” which stabilizes developing negative charge in the Burgi–Dunitz TS for alcoholysis of amides. The negative charge is unique to the tetrahedral intermediate and the high-energy TSs to either side, so reactant/product inhibition isn’t a big issue. This motif can be mimicked by dual hydrogen-bond donor organocatalysts like the one my PhD lab specialized in.) 
</p>

<p>
The downside of this approach to catalyst design is that each new sort of reaction mechanism requires a different sort of catalyst. One TS features increasing negative charge at one place, while another features increasing positive charge at another, and a third is practically charge-neutral the whole way through. What if there were some feature that was common to all transition states? 
</p>

<p>
A recent <a href=https://chemrxiv.org/engage/chemrxiv/article-details/64def1de694bf1540c8b1e3c?s=03>preprint</a> from Diptarka Hait and Martin Head-Gordon suggests an interesting answer to this question: polarizability. (Diptarka, despite just finishing his PhD, is a <a href=https://scholar.google.com/citations?user=R2WrGKMAAAAJ&hl=en>prolific</a> scientist with a ton of different interests, and definitely someone to keep an eye on.) The authors tackle the question of when precisely a stretching bond can be considered “broken.” An intuitive answer might be “the transition state,” but as the paper points out, plenty of bond-breaking potential energy surfaces lack clear transition states (e.g. H<sub>2</sub>, pictured below in red). 
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230820_polariz.png style="width:450px;" />
  <figcaption>
  Hydrogen PES (red), with various polarizability components shown in blue.
  </figcaption>
</figure>

<p>
Instead, the authors propose that polarizability is a good way to study this question. As seen in the following graph, polarizability (particularly α<sub>||</sub>, the component parallel to the bond axis) first increases as a bond is stretched, and then decreases, with a sharp and easily identifiable maximum about where a bond might be considered to be broken. This metric tracks with the conventional PES metric in cases where the PES is well-defined (see Fig. 5), which is comforting. Why does this occur?
</p>

<blockquote>
The evolution of α [polarizability] can be rationalized in the following manner. Upon initially stretching H2 from equilibrium, the bonding electrons fill up the additional accessible volume, resulting in a more diffuse (and thus more polarizable) electron density. Post bond cleavage however, the electrons start localizing on individual atoms, leading to a decrease in polarizability.
</blockquote>

<p>
In other words, electrons that are “caught in the act” of reorganizing between different atoms are more polarizable, whereas electrons which have settled into their new atomic residences are less polarizable again.
</p>

<p>
This is cool, but how can we apply this to catalysis? As it happens, there are already a few publications (<a href=https://pubs.acs.org/doi/abs/10.1021/jo960521y>1</a>, <a href=https://pubs.acs.org/doi/abs/10.1021/jo960521y>2</a>) from Dennis Dougherty and coworkers dealing with exactly this question. They show that cyclophanes are potent catalysts for S<sub>N</sub>2-type reactions that both create and destroy cations, and argue that polarizability, rather than any charge-recognition effect, undergirds the observed catalysis:
</p>

<blockquote>
Since transition states have long, weak bonds, they are expected to be more polarizable than ground-state substrates or products. The role of the host is to surround the transition state with an electron-rich-system that is polarizable and in a relatively fixed orientation, so that induced dipoles in both the host and the transition state are suitably aligned… Note that in this model, it is not sufficient that polarizability contributes to binding. Polarizability must be more important for binding transition states than for binding ground states. Only in this way can it enhance catalysis.
</blockquote>

<p>
To defend this argument, the authors prepare a series of variously substituted cyclophanes, and show that while Hammett-type electronic tuning of the aryl rings has relatively small effects, adding more heavy atoms always increases the rate, with the biggest effect observed with bromine (the heaviest element attempted). Heavier atoms are more polarizable, so this supports the argument that polarizability, rather than any specific electrostatic effect, is responsible for catalysis. 
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230820_cyclophane.png style="width:450px;" />
  <figcaption>
  The cyclophanes used as catalysts: Y=Br is the best.
  </figcaption>
</figure>

<p>
The Dougherty work is performed on a very specific model system, and the absolute rate accelerations seen aren’t massive (about twofold increase relative to the protio analog), so it’s not clear that this will actually be a promising avenue for developing mechanism-agnostic catalysts. 
</p>

<p>
But I think this line of research is really interesting, and merits further investigation. Pericyclic reactions, which involve large electron clouds and multiple forming–breaking bonds and often feature minimal development of partial charges, seem promising targets for this sort of catalysis—what about the carbonyl–ene reaction or something similar? The promise of new catalytic strategies that complement existing mechanism-specific interactions is just too powerful to leave unstudied. 
</p>

<i>
Thanks to Joe Gair for originally showing me the Dougherty papers referenced.
</i>

]]></description>
              <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>How Common Are Different Functional Groups?</title>
              <link>public/blog/20230804_abundance.html</link>
              <description><![CDATA[
<p>
Since the ostensible purpose of organic methodology is to develop reactions that are useful in the real world, the utility of a method is in large part dictated by the accessibility of the starting materials. If a compound is difficult to synthesize or hazardous to work with, then it’s difficult to convince people to use it in a reaction (e.g. most diazoalkanes). Organic chemists are pragmatic, and would usually prefer to run a reaction that starts from a commercial and bench-stable starting material. 
</p>

<p>
For instance, this explains the immense popularity of the Suzuki reaction: although the Neigishi reaction (using organozinc nucleophiles) usually works better for the same substrates, you can buy lots of the organoboron nucleophiles needed to run a Suzuki and leave them lying around without taking any precautions. In contrast, organozinc compounds usually have to be made from the corresponding organolithium/Grignard reagent and used freshly, which is considerably more annoying.
</p>

<p>
The ideal starting material, then, is one which is commercially available and cheap. In recent years, it’s become popular to advertise new synthetic methods by showing that they work on exceptionally cheap and common functional groups, and in particular to compare the abundance of different functional groups to demonstrate that one starting material is more common than another. To pick just one of many examples, Dave MacMillan used this plot to show why cross-coupling reactions of alcohols were important (<a href=https://macmillan.princeton.edu/wp-content/uploads/Zhe.pdf>ref</a>):
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230804_macmillan.png style="width:500px;" />
  <figcaption>
  This visual works really well.
  </figcaption>
</figure>

<p>
When I saw MacMillan’s talk at MIT last year, I was curious what it would take to make additional graphics like this. The “number of reactions” plot can be made pretty easily from Reaxys, but I’ve always been uncertain how the “number of commercial sources” plots are made: I haven’t seen references listed for these numbers, nor is anything usually found in the Supporting Information. 
</p>

<p>
I decided to take a swing at getting this data myself by analyzing the <a href=https://mcule.com/database/>Mcule</a> "building blocks" database, which contains about 3.5 million compounds. Although Mcule doesn't define what a building block is (at least, not that I can find), it’s likely that their definition is similar to that of ZINC, which defines building blocks as “those catalogs of compounds available in preparative quantities, typically 250 mg or more” (<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559>ref</a>). This seems like a reasonable proxy for the sorts of compounds synthetic chemists might use in reactions. I defined patterns to match a bunch of functional groups using SMARTS/SMILES, and then used RDKit to find matches in the Mcule building blocks database. The code can be found on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule>Github</a>, along with the patterns I used.
</p>

<p>
The results are shown below. As expected, ethers, amines, amides, and alcohols are quite common. Surprisingly, aryl chlorides aren't that much more common than aryl bromides—and, except for aliphatic fluorides, all aliphatic halides are quite rare. Allenes, carbodiimides, and SF5 groups are virtually unheard of (&lt;100 examples).
</p>

<table class=left-aligned-table>
  <tr>
    <th>Functional Group</th>
    <th>Number</th>
    <th>Percent</th>
  </tr>
	<tr>
		<td>acid chloride</td>
		<td>6913</td>
		<td>0.19</td>
	</tr>
	<tr>
		<td>alcohol</td>
		<td>1022229</td>
		<td>28.60</td>
	</tr>
	<tr>
		<td>aliphatic bromide</td>
		<td>42018</td>
		<td>1.18</td>
	</tr>
	<tr>
		<td>aliphatic chloride</td>
		<td>70410</td>
		<td>1.97</td>
	</tr>
	<tr>
		<td>aliphatic fluoride</td>
		<td>650576</td>
		<td>18.20</td>
	</tr>
	<tr>
		<td>aliphatic iodide</td>
		<td>3159</td>
		<td>0.09</td>
	</tr>
	<tr>
		<td>alkene</td>
		<td>176484</td>
		<td>4.94</td>
	</tr>
	<tr>
		<td>alkyne</td>
		<td>35577</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>allene</td>
		<td>99</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>amide</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td>anhydride</td>
		<td>1279</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>aryl bromide</td>
		<td>451451</td>
		<td>12.63</td>
	</tr>
	<tr>
		<td>aryl chloride</td>
		<td>661591</td>
		<td>18.51</td>
	</tr>
	<tr>
		<td>aryl fluoride</td>
		<td>618620</td>
		<td>17.31</td>
	</tr>
	<tr>
		<td>aryl iodide</td>
		<td>216723</td>
		<td>6.06</td>
	</tr>
	<tr>
		<td>azide</td>
		<td>5164</td>
		<td>0.14</td>
	</tr>
	<tr>
		<td>aziridine</td>
		<td>748</td>
		<td>0.02</td>
	</tr>
	<tr>
		<td>carbamate</td>
		<td>127103</td>
		<td>3.56</td>
	</tr>
	<tr>
		<td>carbodiimide</td>
		<td>28</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>carbonate</td>
		<td>1231</td>
		<td>0.03</td>
	</tr>
	<tr>
		<td>carboxylic acid</td>
		<td>410860</td>
		<td>11.49</td>
	</tr>
	<tr>
		<td>chloroformate</td>
		<td>250</td>
		<td>0.01</td>
	</tr>
	<tr>
		<td>cyclobutane</td>
		<td>195728</td>
		<td>5.48</td>
	</tr>
	<tr>
		<td>cyclopropane</td>
		<td>349455</td>
		<td>9.78</td>
	</tr>
	<tr>
		<td>diene</td>
		<td>10188</td>
		<td>0.29</td>
	</tr>
	<tr>
		<td>difluoromethyl</td>
		<td>163395</td>
		<td>4.57</td>
	</tr>
	<tr>
		<td>epoxide</td>
		<td>5859</td>
		<td>0.16</td>
	</tr>
	<tr>
		<td>ester</td>
		<td>422715</td>
		<td>11.83</td>
	</tr>
	<tr>
		<td>ether</td>
		<td>1434485</td>
		<td>40.13</td>
	</tr>
	<tr>
		<td>isocyanate</td>
		<td>1440</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>isothiocyanate</td>
		<td>1389</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>nitrile</td>
		<td>209183</td>
		<td>5.85</td>
	</tr>
	<tr>
		<td>nitro</td>
		<td>126200</td>
		<td>3.53</td>
	</tr>
	<tr>
		<td>pentafluorosulfanyl</td>
		<td>18</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>primary amine</td>
		<td>904118</td>
		<td>25.29</td>
	</tr>
	<tr>
		<td>secondary amine</td>
		<td>857290</td>
		<td>23.98</td>
	</tr>
	<tr>
		<td>tertiary amine</td>
		<td>609261</td>
		<td>17.04</td>
	</tr>
	<tr>
		<td>trifluoromethoxy</td>
		<td>18567</td>
		<td>0.52</td>
	</tr>
	<tr>
		<td>trifluoromethyl</td>
		<td>455348</td>
		<td>12.74</td>
	</tr>
	<tr>
		<td>urea</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td><i>Total</i></td>
		<td><i>3574611</i></td>
		<td><i>100.00</i></td>
	</tr>
</table>

<p>
(Fair warning: I’ve spotchecked a number of the SMILES files generated (also on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule/output>Github</a>), but I haven’t looked through every molecule, so it’s possible that there are some faulty matches. I wouldn’t consider these publication-quality numbers yet.)
</p>

<p>
An obvious caveat: there are lots of commercially “rare” functional groups which are easily accessible from more abundant functional groups. For instance, acid chlorides seem uncommon in the above table, but can usually be made from ubiquitous carboxylic acids with e.g. SOCl2. So these data shouldn’t be taken as a proxy for a more holistic measure of synthetic accessibility—they measure commercial availability, that’s all. 
</p>

<p>
What conclusions can we draw from this?
</p>

<ul>
<li>
The most common functional groups are the milquetoast ones: alcohols, amines, esters, etc. Perhaps this explains <a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jmedchem.5b01409>where all the new reactions have gone</a>: unless your new method works on alcohols or amines, it will struggle to get traction in most of chemical space relative to e.g. Williamson ether synthesis or reductive amination. (Kudos to MacMillan for identifying this; <i>vide supra</i>.)
</li>
<li>
Ureas are much more common than you’d expect from academic methods papers. This I think speaks to the difference between what methodologists want and what medicinal chemists want. Ureas are a bit annoying to work with: they’re pretty polar by the standards of academia, they’re not always soluble in organic solvents, and they have a tendency to stick to transition metal catalysts or get deprotonated by strong bases. But they’re easy to make in libraries, since the isocyanate/amine disconnection is so robust, and they’re excellent hydrogen-bond donors and acceptors.
<b>CORRECTION: There's a SMARTS error, so the match for "ureas" actually matches amides—disregard this section. Thanks to <a href=https://twitter.com/wmdhn>@wmdhn</a> for catching this.</a></b>
</li>
<li>
Uncommon functional groups, like SF<sub>5</sub> and allenes, are very uncommon. If you want to introduce an SF<sub>5</sub> group, you are in for a rough time: there aren’t great ways to add it to molecules (although there have been some steps forward <a href=https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/chem.202201491>in recent years</a>), and there are only 18 commercial examples. So people can write as many <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7728329/>papers</a> as they want about how cool SF<sub>5</sub> groups are: I still doubt we’ll see them used very much in the near future.
</li>
<li>
But also, the abundance of a given functional group is very elastic in the long run. Trifluoromethyl groups used to be extremely rare—they’re not found in nature!—but now 1 in 8 molecules has a CF<sub>3</sub> group. CF<sub>3</sub> just turns out to be a very good handle for a lot of molecular design tasks, and so people found ways to introduce it all over the place, and now it’s not hard to get molecules that have trifluoromethyl groups. Synthetic chemists should feel good about this.
</li>
</ul>

<p>
The functional-group-specific SMILES files are in the previously mentioned Github repo, so anyone who wants to e.g. look through all the commercially available alkenes and perform further cheminformatics analyses can do so. I hope the attached code and data helps other chemists perform similar, and better, studies, and that this sort of thinking can be useful for those who are currently engaged in reaction discovery. 
</p>

<i>
Thanks to Eric Jacobsen for helpful conversations about these data.
</i>

]]></description>
              <pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Two Cultures in Atomistic Simulation</title>
              <link>public/blog/20230728_two_cultures.html</link>
              <description><![CDATA[<br>
<br>
<i>
TW: stereotypes about molecular dynamics.
</i>

<p>
In his fantastic essay “<a href=https://en.wikipedia.org/wiki/The_Two_Cultures>The Two Cultures</a>,” C. P. Snow observed that there was (in 1950s England) a growing divide between the academic cultures of science and the humanities:
</p>

<blockquote>
Literary intellectuals at one pole—at the other scientists, and as the most representative, the physical scientists. Between the two a gulf of mutual incomprehension—sometimes (particularly among the young) hostility and dislike, but most of all lack of understanding. They have a curious distorted image of each other. Their attitudes are so different that, even on the level of emotion, they can't find much common ground.
</blockquote>

<p>
He reflects on the origins of this phenomenon, which he contends is new to the 20th century, and argues that it ought to be opposed:
</p>

<blockquote>
This polarisation is sheer loss to us all. To us as people, and to our society. It is at the same time practical and intellectual and creative loss, and I repeat that it is false to imagine that those three considerations are clearly separable. But for a moment I want to concentrate on the intellectual loss.
</blockquote>

<p>
Snow’s essay is wonderful: his portrait of a vanishing cultural intellectual unity should inspire us all, scientists and otherwise, to improve ourselves, and the elegiac prose reminds the reader that even the best cultural institutions are fragile and fleeting things.
</p>

<p>
I want to make an analogous—but much less powerful—observation about the two cultures present in atomistic simulation. I’ll call these the “QM tribe” and the “MD tribe” for convenience: crudely, “people who use Gaussian/ORCA/Psi4 for their research” and “people who use Schrodinger/AMBER/OpenMM/LAMMPS for their research,” respectively. Although this dichotomy is crude, I contend there are real differences between these two groups, and that their disunity hurts scientific progress.
</p>

<h2>
The Nature of Energy Surfaces
</h2>

<p>
The most fundamental disagreement between these two cultures is in how they think about energy surfaces, I think. Most QM-tribe people think in terms of optimizing to discrete critical points on the potential energy surface: one can perform some sort of gradient-informed optimization to a ground state, or follow negative eigenvalues to a transition state.
</p>

<p>
Implicit to this assumption is that there exist well-defined critical points on the PES, and that finding such critical points is meaningful and productive. Conformers exist, and many people now compute properties as Boltzmann-weighted averages over conformational ensembles, but this is usually done for 10–100 conformers, not thousands or millions. Entropy and solvation, if they’re considered at all, are viewed as corrections, not key factors: since QM is so frequently used to study high-barrier bond-breaking processes where enthalpic factors dominate, one can often get reasonable results with cartoonish treatments of entropy.
</p>

<p>
In contrast, MD-tribe scientists generally don’t think about transition states as specific configurations of atoms—rather, a transition state can emerge from some sort of simulation involving biased sampling, but it’s just a position along some abstract reaction coordinate, rather than a structure which can be visualized in CYLView. Any information gleaned is statistical, rather than concretely visual (e.g. “what is the mean number of hydrogen bonds to this oxygen near this transition state”).
</p>

<p>
Unlike the QM tribe, MD-tribe scientists generally cannot study bond-breaking processes, and so focus on conformational processes (protein folding, aggregation, nucleation, transport) where entropy and solvation are of critical importance: as such, free energy is almost always taken into consideration by MD-tribe scientists, and the underlying PES itself is rarely (to my knowledge) viewed as a worthy topic of study in and of itself.
</p>

<h2>
Molecular Representations
</h2>

<p>
This divide also affects how the two cultures view the task of molecular representation. QM-tribe scientists generally view a list of coordinates and atomic numbers as the most logical representation of a molecule (perhaps with charge and multiplicity information). To the QM tribe, a minimum on the PES represents a structure, and different minima naturally ought to have different representations. Bonding and bond order are not specified, because QM methods can figure that out without assistance (and it’s not uncommon for bonds to change in a QM simulation anyway).
</p>

<p>
In contrast, people in the MD tribe generally want a molecular representation that’s independent of conformation, since many different conformations will intrinsically be considered. (See Connor Coley’s <a href=https://drive.google.com/file/d/1PMZ8GHvvJv_jhy8t6fH73wv4e9ZjHwUN/view>presentation</a> from a recent MolSSI workshop for a discussion of this.) Thus, it’s common to represent molecules through their topology, where connectivity and bond order are explicitly specified. This allows for some pretty wild <a href=https://pubs.acs.org/doi/10.1021/ja00241a001>simulations</a> of species that would be reactive in a QM simulation, but also means that e.g. tautomers can be a massive problem in MD (<a href=https://link.springer.com/article/10.1007/s10822-016-9920-5>ref</a>), since protons can’t equilibrate freely.
</p>

<p>
For property prediction, an uneasy compromise can be reached wherein one takes a SMILES string, performs a conformational search, and then Boltzmann-averages properties over all different conformers, to return a set of values which are associated only with the SMILES string and not any individual conformation. (Matt Sigman does this, as does Bobby Paton <a href=https://nova.chem.colostate.edu/cascade/predict/>for NMR prediction.</a>) This is a lot of work, though.
</p>

<h2>
“A Gulf Of Mutual Incomprehension”
</h2>

<p>
These differences also become apparent when comparing the software packages that different tribes use. Take, for instance, the task of predicting partial charges for a given small molecule. A QM-tribe scientist would expect these charges to be a function of the geometry, whereas an MD-tribe scientist would want the results to be explicitly geometry-independent (<a href=https://www.cell.com/biophysj/fulltext/S0006-3495(22)01326-1>e.g.</a>) so that they can be used for subsequent MD simulations.
</p>

<p>
The assumptions implicit to these worldviews mean that it’s often quite difficult to go from QM-tribe software packages to MD-tribe software packages or vice versa. I’ve been <a href=https://github.com/openforcefield/openff-toolkit/issues/611>stymied</a> before by trying to get OpenMM and openforcefield to work on organic molecules for which I had a list of coordinates and not e.g. a SMILES string—although obviously coordinates will at some point be needed in the MD simulation, most workflows expect you to start from a topology and not an xyz file.
</p>

<p>
Similarly, it’s <a href=https://github.com/nglviewer/nglview/issues/589#issuecomment-468339971>very difficult</a> to get the graphics package NGLView to illustrate the process of bonds breaking and forming—NGLView is typically used for MD, and expects that the system’s topology will be defined at the start of the simulation and never changed. (There are kludgy workarounds, like defining a new object for every frame, but it’s nevertheless true that NGLView is not made for QM-tribe people.)
</p>

<p>
(I’m sure that MD-tribe people are very frustrated by QM software as well, but I don’t have as much experience going in this direction. In general, MD tooling seems quite a bit more advanced than QM-tribe tooling; most MD people I’ve talked to seem to interact with QM software as little as possible, and I can’t say I blame them.)
</p>

<h2>
“A Curious Distorted Image of Each Other”
</h2>

<p>
There are also cultural factors to consider here. The questions that QM-tribe scientists think about are different than those that MD-tribe scientists think about: a somewhat famous QM expert once told me that they were “stuck on an ivory tower where people hold their nose when it comes to DFT, forget anything more approximate,” whereas MD-tribe scientists often seem alarmingly unconcerned about forcefield error but are obsessed with proper sampling and simulation convergence.
</p>

<p>
It seems that most people have only a vague sense of what their congeners in the other tribe actually work on. I don’t think most QM-tribe scientists I know have ever run or analyzed a regular molecular dynamics simulation using e.g. AMBER or OpenMM, nor do I expect that most MD-tribe scientists have tried to find a transition state in Gaussian or ORCA. In theory, coursework could remedy this, but education for QM alone already seems chaotic and ad hoc—trying to cram in MD, statistical mechanics, etc is probably ill-advised at the present.
</p>

<p>
Social considerations also play a role. There’s limited crosstalk between the two fields, especially at the trainee level. How many QM people even know who Prayush Tiwary is, or Michael Shirts, or Jay Ponder? How many MD graduate students have heard of Frank Neese or Troy Van Voorhis? As always, generational talent manages to transcend narrow boundaries—but rank-and-file scientists would benefit immensely from increased contact with the other tribe.
</p>

<h2>
“Unite Them”
</h2>

<p>
I’m not an expert on the history of chemistry, but my understanding is that the two fields were not always so different: Martin Karplus, Arieh Warshel, and Bill Jorgensen, key figures in the development of modern MD, were also formidable quantum chemists. (If any famous chemists who read this blog care to share their thoughts on this history, please email me: you know who you are!)
</p>

<p>
And as the two fields advance, I think they will come closer together once more. As QM becomes capable of tackling larger and larger systems, QM-tribe scientists will be forced to deal with more and more complicated conformational landscapes: modern enantioselective catalysts routinely have hundreds of ground-state complexes to consider (<a href=https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc02274e>ref</a>), and QSimulate and Amgen recently reported full DFT calculations on protein–ligand complexes (<a href=https://core.ac.uk/download/pdf/345075772.pdf>ref</a>).
</p>

<p>
Similarly, the increase in computing power means that many MD use cases (like FEP) are now limited not by insufficient sampling but by the poor energetics of the forcefields they employ. This is difficult to prove unequivocally, but I’ve heard this in interviews with industry folks, and there are certainly plenty of references complaining about poor forcefield accuracy (<a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00801>1</a>, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c01081>2</a>): a Psivant <a href=https://pubs.acs.org/doi/10.1021/acs.jcim.7b00564>review</a> dryly notes that “historically solvation energy errors on the order of 2–3 kcal/mol have been considered to be accurate,” which is hardly encouraging.
</p>

<p>
Many QM-tribe professors now work on dynamics: Dean Tantillo and Todd Martinez (who have long been voices “crying out in the wilderness” for dynamics) perhaps most prominently, but also Steven Lopez, Daniel Ess, Fernanda Duarte, Peng Liu, etc. And MD-tribe professors seem more and more interested in using ML mimics of QM to replace forcefields (<a href=https://www.biorxiv.org/content/10.1101/2020.07.29.227959v1>e.g.</a>), which will inevitably lead them down the speed–accuracy rabbit hole that is quantum chemistry. So it seems likely to me that the two fields will increasingly reunite, and that being a good 21st-century computational chemist will require competency in both areas.
</p>

<p>
If this is true, the conclusions for individual computational chemists are obvious: learn techniques outside your specialty, before you get forcibly dragged along by the current of scientific progress! There’s plenty to learn from the other culture of people that deals with more-or-less the same scientific problems you do, and no reason to wait.
</p>

<i>
As a denizen of quantum chemistry myself, I apologize for any misrepresentations or harmful stereotypes about practitioners of molecular dynamics, for whom I have only love and respect. I would be happy to hear any corrections over email.
</i>

]]></description>
              <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Machine Learning for Explicit Solvent Molecular Dynamics</title>
              <link>public/blog/20230720_ml_aimd.html</link>
              <description><![CDATA[
<p>
An important problem with simulating chemical reactions is that reactions generally take place in solvent, but most simulations are run without solvent molecules. This is a big deal, since much of the inaccuracy associated with simulation actually stems from poor treatment of solvation: when gas phase experimental data is compared to computations, the results are often quite good. 
</p>

<p>
Why don’t computational chemists include solvent molecules in their models? It takes a lot of solvent molecules to accurately mimic bulk solvent (enough to cover the system with a few different layers, usually ~10<sup>3</sup>).<sup><a href="#fn1">1</a></sup> Since most quantum chemical methods scale in practice as <i>O</i>(N<sup>2</sup>)–<i>O</i>(N<sup>3</sup>), adding hundreds of additional atoms has a catastrophic effect on the speed of the simulation.
</p>

<p>
To make matters worse, the additional degrees of freedom introduced by the solvent molecules are very “flat”—solvent molecules don’t usually have well-defined positions about the substrate, meaning that the number of energetically accessible conformations goes to infinity (with attendant consequences for entropy). This necessitates a fundamental change in how calculations are performed: instead of finding well-defined extrema on the electronic potential energy surface (ground states or transition states), molecular dynamics (MD) or Monte Carlo simulations must be used to sample from an underlying distribution of structures and reconstruct the free energy surface. Sufficient sampling usually requires consideration of 10<sup>4</sup>–10<sup>6</sup> individual structures,<sup><a href="#fn2">2</sup></a> meaning that each individual computation must be very fast (which is challenging for quantum chemical methods).
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230720_jorgensen.png style="width:550px;" />
  <figcaption>
  The title of this paper makes me so sad, because these techniques are still ignored by most organic chemists.
  </figcaption>
</figure>

<p>
Given the complexity this introduces, it’s not surprising that most computational organic chemists try to avoid explicit solvent at all costs. The typical workaround is to use “implicit solvent” models, which “reduce the complexity of individual solvent−solute interactions such as hydrogen-bond, dipole−dipole, and van der Waals interactions into a fictitious surface potential... scaled to reproduce the experimental solvation free energies” (<a href=https://pubs.acs.org/doi/abs/10.1021/acs.organomet.8b00456?src=recsys>Baik</a>). This preserves the well-defined potential energy surfaces that organic chemists are accustomed to, so you can still find transition states by eigenvector following, etc.
</p>

<p>
Implicit solvent models like PCM, COSMO, or SMD are better than nothing, but are known to struggle for charged species. In particular, they don’t really describe explicit inner-sphere solvent–solute interactions (like hydrogen bonding), meaning that they’ll behave poorly when these interactions are important. Dan Singleton’s paper on <a href=https://pubs.acs.org/doi/10.1021/ja5111392>the Baylis–Hillman reaction</a> is a nice case study of how badly implicit solvent can fail: even high-level quantum chemical methods are useless when solvation free energies are 10 kcal/mol off from experiment!
</p>

<p>
This issue is well-known. To quote from <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201709943>Schreiner and Grimme</a>:
</p>

<blockquote>
An even more important but still open issue is solvation. In the opinion of the authors it is a ‘scandal’ that in 2018 no routine free energy solvation method is available beyond (moderately successful) continuum theories such as COSMO-RS and SMD and classical FF/MD-based explicit treatments.
</blockquote>

<p>
When computational studies have been performed in explicit solvent, the results have often been promising: Singleton has studied <a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c06295>diene hydrochlorination</a> and <a href=https://pubs.acs.org/doi/10.1021/jacs.6b07328>nitration of toluene</a>, and Peng Liu has recently conducted a nice <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.0c12096>study of chemical glycosylation</a>. Nevertheless, these studies all require heroic levels of effort: quantum chemistry is still very slow, and so a single free energy surface might take months and months to compute.<sup><a href="#fn3">3</a></sup>
</p>

<p>
One promising workaround is using machine learning to accelerate quantum chemistry. Since these MD-type studies look at the same exact system over and over again, we could imagine first training some sort of ML model based on high-level quantum chemistry data, and then employing this model over and over again for the actual MD run. As long as (1) the ML model is faster than the QM method used to train it and (2) it takes less data to train the ML model than it would to run the simulation, this will save time: in most cases, a lot of time. 
</p>

<p>
(This is a somewhat different use case than e.g. <a href=https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a>ANI</a>-type models, which aim to achieve decent accuracy for any organic molecule. Here, we already know what system we want to study, and we’re willing to do some training up front.)
</p>

<p>
A lot of people are working in this field right now, but today I want to highlight some work that I liked from Fernanda Duarte and co-workers. Last year, they published a <a href=https://pubs.rsc.org/en/content/articlelanding/2022/cp/d2cp02978b#cit23>paper</a> comparing a few different ML methods for studying quasiclassical dynamics (in the gas phase), and found that atomic cluster expansion (ACE) performed better than Gaussian approximation potentials while training faster than NequIP. They then went on to show that ACE models could be trained automatically through active learning, and used the models to successfully predict product ratios for cycloadditions with post-TS bifurcations.
</p>

<p>
Their new <a href=https://chemrxiv.org/engage/chemrxiv/article-details/64a8085fba3e99daefab8f89>paper</a>, posted on ChemRxiv yesterday, applies the same ACE/active learning approach to studying reactions in explicit solvent, with the reaction of cyclopentadiene and methyl vinyl ketone chosen as a model system. This is more challenging than their previous work, because the ML model now not only has to recapitulate the solute reactivity but also the solute–solvent and solvent–solvent interactions. To try and capture all the different interactions efficiently, the authors ended up using four different sets of training data: substrates only, substrates with 2 solvent molecules, substrates with 33 solvent molecules, and clusters of solvent only. 
</p>

<p>
Previously, the authors used an energy-based selector to determine if a structure should be added to the training set: they predicted the energy with the model, ran a QM calculation, and selected the structure if the difference between the two values was big enough. This approach makes a lot of sense, but has the unfortunate downside that a lot of QM calculations are needed, which is exactly what this ML-based approach is trying to avoid. Here, the authors found that they could use similarity-based descriptors to select data points to add to the training set: these descriptors are both more efficient (needing fewer structures to converge) and faster to compute, making them overall a much better choice. (This approach is reminiscent of the metadynamics-based approach previously <a href=https://arxiv.org/abs/1712.07240>reported</a> by John Parkhill and co-workers.)
</p>

<p>
With a properly trained model in hand, the authors went on to study the reaction with biased sampling MD. They find that the reaction is indeed accelerated in explicit water, and that the free energy surface begins to look stepwise, as opposed to the concerted mechanism predicted in implicit solvent. (Singleton has observed similar behavior <a href=https://pubs.acs.org/doi/10.1021/ja208779k>before</a>, and <a href=https://chemrxiv.org/engage/chemrxiv/article-details/6362b49531107263acfa50e6>I’ve seen this too.</a>) They do some other interesting studies: they look at the difference between methanol and water as solvents, argue that Houk is wrong about the role of water in the TS,<sup><a href="#fn4">4</a></sup> and suggest that the hydrophobic effect drives solvent-induced rate acceleration.<sup><a href="#fn5">5</a></sup>
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230720_pes.png style="width:450px;" />
  <figcaption>
  Figure 4B from the paper, showing the change in the PES.
  </figcaption>
</figure>

<p>
The results they find for this particular system are interesting, but more exciting is the promise that these techniques may soon become accessible to “regular” computational chemists. Duarte and co-workers have shown that ML can be used to solve an age-old problem in chemical simulation; if explicit solvent ML/MD simulations of organic reactions become easy enough for non-experts to run, I have no doubt that they will become a valued and essential part of the physical organic chemistry toolbox. Much work is needed to get to that point—new software packages, further validation on new systems, new ways to assess quality and check robustness of simulations, and much more—but the vision behind this paper is powerful, and I can’t wait until it comes to fruition.
</p>

<i>
Thanks to Croix Laconsay for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    <a href=https://www.youtube.com/watch?v=caxpxBT8JsM>This video</a> from Chris Cramer makes the point nicely.
  </li>
  <li id="fn2">
    This obviously depends on the system in question, and what processes are being studied. But in general insufficient sampling is a big issue in molecular dynamics, which I think is underappreciated by organic chemists wading into the area. Jeff Grossman has a <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.0c00833>nice paper on this</a>.
  </li>
  <li id="fn3">
    If you look carefully, many people who claim to be doing big <i>ab initio</i> molecular dynamics studies are actually doing semiempirical molecular dynamics. This isn’t dishonest per se, but it’s a little underwhelming to a computational chemist, especially when it’s only mentioned in the SI. Things get even more confusing when plane wave DFT is employed: in theory, plane wave DFT can be just as accurate as regular DFT, but in practice there are some sneaky approximations that often get introduced.
  </li>
  <li id="fn4">
  This argument hinges on whether uphill dynamics (starting from reactants, going to transition state) or downhill dynamics (starting from transition state, going to reactants) are more appropriate. The authors argue that "uphill dynamics allow the solvent sufficient time to reorganise [<i>sic</i>] before the trajectory passes the free energy barrier, providing a more realistic view of solvent behaviour [<i>sic</i>] during the reaction." I'm not fully convinced by this—isn't the idea that the system reorganizes to minimize the energy of the transition state a basic precept of transition state theory? But I'm not convinced I understand these issues deeply enough to have an opinion; I will leave this to the experts.
  </li>
  <li id="fn5">
    This argument hearkens back to some old-school computational organic chemistry I love from Bill Jorgensen, studying the <a href=https://pubs.aip.org/aip/jcp/article-abstract/77/11/5757/783263/Monte-Carlo-simulation-of-n-butane-in-water?redirectedFrom=fulltext>hydrophobic effect on conformational preferences of butane</a>. We usually think of the hydrophobic effect as associated with macromolecules (ligands binding to proteins, etc), but it can still matter in tiny systems!
  </li>
</ol>
]]></description>
              <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>For-Profit Micro Focused Research Organizations: A Proposal</title>
              <link>public/blog/20230717_fmufros.html</link>
              <description><![CDATA[
<p>
<i>
TW: sarcasm.
</i>
</p>

<p>
Today, most research is done by academic labs funded mainly by the government. Many articles have been written on the shortcomings with academic research: Sam Rodriques recently had a nice <a href=https://www.sam-rodriques.com/post/academia-is-an-educational-institution>post</a> about how academia is ultimately an educational institution, and how this limits the quality of academic research. (It’s worth a read; I’ve written about these issues from a <a href=https://corinwagen.github.io/public/blog/20221026_structural_diversity.html>few</a> <a href=https://corinwagen.github.io/public/blog/20230502_startups.html>angles</a>, and will probably write more at a later date.)
</p>

<p>
The major alternative to academic research that people put forward is <a href=https://www.nature.com/articles/d41586-022-00018-5>focused research organizations</a> (FROs): large, non-profit research organizations capable of tackling big unsolved problems. These organizations, similar in scope and ambition to e.g. CERN or LIGO, are envisioned to operate with a budget of $20–100M over five years, making them substantially larger and more expensive than a single academic lab. This model is still being tested, but it seems likely that some version of FROs will prove effective for appropriately sized problems. 
</p>

<p>
But FROs have some disadvantages, too: they represent a significant investment on the part of funders, and so it’s important to choose projects where there’s a high likelihood of impact in the given area. (In contrast, it’s expected that most new academic labs will focus on high-risk projects, and pivot if things don’t work out in a few years.) In this piece, I propose a new form of scientific organization that combines aspects of both FROs and academic labs: <u>for-profit micro focused research organizations (FPµFROs)</u>.
</p>

<p>
The key insight behind FPµFROs is that existing financial markets could be used to fund scientific research when there is a realistic possibility for profit as a result of the research. This means that FPµFROs need not be funded by the government or philanthropic spending, but could instead raise capital from e.g. venture capitalists or angel investors, who have access to substantially more money and are used to making high-risk, high-reward investments. 
</p>

<p>
FPµFROs would also be smaller and more nimble than full-fledged FROs, able to tackle high-risk problems just like academia. But unlike academic labs, FPµFROs would be able to spend more freely and hire more aggressively, thus circumventing the human capital issues that plague academic research. While most academic labs are staffed entirely with inexperienced trainees (as Rodriques notes above), FPµFROs could hire experienced scientists, engineers, and programmers, thus accelerating the rate of scientific progress.
</p>

<p>
One limitation of the FPµFRO model is that research would need to be profitable within a reasonable time frame. But this limitation might actually be a blessing in disguise: the need for profitability means that FPµFROs would be incentivized to provide real value to firms, thus preventing useless research through the magic of Adam Smith’s invisible hand. 
</p>

<p>
Another disadvantage of FPµFROs is that they must be able to achieve success with relatively little funding (probably around $10M; big for academia, but small compared to a FRO). This means that their projects would have to be modest in scope. I think this is probably a blessing in disguise, though. Consider the following advice <a href=http://www.paulgraham.com/ambitious.html>from Paul Graham</a>:
</p>

<blockquote>
Empirically, the way to do really big things seems to be to start with deceptively small things.… Maybe it's a bad idea to have really big ambitions initially, because the bigger your ambition, the longer it's going to take, and the further you project into the future, the more likely you'll get it wrong.
</blockquote>

<p>
Thus, the need for FPµFROs to focus on getting a single “minimal viable product” right might be very helpful, and could even lead to more impactful firms later on.
</p>

<p>
In conclusion, FPµFROs could combine the best qualities of academic labs and FROs: they would be agile and risk-tolerant, like academic labs, but properly incentivized to produce useful research instead of publishing papers, like FROs. This novel model should be investigated further as a mechanism for generating new scientific discoveries at scale with immediate short-term utility.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Hopefully it’s clear by now that this is a joke: an FPµFRO is just a startup. 
</p>

<p>
The point of this piece isn’t to criticize FROs or academia: both have their unique advantages relative to startups, and much has been written about the relative advantages and disadvantages of different sorts of research institutions (<a href=https://www.convergentresearch.org/about-fros>e.g.</a>).
</p>

<p>
Rather, I want to remind people that startups can do really good scientific work, something that many people seem to forget. It’s true that basic research can be a public good, and something that’s difficult to monetize within a reasonable timeframe. But most research today isn’t quite this basic, which leads me to suspect that many activities today confined to academic labs could be profitably conducted in startups. 
</p>

<p>
Academics are generally very skeptical of organizations motivated by profit. But all incentives are imperfect, and the drive to achieve profitability pushes companies to provide value to real customers, which is more than many academics motivated by publication or prestige ever manage to achieve. It seems likely that for organizations focused on applied research, profit is the least bad incentive. 
</p>

<p>
I’ll close with a quote from Eric Gilliam’s recent <a href=https://www.freaktakes.com/p/an-alternative-approach-to-deep-tech>essay</a> on a new model for “deep tech” startups:
</p>

<blockquote>
Our corporate R&amp;D labs in most industries have taken a step back in how “basic” their research is. Meanwhile, what universities call ‘applied’ research has become much less applied than it used to be. This ‘middle’ of the deep tech pipeline has been hollowed out.
</blockquote>

<p>
What Eric proposes in his piece, and what I’m arguing here, is that scientific startups can help fill this void: not by replacing FROs and academic research, but by complementing them. 
</p>

<i>Thanks to Ari Wagen for reading a draft of this piece.</i>
]]></description>
              <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>pKa and Nonpolar Media</title>
              <link>public/blog/20230710_pka.html</link>
              <description><![CDATA[
<p>
The concept of p<i>K</i><sub>a</sub> is introduced so early in the organic chemistry curriculum that it’s easy to overlook what a remarkable idea it is.
</p>

<p>
<i>
Briefly, for the non-chemists reading this: p</i>K<i><sub>a</sub> is defined as the negative base-10 logarithm of the acidity constant of a given acid H–A:
</i>
</p>

<p>
p<i>K</i><sub>a</sub> := -log<sub>10</sub>([HA]/[A-][H+]) 
</p>

<p>
<i>
Unlike pH, which describes the acidity of a bulk solution, p</i>K<i><sub>a</sub> describes the intrinsic proclivity of a molecule to shed a proton—a given molecule in a given solvent will always have the same p</i>K<i><sub>a</sub>, no matter the pH. This makes p</i>K<i><sub>a</sub> a very useful tool for ranking molecules by their acidity (e.g. <a href=http://ccc.chem.pitt.edu/wipf/MechOMs/evans_pKa_table.pdf>the Evans p</i>K<i><sub>a</sub> table</a>).
</i>
</p>

<p>
The claim implicit in the definition of p<i>K</i><sub>a</sub> is that a single parameter suffices to describe the acidity of each molecule.<sup><a href="#fn1">1</a></sup> In general, this isn’t true in chemistry—there’s no single “reactivity” parameter which describes how reactive a given molecule is. For various regions of chemical space a two-parameter model can <a href=https://www.cup.lmu.de/oc/mayr/reaktionsdatenbank/>work</a>, but in general we don’t expect to be able to evaluate the efficacy of a given reaction by looking up the reactivity values of the reactants and seeing if they’re close enough. 
</p>

<p>
Instead, structure and reactivity interact with each other in complex, high-dimensional ways. A diene will react with an electron-poor alkene and not an alcohol, while acetyl chloride doesn’t react with alkenes but will readily acetylate alcohols, and a free radical might ignore both the alkene and the alcohol and abstract a hydrogen from somewhere else. Making sense of this confusing morass of different behaviors is, on some level, what organic chemistry is all about. The fact that the reactivity of different functional groups depends on reaction conditions is key to most forms of synthesis!
</p>

<p>
But p<i>K</i><sub>a</sub> isn’t so complicated. If I want to know whether acetic acid will protonate pyridine in a given solvent, all I have to do is look up the p<i>K</i><sub>a</sub> values for acetic acid and pyridinium (pyridine’s conjugate acid). If pyridinium has a higher p<i>K</i><sub>a</sub>, protonation will be favored; otherwise, it’ll be disfavored. More generally, one can predict the equilibrium distribution of protons amongst <i>N</i> different sites from a list of the corresponding p<i>K</i><sub>a</sub>s. 
</p>

<p>
Why is p<i>K</i><sub>a</sub> so well-behaved? The key assumption underlying the above definition is that ions are free and do not interact with one another. This allows us to neglect any specific ion–ion interactions, and makes the scale universal: if the pyridinium cation and the acetate anion never interact, then I can learn everything I need to about pyridinium acetate just by measuring the p<i>K</i><sub>a</sub>s of pyridine and acetic acid in isolation. 
</p>

<p>
This assumption is quite good in solvents like water or DMSO, which excel at stabilizing charged species, but progressively breaks down as one travels to the realm of nonpolar solvents. As ions start to pair with themselves, specific molecule–molecule interactions become important. The relative size of the anions can matter: in a nonpolar solvent, a small anion will be better stabilized by a small cation than by a large, diffuse cation, meaning that e.g. acetate will appear more acidic when protonating smaller molecules. Other more quotidian intermolecular interactions, like hydrogen bonding and π-stacking, can also play a role.
</p>

<p>
And the ions aren’t the only thing that can stick together: aggregation of acids is often observed in nonpolar solvents. Benzenesulfonic acid <a href=https://link.springer.com/article/10.2116/analsci.15.303>forms a trimer</a> in benzonitrile solution, which is still pretty polar, and alcohols and carboxylic acids are known to aggregate under a variety of conditions as well.<sup><a href="#fn2">2</a></sup> Even seemingly innocuous species like tetrabutylammonium chloride will aggregate at high concentrations (<a href=https://pubs.acs.org/doi/pdf/10.1021/ja9723139>ref</a>, <a href=https://pubs.rsc.org/en/content/articlelanding/1995/c3/c39950002513>ref</a>).
</p>

<p>
To reliably extend p<i>K</i><sub>a</sub> scales to nonpolar solvents, one must thus deliberately choose compounds which resist aggregation. As the dielectric constant drops, so does the number of such compounds. The clearest demonstration of this I’ve found is a series of papers (<a href=https://pubs.acs.org/doi/10.1021/jo9713013>1</a>, <a href=https://pubs.acs.org/doi/10.1021/jo0343477>2</a>) by p<i>K</i><sub>a</sub> guru Ivo Leito measuring the acidity of a series of fluorinated compounds in heptane:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230710_leito.png style="width:350px;" />
  <figcaption>
  A portion of the scale, showing the compounds employed.
  </figcaption>
</figure>

<p>
This effort, while heroic, demonstrates the futility of measuring p<i>K</i><sub>a</sub> in nonpolar media from the standpoint of the synthetic chemist. If only weird fluoroalkanes engineered not to aggregate can have p<i>K</i><sub>a</sub> values, then the scale may be analytically robust, but it’s hardly useful for designing reactions! 
</p>

<p>
The key point here is that the difficulty of measuring p<i>K</i><sub>a</sub> in nonpolar media is not an analytical barrier which can be surmounted by new and improved technologies, but rather a fundamental breakdown in the idea of p<i>K</i><sub>a</sub> itself. Even the best p<i>K</i><sub>a</sub> measurement tool in the world can’t determine the p<i>K</i><sub>a</sub> of HCl in hexanes, because no such value exists—the concept itself is borderline nonsensical. Chloride will ion-pair with everything in hexanes, hydrogen chloride will aggregate with itself, chloride will stick to hydrogen chloride, and so forth. Asking for a p<i>K</i><sub>a</sub> in this context just doesn't make much sense.<sup><a href="#fn3">3</a></sup>
</p>

<p>
It’s important to remember, however, that just because the p<i>K</i><sub>a</sub> scale no longer functions in nonpolar solvents doesn’t mean that acids don’t have different acidities. Triflic acid in toluene will still protonate just about <a href=https://pubs.acs.org/doi/abs/10.1021/ol061174+>everything</a>, whereas acetic acid will not. Instead, chemists wishing to think about acidity in nonpolar media have to accept that no one-dimensional scale will be forthcoming. The idealized world of p<i>K</i><sub>a</sub> we’re accustomed to may no longer function in nonpolar solvents, but chemistry itself still works just fine.
</p>

<i>
Thanks to Ivo Leito for discussing these topics with me over Zoom, and to Joe Gair for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    Really, each proton.
  </li>
  <li id="fn2">
   Ivo Leito called carboxylic acids "the world champions of aggregation" when I asked him about these issues.
  </li>
  <li id="fn3">
    Even experimentally nonsensical p<i>K</i><sub>a</sub>s can be simulated, though: Jorgensen <a href=https://pubs.acs.org/doi/10.1021/ja00256a053>famously</a> used free energy perturbation to compute the p<i>K</i><sub>a</sub> of ethane in water.
  </li>
</ol>
]]></description>
              <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Opinionated Advice for Incoming Graduate Students</title>
              <link>public/blog/20230703_advice.html</link>
              <description><![CDATA[
<ol>
<li>
You are a scientist, not a lab monkey. You ought not to view your degree as “six years of hard labor in the chemistry mines.” Always make time to go to interesting seminars, talk with other people about their research, and read the literature. Otherwise, what’s the point of being a scientist?
</li>
<li>
Only one person is really looking out for your best interests: you. Your advisor, your classmates, and your collaborators all have their own distinct incentives and interests, which will often roughly align with yours, but will never align perfectly.
</li>
<li>
Your research interests will also not match up perfectly with those of your advisor. Your job as a graduate student is to find the intersection between what the two of you care about, and work there. Otherwise, one of you will be unhappy.
</li>
<li>
The more non-chemists in your life, the better your mental health will be. (h/t <a href=https://www.theatlantic.com/family/archive/2021/04/deep-friendships-aristotle/618529/>Arthur Brooks</a>)
</li>
<li>
Try to maximize the ratio of “thinking”/publishable work to mindless SI work. Any project will take some grinding, but the thinking work is (ideally) what you’ve been recruited for, what you’ll present on, and what’ll get you a job. If your advisor views you only as a set of hands… be very worried.
</li>
<li>
Cold emails to scientists work much better than it seems they ought to. Most professors spend their entire career trying to get people to care about their work—if you’re interested, they’ll usually talk your ear off.
</li>
<li>
Read your PI’s old papers, as many as possible. Too often students are totally ignorant of the work that occurred a decade before they joined the lab, and end up repeating it or falling into the same pitfalls time and time again.
</li>
<li>
Learn to code; please stop performing curve fits in Excel. (cf. <a href=https://a16z.com/2011/08/20/why-software-is-eating-the-world/>pmarca</a>)
</li>
<li>
Be outcome focused. Each day, ask yourself “What is the biggest problem I’m facing in my research?” If whatever you’re doing isn’t addressing that problem, you’re wasting your time. Sometimes this means stopping all experiments and reading papers for a few weeks; sometimes this looks like running reactions; sometimes this looks like just sitting at your desk and writing. (h/t Brian Liau for giving me this advice when I started graduate school)
</li>
<li>
Be outcome focused on the big scale, too. Figure out what success in graduate school looks like—what your ideal job is, and what it takes to get that sort of job—and then pursue those outcomes relentlessly. Perhaps in an ideal world we could all follow our natural curiosity to our heart’s content, but that’s not real life, not in science as I’ve known it.
</li>
<li>
Success in graduate school may be necessary for your life goals, but it won’t be sufficient. At the end of the day, science is just a job, and your molecules will never love you. So don’t work so hard that you put the rest of your life on hold; don’t make science the highest good in your worldview. (cf. <a href=https://www3.dbu.edu/naugle/pdf/disordered_love.pdf>Augustine</a>)
</li>
</ol>

<i>
Thanks to Joe Gair for reading a draft of this piece.
</i>
]]></description>
              <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Peer Review, Imperfect Feedback</title>
              <link>public/blog/20230626_feedback.html</link>
              <description><![CDATA[
<p>
I’ve been pretty critical of peer review in the past, arguing that it <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>doesn’t accomplish much</a>, <a href=https://corinwagen.github.io/public/blog/20230320_domain_arbitrage.html>contributes to status quo bias</a>, etc. But a few recent experiences remind me of the value that peer review provides: <u>in today’s scientific culture, peer review is essentially the only time that scientists get honest and unbiased feedback on their work.</u> 
</p>

<p>
How can this be true? In experimental science, scientists typically work alongside other students and postdocs under the supervision of a professor. This body of people forms a lab, also known as a research group, and it’s to these people that you present most frequently. Your lab generally knows the techniques and methods that you employ very well: so if you’ve misinterpreted a piece of data or designed an experiment poorly, group meeting is a great place to get feedback. 
</p>

<p>
But a lab is also biased in certain ways. People are attracted to a lab because they think the science is exciting and shows promise, and so they’re likely to be credulous about positive results. Certain labs also develop beliefs or dogmas about how to conduct science: the best ways to perform a mechanistic study, or the most useful reaction conditions. To some extent, every lab is a paradigm unto itself. This means that paradigm-shifting criticism is hard to find among one’s coworkers, even if it’s common in the outside world.
<p>

<p>
Here are some examples of controversial-in-the-field statements that are unlikely to be controversial within given labs:
</p>
<ul>
<li>
Palladium-based catalysis is going to become obsolete due to the scarcity of Pd and must be replaced. <a href=https://www.dal.ca/sites/stradiotto/research.html>Some labs</a> build their research program around this; others think Pd will always be relevant.
</li>
<li>
Water is the greenest possible solvent. Some scientists believe this <a href=https://lipshutz.chem.ucsb.edu/research>wholeheartedly</a>, while others think it’s stupid (for instance, a 2021 <a href=https://link.springer.com/article/10.1007/s10098-021-02188-8>review</a> states that “to meet regulations concerning the discharge of waste water into rivers and other natural waters… [water] often requires very extensive treatment prior to discharge, making the use of water as a reaction medium much less attractive.”)
</li>
<li>
More broadly, the belief that machine learning and generative AI are the future of chemical discovery: while many computational chemists believe this, plenty of experimentalists (even young ones) are pretty skeptical.
</li>
</ul>

<p>
In each of these cases, it’s unlikely that criticism along these lines is available internally: people who’ve chosen to do their PhDs studying ML in chemistry aren’t likely to criticize your paper for overemphasizing the importance of ML in chemistry!
</p>

<p>
More generally, internal criticism works best when a lab serves as a shared repository of expertise, i.e. when everyone in the lab has roughly the same skillset. Some labs focus instead on a single overarching goal and employ many different tools to get to that point: a given chemical biology group might have a synthetic chemist, a MS specialist, a genomics guru, a mechanistic enzymologist, and someone specializing in cell culture. If this is the case, your techniques are opaque to your coworkers: what advice can someone who does cell culture give about improving Q-TOF signal-to-noise?
</p>

<p>
Ideally, one’s professor is well-versed enough in each of the techniques employed that he or she can dispense criticism as needed. But professors are often busy, aren’t always operational experts at each of the techniques they oversee, and suffer from the same viewpoint biases that their students do (perhaps even more so).
</p>

<p>
So, it’s important to solicit feedback from external sources. Unfortunately, at least in my experience most external feedback is too positive: “great talk,” “nice job,” etc. Our scientific culture tries so hard to be supportive that I almost never get any meaningful criticism from people outside my group, either publicly or privately. (Ideally one’s committee would help, but I never really got to present research results to my committee, and this doesn’t help postdocs anyhow.) 
</p>

<p>
Peer review, then, serves as the last bastion against low-quality science: reviewers are outside the lab, have no incentive to be nice, and are tasked specifically with poking holes in your argument or pointing out extra experiments that would improve it. Peer review has improved each one of my papers, and I’m grateful for it.<sup><a href="fn1">1</a></sup>
</p>

<p>
What’s a little sad is that the excellent feedback that reviewers give only comes at the bitter end of a project, which for me has often meant that the results are more than a year old and my collaborators have moved on. Much more useful would be critical feedback delivered early on in a project, when my own thinking is more flexible and the barrier to running additional experiments is lower. And more useful still would be high-quality criticism available at every step of the project, given not anonymously but by people whom you can talk to and learn from.
</p>

<p>
What might this practically look like? 
</p>
<ul>
<li>
A culture of “red teaming,” where students are incentivized to find flaws in others’ projects in somewhat adversarial ways. This would need to be done within a supportive and collegial atmosphere, lest it degenerate into bullying: red teaming need not be red in tooth and claw.
</li>
<li>
Similarly, PIs could invite other professors to come to group meetings and (constructively) criticize the projects, particularly professors from adjacent subfields who might have different perspectives.
</li>
<li>
Poster presentations or talks (at conferences), although often used to present finished projects, can also be used to present unfinished work. I presented some unfinished work at <a href=http://jiwu.chem.uh.edu/tpoc.html>TPOC</a> a few years ago, and got really helpful suggestions from Ken Houk and Dean Tantillo that fundamentally changed how we approached the rest of the project. Maybe this is something that we should encourage more, although finished projects will probably always look more impressive than unfinished projects.
</li>
<li>
Decentralized peer review solutions like <a href=https://www.theseedsofscience.org/><i>Seeds of Science</i></a> or <a href=https://pubpeer.com/><i>PubPeer</i></a> might also help here, but my sense is that it’s unlikely that qualified experts will just spend their time investigating something that they found online; they need to be solicited somehow.
</li>
</ul>

<p>
I don’t know what the right solution looks like here: the burden of peer review is already substantial, and I don’t mean to suggest that this work ought to be arbitrarily multiplied for free. But I do worry that eliminating peer review, absent other changes, would simply mean that one of the only meaningful chances to get unfiltered feedback on one’s science would be eliminated, and that this would be bad.
<p>

<i>
Thanks to Croix Laconsay and Lucas Karas for helpful feedback on this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  It's also true that the threat of peer review increases paper quality. I agree with <a href=https://twitter.com/dasingleton/status/1528877848093954048?s=20>Singleton</a> that this is important today, but am less convinced that this is necessary from an institutional design perspective: if peer review didn't exist, I think some other system of norms or regulations would spring forth to protect good-quality science. See the "Anarchic Preprint Lake" discussion in <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>my piece on journals</a>. (h/t Croix Laconsay for raising this point)
  </li>
</ol>

]]></description>
              <pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>A Farewell to Lab</title>
              <link>public/blog/20230622_farewell.html</link>
              <description><![CDATA[
<p>
I first encountered organic chemistry on Wikipedia, my freshman year of high school. The complexity and arcanity of the field instantly attracted me: here was something interesting that I didn’t know about and which didn’t require years of mathematical training to approach (unlike most of physics).
</p>

<p>
I soon started reading about organic chemistry more and more, albeit with no rhyme or reason to my study. I didn’t know what the good textbooks were, what order to study things in or which concepts ought to be understood in depth before progressing further. Organic chemistry was just a “<a href=https://en.wikipedia.org/wiki/The_Glass_Bead_Game>glass bead game</a>” to me, an art of symbols devoid of any real-world representations. But enthusiasm can sometimes suffice where wisdom is lacking.
</p>

<p>
With the support of a teacher and a half-dozen friends who also wanted to learn more organic chemistry, we started a little independent study. We met in a closet and read a textbook, worked through the problems, and our teacher wrote us tests. We eventually managed to get through all of Paula Bruice, although various misconceptions (and mispronunciations) stayed with me until I took Movassaghi’s course at MIT.
</p>


<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_iodolact.jpg style="width:500px;" />
  <figcaption>
    Here’s a picture of some work from high school, which does not in fact make very much sense. (I think this is from the Evans problem set library.)
  </figcaption>
</figure>

<p>
But in high school we had no lab, and thus no practical knowledge. We split into two groups and tried to come up with experiments for ourselves, but the results were dismal. Here’s the procedure (copied verbatim) for the one and only reaction we ran, synthesis of nitrobenzene via nitration of benzene, which was to be the first step in a multistep synthesis of Kevlar:
</p>

<blockquote>
In a 250 mL beaker, dissolve benzene in a solution of concentrated H<sub>2</sub>SO<sub>4</sub> of twice its volume. Use an ice-salt bath to bring this solution to 0 °C or below, and use a Pasteur pipette to slowly add a 1:1 solution of HNO<sub>3</sub> and H<sub>2</sub>SO<sub>4</sub> (be sure to keep the solution below 15 degrees Celsius at all times, as the reaction is strongly exothermic). Once all the solution has been added, warm it to room temperature and allow it to sit for 15 minutes.
<br><br>
Pour the solution over 50 g of crushed ice in a 250 mL beaker. Once the ice has melted, isolate the product via vacuum filtration via a Buchner funnel and rinse it twice with water and twice with methanol. Recrystallize in a solution of methanol.
</blockquote>

<p>
The astute observer will notice that there aren’t very many details here. How much benzene? How much nitric acid? We didn’t have the glassware mentioned above—neither a beaker nor a Buchner funnel. And, perhaps most damningly, the procedure calls for isolation by filtration, challenging since nitrobenzene is a liquid at room temperature.
</p>

<p>
Despite these problems, we successfully ran this reaction (open to air, not in a fume hood), and obtained the product. I vividly recall the yellow bubbles of nitrobenzene floating to the top of the vial, and the smell of cherries that filled the room, a smell that returns to me in Proustian fashion from time to time when using certain reagents. We didn’t have a separatory funnel (or we didn’t know how to use it if we did), so we fished some of the nitrobenzene out with a Pasteur pipette and threw the rest away.
</p>

<p>
A little bit of knowledge would have served us well, as would have gloves and a fume hood. Here’s <a href=https://en.wikipedia.org/wiki/Nitrobenzene>Wikipedia</a>:
</p>

<blockquote>
Prolonged exposure [to nitrobenzene] may cause serious damage to the central nervous system, impair vision, cause liver or kidney damage, anemia and lung irritation. Inhalation of vapors may induce headache, nausea, fatigue, dizziness, cyanosis, weakness in the arms and legs, and in rare cases may be fatal…
<br><br>
Nitrobenzene is considered a likely human carcinogen by the United States Environmental Protection Agency, and is classified by the IARC as a Group 2B carcinogen which is "possibly carcinogenic to humans".
</blockquote>

<p>
Indeed, a coworker of mine would later be sent to the hospital after spilling nitrobenzene on herself. Surprisingly, my group’s experiment still ended up being the safer one—the lab portion of our course was disbanded the following day after my classmates caused an explosion with thionyl chloride.
</p>

<p>
I was fortunate enough to land a summer research internship in the Sessler lab at UT the following summer, and I started studying chemistry in earnest: column chromatography, Anslyn/Dougherty, NMR spectroscopy, and all the rest. I remember the first reaction I ran at UT: retro-Friedel–Crafts dealkylation of a calix[4]arene, using about 10 g each of phenol and aluminum(III) chloride. The brutal physicality of lab work was a nice contrast to the gnosticism of software (where I’d worked previously), and I was hooked.
</p>

<p>
I’ll fast-forward through the more recent parts of my chemical career: I went to MIT and joined the Imperiali lab to work on essentially a medicinal chemistry project: hit-to-lead optimization, featuring lots of de novo heterocycle synthesis. I got to cook reactions in molten urea, quench 500 mL of phosphorus oxychloride at a time, and even design new routes (with a little oversight from my postdoc). It was awesome.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_imperiali.jpg style="width:500px;" />
  <figcaption>
    My hood in the Imperiali Group. Although this hood appears to have a Schlenk line, do not be fooled—it’s purely decorative.
  </figcaption>
</figure>

<p>
After three semesters, I got tired of my cross couplings mysteriously failing and joined the Buchwald lab, where I learned how to do chemistry more carefully: handling air-sensitive materials with a Schlenk line or in the glovebox, not “as fast as possible.” My tenure in the Buchwald lab also introduced me to the importance of computations, which became a key part of my doctoral work, particularly when we were sent home in March of my first year. COVID gave me the opportunity to pursue some software engineering projects that I wouldn’t have had time to work on otherwise (like <i><a href=https://github.com/ekwan/cctk>cctk</a></i> and <i><a href=https://github.com/corinwagen/presto>presto</a></i>), and simulation started to take up more and more of my time and intellectual bandwidth.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_benzyne.jpg style="width:500px;" />
  <figcaption>
    A large-scale biaryl synthesis proceeding through a benzyne intermediate, from my time in the Buchwald Group.
  </figcaption>
</figure>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_hpc.jpg style="width:400px;" />
  <figcaption>
    One of my first Gaussian jobs ever. I didn't get cluster access for a little while, so I used to run Gaussian on the printer computers in my dorm's basement and just hope that nobody needed the computer overnight.
  </figcaption>
</figure>

<p>
I defended my dissertation on June 5th, and cleaned out my hood last week. For the foreseeable future, I’ll be a purely computational chemist—computation is advancing quickly, and I think that’s where I have the most to offer the field right now. But I’ll miss the sights and sounds of the lab. There’s a satisfaction to making a new molecule and holding the final product in your hands: the knowledge that you’ve reshaped this little corner of reality through your own actions, and that this particular arrangement of matter has never existed before.
</p>

<p>
And simulation is, at the end of the day, only useful insofar as it helps us make real molecules. There may be people who wish to model reactions purely for the sake of modeling them, but I am not one of them. What drew me to simulation initially, and what still attracts me to the field today, is its potential to help experimental scientists do their work faster and better. It was easy for me to ensure that this was true when I was both doing the experiments and running the simulations; my incentives were aligned properly. It will be harder in the future.
</p>

<p>
There’s a seductive appeal in leaving lab work behind altogether, too, and one that’s dangerous. Any experimentalist who’s worked with a computational collaborator knows that nothing ever works precisely as modeled. There are untold depths of chemical behavior still inaccessible to the idealized world of simulation, and it’s all too easy for computational chemists just to look the other way. Life is easier when you don’t have to deal with sludgy workups or poorly soluble intermediates, but they don’t go away just because you can’t model them by DFT—reality has a way of keeping us honest that simulations frequently lack.
</p>

<p>
So, although I bid lab farewell at this point in my career, it’s a bittersweet parting. I hope to return someday; only time will tell.
</p>

<i>Thanks to Jacob Thackston for reading a draft of this piece.</i>
]]></description>
              <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>A Year In The Blog</title>
              <link>public/blog/20230620_year.html</link>
              <description><![CDATA[
<p>
I started this blog one year ago today, with a post on site-selective glycosylation. According to Google Analytics, there have been 24,035 views since then.
</p>

<p>
What have the top posts been? 
</p>

<ol>
<li>
<a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>Against Carbon NMR</a> (3201 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220928_talent.html>Book Review: Talent</a> (1588 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20230616_varda.html>Crystallization in Microgravity</a> (1467 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20230203_poy.html>2022 Paper(s) of the Year</a> (721 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220810_viewpoints_on_simulation.html>Combating Computational Nihilism</a> (694 views)
</li>
</ol>

<p>
The only of these that really surprises me is #5: the <sup>13</sup>C NMR post made a lot of organic chemists really angry, the <i>Talent</i> review was reposted on <i>Marginal Revolution</i>, and Delian Asparouhov (CEO of Varda) retweeted my post about Varda’s crystallization ideas. And everyone loves to share a ranking of the year’s papers, especially when their own work is highlighted. 
</p>

<p>
The least-viewed posts?
</p>

<ol>
<li>
<a href=https://corinwagen.github.io/public/blog/20230413_new_ways.html>New Ways To Read The Blog: RSS and Substack</a> (26 views) 
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221128_business_card_explained.html>Business Card Lennard–Jones Simulation, Explained</a> (32 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220620_glycosylation.html>Site-Selective Glycosylation: Reflections</a> (34 views; my first post)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221121_business_card_lennard_jones.html>Business Card Lennard–Jones Simulation</a> (43 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221231_books.html>Books from 2022</a> (45 views)
</li>
</ol>

<p>
Twitter downranked the Substack post pretty heavily, so it’s not surprising that nobody saw it. The Lennard–Jones posts are more unexpected. Whenever I write about anything computational or coding-related, it seems to attract much less engagement, which is perhaps a reflection of the fact that most of my followers are experimental chemists who don’t really care about obfuscated C code. 
</p>

<p>
A year in, writing blog posts has gotten much easier. The following advice from <a href=https://guzey.com/personal/why-have-a-blog/#writing-helps-you-think-better>Alexey Guzey</a> didn’t seem true when I started, but it does seem true now:
</p>

<blockquote>
Writing not only helps you to understand what’s going on and to crystallize your thoughts, it actually makes you think of new ideas and come up with solutions to your problems.
</blockquote>

<p>
I’ve fallen into a 1x/week update schedule, which seems to work pretty well: enough to keep the routine up, but not so much that it’s a serious distraction from my actual job. I hope to maintain this schedule for the foreseeable future, and recommend it to other bivocational bloggers.
</p>

<p>
Anyhow, thanks for reading!
</p>

<p>
(Also, today in off-blog content: I appeared on my first podcast, <i>Forbidden Conversations</i>, hosted by Harry Wetherall. We talk about why people don’t have kids earlier, how I reconcile being a Christian with being a scientist, the concept of “cope,” and more: you can check it out on <a href=https://podcasts.apple.com/us/podcast/forbidden-conversations/id1676705756>Apple Podcasts</a> or <a href=https://open.spotify.com/episode/38AK3AdSf52gwNAkyA9Lpv>Spotify</a>.)
</p>




]]></description>
              <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Crystallization in Microgravity</title>
              <link>public/blog/20230616_varda.html</link>
              <description><![CDATA[
<p>
<i>Not Boring</i> recently published a <a href=https://www.notboring.co/p/varda-the-space-drug-factory>panegyric</a> about <a href=https://varda.com/>Varda</a>, a startup that’s trying to create “space factories making drugs in orbit.” When I first read this description, alarm bells went off in my head—why would anyone try to make drugs in space? Nevertheless, there’s more to this idea than I initially thought. In this piece, I want to dig a little deeper into the chemistry behind Varda, and discuss some potential advantages and challenges of the approach they’re exploring.
</p>

<p>
Much of my confusion was quickly resolved by realizing that Varda is not actually “making drugs in orbit,” or not in the way that an organic chemist would interpret that sentence. Varda’s proposal is actually much more specific: they aim to crystallize active pharmaceutical ingredients (APIs, i.e. finished drug molecules) in microgravity, allowing them to access crystal forms and particle size distributions which can’t be made under terrestrial conditions. To quote from <a href=https://varda.com/wp-content/uploads/varda-info-sheet-mar23_pharmaceutical.pdf>their website</a>:
</p>

<blockquote>
Varda’s microgravity platform is grounded in decades of proven research conducted on NASA’s space stations. By eliminating factors such as natural convection and sedimentation, processing in a microgravity environment provides a unique path to formulating small molecules and biologics that traditional manufacturing processes cannot address. The resulting tunable particle size distributions, more ordered crystals, and new forms can lead to improved bioavailability, extended shelf-life, new intellectual property, and new routes of administration. 
</blockquote>

<p>
Crystallization is an excellent target for a new and expensive manufacturing process because it’s at once very important and very hard to control. The goal of crystallization is to grow crystals of a given compound, fitting the component molecules together into a perfect lattice that excludes impurities and can be easily handled. To the best of my knowledge, almost every small-molecule API is crystallized at one stage or another; it’s the best way to ensure that the material is extremely pure. 
</p>

<p>
(Crystallizing proteins for administration is less common, since proteins are really difficult to crystallize, but it’s not unheard of—insulin is often administered subcutaneously as a <a href=https://onlinelibrary.wiley.com/doi/10.1002/prot.22213>microcrystalline suspension</a>, which allows higher concentrations to be accessed without excessive viscosity.)
</p>

<p>
But crystallization is also something we can’t really control. We can’t physically put molecules into a lattice or force them to adopt ordered configurations; all we can do is dissolve them in some mixture, tweak the conditions a little bit, and hope that crystals form. Thus, finding good crystallization conditions basically amounts to randomly screening solvents and additives, leaving the solutions for a long time, and checking to see if crystals grow. In the words of <a href=https://www.science.org/content/blog-post/voodoo-nominations>Derek Lowe</a>:
</p>

<blockquote>
I'd like to open up the floor for nominations for the Blackest Art in All of Chemistry. And my candidate is a strong, strong contender: crystallization. When you go into a protein crystallography lab and see stack after stack after stack of plastic trays, each containing scores of different little wells, each with a slight variation on the conditions, you realize that you're looking at something that we just don't understand very well.
</blockquote>

<p>
Why does microgravity matter for crystallization? <i>Not Boring</i> says that crystallization occurs “at the mesoscopic scale, the length scale on which objects are larger than nanoscale (on the order of atoms and molecules) but still small enough that quantum mechanical or other non-trivial ‘microscopic’ behavior becomes apparent.” I found this answer a little confusing—doesn’t crystallization begin on the nanoscale and end on the macroscopic scale? 
</p>

<p>
Clearer to me was the explanation from a 2001 <a href=https://pubs.acs.org/doi/10.1021/cg005511b>review</a> by Kundrot and co-workers:
</p>

<blockquote>
In zero gravity, a crystal is subject to Brownian motion as on the ground, but unlike the ground case, there is no acceleration inducing it to sediment <i>[fall out of solution]</i>. A growing crystal in zero gravity will move very little with respect to the surrounding fluid. Moreover, as growth units leave solution and are added to the crystal, a region of solution depleted in protein is formed. Usually this solution has a lower density than the bulk solution and will convect upward in a 1g field as seen by Schlerien photography (Figure 1). In zero gravity, the bouyant <i>[sic]</i> force is eliminated and no bouyancy-driven convection occurs. Because the positions of the crystal and its depletion zone are stable, the crystal can grow under conditions where its growing surface is in contact with a solution that is only slightly supersaturated. In contrast, the sedimentation and convection that occur under 1g place the growing crystal surface in contact with bulk solution that is typically several times supersaturated. Lower supersaturation at the growing crystal surface allows more high-energy misincorporated growth units to disassociate from the crystal before becoming trapped in the crystal by the addition of other growth units… 
<br><br>
In short, promotion of a stable depletion zone in microgravity is postulated to provide a better ordered crystal lattice and benefit the crystal growth process.
</blockquote>

<p>
To summarize, microgravity serves to immobilize the crystal with respect to the surrounding solution, preventing convection or sedimentation from bringing highly concentrated solutions into contact with the crystal. This slows crystal growth, which might sound bad but is actually really good: in general, the slower a crystal grows, the higher its purity. (See also <a href=https://www.nature.com/articles/npjmgrav201510>this 2015 article</a> for further discussion.)
</p>

<p>
What practical impact does this have? In most cases, crystals grown in space are better than their terrestrial congeners by <a href=https://pubs.acs.org/doi/10.1021/acs.cgd.2c01056>a variety of metrics</a>: larger, structurally better, and more uniform. To quote from the <i>Not Boring</i> piece:
</p>

<blockquote>
Doing crystallization in space is like adding a gravity knob to your instrument—it opens up regions of process design space that would otherwise be inaccessible. Importantly, after the crystallization occurs in space, the drug retains its solid state upon re-entry. Manufacture in space; use on earth. 
<br><br>
This is why pharma is going to space to experiment with a wide range of medicines. Formulations made in microgravity could open the door to improvements in drug shelf life, bioavailability, IP expansion, and even better approaches to drug delivery…
<br><br>
To date, there has been a major disconnect between microgravity research and manufacturing. While it’s been possible to hitch a ride to the ISS and collaborate with NASA on PCG experiments, there is no existing commercial offering to actually manufacture drugs in space. Merck used their research results on Keytruda® crystallization to tinker with their terrestrial approaches to formulation. What if they could actually just manufacture the crystals they discovered in microgravity at commercial scale?
<br><br>
This is Varda’s mission—to make widespread research and manufacturing in microgravity a reality.
</blockquote>

<p>
One concern I have is that to date, the vast majority of space-based crystallization has been aimed at structural biology (elucidating the structure of a protein via crystallography), which only takes one crystal, one time. What Varda is aiming to do is preparative crystallography: crystallizing proteins and small molecules to isolate large quantities of them. Both processes obviously involve growing crystals, but otherwise they’re pretty different: in structural biology, all you care about is isolating a single large and very pure crystal, while uniformity and reproducibility are paramount in preparative crystallography. 
</p>

<p>
There’s some precedent for preparative protein crystallization in microgravity: a 1996 Schering-Plough paper studied crystallization of zinc interferon-α2b on the Space Shuttle. The results are excellent: over 95% of the protein crystallized, and the resulting suspension showed good stability and improved serum half-life in <i>Cynomolgus</i> monkeys (<a href=https://pubs.aip.org/aip/acp/article-abstract/361/1/139/609196/Macroscale-production-and-analysis-of-crystalline?redirectedFrom=fulltext>ref</a>, <a href=https://pubs.aip.org/aip/acp/article-abstract/387/1/671/811857/Protein-crystal-growth-in-microgravity-review-of?redirectedFrom=fulltext>ref</a>). The difference in crystal quality is huge:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230616_interferon.png style="width:450px;" />
  <figcaption>
  Space-grown crystals are much, much larger than crystals grown on Earth.
  </figcaption>
</figure>

<p>
More recently, scientists from Merck found that crystallization of pembrolizumab (Keytruda) in microgravity reproducibly formed a monomodal particle size distribution, as opposed to the bimodal particle size distribution formed under conventional conditions (<a href=https://www.nature.com/articles/s41526-019-0090-3>ref</a>), although the crystals didn’t seem any larger:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230616_keytruda.png style="width:450px;" />
  <figcaption>
  Comparison of pembrolizumab crystal size distribution under different conditions.
  </figcaption>
</figure>

<p>
Of course, until recently access to space was very limited and there wasn’t much reason to study preparative protein crystallography in microgravity, so the lack of studies is hardly surprising. For all the reasons discussed above, it seems very likely that preparative crystallography will be generally better in microgravity, and that the resulting crystals will be more homogenous and more pure than the crystals you could grow on Earth. But it’s not 100% certain, and that’s something Varda will have to establish. The word “can” is doing a lot of work in this text from their website:
</p>

<blockquote>
By eliminating factors such as natural convection and sedimentation, processing in a microgravity environment provides a unique path to formulating small molecules and biologics that traditional manufacturing processes cannot address. The resulting tunable particle size distributions, more ordered crystals, and new forms <b>can</b> lead to improved bioavailability, extended shelf-life, new intellectual property, and new routes of administration <i>(emphasis added)</i>
</blockquote>

<p>
Another concern is that little work (that I’m aware of) has been done on small molecule crystallization in microgravity—the very task Varda intends to start with. Small molecules, in general, are much easier to crystallize than proteins, and there are more parameters for the experimental scientist to tune. While there are certainly cases where small molecule crystals can display unexpected or problematic behaviors (like the famous case of <a href=https://link.springer.com/article/10.1023/A:1011052932607>ritonavir</a>, the molecule Varda is investigating first), in general crystallization of small molecules seems like an easier problem, and one for which there are better state-of-the-art workarounds.
</p>

<p>
The most likely failure mode to me, though, is just that microgravity crystallization is better than crystallization on Earth, but not that much better. Shooting APIs into space, waiting for them to crystallize, and then launching them back to Earth is going to be really expensive, and Varda will have to demonstrate extraordinary results to justify the added hassle—particularly for a technique that they hope to make a key part of the pharmaceutical manufacturing process. Talk about supply chain risk! (Is this all going to be <a href=https://www.fda.gov/drugs/pharmaceutical-quality-resources/facts-about-current-good-manufacturing-practices-cgmp>GMP</a>?)
</p>

<p>
And it’s worth pointing out a fairly obvious consideration too: what Varda is proposing to do in space is only one part of a vast, multi-step operation. No synthesis will take place in space, so all the manufacturing of either proteins or small molecules will take place on Earth. The purified products will then be launched into space, crystallized, and then the crystal-containing solution will undergo reentry and then be formulated into whatever final form the drug needs to be administered in.
</p>

<p>
So, although “there are a number of small molecules that fetch hundreds of thousands or millions of dollars per kg” (<i>Not Boring</i>), Varda can address only a small—albeit important—part of the manufacturing process. I doubt this is likely to change. On average, it takes 100–1000 kg of raw materials to manufacture 1 kg of a small molecule drug (<a href=https://pubs.acs.org/doi/10.1021/acs.oprd.1c00477>ref</a>), so shipping everything to orbit would massively raise costs, without any obvious advantages that I can think of. The TAM for Varda might be large enough to break even, but it’s not going to replace conventional pharmaceutical manufacturing anytime soon.
</p>

<p>
Varda’s pitch is perfect for venture capital: ambitious, risky, and potentially game-changing if it succeeds. And I wish them luck in their quest, since new and better ways to approach formulation would be awesome. But I can’t shake the nagging doubt that they’re so excited about the image of space-based manufacturing that they’re trying to invent a problem that their aerospace engineers have a solution for. We’ll find out soon enough if they’ve succeeded.
</p>

]]></description>
              <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Seven Degrees of Screening for Generality</title>
              <link>public/blog/20230607_degrees.html</link>
              <description><![CDATA[
<p>
<i>
(with apologies to <a href=https://www.chabad.org/library/article_cdo/aid/45907/jewish/Eight-Levels-of-Charity.htm>Maimonides</a> and <a href=https://rintintin.colorado.edu/~vancecd/phil215/Nozick.pdf>Nozick</a>)
</i>
</p>

<ol type="I">
<li>
Screening on only one substrate before assessing the substrate scope. This is the “ordinary means” in methods development. 
</li>

<li>
Screening on one substrate, but choosing a substrate that worked poorly in a previous study (<a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.201705525>e.g.</a>). This can be thought of as serial multi-substrate screening, where each substrate is a separate project, but the body of work achieves greater generality over time.
</li>

<li>
Screening on one substrate at a time, but rescreening catalysts when you find problematic substrates (<a href=https://www.nature.com/articles/s41557-022-00895-3>e.g.</a>). This amounts to serial multi-substrate screening within a single project.
</li>

<li>
Intentionally choosing a variety of catalysts up front and screening this set of catalysts for each new substrate class (<a href=https://pubs.acs.org/doi/10.1021/jacs.3c03182>e.g.</a>), thus achieving a high degree of generality with a family of catalysts, but without attempting to systematically quantify the generality of each catalyst in this set.
</li>

<li>
Choosing a handful of model substrates instead of just one, but otherwise doing everything the same as one would normally (<a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c06139>e.g.</a>, pages S24–S29).
</li>

<li>
Intentionally choosing a large, diverse panel of substrates and screening against this panel to quantify catalyst generality over a given region of chemical space. This is essentially what <a href=https://www.nature.com/articles/s41586-022-05263-2>we</a> and <a href=https://www.science.org/doi/10.1126/science.adf6177>the Miller group</a> did recently (and others, etc). 
</li>

<li>
The same, but incorporating robotics, fancy analytical methods, generative AI, or whatever else.
</li>
</ol>

<p>
When I present the “screening for generality” work, I often get the response “this is cool, but my reaction doesn’t work in 96-well plates/I don’t have an SFC-MS/my substrates are hard to make.” The point of this taxonomy is to illustrate that there are a lot of ways to move towards “screening for generality” that don’t involve 96-well plates. 
</p>

<p>
If you have the time and resources for robotics or SFC-MS, that’s great—you’ll be able to screen more quickly and cover more ground. But you can still start to consider more than a single model substrate even without any specialized equipment. It’s a mindset, not a recipe.
</p>

]]></description>
              <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Data Are Getting Cheaper</title>
              <link>public/blog/20230602_data_cheaper.html</link>
              <description><![CDATA[
<p>
Much ink has been spilled on whether scientific progress is slowing down or not (<a href=https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/>e.g.</a>). I don’t want to wade into that debate today—instead, I want to argue that, regardless of the rate of new discoveries, acquiring scientific data is easier now than it ever has been.
</p>

<p>
There are a lot of ways one could try to defend this point; I’ve chosen some representative anecdotes from my own field (organic chemistry), but I’m sure scientists in other fields could find examples closer to home. 
</p>

<h3>
NMR Spectroscopy
</h3>

<p>
    NMR spectroscopy is now the primary method for characterization and structural study of organic molecules, but it wasn’t always very good. The last half-century has seen a <a href=https://content.iospress.com/download/biomedical-spectroscopy-and-imaging/bsi055?id=biomedical-spectroscopy-and-imaging%2Fbsi055>steady increase</a> in the quality of NMR spectrometers (principally driven by the development of more powerful magnetic fields), meaning that even a relatively lackluster NMR facility today has equipment beyond the wildest dreams of a scientist in the 1980s:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_nmr.png style="width:400px;" />
  <figcaption>
  Most powerful NMR magnet over time (Campbell, Figure 1). Broad adoption lags these records, but the trend is comparable.
  </figcaption>
</figure>

<h3>
Commercially Available Compounds
</h3>

<p>
The number of compounds available for purchase has markedly increased in recent years. In the 1950s, the Aldrich Chemical Company’s listings could fit on a single page, and even by the 1970s Sigma-Aldrich only sold 40,000 or so chemicals (<a href=https://www.encyclopedia.com/books/politics-and-business-magazines/sigma-aldrich>source</a>). 
</p>

<p>
But things have changed. Nowadays new reagents are available for purchase within weeks or months of being reported (<a href=https://enamine.net/news-press-releases/press-releases/1286-enamine-brings-levin-nitrogen-deleting-reagent-to-the-market>e.g.</a>), and companies like Enamine spend all their time devising new and quirky structures for medicinal chemists to buy.
</p>

One way to quantify how many compounds are for sale today is through <a href=https://en.wikipedia.org/wiki/ZINC_database>the ZINC database</a>, aimed at collecting compounds for virtual screening, which is updated every few years or so. The first iteration of ZINC, in 2005, had fewer than a million compounds: now there are almost 40 billion:

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_zinc.png style="width:400px;" />
  <figcaption>
  Number of compounds in the ZINC database over time (graphic made by me).
  </figcaption>
</figure>

<p>
(Most compounds in the ZINC database aren’t available on synthesis scale, so it’s not like you can order a gram of all 40 billion compounds—there’s probably more like <a href=https://mcule.com/database/>3 million</a> “building blocks” today, which is still a lot more than 40,000.)
</p>

<h3>
Chromatography 
</h3>

<p>
Chromatography, the workhorse of synthetic and analytical chemistry, has also gotten a lot better. Separations that took almost an hour can now be performed in <a href=https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/jssc.201500270>well under a minute</a>, accelerating purification and analysis of any new material (this paper focuses on chiral stationary phase chromatography, but many of the advances translate to other forms of chromatography too).
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_chromatography.jpg style="width:450px;" />
  <figcaption>
    Fastest chiral separation for a given compound by year (Regalado, Figure 1). 
  </figcaption>
</figure>

<h3>
Computational Chemistry
</h3>

<p>
Moore’s Law is powerful. In the 1970s, using semiempirical methods and minimal basis set Hartree–Fock to investigate an 8-atom system was cutting-edge, as demonstrated by <a href=https://pubs.acs.org/doi/abs/10.1021/ja00455a056>this paper from Ken Houk</a>:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_houk.png style="width:400px;" />
  <figcaption>
    Figure 3 from the 1977 Houk paper.
  </figcaption>
</figure>

<p>
Now, that calculation would probably take only a few seconds on my laptop computer, and it’s becoming increasingly routine to perform full density-functional theory or post-Hartree–Fock studies on 200+ atom systems. <a href=https://www.science.org/doi/10.1126/science.ade5320>A recent paper</a>, also from Ken Houk, illustrates this nicely: 
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_houk2.png style="width:400px;" />
  <figcaption>
    Figure 3E from the 2022 Houk paper.
  </figcaption>
</figure>

<p>
The fact that it’s now routine to perform conformational searches and high-accuracy quantum chemical calculations on catalyst•substrate complexes with this degree of flexibility would astonish anyone from the past. (To be fair to computational chemists, it’s not all Moore’s Law—advances in QM tooling also play a big role.)
</p>

<h3>
What Now?
</h3>

<p>
There are lots of advances that I haven’t even covered, like the general improvement in synthetic methodology and the rise of robotics. Nevertheless, I think the trend is clear: it’s easier to acquire data than it’s ever been. 
</p>

<p>
What, then, do we do with all this data? Most of the time, the answer seems to be “not much.” <a href=https://pubs.acs.org/doi/10.1021/acs.orglett.2c03246>A recent editorial</a> by Marisa Kozlowski observes that the average number of substrates in <i>Organic Letters</i> has increased from 17 in 2009 to 40 in 2021, even as the information contained in these papers has largely remained constant. Filling a substrate scope with almost identical compounds is a boring way to use more data; we can do better.
</p>

<p>
The availability of cheap data means that scientists can—and must—start thinking about new ways to approach experimental design. Lots of academic scientists still labor under the delusion that “hypothesis-driven science” is somehow superior to HTE, when in fact the two are ideal complements to one another. “Thinking in 96-well plates” is already common in industry, and should become more common in academia; why run a single reaction when you can run a whole screen?
</p>

<p>
New tools are needed to design panels, run reactions, and analyze the resultant data. One nice entry into this space is Tim Cernak’s <a href=https://chemrxiv.org/engage/chemrxiv/article-details/60c75166702a9bcd6918bf39><i>phactor</i></a>, a software package for high-throughput experimentation, and I’m sure lots more tools will spring up in the years to come. (I’m also optimistic that multi-substrate screening, which we and others have used in “screening for generality,” will become more widely adopted as HTE becomes more commonplace.)
</p>

<p>
The real worry, though, is that we will refuse to change our paradigms and just use all these improvements to publish old-style papers faster.  All the technological breakthroughs in the world won’t do anything to accelerate scientific progress if we refuse to open our eyes to the possibilities they create. If present trends continue, it may be possible in 5–10 years to screen for a hit one week, optimize reaction conditions the next week, and run the substrate scope on the third week. Do we really want a world in which every methodology graduate student is expected to produce 10–12 low-quality papers per year? 
</p>

]]></description>
              <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Chaos Monkeys</title>
              <link>public/blog/20230530_chaos_monkeys.html</link>
              <description><![CDATA[
<p>
Recently, I wrote about <a href=https://corinwagen.github.io/public/blog/20230502_startups.html>how scientists could stand to learn a lot from the tech industry.</a> In that spirit, today I want to share a book review of <i>Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley</i>, Antonio García Martínez’s best-selling memoir about his time in tech and “a guide to the spirit of Silicon Valley” (<a href=https://www.nytimes.com/2016/06/29/business/dealbook/review-chaos-monkeys-is-a-guide-to-the-spirit-of-silicon-valley.html>NYT</a>).
</p>

<p>
<i>Chaos Monkeys</i> is one of the most literary memoirs I’ve read. The book itself is a clear homage to Hunter S. Thompson’s <i>Fear and Loathing in Las Vegas</i>; Martínez writes in the “gonzo journalism” style, blending larger-than-life personal exploits with frank accounting of the facts. But Antonio García Martínez’s writing, replete with GRE-level words and classical epigraphs, invites further literary comparisons.
</p>

<h2>
<i>Chaos Monkeys</i> as <i>The Odyssey</i>
</h2>

<p>
The first comparison that springs to mind is <i>The Odyssey</i>. Odysseus, the protagonist of The Odyssey, is frequently described as <i>polytropos</i> (lit. “many turns”), which denotes his wily and cunning nature. Antonio García Martínez (or, per the tech fondness for acronyms, “AGM”) certainly deserves the same epithet.
</p>

<p>
<i>Chaos Monkeys</i> is structured as a recounting of AGM’s escapades in Silicon Valley. In order, he (1) leaves his job at Goldman Sachs and joins an ad-tech company, (2) quits and founds his own company, AdGrok, (3) gets accepted to Y Combinator and survives a lengthy legal battle with his former employer, (4) sells AdGrok simultaneously to both Twitter and Facebook, eventually sending the other employees to Twitter and himself to Facebook, (5) becomes a PM at Facebook and engineers a scheme to fix their ad platform and make them profitable, (6) succeeds, sorta, but pisses people off and gets fired, and then (7) turns around and sells his expertise to Twitter.
</p>

<p>
His circuitous journey around the Bay Area has the rough form of an ancient epic: at each company, he’s faced with new challenges and new adversaries, and his fractious relationships with his superiors mean that he’s often at the mercy of capricious higher powers, not unlike Odysseus. Nevertheless, through a mixture of cunning and hard work he manages to escape with his skin intact every time, ready for the next episode. (And, best of all, he literally lives on a boat while working at Facebook.)
</p>

<p>
(His escapades have only continued since this book was published: he got hired at Apple, unceremoniously fired a few weeks later [for passages in <i>Chaos Monkeys</i>], made it on <a href=https://open.spotify.com/episode/2JMZ0yOuTaGU3bXLymlWYT>Joe Rogan</a>, and has now founded <a href=https://www.spindl.xyz/>spindl</a>, a web3 ad-tech startup, while simultaneously <a href=https://www.thepullrequest.com/p/why-judaism>converting to Judaism</a>.)
</p>

<h2>
<i>Chaos Monkeys</i> as <i>Moby-Dick</i>
</h2>

<p>
<i>Chaos Monkeys</i> bears a structural resemblance to <i>Moby-Dick</i>. Narrative passages alternate with lengthy technical discussions about the minutiae of Silicon Valley: one chapter, you’re reading about how AGM flooded Mark Zuckerberg’s office trying to brew beer inside Facebook, while the next chapter is devoted to a discussion of how demand-side advertising platforms work.
</p>

<p>
The similarities run deeper, though. Venture capitalism, the funding model that dominates Silicon Valley, was originally developed to fund whaling expeditions in the 1800s (<a href=https://newrepublic.com/article/154490/small-world-vc>ref</a>, <a href=https://www.hbs.edu/faculty/Pages/item.aspx?num=43322>ref</a>). While once venture capitalists listened to prospective whaling captains advertising the quality of their crews in a New England tavern, today VCs hear pitches from thousands of startups hoping to develop <s>the next killer app</s> the best ChatGPT frontend and make millions.
</p>

<p>
This isn’t a coincidence: whaling expeditions and tech startups are both high-risk, high-reward enterprises that require an incredible amount of skill and hard work, but also a healthy dose of luck. Both operations have returns dominated by outliers, making picking winners much more important than making safe bets, and in both cases the investment remains illiquid for a long time, demanding trust from the investor.
</p>

<p>
Much like Ishmael in <i>Moby-Dick</i>, AGM’s adventures see him join forces with a motley crew of high-performing misfits from around the globe. And just as Ahab’s quest for the whale is foreshadowed to be a ruinous one, so too does the reader quickly come to understand that AGM’s tenure in Silicon Valley will not, ultimately, end well. A fatalistic haze hangs over the book, coloring his various hijinks with a sense of impending loss.
</p>

<p>
(And did I mention he lives on a boat?)
</p>

<h2>
<i>Chaos Monkeys</i> as <i>The Great Gatsby</i>
</h2>

<p>
The emotional tone of the book, however, is best compared to that favorite of high-school English classes, <i>The Great Gatsby</i>. AGM, like Nick Carraway, is an outsider in the world of the nouveau riche—opulent parties, high-speed road races, conspicuous consumption—and, over the course of the book, is alternately infatuated with and disgusted by his surroundings. When at last AGM retires to a quiet life on <s>Ithaca</s> the San Juan islands, it’s with feelings of disillusionment, betrayal, and frustration, not unlike Carraway withdrawing to the Midwest. As AGM writes in the penultimate paragraph of <i>Chaos Monkeys</i>’s acknowledgements:
</p>

<blockquote>
To Paul Graham, Jessica Livingston, Sam Altman, and the rest of the Y Combinator partners and founders involved in the AdGrok saga. In a Valley world awash with mammoth greed and opportunism masquerading as beneficent innovation, you were the only real loyalty and idealism I ever encountered.
</blockquote>

<p>
But <i>Chaos Monkeys</i> isn’t solely an indictment of Silicon Valley’s worst excesses. Not unlike <i>The Wire</i>, <i>Chaos Monkeys</i> manages to simultaneously portray the positive and negative aspects of its subject matter, refusing to be reduced to “tech good” or “tech bad.” The panoply of grifters, Dilbert-tier bosses, and Machiavellian sociopaths lambasted by AGM can exist only because of the immense value that their enterprises provide to society—and his faith in the ability of tech to create wonders persists even as his own efforts to do so are undermined.
</p>

<p>
So the underlying message of <i>Chaos Monkeys</i>, ultimately, is one of hope for tech. If the excesses of tech are worse than other industries, it's only because the underlying field itself is so much more fertile. Far from condemning it, the depths of the decadence spawned by Silicon Valley bear witness to the immense value it creates. Imitation is the highest form of flattery; grift is the surest sign of productivity.
</p>

<p>
Overall, <i>Chaos Monkeys</i> is an exhilarating and hilarious read, a gentle introduction to the world of term sheets, product managers, and non-competes, and a book replete with anecdotes sure to fulfill the stereotypes of tech-lovers and tech-haters alike.
</p>


]]></description>
              <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Editorial Overreach and Scientific Authority</title>
              <link>public/blog/20230525_jcim.html</link>
              <description><![CDATA[
<p>
Previously, I wrote about various <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>potential future roles for journals</a>. Several of the scenarios I discussed involved journals taking a much bigger role as editors and custodians of science, using their power to shape the way that science is conducted and exerting control over the scientific process.
</p>

<p>
I was thus intrigued when, last week, <i>The Journal of Chemical Information and Modeling</i> (<i>JCIM</i>; an ACS journal) released <a href=https://pubs.acs.org/doi/10.1021/acs.jcim.3c00599>a set of guidelines</a> for molecular dynamics simulations that future publications must comply with. These guidelines <a href=https://twitter.com/LindorffLarsen/status/1659889905843990529>provoked a reaction from the community</a>: various provisions (like the requirement that all simulations be performed in triplicate) were alleged to be <a href=https://twitter.com/PabloGalazDavis/status/1659940766330781696>arbitrary</a> or <a href=https://twitter.com/PiiaBartos/status/1659976208015130626>unscientific</a>, and the fact that these standards were imposed by editors and not determined by the community also <a href=https://twitter.com/daviddesancho/status/1660184730862583808>drew criticism</a>.
</p>

<p>
The authors <a href=https://twitter.com/RommieAmaro/status/1660050832157716481>say</a> that the editorial “is *not* intended to instruct on how to run MD”, but this defense rings hollow to me. See, for instance, the section about choosing force fields:
</p>

<blockquote>
JCIM will not accept simulations with old force field versions unless a clear justification is provided. Specialized force fields should be used when available (e.g., for intrinsically disordered proteins). In the case of the reparametrization or development of new parameters compatible with a given force field, please provide benchmark data to support the actual need for reparameterization, proper validation of novel parameters against experimental or high-level QM data…
</blockquote>

<p>
I’m not a molecular dynamics expert, so I’m happy to stay out of the scientific side of things (although the editorial’s claim that “MD simulations are not suitable to sample events occurring between free energy barriers” seems clearly false for sufficiently low-barrier processes). Nor do I wish to overstate the size of the community’s reaction: a few people complaining on Twitter doesn’t really matter.
</p>

<p>
Rather, I want to use this vignette to reflect on the nature of scientific authority, and return to a piece I’ve cited <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>before</a>: Geoff Anders’ <a href=https://www.palladiummag.com/2022/10/10/the-transformations-of-science/>“The Transformations of Science.”</a> Anders describes how the enterprise of science, initially intended to be free from authority, has evolved into a magisterium of knowledge that governments, corporations, and laypeople rely upon:
</p>

<blockquote>
The original ideal of <i>nullius in verba</i> sometimes leads people to say that science is a never-ending exploration, never certain, and hence antithetical to claims on the basis of authority. This emphasizes one aspect of science, and indeed in theory any part of the scientific corpus could be overturned by further observations.
<br><br>
There is, however, another part of science—<i>settled science.</i> Settled science is safe to rely on, at least for now. Calling it into question should not be at the top of our priorities, and grant committees, for example, should typically not give money to researchers who want to question it again.
</blockquote>

<p>
While each of these forms of science is fine on its own, they ought not to be conflated:
</p>

<blockquote>
When scientists are meant to be authoritative, they’re supposed to know the answer. When they’re exploring, it’s okay if they don’t. Hence, encouraging scientists to reach authoritative conclusions prematurely may undermine their ability to explore—thereby yielding scientific slowdown. Such a dynamic may be difficult to detect, since the people who are supposed to detect it might themselves be wrapped up in a premature authoritative consensus.
</blockquote>

<p>
This is tough, because scientists like settled science. We write grant applications describing how our research will bring clarity to long-misunderstood areas of reality, and fancy ourselves explorers of unknown intellectual realms. How disappointing, then, that so often science can only be relied upon when it settles, long after the original discoveries have been made! An intriguing experimental result might provoke further study, but it’s still “in beta” (to borrow the software expression) for years or decades, possibly even forever.
</p>

<p>
Applying the settled/unsettled framework of science to the <i>JCIM</i> question brings some clarity. I don’t think anyone would complain about settled science being used in editorial guidelines: I wouldn’t want to open <i>JACS</i> and read a paper that questioned the existence of electrons, any more than I want <i>The Economist</i> to publish articles suggesting that Switzerland is an elaborate hoax.
</p>

<p>
Scientific areas of active inquiry, however, are a different matter. Molecular dynamics might be a decades-old field, but the very existence of journals like <i>JCIM</i> and <i>JCTC</i> points to its unsettled nature—and <a href=https://twitter.com/LindorffLarsen/status/1659931209659457539>AlphaFold2</a>, discussed in the editorial, is barely older than my toddler. There are whole hosts of people trying to figure out how to run the best MD simulations, and editors giving them additional guidelines is unlikely to accelerate this process. (This is separate from mandating they report what they actually did, which is fair for a journal to require.)
</p>

<p>
Scientists, especially editors confronted with an unending torrent of low-quality work, want to combat bad science. This is a good instinct. And I’m sympathetic to the idea that journals need to become something more than a neutral forum in the Internet age—the editorial aspect of journals, at present, seems underutilized. But prematurely trying to dictate rules for exploring the frontier of human knowledge is, in my opinion, the wrong way to do this. What if the rules are wrong?
</p>

<p>
There may be a time when it’s prudent for editors to make controversial or unpopular decisions: demanding pre-registration in psychology, for instance, or mandating external validation of a new synthetic method. But I’m not sure that “how many replicates MD simulations need” is the hill I would choose to die on. In an age of declining journal relevance, wise editorial decisions might be able to set journals apart from the “anarchic preprint lake”—but poor decisions may simply hasten their decline into irrelevance.
</p>

]]></description>
              <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: The Art of Doing Science and Engineering</title>
              <link>public/blog/20230516_hamming.html</link>
              <description><![CDATA[
<br>
<br>
<p class=epigraph>
“They performed his signs among them, and miracles in the land of Ham.”
</p>
<p class=epigraph-byline>
—Psalm 105:27
</p>

<p>
Who was Richard Hamming, and why should you read his book?
</p>

<p>
If you’ve taken computer science courses or messed around enough with <i>scipy</i>, you might recognize his name in a few different places—<a href=https://en.wikipedia.org/wiki/Hamming_code>Hamming error-correction codes</a>, the <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hamming.html>Hamming window function</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_distance>Hamming distance</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_bound>Hamming bound</a>, etc. I had heard of some of these concepts, but didn’t know anything concrete about him before I started reading this book.
</p>

<p>
His brief biography is as follows: Richard Hamming (1915–1998) studied mathematics at the University of Chicago, earned a PhD in three years from Illinois, and then started as a professor at the University of Louisville… in 1944. He was almost immediately recruited for the Manhattan Project, where he worked on large-scale simulations of imploding spherical shells of explosives, and generally acted as a “computational janitor” for various projects.
</p>

<p>
In 1946, he moved to Bell Telephone Laboratories, <a href=https://www.theverge.com/2012/3/21/2887206/jon-gertner-idea-factory-bell-labs-great-american-age-innovation-book-review>“arguably the most innovative research institution in the country,”</a> and worked there until 1976. During his time at Bell Labs, he was involved in “nearly all of the laboratories' most prominent achievements” (<a href=https://en.wikipedia.org/wiki/Richard_Hamming>Wikipedia</a>), and was rewarded with such accolades as the third-ever Turing Award (essentially the Nobel Prize for computer science) and the IEEE Richard W. Hamming medal. (You know you’re successful when someone else names an award after you!)
</p>

<p>
After he retired from Bell Labs, Hamming went on to go teach classes at the Naval Postgraduate School. This book—published in 1996, just before his death—is based on the capstone engineering class he taught there, which attempted to prepare students for their technical future by conveying the “style of thinking” necessary to be a great scientist and engineer. Stripe Press calls it “a provocative challenge to anyone who wants to build something great” and “a manual of style for how to get there,” while the foreword calls it a “tour of scientific greatness” which prepares “the next generation for even greater greatness” while challenging them to serve the public good.
</p>

<p>
I was excited to read this book because:
</p>

<ol>
<li>
Hamming was present at arguably the two most successful scientific institutions of the 20th century—the Manhattan Project and Bell Labs—and so was witness to more innovative scientific discoveries than almost anyone today. So if anyone has a shot at teaching how to be an effective scientist, it’s Hamming.
</li>

<li>
The interface between academic science and real-world advances seems to be one of the most broken elements of the modern research ecosystem, and I was interested to see what Hamming, as a self-proclaimed scientist and engineer, had to say on the subject.
</li>

<li>
It’s a first-hand account of the creation of the most important field of the past century—computer science—and thus a way to witness, albeit second-hand, the important process of <a href=https://freaktakes.substack.com/p/when-do-ideas-get-easier-to-find>scientific branch-creation</a>.
</li>

<li>
The book is a chance to see computing, and computer science, as it was before the implications and importance of the field became obvious, and thus a way to understand what a promising area of research looks like without the benefit of hindsight.
</li>

<li>
This is a chance to read frank and honest reflections from someone who both was a great scientist and was adjacent to a huge number of other great scientists, and thus learn about the culture of successful science (especially successful science in the mid-20th century, which might be different than the science of today).
</li>
</ol>

<h2>
1. What’s In The Book?
</h2>

<p>
<i>The Art of Doing Science and Engineering</i> is not a conventional textbook, but neither is it a self-help book, a memoir, or a guide to personal strategy. The two books that remind me of it most are <i>Godel, Escher, Bach</i>, by Douglas Hofstadter, and <i>Zero To One</i>, by Peter Thiel. All three books are composed of various pseudo-independent chapters which, in isolation, could likely function as essays, but which echo certain themes over and over again in a way that makes the whole greater than the sum of its parts in a hard-to-summarize way.
</p>

<p>
Hamming’s goal in structuring the book this way is clear, and explicitly stated: he doesn’t want to teach object-level facts, because the facts needed to succeed in any discipline will inevitably change over the course of time. Instead he seeks to teach the patterns of thought which will enable anyone to succeed in an evolving technical landscape. To do so, he uses a mix of first-person stories, historical reflections, mathematical proofs, and graphs, all with the goal of teaching something that’s essentially incommunicable: style. I quote:
</p>

<blockquote>
I will show you my style as best I can, but, again, you must take those elements of it which seem to fit you, and you must finally create your own style. Either you will be a leader or a follower, and my goal for you is to be a leader. You cannot adopt every trait I discuss in what I have observed in myself and others; you must select and adapt, and make them your own if the course is to be effective.
</blockquote>

<p>
Despite Hamming’s insistence that there is “really no technical content” in the book and that any mathematics is only “window dressing,” I found that my lack of background knowledge made several chapters—particularly those on digital filters—quite difficult to understand.
</p>

<h2>
2. Representative Anecdotes
</h2>

<p>
In keeping with Hamming’s desire to convey the “style of thinking” rather than actual object-level scientific ideas, I’ll share a few anecdotes, insights, and quotes in an attempt to reproduce the style of his writing.
</p>

<h3>
2.1 The Second Mouse Gets The Cheese
</h3>

<p>
Hamming recounts the invention of interpreters and compilers and then reflects on how hard it was for early computer pioneers to think of computers as “symbol manipulators and not just number crunchers,” observing that often the first people in a field do not understand it as well as those who come after them. Why is this?
</p>

<blockquote>
The reason this happens is that the creators have to fight through so many dark difficulties, and wade through so much misunderstanding and confusion, they cannot see the light as others can, now the door is open and the path made easy…. in time the same will probably be true of you.
</blockquote>

<p>
This is a nice observation, and perhaps explains the value of startups: incumbents in a market can be inefficient not only for bureaucratic reasons, but also because they’re intellectually less suited to see new opportunities—the young can see the status quo more clearly than those who’ve had to create it. This also explains why “really new ideas seldom arise from the experts in the field”—experts always bring their expertise when looking at something new, which makes them more likely to be correct, but also disincentivizes new ways of thinking and thus creates a sort of status quo bias.
</p>

<h3>
2.2 Order-of-Magnitude Changes
</h3>

<p>
Many people, faced with early computers, dismissed them as simply a faster way to do rote calculations—which seems silly in hindsight, but was a real barrier for early computer pioneers to overcome. Hamming argues “a single order of magnitude change (a factor of ten) produces fundamentally new effects” in any piece of technology, and immediately thereafter reflects on how it’s difficult for people to accept something new:
</p>

<blockquote>
People always want to think that something new is just like the past—they like to be comfortable in their minds as well as their bodies—and hence they prevent themselves from making any significant contribution to the new field being created under their noses. Not everything which is said to be new really is new, and it is hard to decide in some cases when something is new, yet the all too common reaction of “it’s nothing new” is stupid.
</blockquote>

<p>
I had previously attributed the idea that ten-fold improvement creates a fundamentally new product to Peter Thiel (<i>Zero To One</i>, pp. 48–49), but it seems Hamming (as usual) got there first.
</p>

<h3>
2.3 Intuition In High Dimensions
</h3>

<p>
Since most complex problems occur in high-dimensional spaces, Hamming illustrates a few ways that our 2D or 3D intuition can fail us.
</p>

<p>
One particularly nice thought experiment is this: take a square with side length 4 and divide it into four squares, each containing a unit circle. Now draw a circle in the middle of the square, such that the circle just touches each of the four unit circles. Realizing that the distance from the center of this circle to the center of each unit circle must be √2, and that the radius of the unit circle is, of course, 1, we can see that the radius of the inner circle must be √2 - 1 ≈ 0.414.
</p>

<p>
Now, we can perform the same mental exercise for a cube with side length four, and find that the analogous inner sphere has side length √3 - 1 ≈ 0.732. More generally, as we extend this exercise to higher dimensions, we find that the radius of the inner <i>n</i>-dimensional hypersphere is √<i>n</i> - 1, which is bizarre! For instance, in ten dimensions, the inner sphere has radius √10 - 1 ≈ 2.162, meaning that it reaches outside of the surrounding cube:
</p>

<blockquote>
Yes, the sphere is convex, yes it touches each of the 1,024 packed spheres on the inside, yet it reaches outside the cube! So much for your raw intuition about n-dimensional space, but remember the n-dimensional space is where the design of complex objects generally takes place.
</blockquote>

<p>
This sort of intuition is difficult to obtain, but Hamming gives a few examples. One that stuck with me was the claim that, in high-dimensional shapes, almost all of the volume is on the surface—so “almost surely the optimal design will be on the surface and will not be inside, as you might think from taking the calculus and doing optimizations in that course.”
</p>

<h3>
2.4 The Fast Fourier Transform
</h3>

<p>
Here’s a memorable story from Hamming’s life:
</p>

<blockquote>
You have all heard about the fast Fourier transform [FFT] and the Tukey-Cooley paper. It is sometimes called the <i>Tukey-Cooley</i> transform or algorithm. Tukey had suggested to me, sort of, the basic ideas of the FFT. I had at the time an IBM Card Programmed Calculator (CPC) and the “butterfly” operation meant it was completely impractical to do with the equipment I had. Some years later I had an internally programmed IBM 650 and he remarked on it again. All I remembered was that it was one of Tukey’s few bad ideas; I completely forgot why it was bad—namely because of the equipment I had at the time. So I did not do the FFT, though a book I had already published (1961) shows I knew all the facts necessary, and could have done it easily!
<br><br>
Moral: when you know something cannot be done, also remember the essential reason why, so later, when the circumstances have changed, you will not say “It can’t be done.” Think of my error! How much more stupid can anyone be?
</blockquote>

<p>
Later in the book, Hamming puts forward the following “old statement” about experts:
</p>

<blockquote>
If an expert says something can be done he is probably correct, but if he says it is impossible then consider getting another opinion.
</blockquote>

<h3>
2.5 The Mixed Blessings of Jargon
</h3>

<p>
Hamming emphasizes the importance, as an interdisciplinary scientist, of mastering the language of the field in which you’re working, but warns against embracing jargon too much. Why? Jargon serves “to facilitate communication over a restricted area of things or events… [but] also blocks thinking outside the original area it was designed to cover.” So jargon makes intra-domain communication easier, but makes inter-domain communication harder.
</p>

<p>
More philosophically, jargon is a consequence of how “we have been mainly selected by evolution to resent outsiders,” and thus the “instinctive use of jargon” is a base instinct that must be consciously resisted.
</p>

<h3>
2.6 Optimal Components, Suboptimal Systems
</h3>

<p>
Hamming discusses the field of systems engineering, which he defines as “the attempt to keep at all times the larger goals in mind and to translate local actions into global results” (emphasis original), and coins the first rule of systems engineering:
</p>

<blockquote>
If you optimize the components, you will probably ruin the system performance.
</blockquote>

<p>
This point is illustrated with a few examples. One of these examples is the progressive optimization of calculus and linear algebra classes in college, where “we have stripped out anything not immediately relevant to each course,” leading to “large parts of any mathematical education being omitted in the urge to optimize the individual courses.” Only when the proper goal of a mathematical education is taken into account—producing well-trained students with a firm grasp of math and the ability to apply it to important problems—can the constituent courses sanely be optimized.
</p>

<p>
I found this idea pretty insightful, and have thought about it a lot since reading this book. For instance, one can see many researchers as over-optimizing for “publishing papers” or “winning grants” rather than working towards maximizing total scientific progress. (Alex Danco has a great piece discussing the same ideas in the context of the Canadian tech ecosystem, which I wrote about <a href=https://corinwagen.github.io/public/blog/20221206_definite_games_indefinite_optimism.html>here</a>.)
</p>

<h3>
2.7 Learning Should Be Difficult
</h3>

<p>
I like this story so much I’ll just reproduce it in its entirety:
</p>

<blockquote>
When I first came to the Naval Postgraduate School in 1976 there was a nice dean of the extension division concerned with education. In some hot discussions on education we differed. One day I came into his office and said I was teaching a weightlifting class (which he knew I was not). I went on to say that graduation was lifting 250 pounds, and I had found many students got discouraged and dropped out, some repeated the course, and a very few graduated. I went on to say thinking this over last night I decided the problem could be easily cured by simply cutting the weights in half—the student in order to graduate, would lift 125 pounds, set them down, and then lift the other 125 pounds, thus lifting the 250 pounds.
<br><br>
I waited a moment while he smiled (as you probably have) and I then observed that when I found a simpler proof for a theorem in mathematics and used it in class, was I or was I not cutting the weights in half? What is your answer? Is there not an element of truth in the observation that the easier we make the learning for the student, the more we are cutting the weights in half?
</blockquote>

<p>
This story reflects a key belief of Hamming’s: that creativity and talent in technical disciplines are not innate traits given only to rare geniuses, but trainable skills which anyone can hope to acquire and improve at, given the appropriate training regimen. In Hamming’s worldview, staring at a confusing math problem is not a sign that you’re a failure, but the process by which you become successful.
</p>

<h3>
2.8 The Importance of Presentation
</h3>

<p>
Hamming emphasizes that being able to “sell” one’s ideas is a key part of being a scientist:
</p>

<blockquote>
All [methods of conveying ideas] are essential—you must learn to sell your ideas, not by propaganda, but by force of clear presentation. I am sorry to have to point this out; many scientists and others think good ideas will win out automatically and need not be carefully presented. They are wrong; many a good idea has had to be rediscovered because it was not well presented the first time, years before! New ideas are automatically resisted by the establishment, and to some extent justly. The organization cannot be in a continual state of ferment and change, but it should respond to significant changes.
</blockquote>

<p>
In this view, a certain degree of institutional conservatism is necessary to avoid being swept up by any new fad (in machine learning terms, we might say that organizations need to limit their learning rate), and so you alone must convince your peers that your insights are the real deal and deserve to be taken seriously.
</p>

<p>
Hamming then extends this idea to needing to sell your abilities more broadly:
</p>

<blockquote>
You do not hire a plumber to learn plumbing while trying to fix your trouble; you expect he is already an expert. Similarly, only when you have developed your abilities will you generally get the freedom to practice your expertise, whatever you choose to make it, including the expertise of “universality,” as I did.
</blockquote>

<p>
My experience within science is that most people are a bit allergic to the idea of “selling” themselves or their research—with the exception of a few people who become almost addicted to it. Hamming (who never shies away from quoting a Greek philosopher) would probably think that there’s an Aristotelian mean between these two extremes: the ideal scientist/engineer recognizes that self-promotion is a necessary means to an end, but never engages in self-promotion absent a higher goal.
</p>

<h2>
3. Overall Themes
</h2>

<p>
I would summarize Hamming’s key themes—those leitmotifs which pop up time and time again in the book—as the following:
</p>

<h3>
3.1 Fundamentals Are Key
</h3>

<p>
Wherever possible, try to understand the intellectual underpinnings of a field as well as possible, rather than the surface-level results. If you do so, you will not only understand the field better than most of its practitioners, but also be better at transferring knowledge between fields. Perceiving “the essential unity of all knowledge rather than the fragments which appear as the individual topics are taught” allows one to quickly access relevant information to the problem at hand, no matter the field of origin.
</p>

<p>
Hamming frequently points out the failings of domain experts to perceive the fundamental underpinnings of their knowledge:
</p>

<blockquote>
Lo and behold, the famous <i>transfer function</i> is exactly the <i>eigenvalues</i> of the corresponding eigenfunctions! Upon asking various electrical engineers what the transfer function was, no one has ever told me that! Yes, when pointed out to them that it is the same idea they have to agree, but the fact it is the same idea never seemed to have crossed their minds! The same, simple idea, in two or more different disguises in their minds, and they knew of no connection between them! Get down to the basics every time! <i>(emphasis original)</i>
</blockquote>

<p>
Not only is a good grasp of fundamentals important for understanding your own domain, it also helps with creativity. Hamming argues that creative insights come from the subconscious, and that “flexible access to pieces of knowledge” is the most important way to give the subconscious the tools it needs to solve a problem. This flexible access arises from “looking at knowledge while you are acquiring it from many different angles,” making sure to capture its key features rather than simply memorizing the aspect relevant to the task at hand.
</p>

<p>
(The idea that creativity comes from the subconscious is pretty common—see, for instance, Nisbett and Wallace’s article <a href=https://home.csulb.edu/~cwallis/382/readings/482/nisbett%20saying%20more.pdf>“Telling More Than We Can Know,”</a> which argues that basically all higher order cognitive processes are subconscious.)
</p>

<h3>
3.2 Gain Insight Where Possible
</h3>

<p>
No matter the task at hand, Hamming argues that the correct immediate step is always to gain insight about the situation, and then go from there. He uses the example of Planck and the “ultraviolet catastrophe” to illustrate how crucial insight can be:
</p>

<blockquote>
Max Planck (1858–1947) fit the black-body radiation experimental data with an empirical curve, and it fit so well he “knew” it was “the right formula.” He set out to derive it, but had trouble. Finally he used a standard method of breaking up the energy into finite sizes and then going to the limit… Fortunately for Planck the formula fit only so long as he avoided the limit, and no matter how he took the limit the formula disappeared. He finally, being a very good, honest physicist, decided he had to stop short of the limit, and that is what defines Planck’s constant!
<br><br>
<i>[another historical paragraph omitted]</i>
<br><br>
Before going on, let me discuss how this piece of history has affected my behavior in science. Clearly Planck was led to create the theory because the approximating curve fit so well, and had the proper form. I reasoned, therefore, if I were to help anyone do a similar thing, I had better represent things in terms of functions they believed would be proper for their field rather than in the standard polynomials. I therefore abandoned the standard polynomial approach to approximation, which numerical analysts and statisticians among others use most of the time, for the harder approach of finding which class of functions I should use.
</blockquote>

<p>
This episode demonstrates how insight can arise from a simulation, and enable future work (like, in this case, all of quantum mechanics), and also illustrates how the manner in which one performs simulations can make it either easier or harder to obtain underlying insights about the problem. Hamming emphasizes this point with a few stories from his time at Bell Labs, and argues that there are times where more computational power is actually counterproductive:
</p>

<blockquote>
I have often wondered what would have happened [in the Nike guided missile project] if I had had a modern, high-speed computer. Would I ever have acquired the feeling for the missile, upon which so much depended in the final design? I often doubt hundreds more trajectories would have taught me as much—I simply do not know. But that is why <i>I am suspicious, to this day, of getting too many solutions and not doing enough very careful thinking about what you have seen.</i> Volume output seems to me to be a poor substitute for acquiring an intimate feeling for the situation being simulated… doing simple simulations at the early stages lets you get insights into the whole system which would be disguised in any full-scale simulation. <i>(emphasis original)</i>
</blockquote>

<p>
This point—that simulation is not the same as understanding—is not unique to Hamming (see <i>inter alia</i> Roald Hoffmann on the subject: <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201902527>1</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910283>2</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910285>3</a>), but hearing this from the father of scientific simulation certainly drives the message home!
</p>

<h3>
3.3 Vision Matters
</h3>

<p>
Hamming goes to great lengths to emphasize the importance of having a vision for your life:
</p>

<blockquote>
It is well known the drunken sailor who staggers to the left or right with n independent random steps will, on the average, end up about √<i>n</i> steps from the origin. But if there is a pretty girl in one direction, then his steps will tend to go in that direction and he will go a distance proportional to <i>n</i>. In a lifetime of many, many independent choices, small and large, a career with vision will get you a distance proportional to <i>n</i>, while no vision will get you only the distance √<i>n</i>. In a sense, the main difference between those who go far and those who do not is some people have a vision and the others do not and therefore can only react to the current events as they happen… having a vision is what tends to separate the leaders from the followers.
</blockquote>

<p>
This theme permeates his discussion of systems engineering: a successful systems engineer must, at all times, keep the overall vision and purpose of the system in mind, so as to optimize in the right direction. It also motivates what problems you must choose:
</p>

<blockquote>
If you do not work on important problems, how can you expect to do important work? Yet direct observation and direct questioning of people show most scientists spend most of their time working on things they believe are not important and are not likely to lead to important things.
</blockquote>

<p>
In Hamming’s view, it seems the precise vision one follows is less important than the simple act of having a vision at all. Forcing oneself to decide on goals and then strive to fulfill them will naturally lead you to excellence, even if the goals you choose aren’t the same as the one Hamming would have chosen:
</p>

<blockquote>
The chief gain is in the effort to change yourself, in the struggle with yourself, and it is less in the winning than you might expect. Yes, it is nice to end up where you wanted to be, but the person you are when you get there is far more important. I believe a life in which you do not try to extend yourself regularly is not worth living—but it is up to you to pick the goals you believe are worth striving for.
</blockquote>

<h2>
4. Should You Read This Book?
</h2>

<p>
I began this book review by claiming that <i>The Art of Doing Science and Engineering</i> isn’t a textbook, or a self-help book, or a memoir, but failed to offer a positive vision of what it was. I now reveal my true opinion: <i>The Art of Doing Science and Engineering</i> is best viewed as a modern example of <a href=https://en.wikipedia.org/wiki/Wisdom_literature>“wisdom literature,”</a> in the style of ancient scriptures.
</p>

<p>
Why is this? Wisdom literature frequently has the curious property that it’s accessible only to the wise. For instance, the book of Proverbs is ostensibly written to convey wisdom to those who seek it, but this hardly seems compatible with the following passage (Proverbs 26:7–9):
</p>

<blockquote>
Like a lame man's legs, which hang useless, is a proverb in the mouth of fools.<br>
Like one who binds the stone in the sling is one who gives honor to a fool.<br>
Like a thorn that goes up into the hand of a drunkard is a proverb in the mouth of fools.<br>
</blockquote>

<p>
If proverbs are useless—or worse than useless, a thorn in the hand of a drunkard—to those without wisdom, then what is the point of proverbs? If only the wise can understand your book of wisdom, why bother writing it at all?
</p>

<p>
There are several ways to resolve this tension (one being “the book of Proverbs is stupid”), but I think the right answer goes something like this: wisdom is accessible to those who seek it, but simply reading through a book of wisdom isn’t sufficient to make one wise. Rather, the search for wisdom requires discipline and vigilance—one must meditate on wise sayings, appreciate the underlying principles, and learn to discern what’s right even in complicated circumstances. So, wisdom literature can help us on the journey to wisdom, but ultimately we will have to take the intellectual burden upon ourselves if we hope to get anywhere. (This is roughly how <a href=https://www.desiringgod.org/articles/the-best-discoveries-begin-as-problems>John Piper interprets Proverbs</a>.)
</p>

<p>
Viewing Hamming’s book as essentially modern wisdom literature makes sense of his focus on the “style” of thinking, his insistence that the reader must rediscover much of what he’s saying for themselves, and his admonitions not to accept what he’s saying blindly but to think it over at length:
</p>

<blockquote>
You the reader should take your own opinions and try first to express them clearly, and then examine them with counterarguments, back and forth, until <i>you are fairly clear as to what you believe and why you believe it</i>. It is none of the author’s business in this matter what you believe, but it is the author’s business to get you to think and articulate your position clearly. <i>(emphasis original)</i>
</blockquote>

<p>
If we view this book—Hamming’s guide to future scientists and engineers, his <i>magnum opus</i> as a teacher and mentor—as wisdom literature, it implies that wisdom, not any specific technical skill, is rate-limiting for technical progress. This is very encouraging, because wisdom, unlike innate intelligence, is an acquired trait, and one which we can all cultivate in ourselves. We can’t all be Ramanujan or von Neumann, but (at least as Hamming tells it) we can all be Hamming.
</p>

<p>
Thus far, I’ve mostly given reasons why you should read this book. Why shouldn’t you read this book? One reason is that this book is aimed at scientists and engineers, and furthermore it seems primarily aimed at people with an interest in the “hard” sciences—much of the advice assumes some contact with simulation, math, or physics. So a reader without at least a glancing interest in these topics might struggle to find some of the content relevant. (But maybe the act of extending his advice to other domains would prompt deeper consideration of the fundamental principles at play, and thus serve to cultivate wisdom!)
</p>

<p>
Another reason you shouldn’t read this book is that it’s very much framed as a personal guide—it addresses the needs of an individual scientist, not ideas for how science writ large could be improved. So aspiring metascientists might be disappointed by Hamming’s perspective; he dedicates a lot of time to thinking about how one can navigate imperfect organizations, and much less time to thinking about what a perfect organization would look like.
</p>

<p>
The strongest reason for reading this book, though, is that the world Hamming hopes to write for is almost exactly our world today. Hamming anticipates “the inevitable and extensive computerization of your organization and society generally,” a world in which scientists are frequently overwhelmed by the “rate of innovation and change of the dominant paradigm,” and a world where there is “not time to allow us the leisure which comes from separating the two fields” of science and engineering. From my perspective, this almost perfectly captures the feeling of working in science or tech today.
</p>

<p>
And so Hamming’s message—the vision of scientists “trying to achieve excellence” through making “significant contributions to humanity” on important problems—seems more relevant today than ever. If you yourself work in a scientific field, and want to know how to have the greatest positive impact on your own character and on society, then Hamming’s wisdom is for you: but not without some struggle
</p>

<i>
A transcript of Hamming’s talk “You and Your Research” (a shorter exposition of some of the ideas discussed above) is available <a href=https://www.cs.virginia.edu/~robins/YouAndYourResearch.html>here</a>, and </i>The Art of Doing Science and Engineering<i> can be purchased from Stripe Press <a href=https://press.stripe.com/the-art-of-doing-science-and-engineering>here</a>.
</i>
<br>
<br>
<i>Thanks to Michael Tartre for giving me this book originally, and to Jacob Thackston and Ari Wagen for extensive edits.</i>
]]></description>
              <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>What Happened to IRMS?</title>
              <link>public/blog/20230512_irms.html</link>
              <description><![CDATA[
<p>
A few days ago, <a href=https://corinwagen.github.io/public/blog/20230508_aldehyde_eie.html>I wrote</a> about kinetic isotope effects (KIEs), probably my favorite way to study the mechanism of organic reactions. To summarize at a high level: if the bonding around a given atom changes over the course of a reaction, then different isotopes of that atom will react at different rates. The exact magnitude of the effect depends on the vibrational modes involved, but is often quite different for different mechanisms, meaning that you can computationally predict isotope effects for a lot of mechanisms and then use KIE measurements to figure out which one is actually happening.
</p>

<p>
The trouble is that the magnitude of the effect depends on the difference in the mass of the two isotopologues. <sup>1</sup>H/<sup>2</sup>H isotope effects are quite large: H reacts up to 7x faster than D (more for mechanisms that involve quantum tunneling), meaning that it’s not too hard to measure the value accurately. But as the atom gets heavier, the effects get smaller. For the next most common pair of isotopomers,<sup>12</sup>C/<sup>13</sup>C, the effect is usually 5% or less.
</p>

<p>
Small KIEs are usually measured by one-pot competition experiments: a mixture of the two isomers is reacted to partial conversion, and then the isotopic composition of either the starting material or the product is determined. The product will be enriched in the isotope that reacts more quickly, and the starting material will be enriched in the isotope that reacts more slowly. If you know the starting ratio of isotopes, the conversion, and the ratio of isotopes at partial conversion, then you can use the Bigeleisen−Mayer equation to figure out the KIE.
(<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.1c07351>This</a> is a really good review on isotope effects in general, if you want more than this cursory summary.)
</p>

<p>
The accuracy of the KIE measurement is thus limited by (1) how accurately you can determine conversion and (2) how accurately you can measure the isotopic composition of a sample. Although conversion can be annoying, the second is the more serious limitation—<i>a priori</i> it’s not obvious how to figure out what the relative abundance of various isotopes is.
</p>

<p>
Today, most people use approaches based on NMR spectroscopy: since <sup>1</sup>H and <sup>13</sup>C are both NMR-active nuclei, you can just integrate the peak of interest against another peak to figure out how much there is. (Quantitative <sup>13</sup>C NMR is super slow, so <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>various</a> <a href=https://www.nature.com/articles/s41557-018-0079-7>tricks</a> can be employed to speed things up.)
</p>

<p>
But there was an age before the advent of accurate NMR spectroscopy where people measured isotope effects differently. I was awestruck by <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00834a064>this 1975 paper</a> from Cromartie and Swain reporting the measurement of a <sup>35</sup>Cl/<sup>37</sup>Cl isotope effect in the cyclization of 4-chlorobutanol: they report an isotope effect of 1.000757 ± 0.00015 using hydroxide as base, which they differentiate from an isotope effect of 1.000796 ± 0.00013 using water as base by Student’s <i>t</i> test. These numbers are way, way smaller and more precise than any isotope effect I’ve seen measured in the last few decades.
</p>

<p>
Digging a little deeper reveals a <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00418a038>whole</a> <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00426a047>wealth</a> of papers using <sup>35</sup>Cl/<sup>37</sup>Cl isotope effects to study various mechanistic phenomena. The instrument Swain and others use (described <a href=https://pubs.acs.org/doi/pdf/10.1021/jo00972a016>here</a>) is an isotope-ratio mass spectrometer, which as the name implies is a special sort of mass spectrometer designed specifically to measure isotopic composition. These instruments, although a little obscure from my point of view, are <a href=https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry/isotope-ratio-mass-spectrometry-irms.html>commercial</a>!
</p>

<p>
So, why isn’t IRMS used more frequently in organic chemistry today? I think it’s for a few reasons. IRMS, at least historically, only works on gases, meaning that you have to either use gaseous reactants or convert your analytes to gases, both of which are pretty annoying. In the Swain work, they (i) incinerate their samples with nitric acid, (ii) precipitate out silver chloride by adding silver nitrate, and then (iii) convert silver chloride to gaseous methyl chloride by <a href=https://pubs.acs.org/doi/abs/10.1021/ja00873a025>heating with methyl iodide</a> in a sealed tube. This is certainly a lot of hassle to put up with for a single measurement—and you generally want to get a good number of replicates.
</p>

<p>
(There are some <a href=https://www.thermofisher.com/order/catalog/product/11206175>all-in-one solutions</a> available for sale, which automatically combust samples à la elemental analysis, but they don’t seem to work on non-standard isotopes like chlorine.)
</p>

<p>
Another reason why IRMS might have fallen out of favor is that it requires a dedicated instrument, whereas NMR-based methods can be done using the NMR spectrometers that any university already has. Most labs only have budgets for a handful of instruments—is an IRMS really worth the investment? (Owing to the typical aura of secrecy around instrument prices, I’m not sure how much one costs, but I’m guessing it’s a few hundred thousand dollars or so.)
</p>

<p>
These downsides notwithstanding, I think there is a lot of good science that could be done if a mechanistic group decided to make IRMS a core part of their program. In particular, <sup>35</sup>Cl/<sup>37</sup>Cl KIEs seem really powerful: there are a growing number of organometallic reactions which involve chlorine atoms in the key step(s), and for which Cl KIEs might be complementary or superior to more conventional KIEs. I’m envisioning studying <a href=https://www.science.org/doi/10.1126/science.aad6981>transmetallation from Pd(II) chlorides</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.1c13333>chlorine radical-mediated C–H activation</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.2c01356>photolysis of Ni(II) chlorides</a>.
</p>

<p>
(And why stop at Cl? According to ThermoFisher, thermal ionization mass spectrometry lets you analyze the isotopic composition of metals with really high accuracy [five decimal places, per their <a href=https://assets.thermofisher.com/TFS-Assets/CMD/brochures/br-30537-triton-xt-tims-br30537-en.pdf>brochure</a>]. Would a <sup>58</sup>Ni/<sup>60</sup>Ni isotope effect be possible to measure? This might provide a handle on some mechanistically ambiguous Ni(III) scenarios, like those reported <a href=https://pubs.acs.org/doi/10.1021/jacs.5b13211>here</a>: is radical trapping or reductive elimination rate- and enantioselectivity-determining?)
</p>

<p>
It doesn’t seem like it’s that easy to start a purely mechanistic research group these days, so maybe this is an unfundable idea. But it seems sad that a technique as powerful for physical (in)organic chemists as IRMS could just fade into obscurity, and I hope somebody finds the time and resources to apply it to modern mechanistic problems.
</p>

]]></description>
              <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Isotope Effects in Aldehyde Protonation</title>
              <link>public/blog/20230508_aldehyde_eie.html</link>
              <description><![CDATA[
<p>
I’m writing my dissertation right now, and as a result I’m going back through a lot of old slides and references to fill in details that I left out for publication.
</p>

<p>
One interesting question that I’m revisiting is the following: when protonating benzaldehyde, what is the H/D equilibrium isotope effect at the aldehyde proton? This question was relevant for the H/D KIE experiments we conducted in <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c06688>our study of the asymmetric Prins cyclization</a>. (The paper hasn’t gotten much attention, but it’s probably the most “classic” organic chemistry paper I’ve worked on, with a minimum of weird computational details or bizarre analytical techniques.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230508_aldehyde_nores.png" style="width:375px;" />
</figure>

<p>
Since the H/D bond isn’t involved in the reaction, we won’t see a primary effect; so we know we have to be thinking in terms of secondary effects. The most common reason to observe a secondary isotope effect is changes in hybridization: sp<sup>3</sup> to sp<sup>2</sup> gives a normal effect, whereas sp<sup>2</sup> to sp<sup>3</sup> gives an inverse effect. From this perspective, it looks like the effect should be unity, since the carbon in question is sp<sup>2</sup> in both structures.
</p>

<p>
Reality, however, disagrees. <a href=https://www.sciencedirect.com/science/article/abs/pii/0040403976800573>Hall and Milosevich</a> report a EIE of 0.94 for benzaldehyde in aq. sulfuric acid, and <a href=https://pubs.acs.org/doi/pdf/10.1021/ja982504r>Gajewski and co-authors</a> compute an EIE of 0.83 for acetaldehyde at the MP2/6-31G(d,p) level of theory. I performed my own calculations at the M06-2X/jun-cc-pVTZ level of theory and obtained an EIE of 0.851 with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>, qualitatively consistent with the above results.
</p>

<p>
Where does this EIE come from? It’s helpful to think of benzaldehyde as possessing multiple resonance forms:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230508_aldehyde_res.png" style="width:400px;" />
</figure>

<p>
We typically think of the neutral resonance form on the top left, but you can also imagine putting a positive charge on carbon and a negative charge on oxygen to create a zwitterion with a C–O single bond (bottom left). In neutral benzaldehyde, this resonance form is substantially disfavored, but in protonated benzaldehyde it doesn’t look any worse than the “normal” top resonance form!
</p>

<p>
If this is true, we’d expect the C–O bond order to decrease from 2 in neutral benzaldehyde to ~1.5 in protonated benzaldehyde. Indeed, in my calculations the bond length increases from 1.20 Å to 1.28 Å upon protonation—so it seems the double bond character is decreasing! It’s not quite the same as going from sp<sup>2</sup> to sp<sup>3</sup>, but the inverse KIE begins to make sense.
</p>

<p>
(This is purely guesswork, but my guess would be that the differences between the two structures are attenuated in a polar solvent like water. The zwitterionic resonance form of the neutral structure will be stabilized and thus the neutral aldehyde will be more polar, making the change to the oxocarbenium less drastic. This might explain why the measured EIE in water is smaller—although this might also be due to counterion effects, or something completely unrelated.)
</p>

<p>
Let’s go a level deeper. According to <a href=https://pubs.acs.org/doi/abs/10.1021/ja01542a075>Streitwieser</a>, secondary KIEs associated with hyperconjugation originate from the creation or destruction of the c. 800 cm<sup>-1</sup> out-of-plane bending vibrations of Csp<sup>2</sup>–H hydrogens, which are markedly lower in frequency than the c. 1350 cm<sup>-1</sup> bending vibrations associated with Csp<sup>3</sup>–H hydrogens.
</p>

<p>
Raising the frequency of a mode increases the energy required to inhabit the ground vibrational state (the “zero-point energy”)—but deuterium is heavier and vibrates more slowly, meaning that it possesses less ZPE and is less affected by these changes. So when an 800 cm<sup>-1</sup> sp<sup>2</sup> mode transforms to a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode, the ZPE increases, but less for D than for H, so D is favored. Conversely, when a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode transforms to a 800 cm<sup>-1</sup> sp<sup>2</sup> mode, the ZPE decreases, but less for D than for H, so H is favored. (For a more complete explanation, see <a href=https://macmillan.princeton.edu/wp-content/uploads/RRK-KIE.pdf>this presentation by Rob Knowles</a>.)
</p>

<p>
This effect is complicated for benzaldehyde by the fact that the out-of-plane bend of the aldehyde couples to the out-of-plane bend of the phenyl ring, so there are several modes involving out-of-plane vibration of the aldehyde proton. When I compared the out-of-plane bend of the aldehyde H in both structures, I saw only minimal differences: 771, 963, 1040, and 1051 cm<sup>-1</sup> for the neutral species, as compared to 790, 1003, and 1061 cm<sup>-1</sup> for the protonated species. These small differences can’t be responsible for the observed effect.
</p>

<p>
In contrast, the in-plane C–H bend shows a big change—1430 cm<sup>-1</sup> for benzaldehyde, but 1644 cm<sup>-1</sup> for the oxocarbenium (it seems to couple to the C–O stretch; the reduced mass increases from 1.26 amu to 3.52 amu). Applying Streitweiser’s formula for estimating the isotope effect for a specific mode gives a pretty good match:
</p>

<p>
kH/kD ≈ exp(0.187/T * ∆ν) = exp(0.187/298 * (-214)) = 0.87
</p>

<p>
I don’t understand this area well enough to comment on why there’s a change in the in-plane vibrational frequency and not the out-of-plane vibrational frequency, nor do I understand how to deconvolute the effects of mode-to-mode coupling. Nevertheless, this provides a tentative physical rationale for the observation.
</p>

<p>
On a more abstract level, this case study illustrates why isotope effects are such a good tool. Any transformation that perturbs the vibrational frequencies of a given molecule can, in principle, be monitored by isotope effects without affecting the electronic energy surface at all. So, although the precise nature and magnitude of the effect might be hard to predict <i>a priori</i>, it’s not surprising that a transformation as dramatic as protonating a functional group produces a sizable isotope effect.
</p>
]]></description>
              <pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Tech As Control Group For Science</title>
              <link>public/blog/20230502_startups.html</link>
              <description><![CDATA[
<p>
I frequently wonder what the error bars on my life choices are. What are the chances I ended up a chemist? A scientist of any type? Having two children in graduate school? 
</p>

<p>
If I had the ability, I would want to restart the World Simulator from the time I started high school, run a bunch of replicates, and see what happened to me in different simulations. And this wouldn’t just be useful for me personally—there are lots of things in the world that are just as contingent and path-dependent as one’s life choices. What would have happened if Charles the Bold hadn’t died in 1477 and Burgundy had preserved its independence? If the 1787 convention were rerun several times, how might the US Constitution differ? 
</p>

<p>
Sadly, we’ll never know the answer to these questions. But what we can do instead is find cases in which analogous institutions evolved in parallel, and try to learn from the similarities and differences between them. It’s an imperfect substitute for rerunning the World Simulator, but it’s still pretty cool. (This is far from an original idea: see for instance <a href=https://www.amazon.com/Legal-Systems-Very-Different-Ours/dp/1793386722/><i>Legal Systems Very Different From Ours</i></a>.)
</p>

<p>
Lately, I’ve come to think about the tech/startup world as somewhat parallel to academic science in this manner. Why? For one, both tech and academia deal with hard problems that demand obscure/arcane domain-specific knowledge inaccessible to non-experts. (It’s true that the problems are typically scientific in academia and engineering-related in tech, but I’ve argued <a href=https://corinwagen.github.io/public/blog/20230215_science_engineering.html>previously</a> that this distinction is flimsier than it seems.) And in both fields, a few high performers <a href=https://nadia.xyz/top-talent>vastly outperform</a> the rest of the field, be it a “10x engineer” or a Nobel laureate.
</p>

<p>
Startups, like academic labs, are small and agile institutions which face the task of raising money, building a team, selecting a hard yet solvable problem, and finding a solution all within a few years. In both cases, too, there are nonlinear returns to success: moderate success is not much better than failure, pushing founders/assistant professors to be as ambitious as possible.
</p>

<p>
If we accept these two fields as vaguely analogous, what interesting differences can we observe? 
</p>

<h3>
Startups Have Multiple Founders
</h3>

<p>
I’ll quote from <a href=http://www.paulgraham.com/startupmistakes.html>an essay by Paul Graham</a>, founder of Y Combinator and noted startup sage:
</p>

<blockquote>
Have you ever noticed how few successful startups were founded by just one person? Even companies you think of as having one founder, like Oracle, usually turn out to have more. It seems unlikely this is a coincidence.
<br><br>
What's wrong with having one founder? To start with, it's a vote of no confidence. It probably means the founder couldn't talk any of his friends into starting the company with him. That's pretty alarming, because his friends are the ones who know him best.
<br><br>
But even if the founder's friends were all wrong and the company is a good bet, he's still at a disadvantage. Starting a startup is too hard for one person. Even if you could do all the work yourself, you need colleagues to brainstorm with, to talk you out of stupid decisions, and to cheer you up when things go wrong.
</blockquote>

<p>
Ever since I read this, I’ve wondered why no labs ever have multiple PIs. I guess this would mess with the semi-feudal organization of university bureaucracy, but it doesn’t seem intrinsically bad—after all, lots of startups seem to do just fine.
</p>

<h3>
Startup Winners Can’t Be Picked <i>Ex Ante</i>
</h3>

<p>
The VC strategy, as I understand it, is basically “fund a bunch of companies, and one or two of them will make it all worth our while.” This is a little bit different than how universities approach hiring assistant professors: each university will typically hire a small number of professors each year, after much deliberation, and they have a pretty high likelihood of giving them tenure, at least relative to the likelihood of any given startup succeeding. (Basically, <a href=https://www2.nau.edu/lrm22/lessons/r_and_k_selection/r_and_k.html>startups are r-selected, whereas academic labs are K-selected.</a>)
</p>

<p>
There are a lot of reasons why this might be. For one, faculty members aren’t just trying to pick a winner but also their future colleague, so personal considerations probably matter more. Failure in science seems more cruel, too: while a failed startup founder can often negotiate the “sale” of their company and parlay that into new jobs and the constant churn of tech means that there are always new openings for talented ex-startup employees, a lab that doesn’t get tenure takes a toll on professor and students alike. 
</p>

<p>
A hypothesis for why the success rate for new labs is so much higher than the success rate for new businesses is that many labs only succeed a little bit. They don’t actually achieve what they dreamed about in their initial proposals, but they pivot and accrue enough publications and cachet to earn tenure nevertheless. In business, it seems harder to succeed a little bit—the market is a harsher critic than one’s peers.
</p>

<h3>
Founders Should Be Focused
</h3>

<p>
Paul Graham again, this time talking about <a href=http://www.paulgraham.com/ramenprofitable.html>the dangers of fundraising</a>:
</p>

<blockquote>
Raising money is terribly distracting. You're lucky if your productivity is a third of what it was before. And it can last for months.
<br><br>
I didn't understand (or rather, remember) precisely why raising money was so distracting till earlier this year. I'd noticed that startups we funded would usually grind to a halt when they switched to raising money, but I didn't remember exactly why till YC raised money itself. We had a comparatively easy time of it; the first people I asked said yes; but it took months to work out the details, and during that time I got hardly any real work done. Why? Because I thought about it all the time.
</blockquote>

<p>
The broader conclusion, from this and other essays, is that any distractions from the core mission of the startup are very dangerous, and should be avoided at all costs. This is very different from the lifestyle of new PIs, who are typically juggling departmental responsibilities, writing a curriculum, lecturing for the first time, and writing grants all while trying to get their lab up and running.
</p>

<h3>
Talent Acquisition Is Crucial
</h3>

<p>
In tech, people obsess about recruiting the best people possible—I reviewed <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>a whole book about this</a> last year. Hiring bad programmers is #6 on PG’s <a href=http://www.paulgraham.com/startupmistakes.html>list of mistakes that kill startups</a>, and there seems to be a general consensus that a great company takes great engineers, no matter what. 
</p>

<p>
In contrast, professors don’t have full control over whom they hire (for graduate students), making recruiting much harder. Graduate students are selected through a complex two-stage system involving admission to a school and then a subsequent group-joining process (and new assistant professors sometimes aren’t even around for the first of these stages). You can obviously try to coax talented students to work for you, but the pool of accepted students interested in your subfield might be tiny, and they might all prefer to work for an established group… 
</p>

<p>
(Plus, there’s not a good way to reward top performers in academia. All graduate students are equal, at least on paper—you can’t give someone a year-end bonus, or a promotion.)
</p>

<p>
A nice concrete example of this is how professors <a href=https://www.jefftk.com/p/hiring-programmers-in-academia>struggle to hire competent programmers</a>, even as research scientists—they aren’t allowed to pay enough to match market rates, even when the expense would be well worth the money. To quote <a href=https://acoup.blog/2023/04/28/collections-academic-ranks-explained-or-what-on-earth-is-an-adjunct/?s=03#easy-footnote-bottom-8-18391>Bret Devereaux</a>: “academic hiring, to be frank, is not conducted seriously” (he’s discussing the humanities, but the point stands). 
</p>

<h3>
Successful Startups Grow
</h3>

<p>
As a startup succeeds, it grows: while a seed-stage startup typically has &lt;15 people, startups at Series A often have 20–40, and startups at Series B–C might have as many as 300 employees (<a href=https://www.quora.com/How-many-employees-do-early-stage-startups-have>one ref</a>; rough numbers broadly consistent with other sources). Good companies grow, while bad ones die. 
</p>

<p>
In contrast, it’s rare for even the most successful US academic labs to grow past 30 people (although <a href=https://go.gale.com/ps/i.do?id=GALE%7CA195680544&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=00280836&p=HRCA&sw=w&userGroupName=mlin_oweb&isGeoAuthType=true>it occasionally happens</a>), limiting the reach of top-performing professors. While a huge proportion of tech employees work for the best companies (Google, Meta, Amazon, etc), only a very small number of students work for the best professors. 
</p>

<h3>
Concluding Thoughts
</h3>

<p>
The imperfect nature of the analogy means that some of these points might not be useful in a normative sense: universities are not really optimized to produce research as efficiently as possible, and maybe that’s fine. Likewise, startups aren’t optimized to produce unprofitable research or train future scientists, even if these activities may in the long run be beneficial. (This is <a href=https://corinwagen.github.io/public/blog/20220728_consulting_as_grad_school.html#:~:text=Accordingly%2C%20the%20government%20sponsors%20research%20into%20interesting%20problems%20with%20uncertain%20timeframes%20to%20do%20what%20the%20free%20market%20cannot>why basic science is considered a public good</a>, and why the government funds it at all!)
</p>

<p>
Nevertheless, I think there’s a lot that scientists can learn from startups. There is a whole army of people working to solve challenging technical problems in the most efficient way, and it’d be prudent to study the wisdom that emerges. 
</p>

<i>
Thanks to Ari Wagen and Jacob Thackston for reading drafts of this piece. 
</i>

]]></description>
              <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Do We Still Need Journals?</title>
              <link>public/blog/20230427_journals.html</link>
              <description><![CDATA[
<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_compass.png" style="width:550px;" />
  <figcaption> 
    This image inspired by the rightly famous <a href=https://images.are.na/eyJidWNrZXQiOiJhcmVuYV9pbWFnZXMiLCJrZXkiOiIxMDI4NDUyNy9vcmlnaW5hbF80ZmNlOTY3ODYxZTFiYWEwZjkwODMxYzlkZTNhNjhhZS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjE4MDAsImhlaWdodCI6MTgwMCwiZml0IjoiaW5zaWRlIiwid2l0aG91dEVubGFyZ2VtZW50Ijp0cnVlfSwid2VicCI6eyJxdWFsaXR5Ijo5MH0sImpwZWciOnsicXVhbGl0eSI6OTB9LCJyb3RhdGUiOm51bGx9fQ==?bc=0>original</a>.
  </figcaption>
</figure>

<p>
One of the most distinctive parts of science, relative to other fields, is the practice of communicating findings through peer-reviewed journal publications. Why do scientists communicate in this way? As I see it, scientific journals provide three important services to the community: 
</p>

<ol>
  <li>
    Journals help scientists communicate; they disseminate scientific results to a broad audience, both within one’s community and to a broader scientific audience.
  </li>
  <li>
    Through the peer review process, journals ensure scientific correctness and keep standards high. You never know which of your scientific adversaries might be scrutinizing your work for flaws, so you’re incentivized to do the best job possible.
  </li>
  <li>
    Journals, and peer review, help scientists to read high-quality, impactful work by filtering out low-impact papers (even those which are scientifically correct). This makes journals a somewhat “fair” way to score the importance of publications without being a subject-matter expert; I might not know what’s happening these days with topological quantum materials, but if I see three <i>Science</i>/<i>Nature</i> papers on a CV, I’ll certainly pay attention! 
  </li>
</ol>

<p>
(There are certainly other services that journals provide, like DOIs and typesetting, but these seem much less important to me.)
</p>

<p>
In this post, I want to (1) discuss the problems with scientific journals today, (2) briefly summarize the history of journals and explain how they came to be the way they are today, and (3) imagine how journals might evolve in the coming decades to adapt to the changing landscape of science. My central claim is that <u>the scientific journal, as defined by the above criteria, has only existed since about the 1970s, and will probably not exist for very much longer—and that’s ok.</u> (I’ll also try and explain the esoteric meme at the top.)
</p>

<h2>
1. Journals Present
</h2>

<p>
Many people are upset about scientific journals today, and for many different reasons. 
</p>

<h3>
1.1 Profits and Paywalls
</h3>

<p>
The business model of scientific journals is, to put it lightly, unusual. <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Writing for <i>The Guardian</i></a>, Stephen Buranyi describes how “scientific publishers manage to duck most of the actual costs” that normal magazines incur by outsourcing their editorial duties to scientists: the very people who both write and read the articles:
</p>

<blockquote>
It is as if the New Yorker or the Economist demanded that journalists write and edit each other’s work for free, and asked the government to foot the bill. Outside observers tend to fall into a sort of stunned disbelief when describing this setup. A 2004 parliamentary science and technology committee report on the industry drily observed that “in a traditional market suppliers are paid for the goods they provide”. A 2005 Deutsche Bank report referred to it as a “bizarre” “triple-pay” system, in which “the state funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product”.
</blockquote>

<p>
And this cost-dodging is very successful: scientific journals are a huge moneymaker, with Elsevier (one of the largest publishers) having a margin <a href=https://tidsskriftet.no/en/2020/08/kronikk/money-behind-academic-publishing>in excess of 30%</a>, and ACS’s “information services” (publication) division close behind, <a href=https://www.acs.org/content/dam/acsorg/about/aboutacs/financial/2022-audited-financial-statements.pdf>with a profit margin of 27%</a>.
</p>

<p>
The exorbitant fees charged by journals, and the concomitantly huge profits they earn, have led to increasing pushback against the paywall-based status quo. The Biden administration <a href=https://www.nature.com/articles/d41586-022-02351-1>has called</a> for all government-funded research to be free-to-read without any embargo by 2025, and other countries have been pursuing similar policies <a href=https://www.chemistryworld.com/news/eu-and-uk-bitten-by-the-open-access-bug/5236.article>for some time</a>. Similarly, <a href=https://news.mit.edu/2020/guided-by-open-access-principles-mit-ends-elsevier-negotiations-0611>MIT</a> and <a href=https://www.universityofcalifornia.edu/news/why-uc-split-publishing-giant-elsevier>the UC system</a> recently terminated their subscriptions to Elsevier over open-access issues. (And the rise of SciHub means that, even without a subscription, most scientists can still read almost any article they want—threatening to completely destroy the closed-access model.) 
</p>

<p>
In response to this pressure, journals have begun offering open-access alternatives, where the journal’s fees are paid by the submitting author rather than the reader. While in theory this is a solution to this problem, in practice the fees for authors are so high that it’s not a very good solution. The board of editors of NeuroImage <a href=https://www.nature.com/articles/d41586-023-01391-5>recently resigned</a> over their journal’s high open-access fees—and they’re <a href=https://www.nature.com/articles/d41586-019-00135-8>not the first</a> board of editors to do this. As <a href=https://www.vox.com/the-highlight/2019/6/3/18271538/open-access-elsevier-california-sci-hub-academic-paywalls>a 2019 <i>Vox</i> summary</a> put it: “Publishers are still going to get paid. Open access just means the paychecks come at the front end.” 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_oa_tweet.png" style="width:475px;" />
</figure>

<h3>
1.2 Efficacy of Peer Review
</h3>

<p>
In parallel, the <a href=https://en.wikipedia.org/wiki/Replication_crisis>“replication crisis”</a> has led to growing skepticism about the value of peer review. In his article <a href=https://www.experimental-history.com/p/the-rise-and-fall-of-peer-review>“The Rise and Fall of Peer Review,”</a> Adam Mastroianni describes how experiments to measure its value have yielded dismal outcomes:
</p>

<blockquote>
Scientists have run studies where they deliberately add errors to papers, send them out to reviewers, and simply count how many errors the reviewers catch. Reviewers are pretty awful at this. In <a href=https://www.sciencedirect.com/science/article/pii/S019606449870006X?casa_token=D5MklJnYP0MAAAAA:CzyYl8VLg-M-bvKIHxl7vWCIh8AVrkTUizQ9LZPZWh_eVT5jLf3WJlGaJQzYCsHMraF5Fh8mqw>this study</a> reviewers caught 30% of the major flaws, in <a href=https://jamanetwork.com/journals/jama/article-abstract/187748>this study</a> they caught 25%, and in <a href=https://journals.sagepub.com/doi/full/10.1258/jrsm.2008.080062>this study</a> they caught 29%. These were critical issues, like “the paper claims to be a randomized controlled trial but it isn’t” and “when you look at the graphs, it’s pretty clear there’s no effect” and “the authors draw conclusions that are totally unsupported by the data.” Reviewers mostly didn’t notice.
</blockquote>

<p>
While the worst of the replication crisis seems to be contained to the social sciences, my own field—chemistry—is by no means exempt. As I wrote <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>previously</a>, “<a href=https://pubs.acs.org/doi/10.1021/acscentsci.2c00325>elemental analysis doesn’t work</a>, <a href=https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e>integration grids cause problems</a>, and even <a href=http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html>reactions from famous labs can’t be replicated</a>.” There are a lot of bad results in the scientific literature, even in top journals—I don’t think many people in the field actually believe that a generic peer-reviewed publication is guaranteed to be correct.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_pr_tweet.png" style="width:475px;" />
</figure>

<p>
And the process of soliciting peer reviews is by no means trivial: prominent professors are commonly asked to peer review tons of articles as an unpaid service to the community, which isn’t really rewarded in any way. As the number of journals and publications grows faster than the number of qualified peer reviewers, burnout can result:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_bo_tweet.png" style="width:475px;" />
</figure>

<h3>
1.3 Preprint Servers
</h3>

<p>
The rise of preprint servers like arXiv, BioRxiv, and ChemRxiv also means journals aren’t necessary for communication of scientific results. More and more, preprints dominate discussions of cutting-edge science, while actual peer-reviewed publications lag months to years behind. 
</p>

<p>
While in theory preprints aren’t supposed to be viewed as scientifically authoritative—since they haven’t been reviewed—in practice most preprints are qualitatively identical to the peer-reviewed papers that they give rise to. <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001285>A retrospective analysis</a> of early COVID preprints found that the vast majority of preprints survived peer review without any substantive changes to their conclusions (although this might be biased by the fact that the worst pre-prints will never be accepted at all.)
</p>

<p>
If this is the case, why bother with journals at all? To a growing degree this seems to be the norm in CS and CS-adjacent fields: the landmark Google transformer paper from 2017, <a href=https://arxiv.org/pdf/1706.03762.pdf>“Attention Is All You Need,”</a> is still just a PDF on arXiv six years later, despite being potentially the most impactful discovery of the 2010s. Similarly, UMAP, <a href=https://corinwagen.github.io/public/blog/20230417_dimensionality_reduction.html>which I discussed last week,</a> is also just <a href=https://arxiv.org/abs/1802.03426>hanging out on arXiv</a>, no peer-reviewed publication in sight. Still, in chemistry and other sciences we’re expected to publish in “real journals” if we want to graduate or get jobs. 
</p>

<h3>
1.4 Impact-Neutral Reviewing
</h3>

<p>
An implicit assumption of the scientific journal is that high-impact publications can be distinguished from low-impact publications without the benefit of hindsight. Yet many of the most impactful scientific discoveries—like the Krebs cycle, the weak interaction, lasers, continental drift, and CRISPR—<a href=https://nintil.com/discoveries-ignored>were rejected</a> when first submitted to journals. How is this possible? 
</p>

<p>
I’d argue that peer review creates a bias towards incrementalism. It’s easy to see how an improvement over something already known is significant; it’s perhaps harder to appreciate the impact of a field-defining discovery, or to believe that such a result could even be possible. To quote Antonio Garcia Martinez on startups: “If your idea is any good, it won’t get stolen, you’ll have to jam it down people’s throats instead.” True zero-to-one thinking can provoke a strong reaction from the establishment, and rarely a positive one. 
</p>

<p>
(It’s worth noting that some of the highest profile organic chemistry papers from 2022 were new takes on old, established, reactions: <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c05648>Parasram</a> and <a href=https://www.nature.com/articles/s41586-022-05211-0>Leonori’s</a> “ozonolysis without ozone” and <a href=https://www.science.org/doi/10.1126/science.abo6443>Nagib’s</a> “carbenes without diazoalkanes.” I love both papers—but I also think it’s easier for audiences to appreciate why “ozonolysis without ozone” is a big deal than to process an entirely new idea.)
</p>

<p>
Even for more quotidian scientific results, the value of impact-based peer review is limited. Matt Clancy at <a href=https://www.newthingsunderthesun.com/pub/nc5341ua/release/3><i>New Things Under the Sun</i></a> writes that, for preprints, paper acceptance is indeed correlated with number of eventual citations, but that the correlation is weak: reviewers seem to be doing better than random chance, but worse than we might hope. (Similar results emerge when studying the efficacy of peer review for grants.) On the aggregate, it does seem true that the average <i>JACS</i> paper is better than the average <i>JOC</i> paper, but the trend is far from monotonic.
</p>

<p>
These concerns aren’t just mine; indeed, a growing number of scientists seek to reject impact-based refereeing altogether. The <a href=http://proteinsandwavefunctions.blogspot.com/2016/01/writing-impact-neutral-review.html?m=1&s=03>“impact-neutral reviewing”</a> movement thinks that papers should be evaluated only on the basis of their scientific correctness, not their perceived potential impact. Although I wouldn’t say this is a mainstream idea, journals like <a href=https://journals.plos.org/plosone/s/journal-information><i>PLOS One</i></a>, <a href=https://blog.frontiersin.org/2015/12/21/4782/><i>Frontiers</i></a>, and <a href=https://elifesciences.org/inside-elife/54d63486/elife-s-new-model-changing-the-way-you-share-your-research><i>eLife</i></a> have adopted versions of it, and perhaps more journals will follow in the years to come.
</p>

<p>
Taken together, these anecdotes demonstrate that all three pillars of the modern scientific journal—communication, peer review, and impact-based sorting—are threatened today. 
</p>

<h2>
2. Journals Past
</h2>

<p>
How did we get here? 
</p>

<p>
The importance of journals as a filter for low-quality work is a modern phenomenon. Of course, editors have always had discretion over what to publish, but until fairly recently <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>the total volume of papers was much lower</a>, meaning that it wasn’t so vital to separate the wheat from the chaff. In fact, <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Stephen Buranyi</a> attributes the modern obsession with impact factor and prestige to the founding of <i>Cell</i> in 1974:
</p>

<blockquote>
[<i>Cell</i>] was edited by a young biologist named Ben Lewin, who approached his work with an intense, almost literary bent. Lewin prized long, rigorous papers that answered big questions – often representing years of research that would have yielded multiple papers in other venues – and, breaking with the idea that journals were passive instruments to communicate science, he rejected far more papers than he published….
<br><br>
Suddenly, where you published became immensely important. Other editors took a similarly activist approach in the hopes of replicating <i>Cell</i>’s success. Publishers also adopted a metric called “impact factor,” invented in the 1960s by Eugene Garfield, a librarian and linguist, as a rough calculation of how often papers in a given journal are cited in other papers. For publishers, it became a way to rank and advertise the scientific reach of their products. The new-look journals, with their emphasis on big results, shot to the top of these new rankings, and scientists who published in “high-impact” journals were rewarded with jobs and funding. Almost overnight, a new currency of prestige had been created in the scientific world.
</blockquote>

<p>
As Buranyi reports, the changes induced by <i>Cell</i> rippled across the journal ecosystem. The acceptance rate at <i>Nature</i> <a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>dropped from 35% to 13%</a> over the following decade-and-a-half (coincidentally also the years when peer review was introduced), making journal editors the “kingmakers of science” (Buranyi). 
</p>

<p>
Peer review is also a modern addition. In <a href=https://physicstoday.scitation.org/doi/10.1063/PT.3.3463><i>Physics Today</i></a>, Melissa Baldwin recounts how peer review only became ubiquitous following a series of contentious House subcommittee hearings in 1974 that questioned the value of NSF-funded science:
</p>

<blockquote>
Spending on both basic and applied research had increased dramatically in the 1950s and 1960s—but when doubts began to creep in about the public value of the work that money had funded, scientists were faced with the prospect of losing both public trust and access to research funding. Legislators wanted publicly funded science to be accountable; scientists wanted decisions about science to be left in expert hands. Trusting peer review to ensure that only the best and most essential science received funding seemed a way to split the difference.
</blockquote>

<p>
Our expectation that journals ought to validate the correctness of the work they publish, too, is quite modern. Baldwin again:
</p>

<blockquote>
It also seems significant that refereeing procedures were not initially developed to detect fraud or to ensure the accuracy of scientific claims…. Authors, not referees, were responsible for the contents of their papers. It was not until the 20th century that anyone thought a referee should be responsible for the quality of the scientific literature, and not until the Cold War that something had to be peer-reviewed to be seen as scientifically legitimate.
</blockquote>

<p>
If journals didn’t do peer review and they didn’t do (as much) impact-based filtering before the 1970s, what <i>did</i> they do? The answer is simple: communication. Scientists started communicating in journals because writing books was too slow, and it was important that they be able to share results and get feedback on their ideas quickly. This was a founding aim of <i>Nature</i>:
</p>

<blockquote>
…to aid Scientific men themselves, by giving early information of all advances made in any branch of Natural knowledge throughout the world, and by affording them an opportunity of discussing the various Scientific questions which arise from time to time.
</blockquote>

<p>
Although perhaps underwhelming to a modern audience, this makes sense. Scientists back in the day didn’t have preprints, Twitter, or Zoom—so they published journal articles because it was “one of the fastest ways to bring a scientific issue or idea to their fellow researchers’ attention” (<a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>ref</a>), not because it would look good on their CV. Journals became “the place to discuss big science questions” among researchers, and frequently featured acrimonious and public disputes between researchers—far from celebrated storehouses of truth, journals were simply the social media of pre-communication age scientists. 
</p>

<h2>
3. Journals’ Future?
</h2>

<p>
So, is the solution “reject modernity, embrace tradition”? Should we go back to the way things used to be and stop worrying about whether published articles are correct or impactful?
</p>

<p>
Anyone who’s close to the scientific publishing process knows that this would be ridiculous and suicidal. We’ve come a long way from the intimate scientific community of 18th-century England, where scientists had reputations to uphold and weren’t incentivized to crank out a bunch of <i>Tet. Lett.</i> papers. Like it or not, today’s scientists have been trained to think of their own productivity in terms of publications, and the editorial standards we have today are just barely keeping a sea of low-quality crap at bay (cf. <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>Goodhart’s Law</a>). Sometimes it feels like peer reviewers are the only people who are willing to give me honest criticism about my work—if we get rid of them, what then? 
</p>

<p>
We can understand the changes in journals by borrowing some thinking from economics: as the scale of communities increases, the norms and institutions of the community must progress from informal to formal. This process <a href=https://www.cambridge.org/core/journals/journal-of-economic-history/article/abs/development-of-property-rights-on-frontiers-endowments-norms-and-politics/23F4D5DB23AE6EC415ADF049F6CD0B54>has been documented nicely</a> for the development of property rights on frontiers: at first, land is abundant, and no property rights are necessary. Later on, inhabitants develop a <i>de facto</i> system of informal property rights to mediate disputes—and still later, these <i>de facto</i> property rights are transformed into <i>de jure</i> property rights, raising them to the status of law. Communities with 10,000 people need more formal institutions than communities with 100 people. 
</p>

<p>
If we revisit the history of scientific journals, we can see an analogous process taking place. Centuries ago there were relatively few scientists, and so journals could simply serve as a bulletin board for whatever these scientists were up to. As the scale and scope of science expanded in the late 20th century, peer review became a way to deal with the rising number of scientific publications, sorting the good from the bad and providing feedback. Today, as the scale of science continues to increase and the communication revolution renders many of the historical functions of journals moot, it seems that journals will have to change again, to adapt to the new needs of the community.
</p>

<p>
To the extent that this post has a key prediction, it’s this: <u>scientific journals are going to change a lot in the decade or two to come.</u> If you’re a scientist today—even a relatively venerable one—you’ve lived your whole career in the post-peer review era, and so I think people have gotten used to the status quo. Submitting papers to journals, getting referee reviews, etc are part of what we’re taught “being a scientist” means. But this hasn’t always been true, and it may not be true within your lifetime! 
</p>

<p>
Sadly, I don’t really have a specific high-confidence prediction for how journals will change, or how they should change. Instead, I want to sketch out nine little vignettes of what could happen to journals, good or bad. These options are neither mutually exclusive nor collectively exhaustive; it’s meant simply as an exercise in creativity, and to provide a little basis set with which to imagine the future. 
</p>

<p>
I’ll repost the initial image of the post here, for ambiance, and then walk through the possibilities. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_compass.png" style="width:550px;" />
</figure>

<h3>
3.1 The Journal as Bastion of Verified Truth
</h3>

<p>
One scenario is that journals, no longer being needed to distribute results widely, will double down on their role as defenders of scientific correctness. To a much greater degree, journals will focus on only publishing truly correct work, and thus make peer review their key “value add.” This is already being done post-replication crisis in some fields; Michael Nielsen and Kanjun Qiu describe the rise of “Registered Reports” in their <a href=https://scienceplusplus.org/metascience/>essay on metascience</a>:
</p>

<blockquote>
The idea [behind Registered Reports] is for scientists to design their study in advance: exactly what data is to be taken, exactly what analyses are to be run, what questions asked. That study design is then pre-registered publicly, and before data is taken the design is refereed at the journal. The referees can't know whether the results are "interesting", since no data has yet been taken. There are (as yet) no results! Rather, they're looking to see if the design is sound, and if the questions being asked are interesting – which is quite different to whether the answers are interesting! If the paper passes this round of peer review, only then are the experiments done, and the paper completed.
</blockquote>

<p>
This makes more sense for medicine or psychology than it does for more exploratory sciences—if you’re blundering around synthesizing novel low-valent Bi complexes, it’s tough to know what you’ll find or what experiments you’ll want to run! But there are other ways we could make science more rigorous, if we wanted to.
</p>

<p>
A start would be requiring original datafiles (e.g. for NMR spectra) instead of just providing a PDF with images, and having reviewers examine these data. ACS has made some moves in this direction (<a href=https://publish.acs.org/publish/data_policy>e.g.</a>), although to my knowledge no ACS journal yet requires original data. One could also imagine requiring all figures to be linked to the underlying data, with code supplied by the submitting group (like a Jupyter notebook). A more drastic step would be to require all results to be independently reproduced by another research group, <a href=http://www.orgsyn.org/about.aspx>like <i>Organic Syntheses</i> does</a>. 
</p>

<p>
These efforts would certainly make the scientific literature more accurate, but at what cost? Preparing publications already consumes an excessive amount of time and energy, and making peer review stricter might just exacerbate this problem. Marc Edwards and Siddhartha Roy discuss this in a nice <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5206685/>perspective</a> on perverse incentives in modern science:
</p>

<blockquote>
Assuming that the goal of the scientific enterprise is to maximize true scientific progress, a process that overemphasizes quality might require triple or quadruple blinded studies, mandatory replication of results by independent parties, and peer-review of all data and statistics before publication—such a system would minimize mistakes, but would produce very few results due to overcaution.
</blockquote>

<p>
It seems good that there are some “overcautious” journals, like <i>Org. Syn.</i>, but it also seems unlikely that all of science will adopt this model. In fact, a move in this direction might create a two-tiered system: some journals would adopt stringent policies, but there’s a huge incentive for some journals to defect and avoid these policies, since authors are lazy and would prefer not to do extra work. It seems unlikely that all of science could realistically be moved to a “bastion of truth” model in the near future, although perhaps we could push the needle in that direction.
</p>

<h3>
3.2 The Journal as Guild-Approved Periodical
</h3>

<p>
If peer review is so vital, why not make it a real career? Imagine a world in which journals like <i>Nature</i> and <i>Science</i> have their own in-house experts, recruited to serve as professional overseers and custodians of science. Instead of your manuscript getting sent to some random editor, and thence to whomever deigns to respond to that editor’s request for reviewers, your manuscript would be scrutinized by a team of hand-picked domain specialists. This would certainly cost money, but journals seem to have a bit of extra cash to spare.
</p>

<p>
I call this scenario the “guild-approved periodical” because the professionals who determined which papers got published would essentially be managers, or leaders, of science—they would have a good amount of power over other scientists, to a degree that seems uncommon today. Thus, this model would amount to a centralization of science: if <i>Nature</i> says you have to do genomics a certain way, you have to do it that way or <i>Nature</i> journals won’t publish your work! I’m not sure whether this would be good or bad. 
</p>

<p>
(It is a little funny that the editors of high-tier journals—arguably the most powerful people in their field—are chosen without the knowledge or consent of the field, through processes that are completely opaque to rank-and-file scientists. To the extent that this proposal allows scientists to choose their own governance, it might be good.)	
</p>

<h3>
3.3 The Journal as Living Knowledge Aggregator
</h3>

<p>
This scenario envisions a world in which “publications” are freed from the tyranny of needing to be complete at a certain point. While that was true in the days when you actually got a published physical issue in the mail, it’s not necessary in the Internet age! Instead, one can imagine a dynamic process of publishing, where a journal article is continually updated in response to new data. 
</p>

<p>
A 2020 article in <a href=https://www.facetsjournal.com/doi/10.1139/facets-2019-0012><i>FACETS</i></a> proposes exactly this model:
</p>

<blockquote>
The paper of the future may be a digital-only document that is not only discussed openly after the peer-review process but also regularly updated with time-stamped versions and complementary research by different authors… Living systematic reviews are another valuable way to keep research up to date in rapidly evolving fields. The papers of the future that take the form of living reviews can help our understanding of a topic keep pace with the research but will also add complexities. <i>(citations removed from passage)</i>
</blockquote>

<p>
The idea of the living systematic review is being tried out by the <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031><i>Living Journal of Computational Molecular Science</i></a>, which (among other things) has published a 60-page <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031>review of enhanced sampling methods in MD</a>, which will continue being updated as the field evolves.
</p>

<p>
These ideas are cool, but I wonder what would happen if more research became “living.” Disputes and acrimony are part of the collective process of scientific truth-seeking. What will happen if bitter rivals start working on the same “living” publications—who will adjudicate their disputes? 
</p>

<p>
Wikipedia manages to solve this problem through a plethora of editors, who can even lock down particularly controversial pages, and perhaps editors of living journals will assume analogous roles. But the ability of our collective scientific intelligence to simultaneously believe contradictory ideas seems like a virtue, not a vice, and I worry that living journals will squash this. 
</p>

<p>
An even thornier question is who adjudicates questions of impact. The enhanced sampling review linked above has over 400 references, making it a formidable tome for a non-expert like myself. There’s a lot of merit in a non-comprehensive and opinionated introduction to the field, which takes some subjective editorial liberties, but it’s not clear to me how that would work in a collaborative living journal. What’s to stop me from linking to my own papers everywhere? 
</p>

<p>
(I’m sure that there are clever organizational and administrative solutions to these problems; I just don’t know what they are.)
</p>

<h3>
3.4 The Journal as Curated Scientific Vision
</h3>

<p>
If “objective impact” is so hard to determine fairly, why not just accept that we’re basically just subjectively scoring publications based on how much we like them, and abandon the pretense of objectivity? One can imagine the rise of a new kind of figure: the editor with authorial license, who has a specific vision for what they think science should look like and publishes work in keeping with that vision. The role is as much aesthetic as it is analytic. 
</p>

<p>
There’s some historical precedent for this idea—Eric Gilliam’s written about how Warren Weaver, a grant director for the Rockefeller Foundation, essentially <a href=https://freaktakes.substack.com/p/a-report-on-scientific-branch-creation>created the field of molecular biology</a> <i>ex nihilo</i> by following an opinionated thesis about what work ought to be funded. Likewise, one can envision starting a journal as an act of community-building, essentially creating a Schelling point for like-minded scientists to collaborate, share results, and develop a common approach to science.
</p>

<p>
We can see hints of this today: newsletters like <a href=https://pubs.acs.org/doi/10.1021/acs.oprd.3c00060>“Some Items of Interest to Process Chemists”</a> or Elliot Hershberg’s <a href=https://centuryofbio.substack.com/about><i>Century of Bio</i> Substack</a> highlight a particular vision of science, although they haven’t quite advanced to the stage of formally publishing papers themselves. But perhaps it will happen soon; new movements, like molecular editing or digital chemistry, might benefit from forming more tightly-knit communities. 
</p>

<h3>
3.5 The Journal as Post Hoc Impact Archive
</h3>

<p>
If preprints take over every field of science as thoroughly as they have computer science, journals may find themselves almost completely divorced from the day-to-day practice of science, for better or for worse. Papers might still be submitted to journals, and the status of the journal might still mean something, but it wouldn’t be a guess anymore—journals could simply accept the advances already proven to be impactful and basically just publish a nicely formatted “version of record,” like a scientific Library of Congress. 
</p>

<p>
This is essentially equivalent to the “publish first, curate second” proposal of <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000116>Stern and O’Shea</a>—preprints eliminate the need for journals to move quickly, so we can just see what work the community finds to be best and collect that into journals. The value of journals for specialists, who already need to be reading a large fraction of the papers in their area, would be much lower—journals would mainly be summarizing a field’s achievements for those out-of-field. In this scenario, “many specialized journals that currently curate a large fraction of the literature will become obsolete.”
</p>

<p>
(This already happens sometimes; I remember chuckling at the 2020 Numpy <i>Nature</i> <a href=https://www.nature.com/articles/s41586-020-2649-2>paper</a>. Numpy isn’t successful because it was published in <i>Nature</i>; Numpy got into <i>Nature</i> because it was already successful.)
</p>

<h3>
3.6 The Journal as Antediluvian Status Symbol
</h3>

<p>
Pessimistically, one can imagine a world in which journal publications still carry weight with the “old guard” and certain sentimental types, but the scientific community has almost completely moved to preprints for day-to-day communication. In this scenario, one might still have to publish journal articles to get a job, but it’s just a formality, like a dissertation: the active work of science is done through preprints. Like Blockbuster, journals might limp along for some time, but their fate is pretty much sealed.
</p>

<h3>
3.7 The Journal as Philanthropic Pravda
</h3>

<p>
Another reason why journals might persist in a world driven by preprints is the desire of philanthropic agencies to appear beneficent. If a certain organization, public or private, is giving tens of millions of dollars to support scientific progress, the only real reward it can reap in the short term is the prestige of having its name associated with a given discovery. Why not go one step further and control the means of publication?
</p>

<p>
In this <i>Infinite Jest</i>-like vision, funding a certain project buys you the right to publish its results in your own journal. We can imagine <i>J. Pfizer-Funded Research</i> and <i>J. Navy Research</i> competing to fund and publish the most exciting work in a given area, since no one wants to sponsor a loser. (Why stop there? Why not name things after corporate sponsors? We could have the Red Bull–Higgs Boson, or the Wittig–Genentech olefination.)
</p>

<p>
As discussed at the beginning of this article, the government “funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product.” There’s a certain simplicity in a funding agency just taking over the whole process, but  I doubt this would be good for scientists. Unifying the roles of funder, publisher, and editor would probably lower the agency of actual researchers to an untenably low level. 
</p>

<h3>
3.8 The Journal as Rent-Seeking Data Troll
</h3>

<p>
Another depressing scenario is one in which journals cease contributing to the positive progress of science, and start essentially just trying to monetize their existing intellectual property. As ML and AI become more important, legal ownership of data rights will presumably increase in economic value, and one can easily imagine the Elseviers of the world vacuuming up any smaller journals they can and then charging exorbitant fees for access to their data. (Goodbye, Reaxys…)
</p>

<p>
I hope this doesn’t happen. 
</p>

<h3>
3.9 No Journals; Just an Anarchic Preprint Lake
</h3>

<p>
The obvious alternative to these increasingly far-fetched scenarios is also the simplest; we get rid of journals all together, and—just like in the 1700s—rely solely on communication-style preprints on arXiv, bioRxiv, ChemRxiv, etc. This has been termed a “preprint lake,” in analogy to <a href=https://en.wikipedia.org/wiki/Data_lake>“data lakes.”</a> 
</p>

<p>
To help scientists make sense of the lake, one can envision some sort of preprint aggregator: a Reddit or Hacker News for science, which sorts papers by audience feedback and permits <a href=https://en.wikipedia.org/wiki/PubPeer>PubPeer</a>-type public comments on the merits and shortcomings of each publication. The home page of Reddit-for-papers could serve as the equivalent to <i>Science</i>; the chemistry-specific subpage, the equivalent to <i>JACS</i>. Peer review could happen in a decentralized fashion, and reviews would be public for all to see.
</p>

<p>
There’s an anarchic appeal to this proposal, but it has potential drawbacks too:
</p>

<ol type="i">
  <li>
  For those not immersed in a given field, it’s very difficult to know what’s good research and what isn’t. This is doubly true for non-scientists—what will become of high-school students trying to write papers?
  </li>

  <li>
    Existing status symbols will become more important absent journal status. To quote <a href=https://luispedro.substack.com/p/against-publication-lakes-glam-journals?s=03>Luis Pedro Coelho</a>:

    <blockquote>
    I mostly read preprints by people whose names I already recognize. When thousands of papers are thrown into the “level playing field” of biorxiv, pre-existing markers of prestige end up taking an even greater role.
    </blockquote>

    This presumably will disadvantage up-and-coming scientists, or scientists without access to existing networks of prestige. That being said, one might make the same arguments for the Internet, and the real effect seems to have been exactly the opposite! So I’m not quite sure how to think about this.
  </li>

  <li>
    What “goes viral” may not always be what’s the best science. Rarely do thoughtful or contemplative ideas rise to popularity out of the unstructured morass of the Internet, and I find it naïve to expect that scientists would be any different here. That being said, the wisdom of crowds might be the lesser of two evils, given that our current system is basically “ask three random rivals in the field.”
  </li>

  <li>
    There are also just so many papers out there today that it might just become overwhelming, even to specialists. I miss papers in “my areas” constantly, and I try pretty hard to keep up with the literature! (Some have proposed that <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/leap.1514>AI might help us sift through things</a>, but AI might also help people write more papers faster—tough to say who will win.)
  </li>

  <li>
Without the implicit threat of peer review, standards might ebb across the board. I think this is a real concern, but it’s possible that the same community norms enforced through today’s peer review might also be enforced through whatever decentralized review process replaces it. <a href=https://link.springer.com/article/10.1007/s10657-013-9420-1>There’s some evidence</a> that, in knowledge industries with high information asymmetry (like science), communities tend to spontaneously develop strong systems of self-regulation.
  </li>
</ol>

<h2>
4. Conclusion: Archipelagic Multiverse
</h2>

<p>
The most likely scenario, to me, is that all of this sorta happens simultaneously. Most cutting-edge scientific discussion will move to the anarchic world of preprints, but there will still be plenty of room for more traditional journals: some journals will have very high standards and represent the <a href=https://www.palladiummag.com/2022/10/10/the-transformations-of-science/>magisterium of scientific authority</a>, while other journals will act as living repositories of knowledge and still others will become subjectively curated editorial statements. 
</p>

<p>
We can see journals moving in different directions even today: some journals are indicating that they’ll start requiring original data and implement more aggressive fraud detection, while others are moving away from impact-based reviewing. And I can’t help but notice that it seems to be increasingly acceptable to cite preprints in publications, suggesting that the needle might be moving towards the “anarchic preprint lake” scenario ever so slightly. 
</p>

<p>
For my part, I plan to continue writing and submitting papers as necessary, reviewing papers when asked, and so forth—but I’m excited for the future, and to see how the new world order compares to the old. 
</p>

<i>
Thanks to Melanie Blackburn, Jonathan Wong, Joe Gair, and my wife for helpful discussions, and Ari Wagen, Taylor Wagen, and Eugene Kwan for reading drafts of this piece.
</i>

]]></description>
              <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Dimensionality Reduction in Cheminformatics</title>
              <link>public/blog/20230417_dimensionality_reduction.html</link>
              <description><![CDATA[
<p>
In many applications, including cheminformatics, it’s common to have datasets that have too many dimensions to analyze conveniently. For instance, <a href=https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf>chemical fingerprints</a> are typically 2048-length binary vectors, meaning that “chemical space” as encoded by fingerprints is 2048-dimensional. 
</p>

<p>
To more easily handle these complex datasets (and to bypass the <a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality>“curse of dimensionality”</a>), it’s common practice to use a dimensionality reduction algorithm to convert the data to a low-dimensional space. In this post I want to compare and contrast three approaches to dimensionality reduction, and discuss the challenges with low-dimensional embeddings in general.
</p>

<h2>
Dimensionality Reduction Algorithms
</h2>

<p>
There are many approaches to dimensionality reduction, but I’m only going to talk about three here: PCA, tSNE, and UMAP.
</p>

<p>
<a href=https://en.wikipedia.org/wiki/Principal_component_analysis>Principal component analysis</a> (PCA) is perhaps the most famous dimensionality reduction algorithm, and is commonly used in a variety of scientific fields. PCA works by transforming the data into a new set of coordinates such that the first coordinate vector explains the largest amount of the variance, the second coordinate vector the next most variance, and so on and so forth. It’s pretty common for the first 5–20 dimensions to capture &gt;99% of the variance, meaning that the subsequent dimensions can essentially be discarded wholesale. 
</p>

<p>
<a href=https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding>tSNE</a> (t-distributed stochastic neighbor embedding) and <a href=https://umap-learn.readthedocs.io/en/latest/how_umap_works.html>UMAP</a> (uniform manifold approximation and projection) are alternative dimensionality reduction approaches, based on much more complex algorithms. To quote Wikipedia: 
</p>

<blockquote>
The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map.
</blockquote>

<p>
UMAP, at a high level, works in a very similar way, but uses some fancy topology to construct a “fuzzy simplicial complex” representation of the data in high-dimensional space, and then projects this representation down into a lower dimension (<a href=https://pair-code.github.io/understanding-umap/>more detailed explanation</a>). Practically, UMAP is a lot faster than tSNE, and is becoming the algorithm of choice for most cheminformatics applications. (Although, in fairness, there are <a href=https://arxiv.org/abs/1712.09005>ways to make tSNE faster</a>.)
</p>

<h2>
Data Visualization 
</h2>

<p>
For the purposes of this post, I chose to study Abbie Doyle’s set of <a href=https://pubs.acs.org/doi/10.1021/jacs.1c12203>2683 aryl bromides</a> (obtained from Reaxys, with various filters applied). I used the RDKIT7 fingerprint to generate a 2048-bit encoding of each aryl bromide, computed a distance matrix using Tanimoto/Jaccard distance, and then used each dimensionality reduction technique to generate a 2-dimensional embedding.
</p>

<p>
Let’s look at PCA first:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_pca_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using PCA.
  </figcaption>
</figure>

<p>
PCA generally creates fuzzy-looking blobs, which sometimes show some amount of meaningful structure but don’t really display many sharp boundaries.
</p>

<p>
Now, let’s compare to tSNE:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_tsne_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using tSNE (perplexity 20).
  </figcaption>
</figure>

<p>
tSNE creates “blob-of-blob” plots which show many tight clusters arranged together in some sort of vague pattern. The size and position of the clusters can be tuned by changing the “perplexity” hyperparameter (see <a href=https://stats.stackexchange.com/questions/399868/why-does-larger-perplexity-tend-to-produce-clearer-clusters-in-t-sne>this StackOverflow post</a> for more discussion, and <a href=https://distill.pub/2016/misread-tsne/?s=03>this excellent post</a> for demonstrations of how tSNE can be misleading).
</p>

<p>
What about UMAP?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_umap_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using UMAP (30 neighbors, 0.1 minimum distance).
  </figcaption>
</figure>

<p>
UMAP also creates tight tSNE-like clusters, but UMAP plots generally have a much more variable overall shape—the clusters themselves are tighter and scattered across more space. (These considerations are complicated by the fact that UMAP has multiple tunable hyperparameters, meaning that the exact appearance of the plot is substantially up to the end user.)
</p>

<p>
The debate between tSNE and UMAP is spirited (<a href=https://www.biorxiv.org/content/10.1101/2019.12.19.877522v1.full.pdf>e.g.</a>), but for whatever reason people in chemistry almost exclusively use UMAP. (See, for instance, pretty much every paper I taked about <a href=https://corinwagen.github.io/public/blog/20230118_meta_optimization.html>in this post</a>.)
</p>

<p>
An important thing that I’m not showing here, but which bears mentioning, is that the clusters in all three plots are actually chemically meaningful. For instance, each cluster in the tSNE plot generally corresponds to a different functional group: carboxylic acids, alkynes, etc. So the graphs do in some real sense correspond to the intuition we have about molecular similarity, which is good! (You can use <a href=https://github.com/wjm41/molplotly>molplotly</a> to visualize these plots very easily.)
</p>

<h2>
Distance Preservation
</h2>

<p>
How well are distances from the high-dimensional space preserved in the 2D embedding? Obviously the distances won’t all be the same, but ideally the mapping would be monotonic: if distance A is greater than distance B in the high-dimensional space, we would like distance A to also be greater than distance B in the low-dimensional space. 
</p>

<p>
We can measure this with <a href=https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>Spearman correlation</a>, which is like a Pearson correlation (AKA “r-squared”) but without the assumption of linearity. A Spearman correlation coefficient of 1 indicates a perfect monotonic relationship, while a coefficient of 0 indicates no relationship. Let’s plot the pairwise distances from each embedding against the true distances and compare the Spearman coefficients:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_spearman_plots.png" style="width:300px;" />
  <figcaption> 
  Comparison of distances in high-dimensional space against distances in embedding space, and associated Spearman coefficients. (Only one in every hundred points is plotted, but all points are used for the Spearman coefficient calculation.)
  </figcaption>
</figure>

<p>
In each case, the trend is in the right direction (i.e. increased distance in high-dimensional space is correlated with increased distance in low-dimensional space), but the relationship is far from monotonic. It’s clear that there will be plenty of cases where two points will be close in low-dimensional space and far in high-dimensional space. 
</p>

<p>
Does this mean that UMAP, tSNE, and PCA are all failing? To understand this better, let’s plot a histogram of all the distances in each space:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_dist_hist.png" style="width:400px;" />
  <figcaption> 
  Histogram of all distances in each space. Distances have been scaled to the range [0,1] to match distances obtained with the Jaccard metric.
  </figcaption>
</figure>

<p>
We can see that the 2048-dimensional space has a very distinct histogram. Most of the compounds are pretty different from one another, and—crucially—most of the distances are about the same (0.8 or so). In chemical terms, this means that most of the fingerprints share a few epitopes in common, but otherwise are substantially different, which is unsurprising since fingerprints in general are quite sparse. 
</p>

<p>
Unfortunately, “lots of equidistant points” is an extremely tough pattern to recapitulate in a low-dimensional space. We can see why with a toy example: in 2D space, we can only have 3 equidistant points (an equilateral triangle), and in 3D space, we can only have 4 equidistant points (a tetrahedron). More generally, if we want <i>N</i> equidistant points, we need to be in <b>R</b><sup><i>N</i>-1</sup> (<i>N</i>-1 dimensional Euclidean space). We can <a href=https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma>relax this requirement</a> a little bit if we’re willing to accept approximate equidistance, but the general principle still holds: it’s hard to recapitulate lots of equidistant points in a low-dimensional space. 
</p>

<p>
As expected, then, we can see that the histogram of each of our algorithms looks very different from the ideal distance histogram.
</p>

<h2>
Local Structure
</h2>

<p>
Both tSNE and UMAP take the nearest neighbors of each point explicitly into account, and claim to preserve the local structure of the points as much as possible. To put these claims to the test, I looked at the closest 30 neighbors of each point in high-dimensional space, and then checked how many of those neighbors made it into the closest 30 neighbors in low-dimensional space. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_neighbor_hist.png" style="width:400px;" />
  <figcaption> 
  Histogram of how many of the closest 30 neighbors of each point are recapitulated after dimensionality reduction.
  </figcaption>
</figure>

<p>
We can see that PCA only preserves about 30–40% of each point’s neighbors, whereas PCA and UMAP generally preserve 60% of the neighbors: not perfect, but much better.
</p>

<p>
I chose to look at 30 neighbors somewhat arbitrarily: what happens if we change this number?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_neighborhood_scan.png" style="width:400px;" />
  <figcaption> 
  The percent of neighbors recapitulated correctly, as neighborhood size increases.
  </figcaption>
</figure>

<p>
We can see that UMAP and tSNE both preserve about 60% of the neighbors across a wide range of neighborhood sizes, while PCA gets better as we zoom out more. (At the limit where we consider all 2683 points as neighbors, every method will trivially achieve perfect accuracy.) tSNE does much better than UMAP for small neighborhoods; I’m not sure why!
</p>

<p>
Another way to think about this is in terms of the <a href=https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html>precision–recall tradeoff</a>. In classification, “precision” refers to a classifier’s ability to avoid false positives, while “recall” refers to a classifier’s ability to avoid false negatives. What does this mean in the context of embedding? 
</p>

<p>
Imagine looking at all points in the neighborhood of our central point in high-dimensional space, and then comparing to the points within a certain radius of our point in low-dimensional space. As we increase the radius, we expect to see more of the correct neighbor points in low-dimensional space, but we also expect to see more “incorrect neighbors” that aren’t really there in the high-dimensional space. (<a href=https://jmlr.org/papers/volume11/venna10a/venna10a.pdf>This paper</a> discusses these issues nicely, as does <a href=https://coursepages2.tuni.fi/mttts17/wp-content/uploads/sites/136/2020/03/drv_2020_lecture_7.pdf>this presentation</a>.)
</p>

<p>
So low radii lead to high precision (most of the points are really neighbors) but low recall (we’re not finding most of the neighbors), while high radii lead to low precision and high recall. We can thus study the performance of our embedding by graphing the precision–recall curve for various neighborhood sizes. The better the embedding, the closer the curve will come to the top right:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_precision_recall.png" style="width:400px;" />
  <figcaption> 
  Precision–recall tradeoff for all three methods.
  </figcaption>
</figure>

<p>
We can see that tSNE does better in the high precision/low recall area of the curve (as we saw in the previous graph), but otherwise tSNE and UMAP are quite comparable. In contrast, PCA is just abysmal.
</p>

<p>
The big conclusion of this section is that, if you’re doing something that depends on the local structure of the data, you should avoid PCA. 
</p>

<h2>
Do Higher Dimensions Help Things? 
</h2>

<p>
Since the root of our issues here is trying to represent a 2048-dimensional distance matrix in 2 dimensions, one might wonder if we could do better by expanding to 3, 4, or more dimensions. This would make visualization tricky, but might still be suitable for other operations (like clustering). 
</p>

<p>
tSNE gets very, very slow in higher dimensions, so I focused on PCA and UMAP for this study. I started out by comparing the Spearman correlation for PCA and UMAP up to 20 dimensions:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_spearman_dim.png" style="width:400px;" />
  <figcaption> 
  Precision–recall tradeoff for all three methods.
  </figcaption>
</figure>

<p>
Surprisingly, UMAP doesn’t seem to get any better in high dimensions, but PCA does. (Changing the number of neighbors didn’t help UMAP at all.)
</p>

<p>
How do our other metrics look with high-dimensional PCA?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_dist_hist.png" style="width:400px;" />
  <figcaption> 
  Distance histogram for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
As we increase the number of dimensions, the distance histogram starts to approach the correct distribution. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_neighbor_hist.png" style="width:400px;" />
  <figcaption> 
  Neighbor histogram for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
We also start to do a better job capturing the local structure of the graph, although we’re still not as good as tSNE or UMAP even at 10 dimensions. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_precision_recall.png" style="width:400px;" />
  <figcaption> 
  Precision–recall curve for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
And our precision–recall graph is still pretty dismal when compared to tSNE or UMAP. So, it seems like if distances are what matters, then high-dimensional PCA is an appealing choice—but if local structure is what matters, tSNE or UMAP is still superior.
</p>

<h2>
Conclusions
</h2>

<p>
My big takeaway from all of this is: dimensionality reduction is a lossy process, and one where you always have to make tradeoffs. You’re fundamentally throwing away information, and that always has a cost: there’s no such thing as a free lunch. As such, if you don’t have to perform dimensionality reduction, then my inclination would be to avoid it. (People in single-cell genomics seem to have come to <a href=https://www.biorxiv.org/content/10.1101/2021.08.25.457696v4.full.pdf>a similar conclusion</a>.)
</p>

<p>
If you really need your data to be in a low-dimensional space (e.g. for plotting), then keep in mind what you’re trying to study! PCA seems to do a slightly better job with distances (although I’m sure there are more sophisticated strategies for distance-preserving dimensionality reduction), while tSNE and UMAP seem to do much, much better with local structure. 
</p>

<i>
Thanks to Michael Tartre for helpful conversations, and the students in Carnegie Mellon’s “Digital Molecular Design Studio” class for their thought-provoking questions on these topics.  
</i>

]]></description>
              <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>New Ways To Read The Blog: RSS and Substack</title>
              <link>public/blog/20230413_new_ways.html</link>
              <description><![CDATA[
<p>
<i>
(This is more of a housekeeping post than an actual post with content; apologies.)
</i>
</p>

<p>
Up until now, my blogging strategy has been to write new posts about once a week and publicize them on Twitter, which works great for people who are on Twitter but (obviously) fails for people who aren’t on Twitter. I’m frequently asked if there are non-Twitter ways to subscribe to the blog updates: given that I myself don’t love relying on Twitter to bring me content, and that Twitter itself feels increasingly dicey, I feel bad saying no every time.
</p>

<p>
I’m happy to announce that there are now two additional ways to read the blog: RSS and Substack. 
</p>

<h2>
RSS
</h2>

<p>
RSS is a lovely way to get updates from sites, which is sadly limited by the fact that nobody uses it anymore. (Half the people I talk to these days don’t even know what it is.) You can use an RSS aggregator like <a href=https://feedly.com/>Feedly</a>, and simply subscribe to various sites, so that they’ll dependably show up in your feed. This is the main way I get <a href=https://corinwagen.github.io/public/blog/20230329_literature.html>journal updates</a> and my news. 
</p>

<p>
So, if you like using RSS, you can simply search “corinwagen.github.io” in Feedly, and the blog will come up:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230413_feedly.jpg" style="width:300px;" />
  <figcaption> 
  What it looks like on Feedly. The Twitter preview images sadly don't display.
  </figcaption>
</figure>

<h2>
Substack
</h2>

<p>
Substack is a platform that helps people write and manage newsletters. It essentially solves the problem of “how do I create an email list”/“how do I manage subscriptions” for people who would rather not take care of hosting a web service and handling payments themselves, like me.
</p>

<p>
I initially didn’t want to use Substack because (1) I wanted the blog to be part of my website, (2) I liked being able to control every aspect of the design, and (3) I wasn’t sure if anyone would read the blog, and there’s nothing sadder than an empty Substack. As things stand, (3) is a non-issue, so the question is whether the added convenience of Substack outweighs my own personal design and website preferences.
I suspect that it may, so I’ve capitulated and copied all existing posts over to <a href=https://cwagen.substack.com>my new Substack</a>. (There are a few formatting issues in old posts, but otherwise things copied pretty well.)
</p>

<p>
For now, I plan to continue posting everything on the blog, and manually copying each post over to Substack (I write in plain HTML so this is not too hard). If Substack ends up totally outperforming the blog in terms of views, then I’ll probably switch to Substack entirely for blogging and just leave my website up as a sort of virtual CV. 
</p>

<p>
(I have no plans to enable subscriptions at this point; that being said, if for some bizarre reason there’s sufficient demand I’ll probably try to think of something to reward subscribers.)
</p>

<p>
If you’d like to receive updates on Substack, you can subscribe below: 
</p>

<br>
<iframe src="https://cwagen.substack.com/embed" width="480" height="320" style="display:block; border:1px solid #EEE; background:white; margin:auto;" frameborder="0" scrolling="no"></iframe>

]]></description>
              <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Why New Ventures Are So Important</title>
              <link>public/blog/20230411_newcomers.html</link>
              <description><![CDATA[
<p>
This Easter week, I’ve been thinking about why new ventures are so important. Whether in private industry, where startups are the most consistent source of innovative ideas, or in academia, where new assistant professors are hired each year, newcomers are often the most consistent source of innovation. Why is this?
</p>

<p>
One explanation is the <a href=https://www.nber.org/system/files/chapters/c2144/c2144.pdf>Arrow replacement effect</a> (named after <a href=https://en.wikipedia.org/wiki/Kenneth_Arrow>Kenneth Arrow</a>), which states that “preinvention monopoly power acts as a strong disincentive to further innovation.” Arrow’s argument goes like this: suppose there’s an organization that is earning profit <i>P<sub>old</sub></i>, and there is some innovation that will increase profit to <i>P<sub>new</sub></i> (<i>P<sub>new</sub></i> &gt; <i>P<sub>old</sub></i>). If the existing organization pursues the innovation, their profits will thus increase by <i>∆P</i> := <i>P<sub>new</sub></i> - <i>P<sub>old</sub></i>. But a new organization will see its profits increase by <i>P<sub>new</sub></i>: since the startup has no existing profit to replace, the rewards to innovation are higher. Thus innovation is more appealing for those without any economic stake in the status quo.<sup><a href="#fn1">1</a></sup>
</p>

<p>
We can see this play out today in the dynamic between Google and OpenAI/Microsoft: Google already has a virtual monopoly in search, and so is hesitant to replace what they have, whereas Microsoft has already been losing in search and so is eager to replace Bing with <s>Sydney</s> an AI-powered alternative. (It’s to Apple’s credit that they so eagerly pursued the iPhone when it meant effectively destroying the iPod, one of their top money-makers.<sup><a href="#fn2">2</a></sup>)
</p>

<p>
One can also see this scenario in academia—plenty of established labs have programs built up around studying specific systems, and are thus disincentivized to study areas which might obviate projects they’ve spent decades working on. For instance, labs dedicated to “conventional” synthetic methodology might be slower to turn to biocatalysis than a new assistant professor with nothing to lose; labs that have spent decades studying protein folding might be slower to turn to AlphaFold than they ought to.
</p>

<p>
Another reason is that new entrants often have an advantage in understanding the status quo. In <i>The Art of Doing Science and Engineering</i> (book review coming, eventually), computing legend <a href=https://en.wikipedia.org/wiki/Richard_Hamming>Richard Hamming</a> discusses how there’s often a disadvantage to being a pioneer in a field. Hamming’s argument, essentially, is that those who’ve had to invent something new never understand it as intuitively as those who have simply learned to take it for granted:
</p>

<blockquote>
The reason this happens is that the creators have to fight through so many dark difficulties, and wade through so much misunderstanding and confusion, they cannot see the light as others can, now the door is open and the path made easy…. in time the same will probably be true of you.
</blockquote>

<p>
In Hamming’s view, it’s the latecomers to a field who can see more clearly the new possibilities opened by various innovations, and take the next steps towards previously unimaginable frontiers. There’s a deep irony in this: the very act of inventing something new makes you less able to see the innovations enabled by your own work. The process of invention thus acts like a relay race, where newer generations continually take the baton and push things forward before in turn dropping back.
</p>

<p>
I’ve heard these ideas discussed in terms of naïvete before—the idea being that innovation requires a sort of “beginner’s luck,” a blind optimism about what’s possible that the experienced lack—but I think that’s wrong. A belief in naïvete as the key driver of innovation implies that excessive knowledge is detrimental: that it’s possible to “know too much” and cripple oneself. If anything, the opposite is true in my experience. The most creative and productive people I’ve met are those with an utter mastery of the knowledge in their domain.
</p>

<p>
Hamming’s proposal, which is more cognitive/subconscious, is thus complementary to the more calculated logic of the Arrow replacement theorem: existing organizations are both less incentivized to innovate and less able to see potential innovations. These ideas should be encouraging to anyone at the beginning of their career: you are uniquely poised to discover and exploit new opportunities! So consider this an exhortation to go out and do so now (rather than waiting until you are older and more secure in your field).
</p>

<i>Credit to <a href="https://marginalrevolution.com/marginalrevolution/2023/04/the-arrow-replacement-effect-and-the-dynamics-of-us-inventors.html?utm_source=feedly&utm_medium=rss&utm_campaign=the-arrow-replacement-effect-and-the-dynamics-of-us-inventors">Alex Tabarrok</a> for introducing me to the Arrow replacement effect, and ChatGPT for some edits.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  This is a bit of a cartoonish depiction of the Arrow replacement theorem—the original paper (linked above) is quite readable, and performs a more sophisticated analysis. See the heading “Competition, Monopoly, and the Incentive to Innovate” on page 12 of the PDF (journal page 619).
  </li>
  <li id="fn2">
  Tony Fadell discusses this in <a href=https://www.amazon.com/Build-Unorthodox-Guide-Making-Things/dp/0063046067><i>Build</i></a>: suffice it to say this was not an internally popular decision at Apple.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Industry Research Seems Underrated</title>
              <link>public/blog/20230403_industry.html</link>
              <description><![CDATA[
<p>
While scientific companies frequently publish their research in academic journals, it seems broadly true that publication is not incentivized for companies the same way it is for academic groups. Professors need publications to get tenure, graduate students need publications to graduate, postdocs need publications to get jobs, and research groups need publications to win grants. So the incentives of everyone in the academic system are aligned towards publishing papers, and lots of papers get published.
</p>

<p>
In contrast, the success or failure of a private company is—to a first approximation—unrelated to its publication record. Indeed, publication might even be harmful for companies, insofar as time spent preparing manuscripts and acquiring data <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>only needed for publication</a> is time that could be spent on more mission-critical activities.
</p>

<p>
That’s why I generally believe industry publications, especially those where no academic co-authors are involved, are underrated, and are probably better than the journal they’re in might indicate. Getting a publication into a prestigious journal like <i>Science</i> or <i>Nature</i> is pretty random, requires a lot of effort, and frequently has a slow turnaround time, whereas lower-tier journals are likely to accept your work, and typically review and publish papers much, much faster. (In particular, ACS is <a href=https://scholarlykitchen.sspnet.org/2022/11/08/guest-post-publishing-fast-and-slow-a-review-of-publishing-speed-in-the-last-decade/>among the fastest of all scientific publishers</a>, and is generally a pleasure to work with.)
</p>

<p>
The above reflections were prompted by reading <a href=https://pubs.acs.org/doi/full/10.1021/acs.jmedchem.0c00452>an absolute gem of a paper</a> in <i>J. Med. Chem.</i>, a collaboration between X-Chem, ZebiAI, and Google Research. The paper is entitled “Machine Learning on DNA-Encoded Libraries: A New Paradigm for Hit Finding” and describes how data from DNA-encoded libraries (DELs) can be used to train ML models to predict commercially available compounds with activity against a given target. This is a really, really big deal. As the authors put it in their conclusion:
</p>

<blockquote>
[Our approach] avoids the time-consuming and expensive process of building new chemical matter into a DEL library and performing new selections or incorporating new molecules into a HTS screening library. This ability to consider compounds outside of the DEL is the biggest advantage of our approach; notably, this approach can be used at a fraction of the cost of a traditional DEL screening follow-up, driven primarily by the large difference in synthesis cost.
</blockquote>

<p>
Now, the precise impact of this discovery will of course be determined in the years to come; Derek Lowe raises some fair concerns <a href=https://www.science.org/content/blog-post/machine-learning-top-dna-encoded-libraries>on his blog</a>, pointing out that the targets chosen are relatively easy to drug, and so probably wouldn’t be the subject of a high-tech DEL screen anyway, and it’s entirely possible that there will be other unforeseen complications with this technology that are only revealed in the context of a real-world discovery pipeline. (Given that Relay <a href=https://www.biopharmadive.com/news/relay-acquire-zebiai-ai-drug-discovery/598550/>acquired ZebiAI</a> for $85M in 2021 essentially on the strength of this paper alone, I’m guessing plenty of real-world testing is already underway.)
</p>

<p>
The point I want to make is that if this paper had come from an academic group, I would be very, very surprised to see it in <i>J. Med Chem</i>. This project has everything that one expects in a <i>Science</i> paper: a flashy new piece of technology, a problem that’s understandable to a broad audience, clear clincal relevance, even a domain arbitrage angle. Yet this paper is not in <i>Science</i>, nor <i>ACS Central Science</i>, nor even <i>JACS</i>, but in <i>J. Med. Chem.</i>, a journal I don’t even read regularly.
</p>

<p>
My conclusions from this are (1) to remember that not everyone is incentivized to market their own findings as strongly as academics are and (2) to try and look out for less-hyped industry results that I might neglect otherwise.
</p>

]]></description>
              <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>You Should Read The Literature More</title>
              <link>public/blog/20230329_literature.html</link>
              <description><![CDATA[
<p>
If you are a scientist, odds are you should be reading the literature more. This might not be true in every case—one can certainly imagine someone who reads the literature too much and never does any actual work—but as a heuristic, my experience has been that most people would benefit from reading more than they do, and often much more. Despite the somewhat aggressive title, my hope in writing this is to encourage you to read the literature more: to make you excited about reading the literature, not to guilt you into it or provoke anxiety.
<p>

<h2>
Why You Should Read The Literature More
</h2>

<p>
You should read the literature because you are a scientist, and your business is ideas. The literature is the vast, messy, primal chaos that contains centuries of scientific ideas. If you are an ideas worker, this is your raw material—this is what you work with. Not reading the literature as an ideas worker is like not going to new restaurants as a new chef, or not looking at other people’s art as an artist, or not listening to music as a composer. Maybe the rare person has an internal creativity so deep that they don’t need any external sources of inspiration, but I’m not sure I know anyone like that.
</p>

<p>
If you buy the concept of <a href=https://corinwagen.github.io/public/blog/20230320_domain_arbitrage.html>“domain arbitrage”</a> I outlined last week, then reading the literature becomes doubly important for up-and-coming arbitrageurs. Not only do you need to stay on top of research in your own field, but you also need to keep an eye on other fields, to look for unexpected connections. It was only after months of reading physical chemistry papers about various IR spectroscopy techniques, with no direct goal in mind, that I realized I could use <i>in situ</i> IR to <a href=https://pubs.acs.org/doi/10.1021/acs.orglett.2c03622>pin down the structure of ethereal HCl</a>; simply reading organic chemistry papers would not have given me that insight.
</p>

<h2>
How You Can Read The Literature More
</h2>

<p>
If you don’t read the literature at all—like me, when I started undergrad—then you should start small. I usually recommend <i>JACS</i> to chemists. Just try to read every paper in your subfield in <i>JACS</i> for a few months; I began by trying to read every organic chemistry paper in <i>JACS</i>. At the beginning, probably only 10–20% will make sense. But if you push through and keep trying to make sense of things, eventually it will get easier. You’ll start to see the same experiments repeated, understand the structure of different types of papers, and even recognize certain turns of phrase. (This happened to me after about a year and a half of reading the literature.)
</p>

<p>
Reading more papers makes you a faster reader. Here’s <a href=https://marginalrevolution.com/marginalrevolution/2006/12/how_to_read_fas.html>Tyler Cowen</a> on how he reads so quickly (not papers specifically, but still applicable):
</p>

<blockquote>
The best way to read quickly is to read lots. And lots. And to have started a long time ago. Then maybe you know what is coming in the current book. Reading quickly is often, in a margin-relevant way, close to not reading much at all.
<br><br>
Note that when you add up the time costs of reading lots, quick readers don’t consume information as efficiently as you might think. They’ve chosen a path with high upfront costs and low marginal costs. "It took me 44 years to read this book" is not a bad answer to many questions about reading speed.
</blockquote>

<p>
All of Tyler’s advice applies doubly to scientific writing, which is often jargon-filled and ordered in arcane ways. After 7ish years of reading the scientific literature, I can “skim” a <i>JACS</i> paper pretty quickly and determine what, if anything, is likely to be novel or interesting to me, which makes staying on top of the literature much easier than it used to be.
</p>

<p>
Once you are good with a single journal, you can expand to multiple journals. A good starting set for organic chemistry is <i>JACS</i>, <i>Science</i>, <i>Nature</i>, <i>Nature Chemistry</i>, and <i>Angewandte</i>. If you already know how to read papers quickly, it will not be very hard to read more and more papers. But expanding to new journals brings challenges: how do you keep up with all of them at once? Lots of people use an RSS feed to aggregate different journals—I use <a href=https://feedly.com/>Feedly</a>, as do several of my coworkers. (You can also get this blog on Feedly!)
</p>

<p>
I typically check Feedly many times a day on my phone; I can look at the TOC graphic, the abstract, and the title, and then if I like how the paper looks I’ll email it to myself. Every other day or so, I sit down at my computer with a cup of coffee and read through the papers I’ve emailed to myself. This is separate from my “pursuing ideas from my research”/”doing a literature dive for group meeting” way of reading the literature—this is just to keep up with all the cool stuff that I wouldn’t otherwise hear about.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230329_email.png" style="width:550px;" />
  <figcaption> 
  “Inbox Zero” often proves elusive.
  </figcaption>
</figure>

<p>
(I also use Google Scholar Alerts to email me when new labs publish results—I have probably 20-30 labs set up this way, just to make sure I don’t miss results that might be important just because they’re not in a high-profile journal.)
</p>

<p>
Keeping track of papers you actually like and want to remember is another challenge. For the past two years, I’ve put the URLs into a Google Sheet, along with a one-sentence summary of the paper, which helps me look through my “most liked” papers when I want to find something. Sadly, I didn’t do this earlier, so I’m often tormented by papers I dimly remember but can no longer locate.
</p>

<h2>
What Literature You Should Read
</h2>

<p>
This obviously depends on what you’re doing, but I tend to think about literature results in three categories:
</p>

<ol>
<li>Things every scientist should know about</li>
<li>Things I am supposed to be an expert on</li>
<li>Things I’m not supposed to be an expert on, but would still like to know about</li>
</ol>

<p>
Category 1 basically covers the highest profile results (<i>Science</i> and <i>Nature</i>), and these days Twitter makes that pretty easy.
</p>

<p>
Category 2 covers things “in-field” or directly related to my projects—anything it would be somewhat embarrassing not to know about. For me, this means <i>JACS</i>, <i>Angewandte</i>, <i>ACS Catalysis</i>, <i>Org. Lett.</i>, <i>OPRD</i>, <i>Organometallics</i>, <i>J. Org. Chem.</i>, and <i>Chem. Sci.</i> (I also follow <i>Chem. Rev.</i> and <i>Chem. Soc. Rev.</i>, because review articles are nice.)
</p>

<p>
Category 3 covers things that I am excited to learn about. Right now, that’s <i>JCTC</i> and <i>J. Phys. Chem. A–C</i>. In the past, that’s included <i>ACS Chem. Bio.</i>, <i>Nature Chem. Bio.</i>, and <i>Inorganic Chemistry</i>. (Writing this piece made me realize I should follow <i>JCIM</i> and <i>J. Chem. Phys.</i>, so I just added them to Feedly.)
</p>

<h2>
Conclusion
</h2>

<p>
Reading the literature is—in the short term—pointless, sometimes frustrating, and just a waste of time. It’s rare that the article you read today will lead to an insight on the problem you’re currently facing! But the gains to knowledge compound over time, so spending time reading the literature today will make you a much better scientist in the long run.
</p>

<i>
Thanks to Ari Wagen and Joe Gair for reading drafts of this post.
</i>

]]></description>
              <pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Domain Arbitrage</title>
              <link>public/blog/20230320_domain_arbitrage.html</link>
              <description><![CDATA[
<p>
It’s a truth well-established that interdisciplinary research is good, and we all should be doing more of it (e.g. <a href=https://beta.nsf.gov/funding/learn/research-types/learn-about-interdisciplinary-research>this NSF page</a>). I’ve always found this to be a bit uninspiring, though. “Interdisciplinary research” brings to mind a fashion collaboration, where the project is going to end up being some strange chimera, with goals and methods taken at random from two unrelated fields.
</p>

<p>
Rather, I prefer the idea of “domain arbitrage.”<sup><a href="#fn1">1</a></sup> Arbitrage, in economics, is taking advantage of price differences in different markets: if bread costs $7 in Cambridge but $4 in Boston, I can buy in Boston, sell in Cambridge, and pocket the difference. Since this is easy and requires very little from the arbitrageur, physical markets typically lack opportunities for substantial arbitrage. In this case, the efficient market hypothesis works well.
</p>

<p>
Knowledge markets, however, are much less efficient than physical markets—many skills which are cheap in a certain domain are expensive in other domains. For instance, fields that employ organic synthesis, like chemical biology or polymer chemistry, have much less synthetic expertise than actual organic synthesis groups. The knowledge of how to use a Schlenk line properly is cheap within organic chemistry but expensive everywhere else. And organic chemists certainly don’t have a monopoly on scarce skills: trained computer scientists are very scarce in most scientific fields, as are statisticians, despite the growing importance of software and statistics to almost every area of research.
</p>

<p>
Domain arbitrage, then, is taking knowledge that’s cheap in one domain to a domain where it’s expensive, and profiting from the difference. I like this term better because it doesn’t imply that the goal of the research has to be interdisciplinary—instead, you’re solving problems that people have always wanted to solve, just now with innovative methods. And the concept of arbitrage highlights how this can be beneficial for the practitioner. You’re bringing new insights to your field so you can help your own research and make cool discoveries, not because you’ve been told that interdisciplinary work is good in an abstract way.
</p>

<p>
There are many examples of domain arbitrage,<sup><a href="#fn2">2</a></sup> but perhaps my favorite is the recent black hole image, which was largely due to work by Katie Bouman (formerly a graduate student at MIT, <a href=http://users.cms.caltech.edu/~klbouman/>now a professor at Caltech</a>):
</p>


<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230320_black_hole.jpg" style="width:350px;" />
  <figcaption> 
  The black hole picture, extracted from noisy radio telescope data by Bouman’s new algorithms.
  </figcaption>
</figure>

<p>
What’s surprising is that Bouman didn’t have a background in astronomy at all: she “hardly knew what a black hole was” (<a href=https://www.pbs.org/newshour/science/katie-bouman-hardly-knew-what-a-black-hole-was-her-algorithm-helped-us-see-one>in her words</a>) when she started working on the project. Instead, Bouman’s work drew on her background in computer vision, adapting statistical image models to the task of reconstructing astronomical images. In <a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bouman_Computational_Imaging_for_CVPR_2016_paper.pdf>a 2016 paper</a>, she explicitly credits computer vision with the insights that would later lead to the black hole image, and concludes by stating that “astronomical imaging will benefit from the crossfertilization of ideas with the computer vision community.”
</p>

<p>
If we accept that domain arbitrage is important, why doesn’t it happen more often? I can think of a few reasons: some fundamental, some structural, and some cultural. On the fundamental level, domain arbitrage requires knowledge of two fields of research at a more-than-superficial level. This is relatively common for adjacent fields of research (like organic chemistry and inorganic chemistry), but becomes increasingly rare as the distance between the two fields grows. It’s not enough to just try and read journals from other fields occasionally—without the proper context, other fields are simply unintelligible. Given how hard achieving competence in a single area of study can be, we should not be surprised that those with a working knowledge of multiple fields are so scarce.
</p>

<p>
The structure of our research institutions also makes domain arbitrage harder. In theory, a 15-person research group could house scientists from a variety of backgrounds: chemists, biologists, mathematicians, engineers, and so forth, all focused on a common research goal. In practice, the high rate of turnover in academic positions makes this challenging. Graduate students are only around for 5–6 years, postdocs for fewer, and both positions are typically filled by people hoping to learn things, not by already competent researchers. Thus, senior lab members must constantly train newer members in various techniques, skills, and ways of thinking so that institutional knowledge can be preserved.
</p>

<p>
This is hard but doable for a single area of research, but quickly becomes untenable as the number of fields increases. A lab working in two fields has to pass down twice as much knowledge, with the same rate of personnel turnover. In practice, this often means that students end up deficient in one (or both) fields. As Derek Lowe put it <a href=https://www.science.org/content/blog-post/pipeline-1060>when discussing chemical biology</a> in 2007:
</p>

<blockquote>
I find a lot of [chemical biology] very interesting (though not invariably), and some of it looks like it could lead to useful and important things. My worry, though, is: what happens to the grad students who do this stuff? They run the risk of spending too much time on biology to be completely competent chemists, and vice versa.
</blockquote>

<p>
To me, this seems like a case in which the two goals of the research university—to teach students and to produce research—are at odds. It’s easier to teach students in single-domain labs, but the research that comes from multiple domains is superior. It’s not easy to think about how to address this without fundamental change to the structure of universities (although perhaps others have more creative proposals than I).<sup><a href="#fn3">3</a></sup>
</p>

<p>
But, perhaps most frustratingly, cultural factors also contribute to the rarity of domain arbitrage. Many scientific disciplines today define themselves not by the questions they’re trying to solve but by the methods they employ, which disincentivizes developing innovative methods. For example, many organic chemists feel that biocatalysis shouldn’t be considered organic synthesis, since it employs enzymes and cofactors instead of more traditional catalysts and reagents, even though organic synthesis and biocatalysis both address the same goal: making molecules. While it’s somewhat inevitable that years of lab work leaves one with a certain affection for the methods one employs, it’s also irrational.
</p>

<p>
Now, one might reasonably argue that precisely delimiting where one scientific field begins and another ends is a pointless exercise. Who’s to say whether biocatalysis is better viewed as the domain of organic chemistry or biochemistry? While this is fair, it’s also true that the scientific field one formally belongs to matters a great deal. If society deems me an organic chemist, then overwhelmingly it is other organic chemists who will decide if I get a PhD, if I obtain tenure as a professor, and if my proposals are funded.<sup><a href="#fn4">4</a></sup>
</p>

<p>
Given that the success or failure of my scientific career thus depends on the opinion of other organic chemists, it starts to become apparent why domain arbitrage is difficult. If I attempt to solve problems in organic chemistry by introducing techniques from another field, it’s likely that my peers will be confused or skeptical by my work, and hesitate to accept it as “real” organic chemistry (see, for instance, the biocatalysis scenario above). Conversely, if I attempt to solve problems in other domains with the tools of organic chemistry, my peers will likely be uninterested in the outcome of the research, even if they approve of the methods employed. So from either angle domain arbitrage is disfavored.
</p>

<p>
The factors discussed here don’t serve to completely halt domain arbitrage, as successful arbitrageurs like Katie Bouman or Frances Arnold demonstrate, but they do act to inhibit it. If we accept the claim that domain arbitrage is good, and we should be working to make it more common, what then should we do to address these problems? One could envision a number of structural solutions, which I won’t get into here, but on a personal level the conclusion is obvious: if you care about performing cutting-edge research, it’s important to learn things outside the narrow area that you specialize in and not silo yourself within a single discipline.
</p>

<i>
Thanks to Shlomo Klapper and Darren Zhu for helpful discussions. Thanks also to Ari Wagen, Eric Gilliam, and Joe Gair for editing drafts of this post; in particular, Eric Gilliam pointed out the cultural factors discussed in the conclusion of the post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  I did not coin this term—credit goes to Shlomo Klapper. I do think this is the first time it's been used in writing, however.
  </li>
  <li id="fn2">
  I'll actually go a step farther and propose the Strong Theorem of Domain Arbitrage: All non-incremental scientific discoveries arise either from domain arbitrage or random chance. I don't want to defend it here, but I think there's a reasonable chance that this is true.
  </li>
  <li id="fn3">
  Collaborations between differently skilled labs help with this problem, but the logistical and practical challenges involved in collaboration make this an inefficient solution. Plus, the same cultural challenges still confront the individual contributors. 
  </li>
  <li id="fn4">
  <a href=https://twitter.com/Stephen_Curry/status/1637496308935057409>This tweet</a>, quoting the Nielsen/Qiu metascience essay <a href=https://corinwagen.github.io/public/blog/20221026_structural_diversity.html>which I wrote about before</a>, seems relevant.
  </li>
</ol>
]]></description>
              <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Computational NMR Prediction: A Microreview</title>
              <link>public/blog/20230314_nmr_microreview.html</link>
              <description><![CDATA[
<p>
Recently, I’ve been working to assign the relative configuration of some tricky diastereomers, which has led me to do a bit of a deep dive into the world of computational NMR prediction. Having spent the last week or so researching the current state-of-the-art in simulating experimental <sup>1</sup>H NMR spectra, I’m excited to share some of my findings.
</p>

<p>
My main resource in this quest has been <a href=https://www.mdpi.com/1420-3049/28/6/2449>a new NMR benchmarking paper</a>, published on March 7th by authors from Merck (and a few other places). Why this paper in particular? Although there have been many NMR benchmarks, not all of these papers are as useful as they seem. Broadly speaking, there are two ways to benchmark NMR shifts: (1) against high-level computed results or (2) against experimental NMR shifts.
</p>

<p>
The first strategy seems to be popular with theoretical chemists: NMR shifts at a very high level of theory are presumably very accurate, and so if we can just reproduce those values with a cheap method, we will have solved the NMR prediction problem. Of course, effects due to solvation and vibrational motion will be ignored, but these effects can always be corrected for later. In contrast, the second strategy is more useful for experimental chemists: if the calculation is going to be compared to experimental NMR spectra in CDCl<sub>3</sub> solution, the match with experiment is much more important than the gas-phase accuracy of the functional employed.
</p>

<p>
Not only are these two approaches different in theory, they yield vastly different results in practice, as is nicely illustrated by the case of the double-hybrid functional DSD-PBEP86. DSD-PBEP86 was <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.8b00624>first reported in 2018</a> by Frank Neese and coworkers, who found it to be much superior to regular DFT methods or MP2-type wavefunction methods at reproducing CCSD(T) reference data.<sup><a href=#fn1>1</a></sup> <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.1c00919>A subsequent benchmark</a> by Kaupp and coworkers looked at a much larger set of compounds and confirmed that DSD-PBEP86 was indeed superior at reproducing CCSD(T) data, with a mean absolute error (MAE) for <sup>1</sup>H of 0.06 ppm. In contrast, <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.1c00604>de Oliveira and coworkers</a> found that DSD-PBEP86 and related double-hybrid methods were much worse at predicting experimental <sup>1</sup>H NMR shifts, with a MAE of 0.20 ppm, making them no better than conventional DFT approaches.
</p>

</p>
The difference between these two mindsets is nicely demonstrated by Kaupp’s paper, which dismisses de Oliveira’s work as suffering from “methodological inadequacies” and states:
</p>

<blockquote>
[Benchmarking] can be done by comparing approximative calculations to experimental data or to data computed using high-level ab initio methodologies. The latter helps to eliminate a number of factors that often complicate the direct comparison against experiment, such as environmental, ro-vibrational, or thermal contributions (possibly also relativistic effects, see below).
</blockquote>

<p>
While Kaupp is correct that using gas-phase CCSD(T) data does eliminate “environmental” effects (e.g. from solvent), it’s not clear that these effects always ought to be eliminated! Although directly optimizing a computational method to reproduce a bunch of ill-defined environmental effects is perhaps inelegant, it’s certainly pragmatic.
</p>

<p>
The authors of the 2023 benchmark create a new set of well-behaved reference compounds that avoid troublesome heavy-atom effects (poorly handled by most conventional calculations) or low-lying conformational equilibria, and re-acquire experimental spectra (in chloroform) for every compound in the set. They then score a wide variety of computational methods against this dataset: functionals, basis sets, implicit solvent methods, and more.
</p>

<p>
In the end, <a href=https://pubs.acs.org/doi/10.1021/ct6001016>Cramer’s WP04 functional</a> is found to be best, which is perhaps unsurprising given that it was specifically optimized for the prediction of <sup>1</sup>H shifts in chloroform.<sup><a href="#fn2">2</a></sup> The WP04/6-311++G(2d,p)/PCM(chloroform) level of theory is optima, giving an MAE of 0.08 ppm against experiment, but WP04/jul-CC-PVDZ/PCM(chloroform) is cheaper and not much worse. B3LYP-D3/6-31G(d) works fine for geometry optimization, as do wB97X-D/6-31G(d) and M06-2X/6-31G(d).
</p>

<p>
Based on these results, my final workflow for predicting experimental proton spectra is:
</p>

<ol>
<li><a href=https://corinwagen.github.io/public/blog/20221219_low_code_csearch.html>Run a conformational search using <i>crest</i></a>.</li>
<li>Optimize each conformer using B3LYP-D3BJ/6-31G(d).</li>
<li>Remove duplicate conformers with <span class=code>cctk.ConformationalEnsemble.eliminate_redundant()</span>.</li>
<li>Predict NMR shifts for each conformer using WP04/6-311++G(2d,p)/PCM(chloroform).</li>
<li>Combine conformer predictions through <a href=https://corinwagen.github.io/public/blog/20221228_boltzmann_error.html>Boltzmann weighting</a>, and apply a linear correction.</li>
</ol>

<p>
For small molecules, this workflow runs extremely quickly (just a few hours from start to finish), and has produced good-quality results that solved the problem I was trying to solve.
</p>

<p>
Nevertheless, the theoreticians have a point—although WP04 can account for a lot of environmental effects (essentially by overfitting to experimental data), there are plenty of systems for which this pragmatic approach cannot succeed. For instance, the DELTA50 dataset intentionally excludes molecules which might exhibit concentration-dependent aggregation behavior, which includes basically anything capable of hydrogen bonding or π–π stacking! If we hope to get beyond a certain level of accuracy, it seems likely that physically correct models of NMR shieldings, solvent effects, and aggregation will be necessary.
</p>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  CCSD(T) NMR shifts have to be computed in <a href=https://cfour.uni-mainz.de/cfour/>CFOUR</a>.
  </li>
  <li id="fn2">
  The WP04 functional is not technically in Gaussian, but can be employed with the following route card: <span class=code>#p nmr=giao BLYP IOp(3/ 76=1000001189,3/77=0961409999,3/78=0000109999) 6-311++G(2d,p) scrf=(pcm,solvent=chloroform)</span>.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Optimizing Python</title>
              <link>public/blog/20230309_optimizing_python.html</link>
              <description><![CDATA[
<p>
Python is an easy language to write, but it’s also <a href=https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html>very</a> <a href=https://programming-language-benchmarks.vercel.app/problem/nbody>slow</a>. Since it’s a dynamically typed and interpreted language, every Python operation is much slower than the corresponding operation would be in C or FORTRAN—every line of Python must be interpreted, type checked, and so forth (see this <a href=https://www.codingdojo.com/blog/interpreters-run-python-code>little overview</a> of what the Python interpreter does).
</p>

<p>
Fortunately for those of us who like programming in Python, there are a number of different ways to make Python code faster. The simplest way is just to use NumPy, the <i>de facto</i> standard for any sort of array-based computation in Python; NumPy functions are written in C/C++, and so are much faster than the corresponding native Python functions.
</p>

<p>
Another strategy is to use a just-in-time compiler to accelerate Python code, like Jax or Numba. This approach incurs a substantial <i>O</i>(1) cost (compilation) but makes all subsequent calls orders of magnitude faster. Unfortunately, these libraries don’t support all possible Python functions or external libraries, meaning that sometimes it’s difficult to write JIT-compilable code.
</p>

<p>
How do these strategies fare on a real-world problem? I selected pairwise distance calculations for a list of points as a test case; this problem is pretty common in a lot of scientific contexts, including calculating electrostatic interactions in molecular dynamics or quantum mechanics.
</p>

<p>
We can start by importing the necessary libraries and writing two functions. The first function is the “naïve” Python approach, and the second uses <span class=code>scipy.spatial.distance.cdist</span>, one of the most <a href=https://github.com/ekwan/cctk/blob/master/cctk/molecule.py#L170>overpowered</a> functions I’ve encountered in any Python library.
</p>

<pre class=code-block>
import numpy as np
import numba
import cctk
import scipy

mol = cctk.XYZFile.read_file("30_dcm.xyz").get_molecule()
points = mol.geometry.view(np.ndarray)

def naive_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i, A in enumerate(points):
        for j, B in enumerate(points):
            distances[i,j] = np.linalg.norm(A-B)
    return distances

def scipy_get_distance(points):
    return scipy.spatial.distance.cdist(points,points)
</pre>

<p>
If we score these functions in Jupyter, we can see that <span class=code>cdist</span> is almost 2000 times faster than the pure Python function!
</p>

<pre class=code-block>
%%timeit
naive_get_distance(points)

103 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre>

<pre class=code-block>
%%timeit
scipy_get_distance(points)

55.2 µs ± 2.57 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</pre>

<p>
In this case, it’s pretty obvious that we should just use <span class=code>cdist</span>. But what if there wasn’t a magic built-in function for this task—how close can we get to the performance of <span class=code>cdist</span> with other performance optimizations?
</p>

<p>
The first and most obvious optimization is simply to take advantage of the symmetry of the matrix, and not compute entries below the diagonal. (Note that this is sort of cheating, since <span class=code>cdist</span> doesn’t know that both arguments are the same.)
</p>

<pre class=code-block>
def triangle_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i in range(N):
        for j in range(i,N):
            distances[i,j] = np.linalg.norm(points[i]-points[j])
            distances[j,i] = distances[i,j]
    return distances
</pre>

<p>
As expected, this roughly halves our time:
</p>

<pre class=code-block>
%%timeit
triangle_get_distance(points)

57.6 ms ± 409 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre>

<p>
Next, we can use Numba to compile this function. This yields roughly a 10-fold speedup, bringing us to about two orders of magnitude slower than <span class=code>cdist</span>.
</p>

<pre class=code-block>
numba_triangle_get_distance = numba.njit(triangle_get_distance)
</pre>

<pre class=code-block>
%%timeit
numba_triangle_get_distance(points)

5.74 ms ± 36.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre>

<p>
Defining our own norm with Numba, instead of using <span class=code>np.linalg.norm</span>, gives us another nice boost:
</p>

<pre class=code-block>
def custom_norm(AB):
    return np.sqrt(AB[0]*AB[0] + AB[1]*AB[1] + AB[2]*AB[2])

numba_custom_norm = numba.njit(custom_norm)

def cn_triangle_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i in range(N):
        for j in range(i,N):
            distances[i,j] = numba_custom_norm(points[i] - points[j])
            distances[j,i] = distances[i,j]
    return distances

numba_cn_triangle_get_distance = numba.njit(cn_triangle_get_distance)
</pre>

<pre class=code-block>
%%timeit
numba_cn_triangle_get_distance(points)

1.35 ms ± 21.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
What about trying to write this program using only vectorized NumPy functions? This takes a bit more creativity; I came up with the following function, which is a bit memory-inefficient but still runs quite quickly:
</p>

<pre class=code-block>
def numpy_get_distance(points):
    N = points.shape[0]

    points_row = np.repeat(np.expand_dims(points,1), N, axis=1)
    points_col = np.repeat(np.expand_dims(points,0), N, axis=0)

    sq_diff = np.square(np.subtract(points_row, points_col))
    return np.sqrt(np.sum(sq_diff, axis=2))
</pre>

<pre class=code-block>
%%timeit
numpy_get_distance(points)

426 µs ± 6.34 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
Unfortunately, calling <span class=code>np.repeat</span> with arguments <a href=https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html>isn’t supported by Numba</a>, meaning that I had to get a bit more creative to write a Numba-compilable version of the previous program. The best solution that I found involved a few array reshaping operations, which are (presumably) pretty inefficient, and the final code only runs a little bit faster than the Numpy-only version.
</p>

<pre class=code-block>
def numpy_get_distance2(points):
    N = points.shape[0]

    points_row = np.swapaxes(points.repeat(N).reshape((N,3,N)),1,2)
    points_col = np.swapaxes(points_row,0,1)

    sq_diff = np.square(np.subtract(points_row, points_col))
    return np.sqrt(np.sum(sq_diff, axis=2))

numba_np_get_distance2 = numba.njit(numpy_get_distance2)
</pre>

<pre class=code-block>
%%timeit
numba_np_get_distance2(points)

338 µs ± 4.11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
I tried a few other approaches, but ultimately wasn’t able to find anything better; in theory, splitting the loops into chunks could improve cache utilization, but in practice anything clever I tried just made things slower.
</p>

<p>
In the end, we were able to accelerate our code about 250x by using a combination of NumPy and Numba, but were unable to match the speed of an optimized low-level implementation. Maybe in a future post I’ll drop into C or C++ and see how close I can get to the reference—until then, I hope you found this useful.
</p>

<p>
(I’m sure that there are ways that even this Python version could be improved; I did not even look at any other libraries, like Jax, Cython, or PyPy. Let me know if you think of anything clever!)
</p>
]]></description>
              <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Gilliam and Girard on Scientific Innovation</title>
              <link>public/blog/20230228_gilliam_and_girard.html</link>
              <description><![CDATA[
<p>
Eric Gilliam, whose work on the history of MIT I highlighted before, has <a href=https://freaktakes.substack.com/p/irving-langmuir-the-general-electric>a nice piece</a> looking at Irving Langmuir’s time at the General Electric Research Laboratory and how applied science can lead to advances in basic research.
</p>

<p>
Gilliam recounts how Langmuir started working on a question of incredible economic significance to GE—how to make lightbulbs last longer without burning out—and after embarking on a years-long study of high-temperature metals under vacuum, not only managed to solve the lightbulb problem (by adding an inert gas to decrease evaporation and coiling the filament to prevent heat loss), but also starting working on the problems he would later become famous for studying. In <a href=https://www.jstor.org/stable/16349>Langmuir’s own words</a>:
</p>

<blockquote>
The work with tungsten filaments and gases done prior to 1915 [at the GE laboratory] had led me to recognize the importance of single layers of atoms on the surface of tungsten filaments in determining the properties of these filaments.
</blockquote>

<p>
Indeed, Langmuir was awarded the <a href=https://www.nobelprize.org/prizes/chemistry/1932/summary/>1932 Nobel Prize in Chemistry</a> “for his discoveries and investigations in surface chemistry.”
</p>

<figure>
  <img class="centered-img" src=https://upload.wikimedia.org/wikipedia/commons/4/4d/Irving_Langmuir_and_Guglielmo_Marconi_in_lab.jpg style="width:350px;" />
  <figcaption> 
  Langmuir in the GE Research Laboratory.
  </figcaption>
</figure>

<p>
Nor were lightbulbs the only thing Langmuir studied at GE: he invented a greatly improved form of vacuum pump, invented a hydrogen welding process used to construct vacuum-tight seals, and employed thin films of molecules on water to determine accurate molecular sizes with unprecedented accuracy. Gilliam argues that this tremendous productivity can in part be attributed to the fact that Langmuir’s work was in constant contact with practical problems, which served as a source of scientific inspiration:
</p>

<blockquote>
In a developed world that is not exactly beset by scarcity and hardship anymore, it is hard to come up with the best areas to explore out of thin air. Pain points are not often obvious. Fundamental researchers can benefit massively from going to a lab mostly dedicated to making practical improvements to things like light bulbs and pumps and observing/asking questions. It is, frankly, odd that we normalized a system in which so many of our fundamental STEM researchers are allowed to grow so disjoint from the applied aspects of their field in the first place.
</blockquote>

<p>
And <a href=https://www.jstor.org/stable/16349>Langmuir himself</a> seems to agree:
</p>

<blockquote>
As [the GE] laboratory developed it was soon recognized that it was not practicable nor desirable that such a laboratory should be engaged wholly in fundamental scientific research. It was found that at least 75 per cent of the laboratory must be devoted to the development of the practical applications. It is stimulating to the men engaged in fundamental science to be in contact with those primarily interested in the practical applications.
</blockquote>

<p>
Let’s bring in our second thinker. Last weekend, I had the privilege of attending a lecture by Johnathan Bi on Rene Girard and the philosophy of innovation, which discussed (among other things) how a desire for “disruption” above all else actually makes innovation more difficult. To quote Girard’s <a href=https://www.jstor.org/stable/3684663>“Innovation and Repetition,”</a> which Bi discussed at length:
</p>

<blockquote> 
The main prerequisite for real innovation is a minimal respect for the past and the mastery of its achievements, i.e., mimesis. To expect novelty to cleanse itself of imitation is to expect a plant to grow with its roots up in the air. In the long run, the obligation always to rebel may be more destructive of novelty than the obligation never to rebel.
</blockquote>

<p>
What does this mean? Girard is describing two ways in which innovation can fail. The first is quite intuitive—if we hew to tradition too much, if we have an excessive respect for the past and not a “minimal respect,” we’ll be afraid to innovate. This is the oft-derided state of stagnation.
</p>

<p>
The second way in which we can fail to innovate, however, is a bit more subtle. Girard is saying that innovation also requires a mastery of the past’s achievements; we can’t simply ignore tradition, we have to understand what exists before we can innovate on top of it. Otherwise we will be, in Girard’s words, like a plant “with its roots up in the air.” All innovation has to occur within its proper context—to quote Tyler Cowen, <a href=https://marginalrevolution.com/marginalrevolution/2022/02/context-is-that-which-is-scarce-2.html>“context is that which is scarce.”</a>
</p>

<p>
This might seem a little silly. Innovation, “the introduction of new things, ideas or ways of doing something” (<a href=https://www.oxfordlearnersdictionaries.com/us/definition/english/innovation>Oxford</a>), at first inspection seems not to depend on tradition at all. But novelty with no hope of improvement over the status quo is simply a cry for attention; wearing one’s shoes on the wrong feet may be unusual, but is unlikely to win one renown as a great innovator. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230228_tweet.png" style="width:500px;" />
  <figcaption> 
  When innovation, devoid of context, becomes the highest virtue... (I added this to the post a few hours late, sorry.)
  </figcaption>
</figure>

<p>
What does this mean for scientific innovation, and how does this connect to Gilliam’s thoughts about Langmuir and the GE Research Laboratory? I’d argue that much of our fundamental research today, even that which is novel, lacks the context necessary to be transformatively innovative. Often the most impactful discoveries aren’t those which self-consciously aim to be <i>Science</i> or <i>Nature</i> papers, but those which simply aim to address outstanding problems or investigate anomalies. For instance, our own lab’s interest in hydrogen-bond-donor organocatalysis <a href=https://pubs.acs.org/doi/pdf/10.1021/ja980139y>was initiated</a> by the unexpected discovery that omitting the metal from an ostensibly metal-catalyzed Strecker reaction increased the enantioselectivity. Girard again:
</p>

<blockquote>
The principle of originality at all costs leads to paralysis. The more we celebrate "creative and enriching" innovations, the fewer of them there are.
</blockquote>

<p>
Langmuir’s example shows us a different path towards innovation. If we set out to investigate and address real-world problems of known practical import, without innovation in mind, Gilliam and Girard argue that we’ll be more innovative than if we make innovation our explicit goal. I don’t have a concrete policy recommendation to share here, but some of my other blog posts on <a href=https://corinwagen.github.io/public/blog/20220907_mit_elegy.html>applied research at MIT</a> and <a href=https://corinwagen.github.io/public/blog/20230215_science_engineering.html>the importance of engineering</a> perhaps hint at what positive change might look like.
</p>

<em>
In accordance with the themes of this piece, my interpretation of Girard pretty much comes straight from Johnathan Bi. He has a lecture on Youtube where he discusses these ideas: <a href=https://youtu.be/qdWHcBBCaww?t=3906>here’s a link</a> to the relevant segment, which is the only part I’ve watched.
</em>
]]></description>
              <pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Models and Oracles</title>
              <link>public/blog/20230221_models_and_oracles.html</link>
              <description><![CDATA[
<p>
When thinking about science, I find it helpful to divide computations into two categories: models and oracles.
</p>

<p>
In this dichotomy, models are calculations which act like classic ball-and-stick molecular models. They illustrate that something is geometrically possible—that the atoms can literally be arranged in the proper orientation—but not much more. No alternative hypotheses have been ruled out, and no unexpected insights have emerged. A model has no intelligence of its own, and only reflects the thought that the user puts into it.
</p>

<p>
This isn’t bad! Perhaps the most striking example of the utility of models is <a href=https://pubs.acs.org/doi/abs/10.1021/ja00713a007>Tolman’s original cone angle report</a>, where he literally made a wooden model of different phosphine ligands and measured the cone angle with a ruler. The results are excellent!
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_tolman.png" style="width:350px;" />
  <figcaption> 
  Figure 1, from Tolman’s paper.
  </figcaption>
</figure>

<p>
In contrast, an oracle bestows new insights or ideas upon a petitioner—think <a href=https://en.wikipedia.org/wiki/Pythia>the Oracle at Delphi</a>. This is what a lot of people imagine when they think of computation: we want the computer to predict totally unprecedented catalysts, or figure out the mechanism without any human input. We bring our problems to the superintelligence, and it solves them for us. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_miola.jpeg" style="width:450px;" />
  <figcaption> 
  <i>The Oracle</i>, by Camille Miola (1880). Picture from Wikimedia Commons.
  </figcaption>
</figure>


<p>
In reality, every simulation is somewhere between these two limiting extremes. No matter how hard you try, a properly executed DFT calculation will not predict formation of a methyl cation to be a low-barrier process—the computational method used understands enough chemistry to rule this out, even if the user does not. On the flip side, even the most sophisticated calculations all involve some form of human insight or intuition, either explicitly or implicitly. We’re still very far away from the point where we can ask the computer to generate the structures of new catalysts (or medicines) and expect reasonable, trustworthy results. But that’s ok; there’s a lot to be gained from lesser calculations! There’s no shame in generating computational models instead of oracles.
<p>

<p>
What’s crucial, though, is to make sure that everyone—practitioners, experimentalists, and readers—understands where a given calculation falls on the model–oracle continuum. An expert might understand that a semiempirical AIMD study of reaction dynamics is likely to be only qualitatively correct (if that), but does the casual reader? I’ve talked to an unfortunate number of experimental chemists who think a single DFT picture means that we can “predict better catalysts,” as if that were a button in GaussView. The appeal of oracles is seductive, and we have to be clear when we’re presenting models instead. (This ties into <a href=https://corinwagen.github.io/public/blog/20220810_viewpoints_on_simulation.html>my piece about computational nihilism.</a>)
<p>

<p>
Finally, this piece would be incomplete if I didn’t highlight Jan Jensen and co-workers’ <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.202218565>recent work</a> on automated design of catalysts for the Morita–Baylis–Hillman reaction. The authors use a generative model to discover tertiary amines with lower DFT-computed barriers than DABCO (the usual catalyst), and then experimentally validate one of their candidates, finding that it is indeed almost an order-of-magnitude faster than DABCO. It’s difficult to underscore how groundbreaking this result is; as the authors dryly note, “We believe this is the first experimentally verified <i>de novo</i> discovery of an efficient catalyst using a generative model.” On the spectrum discussed above, this is getting pretty close to “oracle.”
<p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_jensen.png" style="width:450px;" />
  <figcaption> 
  Figure 3 from the paper, illustrating discovery of new catalysts.
  </figcaption>
</figure>

<p>
Nevertheless, the choice of model system illustrates how far the field still has to go. The MBH reaction is among the best-studied reactions in organic chemistry, as illustrated by <a href=https://pubs.acs.org/doi/10.1021/ja5111392>Singleton’s 2015 mechanistic <i>tour de force</i></a> (and references therein, and subsequent work), so Jensen and co-workers could have good confidence that the transition state they were studying was correct and relevant. Furthermore, as I understand it, the MBH reaction can be catalyzed by just about any tertiary amine—there aren’t the sort of harsh activity cliffs or arcane structural requirements that characterize many other catalytic reactions. Without either of these factors—well-studied mechanism or friendly catalyst SAR—I doubt this work would be possible. 
<p>

<p>
This point might seem discouraging, but I mean it in quite the opposite way. <i>De novo</i> catalyst design isn’t impossible for mysterious and opaque reasons, but for quite intelligible reasons—mechanisms are complicated, catalysts are hard to design, and we just don’t understand enough about what we’re doing, experimentally or computationally. What Jensen has shown us is that, if we can address these issues, we can expect to start converging on oracular results. I find this very exciting!
<p>

<em>
Jan Jensen was kind enough to <a href=https://twitter.com/janhjensen/status/1628063110572507136>reply to this post on Twitter</a> with a few thoughts and clarifications, which are worth reading.
</em>

]]></description>
              <pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Science and Engineering</title>
              <link>public/blog/20230215_science_engineering.html</link>
              <description><![CDATA[
<blockquote><i>
In science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it.
</i></blockquote>

<p style="text-align:right;">
—Richard Hamming
</p>

<p>
What’s the difference between science and engineering? 
</p>

<p>
Five years ago, I would have said something along the lines of “engineers study known unknowns, scientists study unknown unknowns” (with apologies to <a href=https://en.wikipedia.org/wiki/There_are_unknown_unknowns>Donald Rumsfeld</a>), or made a distinction between expanding the frontiers of knowledge (science) and settling already-explored territory (engineering).
</p>

<p>
These thoughts seem broadly consistent with what others think. Boston University’s <a href=https://www.bu.edu/eng/about-eng/meet-the-dean/engineering-is-not-science/>College of Engineering</a> says:
</p>

<blockquote>
Engineers are not a sub-category of scientists. So often the two terms are used interchangeably, but they are separate, albeit related, disciplines. Scientists explore the natural world and show us how and why it is as it is. Discovery is the essence of science. Engineers innovate solutions to real-world challenges in society. While it is true that engineering without science could be haphazard; without engineering, scientific discovery would be a merely an academic pursuit.
</blockquote>

<p>
And the <a href=https://www.nspe.org/resources/press-room/resources/frequently-asked-questions-about-engineering>National Society of Professional Engineers</a> says:
<p>

<blockquote>
Science is knowledge based on observed facts and tested truths arranged in an orderly system that can be validated and communicated to other people. Engineering is the creative application of scientific principles used to plan, build, direct, guide, manage, or work on systems to maintain and improve our daily lives.
</blockquote>

<p>
As I’ve started thinking more about the structure of the modern research system, and what its proper scope and purpose should be, I’ve grown increasingly skeptical of these distinctions. The claim I want to make in this post is that, following the above definitions of engineering, <u>most chemistry is engineering</u>. I don’t think this is bad! In fact, I think that many chemists could benefit from borrowing from an engineering mindset, and should consider incorporating this perspective in their self-conception.
<p>

<h2>
Much of Organic Chemistry is Engineering
</h2>

<p>
I want to return to the BU and NSPE definitions, because I think they’re concise and well-written, and take as gospel that scientists “explore the natural world and show us how and why it is as it is,” while engineers “innovate solutions to real-world challenges in society” (we’ll revisit issues of definition later). In short, developing something you want other people to use makes you an engineer. Which branches of modern organic chemistry are science, and which are engineering? 
<p>

<p>
Method development—one of my core interests—seems like a good candidate for “engineering.” Researchers in this space identify unsolved problems in organic synthesis, develop methods or catalysts to solve these problems, and then (in many cases) advertise, license, &amp; sell their solutions to consumers! (If you don’t believe me, just look at the <a href=https://www.sigmaaldrich.com/US/en/collections/professor-product-portal>“Organic Synthesis” tab</a> of the Sigma-Aldrich Professor Product Portal.) If these products weren’t molecules and were instead mechanical gadgets, nothing about this would be obviously scientific.
<p>

<p>
And the problems chosen are identified almost purely on the basis of what might be useful to potential users. There’s no clearer illustration of this than the recent gold rush to identify synthetic routes to bicyclic arene bioisosteres, which are useful in medicinal chemistry. Five years ago, I can’t think of a single paper making these compounds; now, I can find nine in high-profile journals just from the past year or so (<a href=https://www.nature.com/articles/s41586-022-05290-z.pdf>1</a>,
<a href=https://www.nature.com/articles/s41557-022-00979-0.pdf>2</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09733>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c11501>4</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acscatal.2c03498>5</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.1c03681>6</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03606>7</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202205103>8</a>,
<a href=https://www.nature.com/articles/s41557-023-01135-y>9</a>).
<p>

<p>
Mechanistic and physical organic chemistry—another love of mine—present a tougher case, since in most cases the outcome of the studies is only knowledge. But I’m still not sure that this makes this field scientific! Let me illustrate why with an example. 
<p>

<p>
An automotive engineer may be confused by why a given transmission is not working. He/she may formulate a hypothesis, take the transmission apart, perform various experiments on its constituent pieces, validate or disprove the initial hypothesis, and generally conduct a thorough mechanistic investigation to understand the origin of the problem. But does that make him/her a scientist?
<p>

<p>
The answer, I think, is no. The subject matter is not scientific, so no amount of scientific thinking can make the work science. Similarly, I’d argue that investigating the mechanism of a system invented and developed by humans—like a Pd-catalyzed cross-coupling reaction—doesn’t count as science. (Does application of the scientific method automatically make one a scientist? See below for a continued discussion.)
<p>

<p>
In contrast, something that I think is a truly scientific area of investigation is the study of enzyme structure and function. Despite extensive study and many Nobel prizes, we’re still learning about how enzymes operate and how they achieve such extraordinary reactivity and selectivity. (For an example of this sort of work, see Arieh Warshel’s <a href=https://pubs.acs.org/doi/full/10.1021/cr0503106>review</a> on electrostatic effects in enzyme catalysis, and references therein.)
<p>

<p>
I don’t think I understand all areas of chemistry well enough to fairly judge whether they’re better understood as engineering or science, so I’ll leave this as an exercise to the reader: What motivates your research? Are you mainly driven by a desire to understand the natural order of things, or do you aim to develop technology to make the world better? Both are important, and neither answer is bad—but if your main goal is inventing a new molecule, material, algorithm, or medicine, you might consider thinking of yourself as more of an engineer than a scientist.
<p>

<h2>
Do Different Definitions Clarify Matters?
</h2>

<p>
Since the claim that “most chemistry is engineering” is weird, we might consider alternative definitions to solve this problem.
<p>

<p>
One appealing definition: “a scientist is anyone who uses the scientific method.” As I discussed above, in the case of the automotive engineer, lots of people use the scientific method who clearly aren’t scientists: engineers, yes, but also detectives, doctors, and many other people. Indeed, according to this definition almost anyone who acquires data to shed light on a problem is “doing science.” So I don’t think this is a very good definition.
<p>

<p>
Another definition might be: “if you’re closely involved with science, you’re a scientist, even if the work you’re doing isn’t literally pushing the frontiers of knowledge forward.” I’m sympathetic to this definition, but I still find it hard to separate scientists and engineers here. What makes an engineer optimizing a new flow reactor less of a scientist than the chemist optimizing a new catalyst? Are they both scientists? What about people who work on chip design and fabrication, or people who design analytical instruments, or people who use them? I can’t find any clean way to divide scientists from engineers that recapitulates the conventional usage of the terms.
<p>

<h2>
Why Does This Matter?
</h2>

<p>
I think the root of this confusion is that <u>the nature of scientific fields has changed over the past half-century</u>. Organic chemistry hasn’t always been largely engineering; a century ago, the structure of natural products and the nature of the chemical bond were mysteries, truly the domain of science, and these issues were studied by chemists. As we’ve grown to understand our field better and better, our work has shifted from science to engineering—the true mysteries in chemistry are now few and far between, and the challenge facing today’s chemists is how to use centuries of accumulated knowledge to better society. But because of our lineage, we think of ourselves as scientists, and have managed to disguise the true nature of our work so well that we’ve deceived even ourselves.
</p>

<p>
By this point, it should be obvious that I don’t think science is superior to engineering. In fact, I’m glad to work in an area that’s largely engineering! But the way that an engineer ought to approach their work is different from the way a scientist ought to approach their work. In writing this piece, I came across a 1996 <a href=https://dl.acm.org/doi/10.1145/227234.227243>article</a> by Frederick Brooks, who argued that computer science was better understood as an engineering discipline and defended the importance of this claim: 
</p>

<blockquote>
If our discipline has been misnamed, so what? Surely computer science is a harmless conceit. What’s in a name? Much. Our self-misnaming hastens various unhappy trends.
<br><br>
First, it implies that we accept a perceived pecking order that respects natural scientists highly and engineers less so, and that we seek to appropriate the higher station for ourselves. That is a self-serving gambit, hence dubious….
<br><br>
Second, sciences legitimately take the discovery of facts and laws as a proper end in itself. A new fact, a new law is an accomplishment, worthy of publication…. But in design, in contrast with science, novelty in itself has no merit. If we recognize our artifacts as tools, we test them by their usefulness and their costs, not their novelty.
<br><br>
Third, we tend to forget our users and their real problems, climbing into our ivory towers to dissect tractable abstractions of those problems, abstractions that may have left behind the essence of the real problem. 
</blockquote>

<p>
I think Brooks’s summary is simple and elegant. If we judge the value of our work based on the utility of our tools, rather than the novelty of our ideas, we’ll spend our time on different problems and get excited about different advances. There’s room for both scientists and engineers in chemistry—but at the margin, I think our field would benefit from becoming less like science, and more like engineering.
</p>

<i>
Thanks to Michael Nielsen, Joe Gair, Ari Wagen, and Michael Tartre for editing drafts of this post. Michael Tartre sent me the Richard Hamming quote, and Phil Brooks sent me his grandfather’s article.
</i>

]]></description>
              <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>2022 Paper(s) of the Year</title>
              <link>public/blog/20230203_poy.html</link>
              <description><![CDATA[
<p>
Every year, our group participates in a “Paper of the Year” competition, where we each nominate five papers and then duke it out in a multi-hour debate. Looking through hundreds of papers in a few weeks is a great exercise: it helps highlight both creativity and its absence, and points towards where the field’s focus might turn next.
</p>

<p>
My picks are very personal—how could they not be?—and also biased towards towards experimental chemistry, owing to what our group focuses on. So, don’t take this as any attempt towards creating an objective list.
</p>

<p>
All the papers I really liked are listed below, with my top five listed in bold:
</p>

<ul>
<li><a href=https://www.science.org/doi/pdf/10.1126/science.abo0039>A lovely electrochemical cross-electrophile coupling</a> (Sevov). Great scope and forms challenging bonds.</li>

<li><a href=https://www.science.org/doi/pdf/10.1126/science.abo4282>Carbon deletion through photochemistry, converting quinolines to indoles</a> (Levin). 
My favorite of the skeletal editing papers, with apologies to the others (<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09616>1</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c10746>2</a>, 
<a href=https://www.science.org/doi/10.1126/science.add1383>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c10570>4</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202202703>5</a>, 
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202208014>6</a>,
<a href=https://pubs.acs.org/doi/10.1021/jacs.2c08464>7</a>). <b>Pick 1/5.</b></li>

<li><a href=https://www.science.org/doi/pdf/10.1126/science.abn1885>High-throughput additive screening reveals a new role for phthalimide in Ni-catalyzed cross-couplings</a> (MacMillan, Dreher).</li>

<li><a href=https://www.science.org/doi/10.1126/science.abo6443>Turning aldehydes into metal carbenoids</a> (Nagib); self-recommending.</li>

<li><a href=https://www.science.org/doi/10.1126/science.add6852>Epimerization at tertiary carbons for “stereochemical editing”</a> (Wendlandt).</li>

<li><a href=https://www.science.org/doi/10.1126/science.ade5320>Setting stereocenters with vinyl carbocations, somehow</a> (Nelson, Houk).</li>

<li><a href=https://www.nature.com/articles/s41586-022-04491-w.pdf>Expanding automated synthesis to Csp<sub>3</sub> centers with “TIDA boronates”</a> (Burke). 
An approach with a real chance of revolutionizing how we approach organic synthesis. <b>Pick 2/5.</b></li>

<li><a href=https://www.nature.com/articles/s41586-022-04524-4.pdf>Some beautiful asymmetric catalysis to form S-chiral sulfinate esters</a> (Tan).</li>

<li><a href=https://www.nature.com/articles/s41586-022-05211-0.pdf>Two</a>
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c05648>papers</a> disclosing the use of photoexcited nitroarenes to perform “ozonolysis without ozone” (Parasram, Leonori).

<li><a href=https://www.nature.com/articles/s41557-022-00895-3.pdf>Enantioselective conversion of carboxylic acids into amino acids</a> (Meggers, Chen).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c00527>Catalytic olefin hydroalkoxylation, through a tri-catalytic system</a> (Ohmiya). Reminds me a little of <a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c04735>Kanai’s tri-catalytic aldehyde allylation</a>; in both cases, it’s hard to believe so many catalysts play nicely together.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c04807>Deoxygenative trifluoromethylation</a> (MacMillan), a reaction which speaks for itself.</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c08778>Functional group-tolerant Kochi–Salamon [2+2] cycloaddition in water, catalyzed by copper sulfate</a> (Burns). Bizarre, unbelievable, outstanding. <b>Pick 3/5.</b> (With apologies to the other 2+2 cycloaddition papers:
<a href=https://www.nature.com/articles/s41586-022-05335-3.pdf>1</a>,
<a href=https://www.nature.com/articles/s41586-022-05342-4.pdf>2</a>,
<a href=https://www.nature.com/articles/s41586-022-04636-x.pdf>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03606>4</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202200725>5</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202208800>6</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202211596>7</a>.)</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c08332>Pd-catalyzed C–H hydroxylation</a> (Yu). Addresses a lot of my misgivings about C–H activation: directed by carboxylic acids, silver-free, scalable, and uses aqueous H<sub>2</sub>O<sub>2</sub> (perhaps the best oxidant possible, after air).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09745>Red-light-mediated C–N coupling, through Os photocatalysis</a> (Rovis). Suppresses a lot of the annoying mass balance issues that plague conventional photoredox.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.1c13151>A nice mechanistic study of an enantioselective C–N coupling reaction</a> (Peters, Fu).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01487>Mechanistic study and an improved phosphetane catalyst for C–N coupling</a> (Radosevich).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01375>Studying the factors that lead to hydroxyl versus chlorine radical transfer in non-heme Fe complexes</a> (Goldberg, de Visser).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c02283>Studying the mechanism of enantioselective enzymatic C–H amination</a> (Yang Yang, Peng Liu). Surprisingly, radical rebound is enantiodetermining! I remember being puzzled by the mechanism of this reaction when it was first published; this is a very satisfying resolution. Cool isotope studies &amp; <i>ab initio</i> molecular dynamics seal the deal. <b>Pick 4/5.</b></li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01356>The latest on photochemistry of Ni oxidative-addition complexes</a> (Hadt). Beautiful physical inorganic chemistry; a lot of the details are beyond me, but clearly important work.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c07099>Mechanistic study of PCET-mediated olefin hydroamination</a> (Knowles).</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c07643>A really beautiful metal-free hydroamination</a> (Wang/Wang).</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c09786>The latest on cobaltacene-based PCET mediators</a> (Peters). Big fan of this program.</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c00675>Using aryl nitriles as sensors for electric field strength</a> (Boxer). 
Also a really cool program: I liked <a href=https://chemrxiv.org/engage/chemrxiv/article-details/626b29f9ed4d8830581c6ed6>this preprint</a> too.
</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c00154>Studying cation–acetonitrile dynamics with 2D IR and isotope labelling</a> (Tokmakoff). I've <a href=https://corinwagen.github.io/public/blog/20220708_apotheosis_of_2d_ir.html>written about this</a> before</li>

<li><a href=https://pubs.acs.org/doi/10.1021/acscatal.1c05802>A very nice <sup>13</sup>C KIE study of the Suzuki reaction, emphasizing the importance of monoligated Pd even when using PPh<sub>3</sub></a> (Vetticatt, Hirschi). And another <a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c02941>cool <sup>13</sup>C KIE study</a> for Hirschi.</li>

<li><a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202114044>The first enantioselective beta-alkoxy elimination I’ve seen!</a> (Streuff)</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03201>An interesting way to functionalize ketones, via phosphorus-mediated umpolung</a> (Ball).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.organomet.2c00504>Investigating a concentration-dependent KIE for protonolysis of (cod)PtMe<sub>2</sub></a> (Bowring).</li>

<li><a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202213056>Biocatalytic N-heterocycle methylation, with unbelievable regioselectivity</a> (Hammer). Hammer also published a <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.202215093>really exciting styrene hydration paper</a>, albeit technically in 2023.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/acscatal.2c04316>Bifunctional redox-active esters add to styrenes to make a variety of heterocycles.</a> (Knowles/Doyle).</li>

<li><a href=https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc02274e>A nice illustration of the importance of conformer sampling workflows</a> (Neese, Bistoni). See also work from 
<a href=https://pubs.rsc.org/en/content/articlepdf/2022/sc/d2sc01714h>Laplaza/Corminboef</a>, 
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01381>Peter Chen</a>, and <a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202113905>Grimme</a>.</li>

<li><a href=https://arxiv.org/pdf/2204.05249.pdf>The latest equivariant neural network potential</a>, faster than <a href=https://www.nature.com/articles/s41467-022-29939-5>their previous work</a> (Kozinsky).</li>

<li><a href=https://arxiv.org/pdf/2010.01196.pdf>Espaloma, a differential neural network forcefield from the OpenForcefield folks.</a></li>

<li><a href=https://chemrxiv.org/engage/chemrxiv/article-details/62b1b0c97da6ce535c19d40c>Studying cycloaddition dynamics with neural network potentials</a> (Young, Duarte).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jpca.2c08301>Studying dynamic control of kinetic product ratios in cyclopropylidine opening using ML-learned potentials</a> (Carpenter).</li>

<li><a href=https://aip.scitation.org/doi/full/10.1063/5.0133026>wB97X-3c, the latest “composite method”</a> (Grimme). If the reported speed and accuracy are true “in the wild,” every computational chemist ought to use this method. <b>Pick 5/5.</b></li>
</ul>
]]></description>
              <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Importance of Integral Screening</title>
              <link>public/blog/20230123_integral_screening.html</link>
              <description><![CDATA[
<p>
For almost all Hartree–Fock-based computational methods, including density-functional theory, the rate-limiting step is calculating electron–electron repulsion. (This isn’t true for semiempirical methods, where matrix diagonalization is generally rate-limiting, or for <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c00876>calculations on very large systems</a>.)
</p>

<p>
When isolated single molecules are the subject of calculations (as opposed to solids or periodic systems), most programs describe electronic structure in terms of atom-centered basis sets, which reduces the electron–electron repulsion problem to one of calculating electron repulsion integrals (ERIs) over quartets of basis shells. Framed this way, it becomes obvious why ERIs are the bottleneck: the number of ERIs will scale as O(N<sup>4</sup>), meaning that millions of these integrals must be calculated even for relatively small molecules.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_dalle.png" style="width:350px;" />
  <figcaption> 
  I tried to get DALL-E to make a visual representation of integral screening; this was the best I got.
  </figcaption>
</figure>

<p>
One big advance in electronic structure calculations was the development of integral screening techniques, the most popular of which is the “Schwartz inequality” (derived from the <a href=https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality>Cauchy–Schwartz inequality</a>, but actually developed by <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540100111>Mario Häser and Reinhart Ahlrichs</a> [<b>EDIT: This is wrong, see correction at end!</b>]). If we denote an ERI over shells A, B, C, and D as (AB|CD), then the Schwartz inequality says:
</p>

<p>
(AB|CD) ≤ (AB|AB)<sup>0.5</sup> (CD|CD)<sup>0.5</sup>
</p>

<p>
This is pretty intuitive: each shell pair will interact with itself most, since it has perfect overlap with itself, and so the geometric mean of the interaction of each shell pair with itself is an upper bound for the interaction between the two shell pairs. (Why would (AB|AB) ever be a small value? Well, A and B might be super far away from each other, and so the “shell pair” has very little overlap is just negligible.)
</p>

<p>
This result is very useful. Since there are many fewer integrals of the form (AB|AB), we can start by calculating all of those, and then use the resulting values to “screen” each shell quartet. If the predicted value is less than some predefined cutoff, the integral is skipped. While these screening methods don’t help much with small molecules, where all of the shells are pretty close to each other, they become crucial for medium-sized molecules and above.
</p>

<p>
(What’s the cutoff value? Orca defaults to 10<sup>-8</sup>, <a href=https://gaussian.com/integral/>Gaussian</a> to 10<sup>-12</sup>, <a href=https://psicode.org/psi4manual/master/autodir_options_c/module__scf.html>Psi4</a> to 10<sup>-12</sup>, and <a href=https://manual.q-chem.com/5.2/Ch4.S3.SS2.html>QChem</a> to 10<sup>-8</sup>–10<sup>-10</sup> depending on the type of calculation.)
</p>

<p>
The Schwartz inequality neglects, however, another way in which (AB|CD) might be very small: if (AB| and |CD) aren’t independently negligible, but are just really far away from each other. One elegant way to address this (out of many) comes from recent-ish work by <a href=https://aip.scitation.org/doi/10.1063/1.4994190>Travis Thompson and Christian Ochsenfeld</a>. They define an intermediate quantity M for each pair of shells, derived from different high-symmetry integrals:
</p>

<p>
M<sub>AC</sub> := (AA|CC) /  ( (AA|AA)<sup>0.5</sup> (CC|CC)<sup>0.5</sup> )
</p>

<p>
M<sub>AC</sub> intuitively represents the distance between the two shells, and is guaranteed to be in the range [0,1]. Thompson and Ochsenfeld then use this quantity to propose an estimate of a shell quartet’s value:
</p>

<p>
(AB|CD) ≈ (AB|AB)<sup>0.5</sup> (CD|CD)<sup>0.5</sup> max(M<sub>AC</sub>M<sub>BD</sub>, M<sub>AD</sub>M<sub>BC</sub>)
</p>

<p>
This is no longer a rigorous upper bound like the Schwartz inequality, but it’s a pretty good estimate of the size of the integral.
</p>

How much of a difference does this make in practice? To test this, I ran HF/STO-3G calculations on dodecane in the fully linear configuration. As shown by <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540030314>Almlöf, Faegri, and Korsell</a>, linear molecules benefit the most from integral screening (since the shells are on average farther apart), so I hoped to see a sizable effect without having to study particularly large molecules.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_graph.png" style="width:450px;" />
  <figcaption> 
  Almlöf, Faegri, and Korsell, Figure 5. This paper is terrific.
  </figcaption>
</figure>

<p>
I compared both the Schwartz (“QQ”) bound and Ochsenfeld’s CSAM bound for integral thresholds ranging from 10<sup>-9</sup> to 10<sup>-13</sup>, and compared the result to a calculation without any integral screening. The total time for the calculation, as a percent of the unscreened time, is plotted below against the error in µHartree (for the organic chemists out there, 1 µH = 0.00063 kcal/mol):
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_graph2.png" style="width:400px;" />
  <figcaption> 
  Comparing CSAM and QQ.
  </figcaption>
</figure>

<p>
A few things are apparent for this data. First, even tight thresholds lead to dramatic speedups relative to the unscreened calculation—and with minimal errors. Secondly, the CSAM bound really does work better than the QQ bound (especially if you ignore the high-error 10<sup>-9</sup> threshold data point). For most threshold values, using CSAM leads to about a 20% increase in speed, at the cost of a 3-fold increase in an already small error. Viewed visually, we can see that the Pareto frontier for CSAM (blue) is just closer to the optimal bottom-left corner than the corresponding frontier for QQ (black).
</p>

<p>
I hope this post serves to explain some of the magic that goes on behind the scenes to make “routine” QM calculations possible. (If you thought these tricks were sneaky, wait until you hear how the integrals that aren’t screened out are calculated!)
</p>

<p>
<i>
<b>CORRECTION</b>: In this post, I credited Mario Häser and Reinhart Ahlrichs with developing the Cauchy–Schwartz method for integral screening. A (famous) theoretical chemist who shall remain nameless reached out to me to correct the record—in fact, Almlöf included an overlap-based screening method in <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540030314>his landmark 1982 paper</a>. To the untrained eye, this appears unrelated to ERI-based screening, but we are using Gaussian basis sets and so “one can therefore write the integrals in terms of overlaps,” meaning that what looked like a different expression is actually the same thing. (Section 9.12 of Helgaker/Jorgensen/Olsen's textbook </i>Molecular Electronic Structure Theory<i>, a book I sadly do not own, apparently discusses this more.) 
</i>
</p>

<p>
<i>
The professor traced this back to <a href=https://aip.scitation.org/doi/abs/10.1063/1.1681647?journalCode=jcp>Wilhite and Eumena</a> in 1974, and ultimately back to the work of Witten in the 1960s. It is a pleasure to get corrected by those you respect, and I welcome any readers who find errors in my writing to reach out; I will do my best to respond and take blame as appropriate.
</i>
</p>
]]></description>
              <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Who Will Optimize The Optimizers?</title>
              <link>public/blog/20230118_meta_optimization.html</link>
              <description><![CDATA[

<p>
While looking over papers from the past year, one theme in particular stood out to me: meta-optimization, or optimizing how we optimize things.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_algo.png" style="width:450px;" />
  <figcaption> 
  A generic optimization (image credit to <a href="https://mpalaourg.me/project/optimization-algorithms/">Georgios Balaouras</a>).
  </figcaption>
</figure>

<p>
Meta-optimization has long been a focus of research in computer science, where new optimization algorithms can have an incredibly high impact (e.g. <a href=https://arxiv.org/pdf/1412.6980.pdf>ADAM</a>, one of the most commonly used optimizers for neural network training). More recently, the advent of directed evolution has made optimization methods a central focus of biocatalysis, since (in many cases) the efficacy of the reaction one discovers is primarily dependent on the efficacy of the optimization method used.
</p>

<p>
In contrast, it seems that meta-optimization has historically attracted less attention from “classic” organic chemists, despite the central importance of reaction optimization to so much of what we do. This post aims to show some of the ways in which this paradigm is changing, and briefly summarize some of what I consider to be the most interesting and exciting recent advances in chemical meta-optimization.
<i>(This is a big and somewhat nebulous area, and I am certainly leaving things out or not paying due homage to everyone. Sorry in advance!)</i>
</p>

<h3>Design of Experiments, and Dealing with Discrete Variables</h3>
<p>
Perhaps the best-known optimization algorithm in chemistry is “design of experiments” (DoE), which uses statistical methods to estimate the shape of a multiparameter surface and find minima or maxima more efficiently than one-factor-at-a-time screening. (DoE is a pretty broad term that gets used to describe a lot of different techniques: for more reading, <a href=https://en.wikipedia.org/wiki/Response_surface_methodology>see</a> <a href=https://en.wikipedia.org/wiki/Central_composite_design>these</a> <a href=https://en.wikipedia.org/wiki/Box%E2%80%93Behnken_design>links</a>.)
</p>

<p>
DoE has been used for a long time, <a href=https://pubs.acs.org/doi/full/10.1021/op500169m>especially in process chemistry</a>, and is very effective at optimizing continuous variables (like temperature, concentration, and equivalents). However, it’s less obvious how DoE might be extended to discrete variables. (<a href=https://pubs.acs.org/doi/pdf/10.1021/ol1020898>This 2010 report</a>, from scientists at Eli Lilly, reports the use of DoE to optimize a palladium-catalyzed pyrazole arylation, but without many details about the statistical methods used.)
</p>

<p>
<a href=https://pubs.acs.org/doi/10.1021/op300275p>A nice paper</a> from Jonathan Moseley and co-workers illustrates why discrete variables are so tricky:
</p>

<blockquote>
How does one compare such different solvents as hexane and DMSO for example? Comparing their relative polarities, which is often related to solubility, might be one way, but this may not be the most important factor for the reaction in question. Additionally, this simple trend may not be relevant in any case, given for example that chlorinated solvents have good solubilising power despite their low-to-medium polarity (as judged by their dielectric constant). On the other hand, the high polarity and solubilising power of alcohols might be compromised in the desired reaction by their protic nature, whilst the “unrelated” hexane and DMSO are both aprotic.
<br><br>
<u>In summary, replacing any one of the discrete parameters with another does not yield a different value on the same axis of a graph, as it would for a continuous parameter; instead it requires a different graph with different axes which may have no meaningful relationship to the first one whatsoever.</u> This means that every single combination of catalyst/ligand/base/solvent is essentially a different reaction for the same two starting materials to produce the same product. <i>(emphasis added)</i>
</blockquote>

<p>
The solution the authors propose is to use principal component analysis (PCA) on molecular descriptors, such as “measurable physical factors (e.g., bp, density, bond length), or calculated and theoretical ones (e.g., electron density, Hansen solubility parameters, Kamlet–Taft solvent polarity parameters),” to convert discrete parameters into continuous ones. This general approach for handling discrete variables—generation of continuous molecular descriptors, followed by use of a dimensionality reduction algorithm—is widely used today for lots of tasks (see for instance <a href=https://pubs.rsc.org/en/content/articlelanding/2016/ob/c5ob01892g>this paper</a> on DoE for solvent screening, and <a href=https://corinwagen.github.io/public/blog/20220926_plotting_diversity.html>my previous post on UMAP</a>).
</p>

<h3>Choosing Intelligent Screening Sets</h3>

<p>
With continuous descriptors for formally discrete variables in hand, a natural next step is to use this data to choose catalyst/substrates that best cover chemical space. (This can be done with several algorithms; see <a href=https://pubs.acs.org/doi/10.1021/jm9700878?ref=PDF>this paper</a> for more discussion) In 2016, this technique was popularized by the Merck <a href=https://pubs.rsc.org/en/content/articlelanding/2016/sc/c5sc04751j>“informer library”</a> approach, which generated sets of aryl boronic esters and aryl halides that could be used to fairly evaluate new reactions against complex, drug-like substrates. (See also <a href=https://pubs.acs.org/doi/full/10.1021/acs.accounts.0c00760>this recent perspective</a> on the Merck informer libraries, and <a href=https://www.pnas.org/doi/full/10.1073/pnas.1409522111>similar work</a> from Matt Sigman a few years earlier.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_merck_informer.png" style="width:600px;" />
  <figcaption> 
  Illustration of the Merck informer library (yellow) in chemical space, compared to drugs (grey) and compounds from recent literature (blue).
  </figcaption>
</figure>

<p>
While the Merck informer libraries were intended to be shared and used by lots of research groups, recently it’s become more common for individual research groups to design their own project-specific screening sets. Abbie Doyle and co-workers <a href=https://pubs.acs.org/doi/10.1021/jacs.1c12203>kicked this off</a> in 2022 by using DFT-based descriptors, UMAP dimensionality reduction, and agglomerative hierarchical clustering to generate a maximally diverse set of commercial aryl bromides. Other groups soon followed suit: <a href=https://pubs.acs.org/doi/10.1021/acscatal.2c01813>Karl Gademann</a> used this approach to study bromotetrazine cross coupling, while Marion Emmert and co-workers at Merck <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c10557>employed similar methods</a> to investigate azole carboxylation. (I’ve also used this approach for substrate selection!)
</p>

<p>
This approach can also be used to design intelligent sets of catalysts/ligands at the outset of a screening project. Using <a href=https://pubs.acs.org/doi/10.1021/jacs.1c09718>their “kraken” dataset of phosphine properties</a>, Tobias Gensch and Matt Sigman <a href=https://pubs.acs.org/doi/10.1021/acscatal.2c01970>proposed a set of 32 commercially available ligands</a> which aims to cover as much of phosphine chemical space as possible in an initial screen. Jason Stevens and co-workers combined this idea with the substrate-selection methods from the previous paragraph to <a href=https://pubs.acs.org/doi/10.1021/acs.organomet.2c00089>perform a detailed study</a> of Ni-catalyzed borylation under many conditions, and tested a variety of ML models on the resulting dataset. (Scott Denmark and co-workers have also used a variant of this idea, called the Universal Training Set, to <a href=https://www.science.org/doi/10.1126/science.aau5631>initialize ML-driven reaction optimization</a>.)
</p>

<h3>New Optimization Algorithms</h3>

<p>
As in every area of life, ML-based approaches have been used a lot for optimization recently. This isn’t new; Klaus Jensen and Steve Buchwald <a href=https://pubs.rsc.org/en/content/articlelanding/2016/RE/C6RE00153J>used machine learning</a> to drive autonomous optimization in 2016, and <a href=https://pubs.acs.org/doi/10.1021/acscentsci.7b00492>Richard Zare</a> published a detailed methodological study in 2017. Nevertheless, as with computational substrate selection, these techniques have come into the mainstream in the past few years.
</p>

<p>
I mentioned the work of Scott Denmark on ML-driven optimization before, and his team published two more papers on this topic last year: <a href=https://pubs.acs.org/doi/10.1021/jacs.2c08820>one on atropselective biaryl iodination</a>, and <a href=https://pubs.acs.org/doi/pdf/10.1021/acs.oprd.1c00155>one on optimization of <i>Cinchona</i> alkaloid-based phase transfer catalysts</a>. In particular, the second paper (conducted in collaboration with scientists at Merck) illustrates how an ML model can be updated with new data as optimization progresses, allowing many sequential rounds of catalyst development to be conducted.
</p>

<p>
Abbie Doyle’s group has done a lot of work on using <a href=https://en.wikipedia.org/wiki/Bayesian_optimization>Bayesian optimization</a> (BO) to drive reaction optimization. <a href=https://www.nature.com/articles/s41586-021-03213-y>Their first paper</a> in this area illustrated the capacity of BO to avoid spurious local minima, and went on to validate this approach in a variety of complex problems. Even better, they compared the results of BO to chemist-guided optimization to see if computer-driven optimization could outcompete expert intuition. To quote the paper:
</p>

<blockquote>
In total, 50 expert chemists and engineers from academia and industry played the reaction optimization game (Fig. 4c). Accordingly, the Bayesian reaction optimizer also played the game 50 times (Fig. 4b), each time starting with a different random initialization. The first point of comparison between human participants and the machine learning optimizer was their raw maximum observed yield at each step during the optimization. Humans made significantly (<i>p</i> &lt; 0.05) better initial choices than random selection, on average discovering conditions that had 15% higher yield in their first batch of experiments. <u>However, even with random initialization, within three batches of five experiments the average performance of the optimizer surpassed that of the humans.</u> Notably, in contrast to human participants, Bayesian optimization achieved &gt;99% yield 100% of the time within the experimental budget. Moreover, Bayesian optimization tended to discover globally optimal conditions (CgMe-PPh, CsOPiv or CsOAc, DMAc, 0.153 M, 105 °C) within the first 50 experiments (Fig. 4b). <i>(emphasis added)</i>
</blockquote>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_bayesian_optimization.png" style="width:300px;" />
  <figcaption> 
  Average yield per batch from Bayesian optimization (black) vs. humans (blue), showing how Bayesian optimization outcompetes humans despite a worse start.
  </figcaption>
</figure>

<p>
<a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c08592>Their subsequent work</a> has made their optimization app available online, and illustrated the application of this strategy to other reactions.
</p>

<p>
Closely related is <a href=https://www.science.org/doi/10.1126/science.adc8743> this work</a> from Aspuru-Guzik, Burke, and co-workers, which uses a “matrix-down” approach to choosing representative substrates for the Suzuki reaction (similar to the substrate-selection algorithms discussed previously). The selected substrates are then subjected to automated high-throughput screening guided by an uncertainty-minimizing ML model (i.e., new reactions are chosen based on the regions of chemical space that the algorithm has the least knowledge about; this is similar to, but distinct from, Bayesian optimization). This is a pretty interesting approach, and I hope they study it further in the future. (Aspuru-Guzik has done <a href=https://arxiv.org/abs/2103.03716>lots of other work</a> in this area, including some <a href=https://pubs.acs.org/doi/10.1021/acscentsci.8b00307>Bayesian optimization</a>.)
</p>

<p>
Finally, two papers this year (that I’m aware of) put forward the idea of using multi-substrate loss functions for optimization: <a href=https://www.nature.com/articles/s41586-022-05263-2>our work on screening for generality</a> and <a href=https://chemrxiv.org/engage/chemrxiv/article-details/636a84ab80c9bf01cc8d95f9>a beautiful collaboration</a> from Song Lin, Scott Miller, and Matt Sigman. These papers used “low-tech” optimization methods that are familiar to practicing organic chemists (e.g. “screen different groups at this position”), but evaluated the output of this optimization not based on the yield/enantioselectivity of a single substrate but on aggregate metrics derived from many substrates. The results that our groups were able to uncover were good, but I’m sure adding robotics and advanced ML optimization will turbocharge this concept and find new and better catalysts with truly remarkable generality.
</p>

<h3>Conclusions</h3>

<p>
Reaction optimization is a common task in organic chemistry, but one that’s commonly done without much metacognition. Instead, many researchers will screen catalysts, substrates, and conditions based on habit or convenience, without necessarily dwelling on whether their screening procedure is optimal. While this may work well enough when you only need to optimize one or two reactions in your whole graduate school career (or when acquiring each data point takes days or weeks), <i>ad hoc</i> strategies will at some point simply fail to scale.
</p>

<p>
Organic chemistry, long the realm of “small data,” is slowly but inexorably catching up with the adjacent sciences. As progress in lab automation and instrumentation makes setting up, purifying, and analyzing large numbers of reactions easier, experimental chemists will have to figure out how to choose which reactions to run and how to handle all the data from these reactions, using tools like the ones discussed above. Like it or not, data science and cheminformatics may soon become core competencies of the modern experimental chemist!
</p>

]]></description>
              <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Misleading H/D KIE Experiments</title>
              <link>public/blog/20230103_hdkie_puzzle.html</link>
              <description><![CDATA[
<p>
<i>
Note: old versions of this post lacked a discussion of S<sub>N</sub>2. I've added an appendix which remedies this.
</i>
</p>

<p>
In <a href=https://corinwagen.github.io/public/blog/20220815_rate_determining_span.html>“The Rate-Limiting Span,”</a> I discussed how thinking in terms of the span from ground state to transition state, rather than in terms of elementary steps, can help prevent conceptual errors. Today, I want to illustrate why this is important in the context of a little H/D KIE puzzle. 
</p>

<p>
Consider the following reaction, which could conceivably proceed via an S<sub>N</sub>2 mechanism (red), an S<sub>N</sub>1 mechanism (blue), or really anywhere on the continuum:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_structures.png" style="width:550px;" />
  <figcaption> 
    This is a bit of a silly reaction, admittedly. The intermolecular version or the <i>endo</i> version would have been better.
  </figcaption>
</figure>

<p>
What experiment can be used to investigate the mechanism of this reaction? One possibility is an alpha H/D KIE experiment at the iminium position. Reactions with a sp<sup>3</sup> ground state and an sp<sup>2</sup> transition state display secondary normal alpha H/D KIEs, while reactions with an sp<sup>2</sup> ground state and an sp<sup>3</sup> transition state display secondary inverse KIEs.
Thus, one might think “if iminium formation is rate-limiting, the KIE will be normal, but if alkene addition is rate-limiting, the KIE will be inverse.”
</p>

<p>
Unfortunately this is not true. Instead, all mechanistic possibilities give secondary normal KIEs! I investigated this model system computationally at the wB97X-D/6-31G(d)/SMD(CH<sub>2</sub>Cl<sub>2</sub>) level of theory. Here’s a More O’Ferrall–Jencks plot of the H/D KIE at the iminium position, computed with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>
(breaking bond on Y axis, forming bond on X axis): 
</p>

<!--
<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_energy.png" style="width:500px;" />
  <figcaption> 
    Some of the grid points didn't finish, which explains the scattered gaps. I've also excluded anything with energy more than 30 kcal/mol above the ground state.
  </figcaption>
</figure>
-->

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_kie.png" style="width:550px;" />
  <figcaption> 
    The raw data from PyQuiver is a little fuzzy, so I applied a convolution to smooth the data. Details later on. 
  </figcaption>
</figure>

<p>
Rather than telling us which step is rate-limiting, all the KIE shows us is how much the transition state resembles an iminium ion (bottom right). Structures with long C–Cl bond distances and short C–C bond distances have substantial isotope effects (around 20%), while structures with forming or breaking bonds have smaller isotope effects. 
</p>

<p>
Why is this? Both ionization and addition proceed through iminium-like structures that are substantially sp<sup>2</sup>-hybridized at carbon, irrespective of whether sp<sup>2</sup> character is technically increasing or decreasing in the elementary step. Relative to the starting material, both transition states look like iminium ions and thus lead to large isotope effects. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_structures2.png" style="width:300px;" />
  <figcaption>
  These both look pretty much like iminium ions.
  </figcaption>
</figure>

<p>
We can also conceptualize this in terms of elementary steps. Taken by itself, alkene addition does lead to an inverse kinetic isotope effect, as seen by the decreasing values as one moves left from the iminium—but an inverse isotope effect relative to the normal equilibrium isotope effect of the iminium. In this system, the equilibrium isotope effect of the iminium is larger than the kinetic isotope effect for alkene addition, and so the combination of these two effects leads to a (smaller) overall normal effect. 
</p>

<p>
(This is the opposite of primary H/D KIEs, where central transition states lead to the largest isotope effects and equilibrium effects are typically small. Here, the isotope effect is mainly a function of hybridization, and so the later the TS, the greater the difference in hybridization and the larger the isotope effect.)
</p>

<p>
In summary, although this experiment seems informative, it’s actually not very useful. It tells you something about the structure of the transition state, but not which step is rate-limiting! In this case, a better experiment would be to measure <sup>13</sup>C KIEs, or an H/D KIE on the nucleophile. 
</p>

<h3>Appendix I: What About S<sub>N</sub>2?</h3>

<p>
On Twitter, <a href="https://twitter.com/LevinChem/status/1610287881993728000">Mark Levin</a> asks about the KIE for the concerted path. I originally meant to include a discussion of this, but then totally forgot to! So let’s fix that.
</p>

<p>
As shown in the graph above, extremely concerted pathways (i.e. going right down the middle of the MOJ plot) will have pretty small isotope effects. These sorts of mechanisms are common where the carbocation would be extremely unstable (methyl iodide) but much less common for stabilized carbocations like we’re discussing here. When oxocarbeniums or iminiums are involved, even “concerted” mechanisms shift towards the bottom right corner: this is the <a href=https://pubs.acs.org/doi/pdf/10.1021/ja506092h>“loose”/“exploded” S<sub>N</sub>2</a> often seen in glycosylation. These pathways will have a modest to large H/D KIE, depending on the exact system and how “exploded” they are (see page 53 of <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>this SI</a> for examples)
</p>

<p>
Putting this together, then, what experimental results would be conclusive? A very small KIE would be diagnostic for a “classic” synchronous S<sub>N</sub>2 process, which I consider to be very unlikely here. But medium or large H/D KIEs are consistent with any possibility: S<sub>N</sub>1 with rate-limiting ionization, S<sub>N</sub>1 with rate-limiting nucleophilic attack, or a loose S<sub>N</sub>2. There’s an annulus of different mechanistic possibilities that all give about the same isotope effect, as shown below:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_sn2_plot.png" style="width:470px;" />
  <figcaption> 
  Any TS in the red zone is consistent with a moderately large KIE (say, 15%).
  </figcaption>
</figure>

<p>
To make matters worse, H/D KIEs are pretty tough to simulate quantitatively, because of tunneling, so the annulus isn’t even that precise. That’s why I think this isn’t a very good experiment.
</p>

<h3>Appendix II: Smoothing the KIE Grid</h3>

<p>
The raw KIE values from PyQuiver were pretty noisy, probably because there are small or multiple imaginary frequencies for some of these non-stationary points, so I used convolution to smooth things out a bit. 
</p>

<pre class=code-block>
import numpy as np
from scipy.ndimage import convolve

#### this code block takes a 2d np.ndarray of KIE values 
#### and returns a smoothed np.ndarray with the same dimensions

corner_w = 0.05
edge_w = 0.2
kernel = np.array([
    [corner_w, edge_w, corner_w],
    [edge_w, 1 ,edge_w],
    [corner_w, edge_w, corner_w]
])
kernel = kernel / np.sum(kernel)

smooth_grid = convolve(kie_grid, kernel, mode="nearest")
</pre>

<p>
I haven’t seen this technique used before, but it doesn’t seem unreasonable to me.
</p>
]]></description>
              <pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Books from 2022</title>
              <link>public/blog/20221231_books.html</link>
              <description><![CDATA[
<p>
Last January, I aimed to read 50 books in 2022. I got through 32, which is at least more than I read in 2021.
</p>

<p>
There’s been a bit of <a href=https://twitter.com/nabeelqu/status/1608874209375313920>discourse</a> around whether setting numerical reading goals for oneself is worthwhile or counterproductive. I don’t have a strong opinion in the abstract, but I noticed that consciously tracking how many books I read served as a little nudge to read more than I would otherwise, without really compromising the quality of books I was reading.
</p>

<p>
In order, then:
</p>

<b>
#1. Neal Stephenson, <i>Snow Crash</i> (reread)
</b>

<p>
I read this in high school, and wanted to read it again in light of recent Metaverse-related discourse. It didn’t disappoint, although it’s a little more “action movie” than Stephenson’s later works.
</p>

<b>
#2. Aristotle, <i>Nicomachean Ethics</i>
</b>
<br>
<b>
#3. Tim Keller, <i>Prayer</i> (reread)
</b>
<br>
<b>
#4–17. Robert Jordan &amp; Brandon Sanderson, <i>Wheel of Time</i>
</b>

<p>
Beyond the surface-level plot (which is fun), <i>Wheel of Time</i> is a fascinating exploration of a number of societies structured around complementarianism at a deep level.
</p>

<b>
#18. Paul Tripp, <i>Parenting</i>
</b>

<br>
<b>
#19. Peter Thiel, <i>Zero To One</i>
</b>

<p>
I’ve discussed this book <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>previously</a>.
</p>

<b>
#20. Peter Scazzero, <i>Emotionally Healthy Spirituality</i>
</b>

<br>
<b>
#21. Eric Berger, <i>Liftoff</i>
</b>

<p>
This is a good account of the early days of SpaceX, and works well as a book-length answer to the question “What decisions or mindsets allowed Elon Musk to succeed in starting a rocket company when so many other billionaires failed?” or equivalently “What—besides money—explains SpaceX’s success?”
</p>

<p>
My summary, based on the book, would be (in no particular order): (1) a focus on recruiting top talent, (2) a “can-do” spirit / commitment to moving quickly and recklessly, (3) decisive decision-making at top levels of the organization, (4) a willingness to internalize lots of tasks to increase efficiency, and (5) luck.
</p>

<b>
#22. Roald Dahl, <i>Going Solo</i>
</b>
<br>
<b>
#23. Yiyun Li, <i>Must I Go</i>
</b>
<br>
<b>
#24. Tyler Cowen &amp; Daniel Gross, <i>Talent</i>
</b>

<p>
I’ve also discussed this book <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>previously</a>.
</p>

<b>
#25. Stanley Gundry (Ed.), <i>Five Views on Law and Gospel</i>
</b>

<p>
This book presents five different theological “takes” on the relationship between Old Testament law and the New Testament—there was much less consensus than I expected! It is interesting but scholarly, and not an easy read.
</p>

<p>
There are a whole bunch of books in this series; each author writes an essay explaining their position, and then writes brief responses to the other authors’ positions. This format should be more common!
</p>

<b>
#26. Albert Hirschman, <i>Exit, Voice, and Loyalty</i>
</b>

<p>
I discussed pieces of this book <a href="https://corinwagen.github.io/public/blog/20221018_omelas_hirschman_altom.html">here</a>; the rest is also good.
</p>

<b>
#27. Celeste Ng, <i>Little Fires Everywhere</i>
</b>
<br>
<b>
#28. Fuchsia Dunlop, <i>The Food of Sichuan</i>
</b>

<p>
As discussed on <a href=https://conversationswithtyler.com/episodes/fuchsia-dunlop/> Conversations with Tyler</a>.
</p>

<b>
#29. Geoffrey Moore, <i>Crossing The Chasm</i>
</b>

<p>
This book is about 100 pages longer than it needs to be.
</p>

<b>
#30. John Owen, <i>The Mortification of Sin</i> (abridged)
</b>

<p>
As recommended <a href=https://twitter.com/timkellernyc/status/1477975783641595905>by Tim Keller</a>!
</p>

<b>
#31. Margaret Atwood, <i>Oryx and Crake</i>
</b>
<br>
<b>
#32. Alison Weir, <i>The Wars of the Roses</i>
</b>

<p>
This book is a nice account of the Wars of the Roses in the style of a novel; I didn’t know anything beyond the broad strokes, so I found it quite gripping. My biggest complaint is that the book only goes through 1471, and so doesn’t cover any of the Bosworth Field-adjacent events.
</p>

<p>
My reading this year was about 50% fiction (18 books), with the remainder mostly divided between business (5 books) and Christianity (5 books). My main goal for next year is to read more history; I didn’t end up reading very much history this year, and I miss it.
</p>

]]></description>
              <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Evaluating Error in Boltzmann Weighting</title>
              <link>public/blog/20221228_boltzmann_error.html</link>
              <description><![CDATA[
<p>
A technique that I’ve seen employed more and more in computational papers over the past few years is to calculate Boltzmann-weighted averages of some property over a conformational ensemble. This is potentially very useful because most complex molecules exist in a plethora of conformations, and so just considering the lowest energy conformer might be totally irrelevant. 
To quote a <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.202205735>recent perspective</a> from Grimme and friends:
</p>

<blockquote>
For highly flexible structures, a molecular property, such as energy, nuclear magnetic resonance spectra, or optical rotation values may not be sufficiently described by a single structure. At finite temperatures, various conformers are populated and the overall property must be described as thermal average over the unique property values of each conformer.
</blockquote>

<p>
What's been bothering me about this, however, is that Boltzmann weights are calculated as <i>e</i> to the power of the relative energy:
</p>

<pre class=code-block>
def boltzmann_weight(conformer_properties, conformer_energies, temp=298):
    """ Some simple Python code to calculate Boltzmann weights. """

    energies = conformer_energies - np.min(conformer_energies)

    R = 3.1668105e-6 # eH/K
    weights = np.exp(-1*energies/(627.509*R*temp))
    weights = weights / np.sum(weights)

    return weights, np.average(conformer_properties, weights=weights)
</pre>

<p>
Since relative energies of conformers are usually calculated with only middling accuracy (±0.2 kcal/mol with <a href=https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j>common methods</a>), we’re taking the exponential of a potentially inaccurate value—which seems bad from the standpoint of error propagation!
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_xkcd.png style="width:375px;" />
  <figcaption>
  Excerpt from <a href="https://xkcd.com/2295/">xkcd</a>, on error propagation (line #3 is what's relevant here).
  </figcaption>
</figure>

<p>
Grimme and co-authors address this point explicitly in their review:
</p>

<blockquote>
At the same time, however, conformational energies need to be accurate to within about 0.1–0.2 kcal mol<sup>−1</sup> to predict Boltzmann populations at room temperature reasonably well. This is particularly important since properties can vary strongly and even qualitatively between populated conformers…
</blockquote>

<p>
Although the best answer is, of course, to just get more accurate energies, it's not always practical to do that in the real world.
If we take imperfect energies as our starting point, what's the best strategy to pursue?
</p>

<p>
One could imagine a scenario in which error causes relatively unimportant conformers to end up with large weights, making the answer even worse than the naïve approach would have been. If the lowest energy conformer accounts for 60-70% of the answer, might it be best to just stick with that, instead of trying to throw in some messy corrections?
</p>

<p>
To test this, I drew a random flexible-looking molecule with a few functional groups, conducted a conformational search using <i>crest</i>, and then optimized it and calculated <sup>19</sup>F NMR shieldings using wB97X-D/6-31G(d). (There are <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.8b00624>better NMR methods</a> out there, but the absolute accuracy of the shift isn’t really the point here.)
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_cylview.png style="width:350px;" />
  <figcaption>
  The lowest energy conformer of the molecule I drew (3-fluorohex-5-enal).
  </figcaption>
</figure>

<p>
I then computed more accurate energies using DLPNO-CCSD(T)/cc-pVTZ, and compared the results from Boltzmann weighting with DFT and coupled-cluster energies.
(<sup>19</sup>F values are just the isotropic shielding tensor, and energies are in kcal/mol.)
</p>

<table>
  <tr>
    <th>Conformer</th> 
    <th><sup>19</sup>F shift</th> 
    <th>DFT energy</th>
    <th>DFT weight</th>
    <th>CC energy</th>
    <th>CC weight</th>
  </tr>
  <tr><td>c00003</td><td>401.76</td><td>0.00</td><td>0.624</td><td>0.00</td><td>0.529</td></tr>
  <tr><td>c00001</td><td>403.08</td><td>1.02</td><td>0.112</td><td>0.68</td><td>0.167</td></tr>
  <tr><td>c00010</td><td>396.63</td><td>1.12</td><td>0.093</td><td>1.10</td><td>0.083</td></tr>
  <tr><td>c00007</td><td>391.45</td><td>1.56</td><td>0.045</td><td>1.54</td><td>0.039</td></tr>
  <tr><td>c00004</td><td>396.77</td><td>1.82</td><td>0.029</td><td>1.64</td><td>0.033</td></tr>
  <tr><td>c00006</td><td>400.16</td><td>2.31</td><td>0.013</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00029</td><td>400.37</td><td>2.36</td><td>0.012</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00032</td><td>393.96</td><td>2.05</td><td>0.020</td><td>1.76</td><td>0.027</td></tr>
  <tr><td>c00027</td><td>394.60</td><td>2.54</td><td>0.009</td><td>2.21</td><td>0.013</td></tr>
  <tr><td>c00017</td><td>394.69</td><td>3.12</td><td>0.003</td><td>2.27</td><td>0.011</td></tr>
  <tr><td>c00018</td><td>402.24</td><td>2.24</td><td>0.014</td><td>2.35</td><td>0.010</td></tr>
  <tr><td>c00011</td><td>381.31</td><td>2.59</td><td>0.008</td><td>2.49</td><td>0.008</td></tr>
  <tr><td>c00023</td><td>388.77</td><td>2.51</td><td>0.009</td><td>2.54</td><td>0.007</td></tr>
  <tr><td>c00013</td><td>390.32</td><td>3.02</td><td>0.004</td><td>2.61</td><td>0.006</td></tr>
  <tr><td>c00020</td><td>394.97</td><td>3.23</td><td>0.003</td><td>2.62</td><td>0.006</td></tr>
  <tr><td>c00015</td><td>398.24</td><td>3.02</td><td>0.004</td><td>2.97</td><td>0.004</td></tr>
  <tr><td>&nbsp;</td><td></td><td></td><td></td><td></td><td></td><tr>
  <tr><th>Final <sup>19</sup>F Shift</th><td></td><td></td><td>400.20</td><td></td><td>400.13</td></tr>
</table>

<p>
The match is really quite good, much better than just guessing the lowest energy conformer would have been! This is despite having a decent number of low-energy conformers, so I don’t think this is a particularly rigged case.
</p>

<p>
But, what if we just got lucky in this case? The relative energies are off by 0.28 kcal/mol on average. If we simulate adding 0.28 kcal/mol of error to each of the “true” energies a bunch of times, we can see how well Boltzmann weighting does on average, even with potentially unlucky combinations of errors.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_randomShifts.png style="width:400px;" />
  <figcaption>
  Shifts from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
The above image shows the predicted shift from 100,000 different randomly generated sets of “wrong” energies. We can see that the Boltzmann-weighted value is almost always closer to the true value than the shift of the major conformer is (99.01% of the time, to be precise). This is despite substantial changes in the weight of the major conformer:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_randomWeights.png style="width:400px;" />
  <figcaption>
  Major conformer weights from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
Thus, we can see that Boltzmann weighting is relatively resistant to random errors in this case. Although this is only one molecule, and no doubt scenarios can be devised where inaccurate energies lead to ludicrously incorrect predictions, this little exercise has helped temper my skepticism of Boltzmann weighting.
</p>

<i>
Thanks to Eugene Kwan and Joe Gair for discussions around these topics over the past few years. Data available upon request.
</i>

]]></description>
              <pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Low-Code Conformational Searching</title>
              <link>public/blog/20221219_low_code_csearch.html</link>
              <description><![CDATA[
<p>
Today I want to engage in some shameless self-promotion and highlight how <a href=https://github.com/ekwan/cctk><i>cctk</i></a>, an open-source Python package that I develop and maintain with Eugene Kwan, can make conformational searching easy.
</p>

<p>
<b>Updated October 2024 with some more shameless self-promotion: you can now run even faster and more advanced conformer searches for free through <a href=rowansci.com>Rowan</a>, my computational chemistry startup. <a href=https://rowansci.com/tools/conformers>Here's some more information</a> about Rowan's conformational searching capabilities!</b>
</p>

<p>
Conformational searching is a really crucial task in computational chemistry, because pretty much everything else you do depends on having the correct structure in the computer. In simple cases you can just draw out every conformer manually, but as the system under study gains degrees of freedom it becomes increasingly impractical to think through every possibility.
</p>

<p>
Failure to identify the correct conformer can lead to completely incorrect results, as demonstrated by Neese and coworkeers in <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">this recent article</a>. They reexamine a reaction <a href="https://www.science.org/doi/10.1126/science.aaq0445">originally studied by Ben List</a> and demonstrate that the conformers examined in the initial publication are almost 5 kcal/mol above the true lowest-energy conformers.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221219_neese_csearch.gif style="width:450px;" />
  <figcaption>
  Figure 1 from the paper; the previously reported conformers are shown in green.
  </figcaption>
</figure>

<p>
Conformational searching approaches attempt to prevent this sort of error by automating the process of finding conformers. There are lots of different algorithms one can use, like <a href="https://pubs.acs.org/doi/10.1021/ja952478m">low-mode searching</a>, metadynamics, and replica exchange (to name just a few), and decades of literature on this topic.
</p>

<p>
Since conformational searching requires many individual calculations, it’s <a href=https://link.springer.com/protocol/10.1007/978-1-0716-0282-9_14>almost never practical</a> to do a conformational search at a high level of theory (e.g. using DFT or <i>ab initio</i> methods). Instead, <a href="https://pubs.acs.org/doi/10.1021/acs.joc.2c00066">forcefields</a> or <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.5b00671">semiempirical</a> methods are generally used, with the caveat that the conformers generated might have somewhat inaccurate geometries.
</p>

<p>
<i>cctk</i> uses <a href="https://crest-lab.github.io/crest-docs/">crest</a> (from Grimme and coworkers), which uses a <a href=https://crest-lab.github.io/crest-docs/page/overview/workflows.html#imtd-gc-algorithm>metadynamics-based algorithm</a> with the <i>GFN2-xtb</i> semiempirical method to generate and score conformers. Although <i>crest</i> isn’t perfect, it’s simple, easy to use, and often generates very reasonable results.
</p>

<p>
I personally find the <i>crest</i> syntax a little tough to remember, so I’ve created a Python script so that I don’t have to look it up every time. 
</p>

<h3>Installing Packages</h3>

<p>
To run this tutorial, you’ll need to have <i>cctk</i> and <i>crest</i> installed. It’s often easiest to manage dependencies using a <i>conda</i> environment; if you don’t already have one, you can create one for this project with this code:
</p>

<pre class=code-block>
conda create --name=chem python=3.8
pip install cctk
pip install pyyaml
conda install -c conda-forge crest
</pre>

<p>
And in the future, you can activate the environment like this:
</p>

<pre class=code-block>
conda activate chem
</pre>

<h3>Running the Tutorial</h3>

<p>
The files for this tutorial can be found <a href="https://github.com/corinwagen/utilities/tree/master/csearch">here</a>. <span class=code>ex.yaml</span>, which is the only file you should need to modify, contains all the information needed for the python script <span class=code>do_crest.py</span>:
</p>

<pre class=code-block>
# list of atoms to constrain
# atom1, atom2, distance (or "auto" to keep distance from initial geometry)
constraints:
    constraint1: 17 31 auto
    constraint2: 30 31 auto

# location of input geometry, either as Gaussian .gjf or .out file
input_geom: pictet_spengler.gjf

# directory in which crest will run (will be created)
directory: crest

# name of logfile
logfile: crest.log

# whether or not this is a noncovalent complex (true or false).
# this simply gets passed to crest; some settings are changed.
noncovalent: false
</pre>

<p>
To generate conformers, simply run:
</p>

<pre class=code-block>
python do_crest.py ex.yaml
</pre>

<p>
This takes about 30 seconds to run on my laptop, and will generate about a hundred output conformers, which can (if desired) be further refined using DFT.
</p>

<p>
Hopefully this is useful! Please feel free to contact me with questions or bug reports.
</p>
]]></description>
              <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Response to Comments on &#34;Against Carbon NMR&#34;</title>
              <link>public/blog/20221216_carbon_nmr_response.html</link>
              <description><![CDATA[
<p>
Since my previous <a href="https://twitter.com/CraftyCarbene/status/1603572282953289729">“based and red pilled”</a> post seems to have struck a nerve, I figured I should address some common objections people are raising. 
</p>

<p>
Although this is obvious, I wanted to preface all of this by saying: this is my opinion, I'm not some expert on systems of science,
and many of the criticisms come from people with much more scientific and institutional expertise than me. 
It's very possible that I'm just totally wrong here! 
But what I'm saying makes sense to me, and (it seems) to a lot of other people, so I think it's at least worth having this discussion.
</p>

<h2>Commenters Who Feel <sup>13</sup>C NMR Is Scientifically Crucial</h2>

<p>
A few people pointed out that there are lots of instances in which carbon NMR <i>is</i> very important
(<a href="https://twitter.com/MuhammadAdilSA/status/1603339486431416320">1</a>,
<a href="https://twitter.com/craigdc1983/status/1603300877208621056">2</a>,
<a href="https://twitter.com/Double_Anne_/status/1603358363580088320">3</a>,
<a href="https://twitter.com/BogdosMichael/status/1603413304885612545">4</a>,
<a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">5</a>).
I don't disagree with this at all; I've also used <sup>13</sup>C NMR to solve problems that <sup>1</sup>H NMR and mass spectrometry alone couldn't solve!
But just because it’s crucial sometimes doesn’t mean it’s crucial all the time.
Does anyone really think that you need carbon NMR to tell if Boc protection of a primary amine worked? 
</p>

<p>
Most of the reactions that people do—especially people for whom synthetic chemistry is a means and not an end—are <a href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409">pretty straightforward</a>, such that I think it’s fair to assume you could deduce the correct product with high confidence without <sup>13</sup>C NMR.
(Again, if carbon spectra were so crucial, it wouldn’t be the case that many people don’t get them until the very end of the project.)

<h2>Commenters Who Feel That It's Important To Have Non-Crucial Data To Test Your Hypotheses</h2>

<p>
This point was also made by a number of people
(<a href=https://twitter.com/OrthaberLab/status/1603120117444919301>1</a>,
<a href=https://twitter.com/dasingleton/status/1603453351592706067>2</a>,
<a href=https://twitter.com/andrechemist/status/1603465213436633088>3</a>), 
perhaps most succinctly by <a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">“Chris Farley”</a>:

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_chris_farley.png style="width:450px;" />
  <figcaption>
  Never meet your heroes.
  </figcaption>
</figure>

<p>
I think this is an important point—part of what we ought to do, as a scientific community, is challenge one another to test our hypotheses and scrutinize our assumptions. Nevertheless, I’m not convinced this is a particularly strong argument for carbon NMR specifically. What makes <sup>13</sup>C{<sup>1</sup>H} spectra uniquely powerful at challenging one’s assumptions, as opposed to other data?
</p>

<p>
<a href=https://twitter.com/kjfritzsc/status/1603224459753959424>Keith Fritzsching</a> points out that HSQC is much faster and gives pretty comparable information (as did other readers, privately), and simply replacing <sup>13</sup>C with HSQC in most cases seems like it would nicely balance hypothesis testing with efficiency.  
</p>

<p>
(Relatedly, <a href=https://twitter.com/XiaoX_chem/status/1603251065293373440>Xiao Xiao</a> recounts how reviewers will request <sup>13</sup>C spectra even when there’s plenty of other data, including HSQC and HMBC. This is a pretty nice illustration of how powerful status quo bias can be.)
</p>

<h2>Commenters Who Say Carbon Spectra Are Easy To Acquire</h2>

<p>
Both <a href="https://twitter.com/VT_Chemist/status/1603153621511806979">@VT_Chemist</a> and <a href="https://twitter.com/spfletcher/status/1603452172510924800">Steve Fletcher</a> made this point:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_steve_fletcher.png style="width:450px;" />
  <figcaption>
  (cue flashbacks to undergraduate me trying to dissolve enough of some tetracyclic monster in pyridine-d5 to see my last quat)
  </figcaption>
</figure>

<p>
I've heard this from plenty of people before, and it's true that sometimes it's not hard at all to get a nice carbon spectrum! But sometimes it <i>is</i> really hard, also.
Based on the other responses, it seems like lots of other people agree with this sentiment.
</p>

<p>
(Is it possible that some of this disagreement reflects whether one has access to a helium cryoprobe?)
</p>

<h2>Commenters Who Feel It's Important To Have Consistent Journal Standards</h2>

<p>
A few people pointed out that making carbon NMR necessary on a case-by-case basis would be burdensome for editors and reviewers, since they'd have to think through each case themselves
(<a href="https://twitter.com/Double_Anne_/status/1603358846734548992">1</a>, <a href="https://twitter.com/rapodaca/status/1603471344468840448">2</a>). 
This is a fair point, and one I don't have a good response to. 
</p>

<p>
However, it's worth noting that this is already what we do for pretty much every other claim, including many complex structural problems: give the data, draw a conclusion, and ask reviewers to evaluate the logic.
Arguments about where the burden of proof should lie are tedious and usually unproductive, but I think we should have a pretty high barrier to making specific methods <i>de jure</i> required for publication.
</p>

<h2>Commenters Who Dislike My Claim That Journals Could Permit More Errors</h2>

<p>
I'm going to highlight <a href="https://twitter.com/dasingleton/status/1603441344747388929">Dan Singleton</a> here, someone I respect a ton:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_dan_singleton.png style="width:450px;" />
  <figcaption>
  The thread goes on, obviously, and is worth reading.
  </figcaption>
</figure>

<p>
I’m not trying to suggest that journals ought not to care about accuracy at all; <i>ceteris paribus</i>, accuracy should be prioritized. But given that we’re all constrained by finite resources, it’s important to consider the tradeoffs we’re making with every policy decision. It’s possible that trying to increase accuracy by asking for more data could have deleterious consequences:
</p>

<blockquote>
There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. 
</blockquote>

<p>
I think it’s easiest to think about these issues in terms of two separate questions: (1) relative to where we are today, should we push for more accuracy in the literature or less, and (2) are we achieving our chosen degree of accuracy in the most efficient manner possible? 
</p>

<p>
The first question is clearly complex, and probably deserves a longer and fuller treatment that I can provide here—although I’ll note that <a href=https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review>others have espoused</a> <a href=https://www.liamkofibright.com/uploads/4/8/9/8/48985425/is_peer_review_a_good_idea_.pdf>more radical positions</a> than mine on peer review (h/t <a href=https://twitter.com/BogdosMichael/status/1603413314868060160>Michael Bogdos</a> for the second link). I hope to write more on this subject later.
</p>

<p>
But the second question seems more straightforward. Is requiring <sup>13</sup>C NMR for every compound a Pareto-optimal way to ensure accuracy, as opposed to HSQC or HMBC? I struggle to see how the answer can be yes.
</p>

]]></description>
              <pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Against Carbon NMR</title>
              <link>public/blog/20221214_against_carbon_nmr.html</link>
              <description><![CDATA[
<p>
<sup>13</sup>C NMR is, generally speaking, a huge waste of time.
</p>

<p>
This isn’t meant to be an attack on carbon NMR as a scientific tool; it’s an excellent technique, and gives structural information that no other methods can. Rather, I take issue with the requirement that the identity of every published compound be verified with a <sup>13</sup>C NMR spectrum.
</p>

<p>
Very few <sup>13</sup>C NMR experiments yield unanticipated results. While in some cases <sup>13</sup>C NMR is the only reliable way to characterize a molecule, in most cases the structural assignment is obvious from <sup>1</sup>H NMR, and any ambiguity can be resolved with high-resolution mass spectrometry. Most structure determination is boring. Elucidating the structure of bizarre secondary metabolites from sea sponges takes <sup>13</sup>C NMR; figuring out if your amide coupling or click reaction worked does not.
</p>

<p>
The irrelevance of <sup>13</sup>C NMR can be shown via the doctrine of revealed preference: most carbon spectra are acquired only for publication, indicating that researchers are confident in their structural assignments without <sup>13</sup>C NMR. It’s not uncommon for the entire project to be scientifically “done” before any <sup>13</sup>C NMR spectra are acquired. In most fields, people treat <sup>13</sup>C NMR like a nuisance, not a scientific tool—the areas where this isn’t true, like total synthesis or sugar chemistry, are the areas where <sup>13</sup>C NMR is actually useful.
</p>

<p>
Requiring <sup>13</sup>C NMR for publication isn’t costless. The low natural abundance of <sup>13</sup>C and poor gyromagnetic ratio means that <sup>13</sup>C NMR spectra are orders of magnitude more difficult to obtain than <sup>1</sup>H NMR spectra. As a result, a large fraction of instrument time in any chemistry department is usually dedicated to churning out <sup>13</sup>C NMR spectra for publication, while people with actual scientific problems are kept waiting. <sup>13</sup>C NMR-induced demand for NMR time means departments have to buy more instruments, hire more staff, and use more resources; the costs trickle down.
</p>

<p>
And it’s not like eliminating the requirement to provide <sup>13</sup>C NMR spectra would totally upend the way we do chemical research. Most of our field’s history, including some of our greatest achievements, were done in the age before carbon NMR—the first <sup>13</sup>C NMR study of organic molecules was done by <a href="https://aip.scitation.org/doi/10.1063/1.1743253">Lauterbur</a> in 1957, and it would take even longer for the techniques to advance to the point where non-specialists could use the technique routinely. Even in the early 2000s you can find <i>JACS</i> papers without <sup>13</sup>C NMR spectra in the SI, indicating that it's possible to do high-quality research without it. 
</p>

<p>
Why, then, do we require <sup>13</sup>C NMR today? I think it stems from a misguided belief that scientific journals should be the ultimate arbiters of truth—that what’s reported in a journal ought to be trustworthy and above reproach. We hope that by requiring enough data, we can force scientists to do their science properly, and ferret out bad actors along the way. (Perhaps the clearest example of this mindset is <i>JOC</i> &amp; <i>Org. Lett.</i>, who maintain an ever-growing <a href="https://publish.acs.org/publish/author_guidelines?coden=joceah#data_requirements">list of standards</a> for chemical data aimed at requiring all work to be high quality.) Our impulse to require more and more data flows from our desire to make science an institution, a vast repository of knowledge equipped to combat the legions of misinformation.
</p>

<p>
But this hasn’t always been the role of science. Geoff Anders, <a href="https://www.palladiummag.com/2022/10/10/the-transformations-of-science/">writing for <i>Palladium</i></a>, describes how modern science began as an explicitly anti-authoritative enterprise:
</p>

<blockquote>
Boyle maintained that it was possible to base all knowledge of nature on personal observation, thereby eliminating a reliance on the authority of others. He further proposed that if there were differences of opinion, they could be resolved by experiments which would yield observations confirming or denying those opinions. The idea that one would rely on one’s own observations rather than those of others was enshrined in the motto of the Royal Society—<i>nullius in verba</i>.
</blockquote>

<p>
<i>nullius in verba</i> translates to “take no one’s word for it,” not exactly a ringing endorsement of science as institutional authority. This same theme can be found in more recent history. Melinda Baldwin’s <a href="https://astralcodexten.substack.com/p/your-book-review-making-nature"><i>Making Nature</i></a> recounts how peer review—a now-core part of scientific publishing—became commonplace at <i>Nature</i> only in the 1970s. In the 1990s it was still common to publish organic chemistry papers without supporting information.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221214_royal_society.jpeg style="width:450px;" />
  <figcaption>
  The Royal Society—a group of folks who definitely didn't take <sup>13</sup>C NMR spectra.
  </figcaption>
</figure>

<p>
The point I’m trying to make is not that peer review is bad, or that scientific authority is bad, but that the goal of enforcing accuracy in the scientific literature is a new one, and perhaps harder to achieve than we think. There are problems in the scientific literature everywhere, big and small. John Ioannidis memorably claimed that <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">“most published findings are false,”</a> and while chemistry may not suffer from the same issues as the social sciences, we have our own problems. 
<a href="https://pubs.acs.org/doi/10.1021/acscentsci.2c00325">Elemental analysis doesn’t work</a>, <a href="https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e">integration grids cause problems</a>, and even <a href="http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html">reactions from famous labs can’t be replicated</a>. Based on this, we might conclude that we’re very, very far from making science a robust repository of truth.
</p>

<p>
Nevertheless, progress marches on. A few misassigned compounds here and there don’t cause too many problems, any more than a faulty elemental analysis report or a sketchy DFT study. Scientific research itself has mechanisms for error correction: anyone who’s ever tried to reproduce a reaction has engaged in one such mechanism. Robust reactions get used, cited, and amplified, while reactions that never work slowly fade into obscurity. Indeed, despite all of the above failures, we’re living through a golden age for our field.
</p>

<p>
Given that we will never be able to eradicate bad data completely, the normative question then becomes “how hard should we try?” In an age of <a href="https://www.agrarheute.com/sites/agrarheute.com/files/2020-01/innovation_scientific_progress.pdf">declining research productivity</a>, we should be mindful not only of the dangers of low standards (proliferation of bad work) but also of the dangers of high standards (making projects take way longer). There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. The claim I hope to make here is that requiring <sup>13</sup>C NMR for every compound does more to slow down good work than it does to prevent bad work, and thus should be abandoned.
</p>

<i>
Update 12/16/2022: see <a href="https://corinwagen.github.io/public/blog/20221216_carbon_nmr_response.html">some followup remarks</a> based on feedback from Twitter.
</i>
]]></description>
              <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Definite Games, Indefinite Optimism</title>
              <link>public/blog/20221206_definite_games_indefinite_optimism.html</link>
              <description><![CDATA[
<p>
One of the more thought-provoking pieces I read last year was Alex Danco’s post <a href="https://alexdanco.com/2021/01/11/why-the-canadian-tech-scene-doesnt-work/">“Why the Canadian Tech Scene Doesn’t Work,”</a> which dissects the structural and institutional factors that make Silicon Valley so much more effective at spawning successful companies than Toronto.
I’ll briefly summarize the piece’s key arguments here, connect it to some ideas from <i>Zero to One</i>, and finish by drawing some conclusions for academia.
</p>

<p>
Danco’s key insight is applying James Carse’s distinction between <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">finite and infinite games</a> to entrepreneurship. What makes a game “finite” or “infinite”?
</p>

<blockquote>
First, <i>finite</i> games are played <i>for the purpose of winning</i>. Whenever you’re engaging in an activity that’s definite, bounded, and where the game can be completed by mutual agreement of all the players, then that’s a finite game. Much of human activity is described in finite game metaphors: wars, politics, sports, whatever. When you’re playing finite games, each action you take is directed towards a pre-established goal, which is to win.
<br>
<br>
In contrast, <i>infinite</i> games are played <i>for the purpose of continuing to play</i>. You do not “win” infinite games; these are activities like learning, culture, community, or any exploration with no defined set of rules nor any pre-agreed-upon conditions for completion. The point of playing is to bring new players into the game, so they can play too. You never “win”, the play just gets more and more rewarding.
</blockquote>

<p>
In entrepreneurship, Danco argues that infinite games are good and finite games are bad. Good founders are playing infinite games: they want to build something important and keep on contributing to society and progress. In contrast, bad founders are playing to win a finite game—acquiring lots of funding, getting high valuation, or exiting with a big IPO.
</p>

<p>
Danco identifies numerous ways that the Canadian startup ecosystem incentivizes finite games at the expense of infinite games. One important factor favoring finitude is the scrutiny given to deals between founders and funders (e.g. in a seed round), which tends to favor conservative or incremental ventures over ambitious, idealistic ones:
</p>

<blockquote>
[High deal scrutiny] is bad, for two reasons. It’s bad because the very best startups, who have the longest time horizon and are most curious about the world, will look disproportionately uninspiring. They’ll have the fewest definite wins relative to their ambition, and the most things that can potentially go wrong.…
<br>
<br>
Conversely, [high deal scrutiny is] bad because startups will learn to optimize for how to get funded. So if seed deals take 3 months, then founders will learn to build companies that look good under that kind of microscope. And that means they’re going to optimize for playing determinate games, so that they can show definable wins that can’t be argued against; rather than what they should be focusing on, which is open-ended growth.
</blockquote>

<p>
Government support for startups also tends to prioritize finite games at the expense of infinite games, since the requisite bureaucracy tends to stifle fast-moving innovation:
</p>

<blockquote>
The problem with [research tax] credits, honestly through no fault of their own, is that you have to say what you’re doing with them. This seems like a pretty benign requirement; and honestly it’s pretty fair that a government program for giving out money should be allowed to ask what the money’s being used for. But in practice, once you take this money and you start filling out time sheets and documenting how your engineers are spending their day, and writing summaries of what kind of R&amp;D value you’re creating, you are well down the path to destroying your startup and killing what makes it work.
</blockquote>

<p>
Overall, Danco paints a picture of a place where an obsession with goals and benchmarks has almost completely crowded out sincere innovation:
</p>

<blockquote>
Quite in character with our love of milestones, Canada loves anything with structure: accelerators, incubators, mentorship programs; anything that looks like an “entrepreneurship certificate”, we can’t get enough of it. <b>We’re utterly addicted with trying to break down the problem of growing startups into bite-size chunks, thoughtfully defining what those chunks are, running a bunch of promising startups through them, and then coming out perplexed when it doesn’t seem to work.</b> <i>(emphasis added)</i>
</blockquote>

<p>
While Danco limits himself to comparing Canada and Silicon Valley, this failure mode is sadly not confined to Canada. Indeed, many of his observations are directly applicable to academic research. The large number of finite games in academia—publishing papers, writing a dissertation, submitting grants, getting tenure—tends to crowd out the more impactful infinite games that lead to real, meaningful progress, and promotes incremental projects with a high chance of success. (This is simply Charles Hummel’s <a href="https://www.theartofsimple.net/fighting-the-tyranny-of-the-urgent-at-home/">“tyranny of the urgent”</a> by a different name.)
</p>

<p>
My claim is that the distinction between finite and infinite games is best understood through <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>Peter Thiel’s</a> concept of definite and indefinite optimism. In Thiel’s dichotomy, indefinite optimists believe the world will get better but have no idea how, whereas definite optimists have a concrete proposal for how to make the world better.
</p>

<p>
How does this connect to finite and infinite games? Paradoxically, indefinite optimism leads to an obsession with finite games, since there’s no higher animating principle at work to drive progress. When you don’t have any positive vision for innovation, the natural solution is to write procedures and hope that progress will arise spontaneously if the right steps are followed. Companies, research groups, and other organizations can learn to mimic what actual innovation looks like from afar, but without the proper motivations their ultimate success is improbable.
</p>

<p>
In contrast, an organization playing an infinite game doesn’t need to be forced to jump through arbitrary hoops. Pharmaceutical companies don’t bother to acquire <sup>13</sup>C NMR and IR spectra for every intermediate; startups putting together a minimum viable product don’t worry about properly formatting all their code. Finite games are only a distraction for properly motivated organizations, and one which should be avoided whenever possible.<sup><a href="#fn1">1</a></sup>
</p>

<p>
What conclusions can we draw from this? On a personal level, seek to make your games as infinite as possible. Every startup has to raise money and every graduate student has to publish papers, but one shouldn’t spend most of one’s time worrying about how to publish papers as efficiently as possible.
If you can treat finite games as the distraction that they are, you can give them as little mental effort as possible and spend your time and talents on worthier pursuits.
</p>

<p>
On a broader level, I’m struck by the fact that finite games are a problem of our own making. Nobody becomes a scientist hoping to write papers or win grants;<sup><a href="#fn2">2</a></sup>
our aspirations start out infinite, and it’s only through exposure to the paradigms of the field that we learn to decrease our ambition. 
Indeed, calling a research proposal “ambitious” is reportedly one of the worst criticisms that an NIH study section can give.
</p>

<p>
We should therefore be skeptical about bureaucratic solutions to research stagnation. If our scientific institutions themselves are part of the problem, expanding their reach and importance is unlikely to fix the problem and may indeed do more harm than good. “If you find yourself in a hole, stop digging.”
</p>

<i>
Thanks to Jacob Thackston and Ari Wagen for reading drafts of this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  I’m intentionally eliding the normative question of “how do we assign funding if not through quantitative measures?”—it’s an excellent question, and one which deserves a larger treatment than I can offer here. The <a href="https://scienceplusplus.org/metascience/">Nielsen/Qiu metascience essay</a> has some ideas here.
  </li>
  <li id="fn2">
  This may not always be true, but I think it’s generally true. Most high schoolers or undergraduates are drawn to science because of a sense of wonder or curiosity, which gets transmuted to “publish JACS papers” through the alchemy of graduate school.
  Ari pointed me towards <a href="https://www.briantimar.com/notes/mimetic/mimetic/">this essay</a> by a physics graduate student, which seems relevant.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Introducing &lt;i&gt;quick_fuoss&lt;/i&gt;</title>
              <link>public/blog/20221205_quick_fuoss.html</link>
              <description><![CDATA[
<p>
Modeling ion-pair association/dissociation is an incredibly complex problem, and one that's often beyond the scope of conventional DFT-based techniques. 
(<a href="https://pubs.acs.org/doi/10.1021/acs.joc.1c01823">This study</a> is a nice demonstration of how hard modeling ion-pairing can be.)
Nevertheless, it's still often important to gain insight into what the relative energy of solvent-separated ion pairs and contact-ion pairs might be;
are solvent-separated configurations energetically accessible or not? 
</p>

<p>
I've run into this problem a few times myself: in measuring pKas in nonpolar solvents, and again recently when trying to understand 
<a href="https://pubs.acs.org/doi/10.1021/acs.orglett.2c03622">the solution structure of ethereal HCl.</a>
When digging through the pKa literature, I was surprised to learn that there's a simple and relatively accurate way to estimate the dissociation constant of contact-ion pairs, 
developed by Raymond Fuoss in the 1950s. 
</p>

<p>
Despite modeling ions as charged spheres and solvent as a featureless dielectric, the "Fuoss model" is surprisingly good at reproducing experimental data.
Here's one such example:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221205_exp_vs_fuoss.png style="width:450px;" />
  <figcaption>
  Comparison of experimental ∆G<sub>diss</sub> for tetraisoamylammonium nitrate in various dioxane–water blends vs. Fuoss-calculated ∆G values.
  (Data taken from <a href="https://pubs.acs.org/doi/10.1021/ja01330a023">Fuoss, <i>J. Am. Chem. Soc.</i>, <b>1933</b></a>, Table II.)
  </figcaption>
</figure>

<p>
It's not trivial to think about how to get similar results using more atomistic methods!
(In principle one could actually model the exact solvent mixture and compute the energy of ion-pair dissociation using biased sampling and MD, but this 
would be horrendously expensive and probably less accurate anyway.)
</p>

<p>
The Fuoss model is pretty simple to implement oneself—but to make things even easier, I've implemented it as an open-source Python package, 
which can be imported using <span class=code>pip</span>.
The package contains only a single function, <span class=code>compute_kd</span>, which accepts the name of the cation, the name of the anion, and the dielectric constant of the medium.
(Alternatively, <span class=code>.xyz</span> files, <span class=code>.gjf</span> files, or <span class=code>cctk.Molecule</span> objects can also be given.)
</p>

<p>
Under the hood, the program builds molecules using <i>cctk</i>, computes their volume, and then applies the Fuoss model.
The end result is a comically simple interface:
</p>

<pre class=code-block>
$ pip install quick_fuoss
$ python
&gt;&gt;&gt; import quick_fuoss
&gt;&gt;&gt; quick_fuoss.compute_kd("sodium", "chloride", 80)
1.0793241279015366
</pre>

<p>
On the associated <a href="https://github.com/corinwagen/quick-fuoss">Github repository</a>, there's also a little command-line script which makes this even simpler:
</p>

<pre class=code-block>
$ python quick_fuoss.py tetraisoamylammonium nitrate 8.5
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00004930 M
Ionization energy: 5.873 kcal/mol
$ python quick_fuoss.py tetraisoamylammonium nitrate 11.9
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00094706 M
Ionization energy: 4.122 kcal/mol
</pre>

<p>
My hope is that this program promotes wider adoption of the Fuoss model, and in general enables more critical thinking about ion-pair energetics in organic solvents.
Please feel free to send any bug reports, complaints, etc. my way!
</p>
]]></description>
              <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Business Card Lennard–Jones Simulation, Explained</title>
              <link>public/blog/20221128_business_card_explained.html</link>
              <description><![CDATA[
<p>
Last week, I posted a simple Lennard–Jones simulation, written in C++, that models the behavior of liquid Ar in only 1561 characters.
By popular request, I'm now posting a breakdown/explanation of this program, both to explain the underlying algorithm and illustrate how it can be compressed.
</p>

<p>
Here's the most current version of the program, now condensed even further to 1295 characters:
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;
#define H return
typedef double d;typedef int i;using namespace std;i N=860;d B=8.314e-7;d S=1544
.8;d q(d x){if(x&gt;17)x-=34;if(x&lt;-17)x+=34;H x;}d rd(){H 34*drand48()-17;}struct v
{d x,y,z;v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}d n(){H x*x+y*y+z*z;}v p(){H v(q(x
),q(y),q(z));}};v operator+(v a,v b){H v(a.x+b.x,a.y+b.y,a.z+b.z);}v operator*(d
 a,v b){H v(a*b.x,a*b.y,a*b.z);}struct p{v r,q,a,b;vector&lt;i&gt; W;p(v l):r(l){}void
 F(v f){b=0.025*f+b;}};vector&lt;p&gt; P;void w(){cout&lt;&lt;N&lt;&lt;"\n\n";for(p l:P)cout&lt;&lt;"Ar
"&lt;&lt;l.r.x&lt;&lt;" "&lt;&lt;l.r.y&lt;&lt;" "&lt;&lt;l.r.z&lt;&lt;"\n";cout&lt;&lt;"\n";}void E(){for(i j=N;j--;){for(
i k:P[j].W){v R=(P[j].r+-1*P[k].r).p();d r=R.n();if(r&lt;70){d O=r*r*r;R=2880*B*(2*
S*S/O/O-S/O)/r*R;P[j].F(R);P[k].F(-1*R);}}}}void e(){for(i j=0;j&lt;N;j++){P[j].W.c
lear();for(i k=j+1;k&lt;N;k++)if((P[j].r+-1*P[k].r).p().n()&lt;90)P[j].W.push_back(k);
}}i main(){i A=1e3;i Y=5e3;for(i j=N;j--;){for(i a=999;a--;){v r=v(rd(),rd(),rd(
));i c=0;for(p X:P){d D=(r+-1*X.r).p().n();if(D&lt;6.8)c=1;}if(!c){P.push_back(p(r)
);break;}}}for(i t=0;t&lt;=3e4;t++){for(p&amp; I:P)I.r=(I.r+I.q+0.5*I.a).p();if(t%20==0
)e();E();d K=0;for(p&amp; I:P){I.q=I.q+0.5*(I.b+I.a);I.a=I.b;I.b=v();K+=20*I.q.n();}
d T=2*K/(3*B*N);if(t&lt;2*Y){d C=75;if(t&lt;Y)C=75*t/Y+(A-75)*(Y-t)/Y;for(p&amp; I:P)I.q=s
qrt(C/T)*I.q;}if(t%100==0&amp;&amp;t&gt;2*Y)w();}}
</pre>

<p>
Let's add some whitespace and break this down, line by line:
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;

// some abbreviations
#define H return
typedef double d;
typedef int i;
using namespace std;

// constants
i N=860;        // number of particles
d B=8.314e-7;   // boltzmann's constant
d S=1544.8;     // the minimum of the Lennard-Jones potential, 3.4 Å, to the 6th power
</pre>

<p>
The program begins by importing the necessary packages, defining abbreviations, and declaring some constants. 
Redefining <span class=code>double</span> and <span class=code>int</span> saves a ton of space, as we'll see.
(This is standard practice in the code abbreviation world.)
</p>

<pre class=code-block>
// given a 1d coordinate, scale it to within [-17,17].
// (this only works for numbers within [-51,51] but that's fine for this application)
d q(d x){
    if(x&gt;17)
        x-=34;
    if(x&lt;-17)
        x+=34;
    H x;
}

// returns a uniform random number in [-17,17]
d rd(){
    H 34*drand48()-17;
}
</pre>

<p>
We now define a helper function to keep coordinates within the boundaries of our cubical box, and create another function to "randomly" initialize particles' positions.
(<span class=code>drand48</span> is not a particularly good random-number generator, but it has a short name and works well enough.)
</p>

<pre class=code-block>
// vector class
struct v{
    d x,y,z;
    v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}

    // return the squared length of the vector
    d n(){
        H x*x+y*y+z*z;
    }
   
    // return a vector with periodic boundary conditions applied
    v p(){
        H v(q(x),q(y),q(z));
    }
};

// vector addition
v operator+(v a,v b){
    H v(a.x+b.x,a.y+b.y,a.z+b.z);
}

// multiplication by a scalar
v operator*(d a,v b){
    H v(a*b.x,a*b.y,a*b.z);
}

</pre>

<p>
Here, we define a vector class to store positions, velocities, and accelerations, and define addition and multiplication by a scalar.
(It would be better to pass <span class=code>const</span> references to the operators, but it takes too many characters.)
</p>

<pre class=code-block>
// particle class
struct p{
    // r is position, q is velocity, a is acceleration, b is acceleration in the next timestep
    v r,q,a,b;
    // neighbor list, the list of particles close enough to consider computing interactions with
    vector&lt;i&gt; W;

    p(v l):r(l){}
   
    // apply a force to this particle. 
    // 0.025 = 1/40 = 1/(Ar mass)
    void F(v f){
        b=0.025*f+b;
    }
};

// global vector of all particles
vector&lt;p&gt; P;

// write current coordinates to stdout
void w(){
    cout&lt;&lt;N&lt;&lt;"\n\n";
    for(p l:P)
        cout&lt;&lt;"Ar "&lt;&lt;l.r.x&lt;&lt;" "&lt;&lt;l.r.y&lt;&lt;" "&lt;&lt;l.r.z&lt;&lt;"\n";
    cout&lt;&lt;"\n";
}

</pre>

<p>
Now, we define a class that represents a single argon atom. 
Each atom has an associated position, velocity, and acceleration, as well as <span class=code>b</span>, which accumulates acceleration for the next timestep.
Atoms also have a "neighbor list", or a list of all the particles close enough to be considered in force calculations. 
(To prevent double counting, each neighbor list only contains the particles with index larger than the current particle's index.)
</p>

<p>
We create a global variable to store all of the particles, and create a function to report the current state of this variable.
</p>

<pre class=code-block>
// compute forces between all particles
void E(){
    for(i j=N;j--;){
        for(i k:P[j].W){
            // compute distance between particles
            v R=(P[j].r+-1*P[k].r).p();
            d r=R.n();

            // if squared distance less than 70 (approximately 6 * 3.4Å**2), the interaction will be non-negligible
            if(r&lt;70){
                d O=r*r*r;
                // this is the expression for the lennard–jones force.
                // the second lennard–jones parameter, the depth of the potential well (120 kB), is factored in here.
                R=2880*B*(2*S*S/O/O-S/O)/r*R;

                // apply force to each particle
                P[j].F(R);
                P[k].F(-1*R);
            }
        }
    }
}
</pre>

<p>
Now, we create a function to calculate the forces between all pairs of particles. 
For each particle, we loop over the neighbor list and see if the distance is within six minima of the adjacent particle, 
using squared distance to avoid the expensive square-root calculation.
If so, we calculate the force and apply it to each particle.
</p>

<pre class=code-block>
// build neighbor lists
void e(){
    for(i j=0;j&lt;N;j++){
        // clear the old lists
        P[j].W.clear();
        for(i k=j+1;k&lt;N;k++)
            // if squared distance between particles less than 90 (e.g. close to above cutoff), add to neighbor list
            if((P[j].r+-1*P[k].r).p().n()&lt;90)
                P[j].W.push_back(k);
    }
}
</pre>

<p>
Finally, we create a function to build the neighbor lists, which is a straightforward double loop.
We add every particle which might conceivably be close enough to factor into the force calculations within 10–20 frames.
</p>

<pre class=code-block>
i main(){
    i A=1e3;    // initial temperature (1000 K)
    i Y=5e3;    // time to reach final temperature (5000 fs)

    // initialize the system. each particle will be randomly placed until it isn't too close to other particles.
    for(i j=N;j--;){
        for(i a=999;a--;){
            // generate random position
            v r=v(rd(),rd(),rd());
            i c=0;

            // check for clashes with each extant particle
            for(p X:P){
                d D=(r+-1*X.r).p().n();
                if(D&lt;6.8)
                    c=1;
            }

            // if no clashes, add particle to list
            if(!c){
                P.push_back(p(r));
                break;
            }
        }
    }
</pre>

<p>
To begin the program, we randomly initialize each particle and test if we're too close to any other particles. 
If not, we save the position and move on to the next particle. This is crude but works well enough for such a simple system.
</p>
   
<pre class=code-block>
    // run MD! this is basically just the velocity verlet algorithm.
    for(i t=0;t&lt;=3e4;t++){
        // update position
        for(p&amp; I:P)
            I.r=(I.r+I.q+0.5*I.a).p();
        
        // every 20 timesteps (20 fs), update neighbor lists
        if(t%20==0)
            e();

        // compute forces
        E();

        // finish velocity verlet, and sum up the kinetic energy.
        d K=0;  // kinetic energy
        for(p&amp; I:P){
            I.q=I.q+0.5*(I.b+I.a);
            I.a=I.b;
            I.b=v();
            K+=20*I.q.n();
        }
        
        d T=2*K/(3*B*N); // temperature

        // in the first 10 ps, apply berendsen thermostat to control temperature
        if(t&lt;2*Y){
            d C=75; // target temperature
            if(t&lt;Y)
                C=75*t/Y+(A-75)*(Y-t)/Y;
            for(p&amp; I:P)
                I.q=sqrt(C/T)*I.q;
        }
       
        // every 100 fs after the first 10 ps, write the configuration to stdout
        if(t%100==0&amp;&amp;t&gt;2*Y)
            w();
    }
}
</pre>

<p>
Finally, we simply have to run the simulation. We use the velocity Verlet algorithm with a 1 fs timestep, 
updating neighbor lists and writing to <span class=code>stdout</span> periodically.
The temperature is gradually lowered from 1000 K to 75 K over 5 ps, and temperature is controlled for the first 10 ps.
</p>

<p>
Hopefully this helps to shed some light on how simple a molecular dynamics simulation can be, and highlights the wonders of obfuscated C++!
</p>
]]></description>
              <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Business Card Lennard–Jones Simulation</title>
              <link>public/blog/20221121_business_card_lennard_jones.html</link>
              <description><![CDATA[
<p>
An (in)famous code challenge in computer graphics is to write a complete ray tracer small enough to fit onto a business card.
I've really enjoyed reading through some of the submissions over the years (e.g. 
<a href="https://fabiensanglard.net/rayTracing_back_of_business_card/">1</a>,
<a href="https://mzucker.github.io/2016/08/03/miniray.html">2</a>,
<a href="https://www.realtimerendering.com/blog/back-of-the-business-card-ray-tracers/">3</a>,
<a href="https://www.taylorpetrick.com/blog/post/business-rt">4</a>), and I've wondered what a chemistry-specific equivalent might be.
</p>

<p>
As a first step in this space—and as a learning exercise for myself as I try to learn C++—I decided to try and write a tiny Lennard–Jones simulation.
Unlike most molecular simulation methods, which rely on heavily parameterized forcefields or complex quantum chemistry algorithms, 
the Lennard–Jones potential has a simple functional form and accurately models noble gas systems.
Nice work from <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.136.A405">Rahman</a> in 1964 showed that Lennard–Jones simulations
of liquid argon could reproduce experimental X-ray scattering results for the radial distribution function:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221121_rahman_rdf.png style="width:350px;" />
  <figcaption>
  Figure 2 from Rahman, showing the radial distribution function of liquid argon with clear first, second, and third solvation shells.
  </figcaption>
</figure>

<p>
This thus seemed like a good target for a "tiny chemistry" program: easy enough to be doable, but tricky enough to be interesting.
After a bit of development, I was able to get my final program down to 1561 characters, easily small enough for a business card.
The final program implements periodic boundary conditions, random initialization, and temperature control with the Berendsen thermostat for the first 10 picoseconds.
Neighbor lists are used to reduce the computational cost, and an xyz file is written to stdout. 
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;
#define H return
typedef double d;typedef int i;using namespace std;d L=17;i N=860;d B=8.314e-7;d
 s=3.4;d M=6*s*s;d S=s*s*s*s*s*s;d q(d x){if(x&gt;L)x-=2*L;if(x&lt;-L)x+=2*L;H x;}d rd
(){H 2*L*drand48()-L;}struct v{d x,y,z;v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}d n(
){H x*x+y*y+z*z;}v p(){x=q(x);y=q(y);z=q(z);H*this;}};v operator+(v a,v b){H v(a
.x+b.x,a.y+b.y,a.z+b.z);}v operator*(d a,v b){H v(a*b.x,a*b.y,a*b.z);}struct p{d
 m;v r,q,a,b;vector&lt;i&gt; W;p(v l,d n=40):r(l),m(n),q(v()),a(v()),b(v()),W(vector&lt;i
&gt;()){}void F(v f){b=(1/m)*f+b;}};vector&lt;p&gt; P;void w(){cout&lt;&lt;N&lt;&lt;"\n\n";for(i j=N;
j--;){v l=P[j].r;cout&lt;&lt;"Ar "&lt;&lt;l.x&lt;&lt;" "&lt;&lt;l.y&lt;&lt;" "&lt;&lt;l.z&lt;&lt;"\n";}cout&lt;&lt;"\n";}void E(
){for(i j=0;j&lt;N;j++){for(i x=0;x&lt;P[j].W.size();x++){i k=P[j].W[x];v R=(P[j].r+-1
*P[k].r).p();d r2=R.n();if(r2&lt;M){d O=1/(r2*r2*r2);d f=2880*B*(2*S*S*O*O-S*O)/r2;
R=f*R;P[j].F(R);P[k].F(-1*R);}}}}void cW(){for(i j=0;j&lt;N;j++){P[j].W.clear();for
(i k=j+1;k&lt;N;k++)if((P[j].r+-1*P[k].r).p().n()&lt;1.3*M)P[j].W.push_back(k);}}i mai
n(){i A=1e3;i e=75;i Y=5e3;for(i j=N;j--;){for(i a=99;a--;){v r=v(rd(),rd(),rd()
);i c=0;for(i k=P.size();k--;){d D=(r+-1*P[k].r).p().n();if(D&lt;2*s)c=1;}if(!c){P.
push_back(p(r));break;}}}if(P.size()!=N)H 1;for(i t=0;t&lt;=3e4;t+=10){for(i j=N;j-
-;){P[j].r=P[j].r+P[j].q+0.5*P[j].a;P[j].r.p();}if(t%20==0)cW();E();d K=0;for(i
j=N;j--;){P[j].q=P[j].q+0.5*(P[j].b+P[j].a);P[j].a=P[j].b;P[j].b=v();K+=P[j].m*P
[j].q.n()/2;}d T=2*K/(3*B*N);if(t&lt;2*Y){d C=e;if(t&lt;Y)C=e*t/Y+(A-e)*(Y-t)/Y;d s=sq
rt(C/T);for(i j=N;j--;)P[j].q=s*P[j].q;}if(t%100==0&amp;&amp;t&gt;2*Y)w();}}
</pre>

<p>
The program can be compiled with <span class=code>g++ -std=gnu++17 -O3 -o mini-md mini-md.cc</span> and run:
</p>

<pre class=code-block>
$ time ./mini-md &gt; Ar.xyz

real	0m4.689s
user	0m4.666s
sys	0m0.014s
</pre>

<p>
Analysis of the resulting file in <i>cctk</i> results in the following radial distribution function, in good agreement with the literature:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221121_rdf.png style="width:450px;" />
  <figcaption>
  Radial distribution function generated by the above code.
  </figcaption>
</figure>

<p>
I'm pretty pleased with how this turned out, and I learned a good deal both about the details of C++ syntax and about molecular dynamics.
There are probably ways to make this shorter or faster; if you write a better version, let me know and I'll happily link to it here! 
</p>
]]></description>
              <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Structural Diversity in Science</title>
              <link>public/blog/20221026_structural_diversity.html</link>
              <description><![CDATA[
<p>
Michael Nielsen and Kanjun Qiu recently published a massive essay entitled <a href="https://scienceplusplus.org/metascience/index.html">“A Vision of Metascience: An Engine of Improvement for the Social Processes of Science.”</a> Metascience, the central topic of the essay, is science about science. As the authors put it, metascience “overlaps and draws upon many well-established fields, including the philosophy, history, and sociology of science, as well as newer fields such as the economics of science funding, the science of science, science policy, and others.”
</p>

<p>
The essay emphasizes how there are likely massive potential improvements to what they call the “social processes” of science, or the ways in which science is practiced by scientists. There are a lot of important ideas and conclusions in the essay, but here I want to focus on one specific theme that caught my attention: <u>the importance of structural diversity in driving scientific progress</u>. In this context, structural diversity means “the presence of many differently structured groups in science.” High structural diversity means many different types of scientific groups, while low structural diversity means only a few different types of scientific groups. To quote Nielsen and Qiu directly:
</p>

<blockquote>
…structural diversity is a core, precious resource for science, a resource enlarging the range of problems humanity can successfully attack. The reason is that different ambient environments enable different kinds of work. Problems easily soluble in one environment may be near insoluble in another; and vice versa. Indeed, often we don't <i>a priori</i> know what environment would best enable an attack on an important problem. Thus it's important to ensure many very different environments are available, and to enable scientists to liquidly move between them. In this view, structural diversity is a resource to be fostered and protected, not homogenized away for bureaucratic convenience, or the false god of efficiency. What we need is a diverse range of very different environments, expressing a wide range of powerful ideas about how to support discovery. In some sense, the range of available environments is a reflection of our collective metascientific intelligence. And monoculture is the enemy of creative work.
</blockquote>

<p>
Today, structural diversity is low: scientific research is overwhelmingly performed in universities by professor-led research groups. These groups typically contain from 5 to 30 people, have at most two additional administrators beyond the principal investigator (i.e. the professor), and are composed of undergrads, graduate students, and postdocs. (There are, of course, many exceptions to the template I’ve outlined above.)
</p>

<p>
This structure influences the sort of scientific work that gets done. To graduate, PhD students need to have papers, which means that they need to work on projects that have sufficiently short time horizons to conclude before they graduate. Additionally, they need to have first-author papers, which means that they can’t work in large teams; if three students work together, they can’t all be first authors. Taken together, these considerations imply that most projects should take 10 person-years or less to accomplish.<sup><a href="#fn1">1</a></sup>
</p>

<p>
This is a long time, but not that long for science: unfortunately, most truly groundbreaking projects are “too big” for a single academic lab. Conversely, students are incentivized to publish in high-impact journals, and so projects that are “too small” are penalized for not being ambitious enough. Skilled academics are able to thread the needle between projects that are “too big” and projects that are “too small” and provide exactly the right amount of effort to generate high-impact publications within a reasonable time horizon.
</p>

<p>
These same challenges are echoed (on grand scale) for assistant professors. New faculty typically have 5 to 7 years before they must submit their tenure package, which means they’re forced to choose projects likely to work in that time frame (with inexperienced students and relatively few resources, no less). This disincentives tool-building, which generally chews up too much time to be an efficient use of resources for young labs, and puts a ceiling on their ambition.
</p>

<p>
These aren’t the only consequences of academia’s structure. “Deep” skills requiring more than a year or two of training are tricky, because even PhD students are only there for 4–6 years, so the time it takes to acquire skills comes directly out of the time they can be doing productive research. Additionally, certain skill sets (e.g. software engineering) command such a premium that it’s <a href="https://www.lesswrong.com/posts/9GweYgHABZAjH6T6f/">difficult to attract such people to academia</a>. Specialized instrumentation is another challenge: a given lab might only have the budget for a few high-end instruments, implying that its projects must be chosen accordingly.
</p>

<p>
A defender of the status quo might reasonably respond that smaller labs do lead to smaller projects, but in a greater number of areas: “what is any ocean, but a multitude of drops?” The academic structure, with its incentives for demonstrable progress, certainly cuts back on the number of costly, high-profile failures: most failed research groups never get tenure, limiting the damage.
</p>

<p>
Nevertheless, it seems that at this point many fields have been picked clean of projects with low enough barriers to entry to make them accessible to academics. Many remaining insights, including those needed to spawn new fields of science, may be simply out of reach of a decentralized array of small, independent academic groups. As Nielsen and Qiu put it, “you can stack up as many canonical researchers as you like and they still won't do the non-canonical work; it's a bottleneck on our capacity for discovery.” To support this point, they cite examples where large organizations were able to produce big advances inaccessible to their smaller counterparts: LIGO, the Large Hadron Collider, Brian Nosek’s Center for Open Science, and the Human Genome Project.
</p>

<p>
If we accept the claim that structural diversity is important, we ought to look for opportunities to expand structural diversity wherever possible. At the margin, this might look like supporting non-traditional hires in academic groups, including people who don’t fit into the established undergrad–graduate student–postdoc pipeline, and allowing for greater flexibility in the structure of labs (i.e. multiple professors within the same lab). More radical solutions might look like scientific start-ups where profitability can realistically be achieved, or <a href="https://www.nature.com/articles/d41586-022-00018-5">“focused research organizations”</a> where it cannot.<sup><a href="#fn2">2</a></sup> What could a well-managed team of professional scientists, focused solely on work “not easily possible in existing environments,” accomplish when pitted against some of the toughest unsolved problems in the world? We won’t know until we try.
</p>

<i>Thanks to Ari Wagen for reading a draft of this post.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    It’s true that there are a lot of efforts to create larger supra-lab organizations to tackle big questions: in chemistry, the NSF has funded <a href="http://www.nsf-cchf.com/aboutCCI.html">“centers for chemical innovation”</a> to unite like-minded researchers. But trying to forge a functional organization from a myriad of independent sovereign teams seems much harder than simply starting a new organization <i>de novo</i>.
  </li> 
  <li id="fn2">
    What if the discoveries needed to advance science are too big for these proposed solutions? For instance, it’s tough to imagine funding a Manhattan Project-style endeavor this way. I don’t think that it’s the case that science requires government-scale resources to push past stagnation, but if that were really true, it might be an argument for using the full might of state capacity to drive research, like something out of Liu Cixin’s novels. Note, however, that this would make the task of “picking the right problems” that much more important.
  </li> 
</ol>


]]></description>
              <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Omelas, Hirschman, Altom: On Healing Academia</title>
              <link>public/blog/20221018_omelas_hirschman_altom.html</link>
              <description><![CDATA[
<p>
<i>Spoilers below for Ursula Le Guin’s short story “The Ones Who Walk Away From Omelas.” If you haven’t read it, it’s short—go and do so now!</i>
</p>

<p>
<i>TW: child abuse, suicide.</i>
</p>

<p>
In her short story “The Ones Who Walk Away From Omelas,” Ursula Le Guin describes an idyllic town (Omelas) built entirely on the misery of a single, innocent child. The inhabitants of Omelas lead a utopian life, but are burdened with the knowledge that this child suffers so that they can prosper. Although the story is brief—only five pages long—Le Guin pulls no punches in the emotional weight of her writing:
</p>

<blockquote>
The child, who has not always lived in the tool room, and can remember sunlight and its mother's voice, sometimes speaks. "I will be good, " it says. "Please let me out. I will be good!" They never answer. The child used to scream for help at night, and cry a good deal, but now it only makes a kind of whining, "eh-haa, eh-haa," and it speaks less and less often. It is so thin there are no calves to its legs; its belly protrudes; it lives on a half-bowl of corn meal and grease a day.
</blockquote>

<p>
The metaphor, and underlying social commentary, is perhaps obvious. Le Guin goes on to (implicitly) attack utilitarians:
</p>

<blockquote>
[The spectators] would like to do something for the child. But there is nothing they can do. If the child were brought up into the sunlight out of that vile place, if it were cleaned and fed and comforted, that would be a good thing, indeed; but if it were done, in that day and hour all the prosperity and beauty and delight of Omelas would wither and be destroyed. Those are the terms. To exchange all the goodness and grace of every life in Omelas for that single, small improvement: to throw away the happiness of thousands for the chance of happiness of one: that would be to let guilt within the walls indeed.
</blockquote>

<p>
And those who are good at rationalizing away injustice:
</p>

<blockquote>
…as time goes on [the people] begin to realize that even if the child could be released, it would not get much good of its freedom: a little vague pleasure of warmth and food, no real doubt, but little more. It is too degraded and imbecile to know any real joy. It has been afraid too long ever to be free of fear. Its habits are too uncouth for it to respond to humane treatment. Indeed, after so long it would probably be wretched without walls about it to protect it, and darkness for its eyes, and its own excrement to sit in. Their tears at the bitter injustice dry when they begin to perceive the terrible justice of reality, and to accept it.
</blockquote>

<p>
And those who think that suffering “gives life meaning”:
</p>

<blockquote>
[The people] know that they, like the child, are not free. They know compassion. It is the existence of the child, and their knowledge of its existence, that makes possible the nobility of their architecture, the poignancy of their music, the profundity of their science. It is because of the child that they are so gentle with children. They know that if the wretched one were not there sniveling in the dark, the other one, the flute-player, could make no joyful music as the young riders line up in their beauty for the race in the sunlight of the first morning of summer.
</blockquote>

<p>
The story concludes by describing the last, and rarest, response to Omelas:
</p>

<blockquote>
At times one of the adolescent girls or boys who go see the child does not go home to weep or rage, does not, in fact, go home at all. Sometimes also a man or a woman much older falls silent for a day or two, then leaves home.... They leave Omelas, they walk ahead into the darkness, and they do not come back. The place they go towards is a place even less imaginable to most of us than the city of happiness. I cannot describe it at all. It is possible that it does not exist. But they seem to know where they are going, the ones who walk away from Omelas.
</blockquote>

<p>
As I read the story, the conclusion is that few people have the moral courage to reject injustice entirely. Rejecting a broken system is scary, and risky; most people would rather lull themselves into complacency than try and build a better world. But implicit in this analysis is that the ones who walk away are making the right decision, and the ones who stay in Omelas are making the wrong one. In our own flawed world, when is this true?
</p>

<p>
The past decades have seen a great purge within American culture. The “Me Too” movement exposed the prevalence of sexual assualt within Hollywood, Boston Globe investigations revealed massive corruption within the Catholic Church, vast protests over the summer of 2020 decried systematic racism within American institutions, and in general institutions of all forms have come under attack for their failings. Every university has a racist legacy to reckon with; every business has an investor with unsavory political beliefs.
</p>

<p>
To some, this is the oft-derided “cancel culture”—finding fault with everyone, and viciously attacking people and organizations for even the slightest offenses. If “all have sinned and fall short,” then evil can never truly be eradicated, and all that this movement can do is destroy venerable institutions without constructing anything better. To others, however, making peace with evil is deplorable. Evil is the original pandemic; its capacity to spread is unparalleled, and its damage immeasurable. No compromise can be made with the enemy; no peace can be made with injustice.
</p>

<p>
Neither heuristic is sufficient; as usual, wisdom resides in the dialectic. For everyone who seeks to improve the world, then, the question presents itself anew: can one work within the system, flaws and all, or must change come from outside?
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Albert Hirschman analyzed these issues in his 1970 book <i>Exit, Voice, and Loyalty</i>, which analyzes how consumers respond to declining quality. Hirschman draws a basic distinction between “exit,” the movement of individuals away from their current allegiance and towards a competitor, and “voice,” when individuals remain loyal and protest the change from within the system. An insightful point that Hirschman makes is that different fields of study view these options in different ways. Economists, who often think in terms of “perfect competition,” tend to assume exit is the most meaningful option, whereas political scientists prefer voice (i.e. engagement with the political process), thinking of exit as craven or traitorous.
</p>

<p>
These two options can be described in other terms. Exit is the mindset of the young reformer, the naïve, the idealistic, who believes the old system is beyond saving and a new world must be birthed. In contrast, voice is the perspective of small-c conservatism and Chesterton’s Fence, the wisdom of the agéd seer who has seen countless evils and knows how fragile civilization can be. This dichotomy does not separate Red from Blue; a love of exit unites Robespierre, Thunberg, and Trump, while voice embraces Henry Clay and Deng Xiaopeng alike.
</p>

<p>
When is exit better, and when is voice better? In certain circumstances exit can preclude voice by allowing only the most motivated and skilled members to escape a failing system. This removes the very elements of the populace necessary for change through voice, resulting in what Hirschman calls “an oppression of the weak by the incompetent and an exploitation of the poor by the lazy.” In this scenario, then, voice is superior. Exit also only functions when there are legitimate alternatives to the current system: in a monopoly, there can be no exit.
</p>

<p>
(Different Christian sects present an interesting case study here. In Catholicism, the Church is one united organization, and so exit is not an option: only voice is permitted. In contrast, Protestants are divided and subdivided into innumerable organizations, and so any individual Protestant can easily “vote with their feet” and join a church they agree with. The myriad failures of both sects suggests that neither solution is perfect.)
</p>

<p>
In contrast, voice presumes that change is possible—that the system is able to be reformed, and that doing so is more effective than simply starting over. For governments or Catholics, this may be true; for smaller organizations, it seems less true. An idealized capitalist market proceeds through relentless “creative destruction”: old firms become stagnant and stop innovating and are replaced by young startups, who last a few decades before themselves becoming stagnant. Creating the next Google, in most cases, is easier than fixing Google.
</p>

<p>
In other cases, the failures of the current system are so all-encompassing, so total, that it’s almost impossible to conceptualize the right reforms from within. Scott Alexander (of <i>Astral Codex Ten</i>, née <i>Slate Star Codex</i>) describes this phenomenon in his piece <a href="https://slatestarcodex.com/2015/06/06/against-tulip-subsidies/">“Against Tulip Subsidies,”</a> which critiques the discourse around rising tuition costs. Reforming how we pay college tuition, Alexander argues, “would subsidize the continuation of a useless tradition that has turned into a speculation bubble” and thus entrench the very thing we ought to uproot. Voice is better suited for marginal or incremental change; if you want to think big, start from scratch.
</p>

<p>
The choice between voice and exit thus hinges on several factors: how hard would it be to fix the current system, and how hard would it be to build a new and better system from the ground up?
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
As a high school student learning about synthetic organic chemistry, I came across a New York Times article entitled <a href="https://web.archive.org/web/20220130071514/https://www.nytimes.com/1998/11/29/magazine/lethal-chemistry-at-harvard.html">“Lethal Chemistry At Harvard.”</a> The article told the story of Jason Altom, a brilliant Ph.D. candidate in organic chemistry who took his life after years of struggling on a challenging total synthesis problem. Altom’s story resonated with me so much that, as an undergraduate, I printed out his <a href="https://pubs.acs.org/doi/10.1021/ja9915201">aspidophytine synthesis</a> and pinned it above my desk as a sort of impromptu <i>memento mori</i>. (The synthesis, and Scheme 4 in particular, is transcendently beautiful.) He died just weeks after I was born; now I work in the same building in which he worked.
</p>

<p>
Altom’s story is by no means unique: <a href="https://www.chronicle.com/article/harvard-faces-the-aftermath-of-a-graduate-students-suicide/">according to the Chronicle of Higher Education</a>, he was one of three graduate students in his lab to commit suicide within an 11-year period, with more suicides occuring in other labs, departments, and universities. Although some of the issues highlighted by his suicide have been addressed (I have an advising committee now in addition to my professor), the core reality remains the same: the success or failure of a given graduate student depends almost completely on their relationship with their advisor, making students willing to sacrifice almost anything to please their PI. Even those who “succeed” often emerge damaged by the process, aspirations lowered and ambition quenched.
</p>

<p>
Nor are graduate-student <a href="https://www.vox.com/2020/4/15/21214734/deaths-of-despair-coronavirus-covid-19-angus-deaton-anne-case-americans-deaths">“deaths of despair”</a> the only problem within academia. Much has been written about the failures of the scientific establishment: <a href="https://newscience.substack.com/p/laws-of-science">problems</a> with <a href="https://www.science.org/content/blog-post/systematic-fraud">fraud</a> and <a href="https://guzey.com/how-life-sciences-actually-work/#large-parts-of-modern-scientific-literature-are-wrong"><i>p</i>-hacking</a>, <a href="https://guzey.com/how-life-sciences-actually-work/#universities-seem-to-maximize-their-profits-with-good-research-being-a-side-effect">parasitic university rent-seeking</a>, <a href="https://guzey.com/how-life-sciences-actually-work/#peer-review-is-a-disaster">problems with peer review</a>, <a href="https://www.notboring.co/p/gassing-the-miracle-machine">a general increase in bureaucracy</a> and concomitant decline in research productivity, <a href="https://newscience.org/how-software-in-the-life-sciences-actually-works-and-doesnt-work/">a failure to fund basic methods</a>, and many more. It’s obvious that the ivory tower is not without blemish. The human cost, typified by Altom, is but the most visible apex of a whole mountain of problems.
</p>

<p>
Nevertheless, although Le Guin might not approve, academia can be defended through a utilitarian argument: the considerable benefits of academic research and education outweigh the human costs and other drawbacks. This argument, while compelling, presumes that the functions of graduate school are irreplaceable—that no better alternative can be created, that exit is impossible. I would also argue that, although academic research is good, it’s likely it could be improved and made better. Indeed, the very importance of research makes it uniquely susceptible to decline; the “ceiling” of scientific progress is so high that even considerable atrophy still leaves behind an important and productive institution. Hirschman again:
</p>

<blockquote>
The wide latitude humans societies have for deterioration is the inevitable counterpart of man’s increasing productivity and control over his environment. Occasional decline as well as prolonged mediocrity—in relation to achievable performance levels—must be counted among the many penalties of progress.
</blockquote>

<p>
If we accept that improvements to the “prolonged mediocrity” of the academic research system are necessary, the question becomes familiar: can the system be fixed from within (voice), or would it be easier to start from scratch (exit)?
</p>

<p>
Reforming any huge system is non-trivial, but academia presents special difficulties; as Hirschman describes, academia protects itself by selecting for those who can stomach its evils. The conscientious and compassionate flee, for the most part, to undergraduate institutions or industrial positions, enriching R1 faculty positions in those less encumbered by moral concerns. (This statement notwithstanding, I don’t mean to condemn faculty <i>en masse</i>; I have the utmost love and admiration for many faculty members I know who push back against the excesses of the system.) This selection effect deprives voice of its strongest assets, making reform that much harder.
</p>

<p>
Exit, too, is not without its challenges. The problems we face cannot be solved simply by moving to a different university or a different country; the interconnectedness of academia makes it monolithic. To contemplate exit from the academic system means a total revolution, erasing centuries of norms and paradigms—the relationship between professors and graduate students that Altom blamed for his death, which echoes the ancient master–apprentice relationship, dates back to <a href="https://academic.oup.com/book/39837/chapter/339974060">Justus von Liebig</a> in 19th-century Germany. Exit means new journals, new funding paradigms, new ways to recruit students and new ways to train them. Proper regime change leaves no stone unturned.
</p>

<p>
Perhaps it's the optimism of youth, or a misguided belief in the possibility of progress. But day by day I find myself believing less in voice and more in exit. The past few years have seen a flourishing of non-academic models for science. New ventures like <a href="https://www.arcadia.science/">Arcadia</a>, the <a href="https://arcinstitute.org/">Arc Institute</a>, <a href="https://newscience.org/">New Science</a>, <a href="https://astera.org/">the Astera Institute</a>, and the more venerable <a href="https://www.santafe.edu/">Santa Fe Institute</a> are demonstrating that science can be done outside a university—and the <a href="https://www.science.org/content/article/data-check-us-government-share-basic-research-funding-falls-below-50">growing interest</a> from industry and philanthropists for funding basic research indicates that the demand is there. But at the same time, these little endeavors all combined represent less than a percent of the current governmental–academic complex. Time will tell, too, if the flaws of our current system are fated to be recapitulated anew in any replacement.
</p>

<p>
Will it be possible to build a robust and practical scientific ecosystem in parallel to the current system, gradually moving more and more pieces into position until at last <a href="https://en.wikipedia.org/wiki/Tl%C3%B6n,_Uqbar,_Orbis_Tertius">Uqbar</a> becomes reality? I don’t know—but I’m excited to find out.
</p>

<i>Thanks to Eugene Kwan, Ari Wagen, and Taylor Wagen for reading a draft of this piece.</i>

]]></description>
              <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Entropy in the Water Dimer</title>
              <link>public/blog/20221006_water_dimer.html</link>
              <description><![CDATA[
<p>
The failure of conventional calculations to handle entropy is well-documented. Entropy, which <a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula">fundamentally</a> depends on the number of microstates accesible to a system, is challenging to describe in terms of a single set of XYZ coordinates (i.e. a single microstate), and naïve approaches to computation simply disregard this important consideration.
</p>

<p>
Most programs get around this problem by partitioning entropy into various components—translational, rotational, vibrational, and configurational—and handling each of these separately. For many systems, conventional approximations perform well. Translational and rotational entropy depend in <a href="https://gaussian.com/thermo/">predictable ways</a> on the mass and moment of inertia of the molecule, and vibrational entropy can be estimated from normal-mode analysis at stationary points. Conformational entropy is less easily automated and as a result is often neglected in the literature (see the <a href="https://pubs.acs.org/doi/10.1021/ja5111392">discussion</a> in the SI), although <a href="https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc00621e">some</a> <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.0c01213">recent</a> <a href="https://pubs.rsc.org/en/content/articlelanding/2022/cp/d1cp05805c">publications</a> are changing that. 
</p>

<p>
In general, however, the approximations above only work for ground states. To quote the Gaussian vibrational analysis <a href="https://gaussian.com/vib/">white paper</a>:
</p>

<blockquote>
Vibrational analysis, as it’s descibed in most texts and implemented in <i>Gaussian</i>, is valid only when the first derivatives of the energy with respect to displacement of the atoms are zero. In other words, the geometry used for vibrational analysis must be optimized at the same level of theory and with the same basis set that the second derivatives were generated with. Analysis at transition states and higher order saddle points is also valid. Other geometries are not valid. 
</blockquote>

<p>
While this isn't a huge issue in most cases, since most processes are associated with a minima or first-order saddle point on the electronic energy surface, it can become a big deal for reactions where entropy significantly shifts the position of the transition state (e.g. Figure 4 in <a href="https://pubs.acs.org/doi/10.1021/ja208779k">this study</a> of cycloadditions). Even worse, however, are cases where entropy constitutes the entire driving force for the reaction: association/dissociation processes. In his elegant <a href="https://pubs.acs.org/doi/pdf/10.1021/acs.organomet.8b00456">review</a> of various failures of computational modelling, Mookie Baik illustrated this point by showing that no transition state could be found for dissociation of the water dimer in the gas phase:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221006_baik.png style="width:500px;" />
  <figcaption>
  Figure 11 from Baik's review.
  </figcaption>
</figure>

<p>
Panel (b) of this figure shows the electronic energy surface for dissociation, which monotonically increases out to infinity—there's never a maximum, and so there's no transition state. To estimate the position of the transition state, Baik proposes computing the entropy (using the above stationary-point approximations) at the first few points, where the two molecules are still tightly bound, and then extrapolating the curve into a sigmoid function. Combining the two surfaces then yields a nice-looking (if noisy) curve with a clear transition state at an O–H distance of around 3 Å.
</p>

<p>
This approach, while clever, seems theoretically a bit dubious—is it guaranteed that entropy must always follow a smooth sigmoidal interpolation between bound and unbound forms? I thought that a more direct solution to the entropy problem would take advantage of <i>ab initio</i> molecular dynamics. While too slow for most systems, AIMD intrinsically accounts for entropy and thus should be able to generate considerably more accurate energy surfaces for association/dissociation events.
</p>

<p>
Using <a href="https://github.com/corinwagen/presto"><i>presto</i></a>, I ran 36 constrained 100 ps <i>NVT</i> simulations of the water dimer with different O–O distances, and used the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540130812">weighted histogram analysis method</a> to stitch them together into a final potential energy surface. I then compared these results to those obtained from a direct <span class=code>opt=modredundant</span> calculation (with frequencies at every point) from Gaussian. (All calculations were performed in Gaussian 16 at the wB97XD/6-31G(d) level of theory, which overbinds the water dimer a bit owing to basis-set superposition error.)
</p>

<p>
The results are shown below (error bars from the AIMD simulation are derived from bootstrapping):
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221006_PES.png style="width:550px;" />
  <figcaption>
  Comparison of different approaches for studying dissociation of the water dimer.
  </figcaption>
</figure>

<p>
As expected, no transition state can be seen on the black line corresponding to the electronic energy surface, or on the green line corresponding to enthalpy. All methods that depend on normal-mode analysis show sizeable variation at non-stationary points, which is perhaps unsurprising. What was more surprising was how much conventional DFT calculations (purple) overestimated entropy relative to AIMD! <a href="https://pubs.acs.org/doi/full/10.1021/jp205508z">Correcting for low-frequency vibrational modes</a> brought the DFT values more into line with AIMD, but a sizeable discrepancy persists.
</p>

<p>
Also surprising is how different the AIMD free energy surface looks from Baik's estimated free energy surface. Although the scales are clearly different (I used O–O distance for the X axis, whereas Baik used O–H distance), the absence of a sharp maximum in both the AIMD data and the corrected Gibbs free energy data from conventional DFT is striking. Is this another instance of <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4124006/">entropy–enthalpy compensation</a>?
</p>

<p>
In the absence of good gas-phase estimates of the free energy surface, it's tough to know how far the AIMD curve is from the true values; perhaps others more skilled in these issues can propose higher-level approaches or suggest additional sources of error. Still, on the meta-level, this case study demonstrates how molecular dynamics holds promise for modelling things that just can't be modelled other ways. Although this approach is still too expensive for medium to large systems, it's exciting to imagine what might be possible in 5 or 10 years!
</p>

<em>
Thanks to Richard Liu and Eugene Kwan for insightful discussions about these issues.
</em>
]]></description>
              <pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Talent</title>
              <link>public/blog/20220928_talent.html</link>
              <description><![CDATA[
<p>
<i>Talent</i>, by Tyler Cowen and Daniel Gross, is a book about talent selection—in other words, a book about hiring. Although I confess this sounded very boring to me initially, the authors address this concern right away:
</p>

<blockquote>
Talent search is one of the most important activities in virtually all human lives. Elon Musk personally interviewed the first <i>three thousand</i> employees at SpaceX because he wanted to make sure the company was hiring the right people. Don’t think of talent search as a problem faced by “the boss” or by human resource departments… Just about everyone is on a quest to find talent in others or to show off their own.
(emphasis original)
</blockquote>

<p>
Not only is finding talented people a necessary prerequisite for any great endeavor, but “excess credentialism and highly bureaucratic hiring procedures” means that existing protocols for finding excellence are at best inefficient and at worst counterproductive. This problem is everywhere, the authors assert: venture capital is full of money looking for people to fund, discrimination against women and minorities means that talented people don’t get the resources they deserve, and increasing globalization means that ever-larger pool of potential talent are entering the global market. The authors summarize by stating that “the world’s inability to find and mobilize enough talent [is] one of the most significant failures of our time,” and hope that this book can be the first of many seeking to address this problem.
</p>

<p>
How, then, do we do better at identifying talent?
</p>

<h2>Interviewing</h2>

<p>
Interviewing, although oft-derided, remains one of the best ways to learn about someone. The authors devote considerable time to the question of how best to interview someone, and especially how to gain useful, non-scripted information about personality:
</p>

<blockquote>
The best interviews are not formal interviews at all. We’re sure you can think of other creative ways to take the candidate out of interview mode and into their everyday self. This is important, because the everyday self is what you’ll get if you hire them.
</blockquote>

<p>
Many potential questions are discussed: some memorable examples focus on examining someone’s self-conception of their past and their current habits (e.g. “what are the open tabs on your browser right now?”). The best questions prompt authentic, off-the-cuff answers that give insight not into what the candidate wants to tell you but into who they really are. (There’s a fundamental pessimism about people’s self-deception that permeates this section.)
</p>

<p>
The authors close by emphasizing the importance of being a good conversationalist, a skill underrated by many technical people:
</p>

<blockquote>
Conversing well with potential hires or award winners is one of the most important things that you can do. Keep in mind that it not only brings you talent, but it helps you retain talent and mobilize those individuals to use their skills better. If you cannot relate to your talent at a conversational level, you will learn less, you will build less trust, and you will end up relying too heavily on direct monetary incentives to motivate people.
</blockquote>

<h2>Zoom Interviews</h2>

<p>
Cowen and Gross devote an entire chapter specifically to Zoom interviews, which they feel are underrated and can be just as useful as in-person interviews. Although many people find Zoom uncomfortable, this may actually be an advantage of the medium:
</p>

<blockquote>
Many women have remarked on Twitter that they feel on more equal footing on a Zoom call… A lot of people used to coming across as high-status and charismatic in person will feel a bit lost through the screen. Witty repartee also can be hard to pull off over an internet call, and that too may diminish the stature of those individuals who are used to using clever banter to command a room.
</blockquote>

<p>
You might be even one of these people:
</p>

<blockquote>
One of the hardest mental adjustments for people to make is to realize how much their positive affect relies on their in-person rejection of high social status. To give a simple example, you might not be as witty as you think! You will do better in the online call if you realize how much your in-person presence relies on a kind of phoniness.
</blockquote>

<p>
I also liked this observation about how Zoom interviews can be more equitable:
</p>

<blockquote>
The supposed information poverty of the online interview also may help some interviewers overcome potential biases against women and also some minority groups… The online interview, by making everyone less charismatic, may help counter your bias against these individuals.
</blockquote>

<h2>Intelligence</h2>

<p>
Cowen and Gross review a variety of data about the importance of intelligence in various careers. The picture they present is complicated; IQ is clearly important for many professions, but perhaps less so than many people think. In general, Cowen and Gross seem to conclude that intelligence is overrated in hiring:
</p>

<blockquote>
In what might seem like a paradox, it can be hard to spot intelligence, drive, and other positive qualities at the very, very top. Why? Well, the very, very top of the market usually is underexplored territory, virtually by definition. The most talented people usually are doing something extraordinary and fairly new, and often they are so unbelievably talented that most of us just don’t have the ability to appreciate their talents, at least not until their final achievements are on full display.
</blockquote>

<p>
Cowen and Gross also reference Marc Andressen’s essay <a href="https://fictivekin.github.io/pmarchive-jekyll/how_to_hire_the_best_people.html">“How To Hire the Best People You’ve Ever Worked With,”</a> which argues that drive, self-motivation, curiosity, and ethics are more important considerations than raw intelligence. Furthermore, they point out that intelligence is already priced into the market—everyone knows smart employees are good, and so “the obviously smart people are not always the obvious bargains.”
</p>

<p>
(It strikes me that humanities PhDs might be an underutilized pool of high-IQ workers, albeit with little technical training. Perhaps a business with an acute need for raw intelligence and few required technical skills might capitalize on this… this probably already exists.)
</p>

<h2>Personality</h2>

<p>
Much as they did for intelligence, Cowen and Gross analyze the five-factor personality model with an eye towards finding good hires at the margin. Their literature review finds that high conscientiousness is “the single best predictor of overall job performance” (other factors being poorly predictive), but they note that certain fields may benefit from a less responsible approach: 
</p>

<blockquote>
Sometimes leaders of organizations can have too much rather than too little conscientiousness…. leadership skills often involve a mix of creativity and daring and ability to reimagine the risky future, and those are not necessarily the traits found in the people who punch the time clock promptly every day. Elon Musk would have gotten in less trouble had he not smoked a joint on the live video stream of Joe Rogan’s podcast, but a more sedate Elon Musk probably would not have built SpaceX and Tesla with the same fervor.”
</blockquote>

<p>
(Additionally, the authors note that conscientiousness, like intelligence, is already priced into the market.)
</p>

<p>
The authors go on to contrast conscientiousness with stamina, which they call “one of the great underrated concepts for talent search, especially when you are looking for top performers and leaders and major achievers.” Stamina refers to perseverance of effort, or a person’s ability to keep working diligently for long periods of time: since returns to learning and improvement compound over time, high stamina ends up making a huge difference in the long run. The authors continue: 
</p>

<blockquote>
Don’t just think in terms of levels of current ability, because over time, rates of change very often prove to be more important. Think in terms of trajectories. When it comes to a job or fellowship candidate, think about the person’s developmental curve and whether the candidate is truly committed to consistent, perpetual self-improvement, as you might expect from a top athlete or musician…. If a person doesn’t seem to think much about self-improvement, they still might be a good hire, but then you had better be pretty content with their currently demonstrated level of expertise.
</blockquote>

<p>
Other “more exotic” traits, both good and bad, that the authors discuss are:
</p>

<ul>
<li>
<b>Morlockism</b>, the capacity to lock oneself “in a cave” and work very hard for several days. 
</li>
<li>
<b>Sturdiness</b>, “the quality of getting work done every day, with extreme regularity.” (Sturdiness and Morlockism appear to be substitutionary traits.) 
</li>
<li>
<b>Generativeness</b>, a capacity of people to “talk quickly, move quickly, and in general seem to be enthralled with life.” (Balaji Srinivasan is cited as an example.)
</li>
<li>
<b>Insecure overachievement</b>, the quality of “never quite feeling comfortable with your output despite knowing at a deep level that it <i>is</i> good” (emphasis original), which often results in high output but problematic psychology. Closely related, but more negative, is <b>pessimistic perfectionism</b>, exhibited by people who “believe that their work is never good enough” and “don’t have the ongoing drive and impetus of insecure overachievers.”
</li>
<li>
<b>Clutteredness</b>, a trait of otherwise smart people who are unable to express their thoughts in a coherent or intelligible manner. Perhaps obviously, such individuals ought not to be put in positions that emphasize clear communication.
</li>
<li>
The distinct trait of <b>vagueness</b>, where someone thinks in “mushy concepts and unspecific terms… not really drawing any conceptual distinctions at all.” This is problematic for strongly analytical roles.
</li>
<li>
<b>Precocity</b>, how young a person first displayed aptitude. Probably more important for fluid intelligence than for skills that require long accumulation of knowledge.
</li>
<li>
<b>Adhesiveness</b>, or the ability to be a good team player and fix problems (related to high social intelligence). This seems largely orthogonal to many other skills discussed, and may be crucial for some jobs and largely irrelevant for others.
</li>
<li>
<b>Ability to perceive and climb the right hierarchy</b>, which lacks a catchy moniker. Many promising and highly skilled people manage to waste time focusing on the wrong problems. Cowen references chess players who never consider the world outside chess, and Gross describes potential founders who focus more on prestige than actually building their company. (To quote Peter Thiel: “be long substance, short status.”) I’m reminded of incredibly brilliant quiz bowl players I knew who never managed to translate their skills into success outside academic competitions.
</li>
<li>
Another interesting trait is <b>how many conceptual frameworks one possesses</b>. A person with many frameworks can put themselves into the mind of an engineer, a salesperson, a regulator, or a user, whereas more limited people struggle to escape their own viewpoint.
</li>
</ul>

<p>
The discussion of these traits was one of my favorite parts of the book. I found it very useful to imagine various personalities and dissect what their strengths and flaws might be; except in rare cases, it seems that every strength has a corresponding flaw. As the authors write, “skill in spotting flaws in other people can lead to very positive matching outcomes, and that is another reason the dialectical perspective of seeing both the good and bad sides of talent is highly useful.”
</p>

<h2>Overcoming Bias</h2>

<p>
The authors first discuss disabilities, observing that often disability can augment talent through either “redirection of effort” or “compensation and adaptation.” The first case is typified by Richard Branson (founder of Virgin Galactic), who recounts how his dyslexia made it difficult for him to focus on details and pushed him towards important big-picture thinking. In contrast, the second case is typified by blind lawyers, who frequently know the law better than their sighted counterparts because they are unable to look it up as quickly. In either case, what appears to be purely a disadvantage in fact leads to subtle advantages which might be easily overlooked: “disability is a highly complex notion and by no means always negative on the whole.”
</p>

<p>
The subsequent section focuses on women and minorities. As alluded to in the section on Zoom interviewing, the authors observe that there are a “fairly limited range of behaviors allowed” for women in the workplace, and as a result that many women’s talents are not fairly assessed. In particular, aggression is viewed as a positive for high-status men but a negative for high-status women. The authors then summarize a variety of literature which supports “the notion of a <i>confidence gap</i> as one of the main differences between men and women in the workplace” (emphasis added). In light of this finding, Cowen and Gross make three points:
</p>

<ol>
<li>
Favor women for jobs where low confidence is advantageous; “for many jobs, including in politics, diplomacy, and prudential supervision, epistemic humility is more important than risk-taking.”
</li>
<li>
Look extra-hard for confident women, because their skills are likely undervalued by the market; you can “gain from the world’s statistical discrimination and in the process rectify an injustice.”
</li>
<li>
Be mindful that risk-taking and competitiveness are often viewed as key values in male-dominated organizations, even when they needn’t be, and that this faulty institutional self-image may lead to unnecessary barriers to the advancement of women.
</li>
</ol>

<p>
The authors conclude this section by citing work that suggests women are better at talent spotting, both “better at assessing the intelligence of both men and women” and “better than men at detecting deceit”; so good talent selection should involve women!
</p>

<p>
Cowen and Gross then discuss hiring minorities. As with women, the main challenge is perceiving the real talents of the people you talk to; cultural differences often lead to more awkward and formal conversations that struggle to escape “interview mode.” It’s hard to overcome this, but the authors propose the exercise of putting oneself into a situation where you feel culturally uncomfortable and observing how you struggle to present yourself and convey your ideas naturally. Emotionally internalizing this feeling can help you while interviewing those from different backgrounds. If nothing else, realizing that you struggle to perceive the abilities of minorities accurately can help you consciously compensate in the other direction. 
</p>

<h2>Conclusion</h2>

<p>
One of my key takeaways from <i>Talent</i> is this: every job requires aptitude along certain dimensions and is relatively insensitive to variation along other dimensions. The key to intelligent recruiting is to attune yourself to evaluating people only along important dimensions of talent while ignoring unimportant dimensions. Everyone has shortcomings; the most efficient hiring strategy is not to hire people without shortcomings, but to make sure their shortcomings are well-tolerated in the job you’re hiring for. (For instance, disagreeability is often viewed as positive for startup founders, but would certainly be deleterious for a salesperson. A disorganized chemist would do better in exploratory synthesis than in a job that required precise kinetic measurements.)
</p>

<p>
Another nice thought I got from <i>Talent</i> was a fundamental positivity about the ability of intelligent hiring to alleviate bias or prejudice. Cowen and Gross point out that if you believe a certain group of people is fundamentally overlooked or discriminated against by the market, the logical implication of that belief is that you should hire from that group: if you’re right, you’re not only helping yourself but also your hires. This little bit of free-market thinking turns the issue of bias from a negative one (“how is society mistreating people?”) to a positive one (“how can my talent search benefit by avoiding existing prejudice?”), which I found helpful.
</p>

<p>
But perhaps the most fundamental conclusion is simply that finding talent is an important, and underrated, skill for many areas of life. In the authors’ own words:
</p>

<blockquote>
The vision that talent search is ‘a thing,’ that it is an art that can be learned and improved on, and that it can be taught and communicated to others—that is the fundamental point of this presentation.
</blockquote>

<p>
I’d recommend this book both for people looking for talent (professors, founders, leaders) and for those hoping to display their own talent accurately to the world. 
</p>

]]></description>
              <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Plotting Chemical Space with UMAP and Molecular Fingerprints</title>
              <link>public/blog/20220926_plotting_diversity.html</link>
              <description><![CDATA[
<p>
In our recently published work on <a href="https://corinwagen.github.io/public/blog/20220901_screening_for_generality.html">screening for generality</a>, we selected our panel of model substrates in part using cheminformatic techniques. We're not the only people to do this, obviously: cheminformatics is a busy and important field, and even in organic chemistry there's lots of papers using similar techniques these days (I liked <a href="https://pubs.acs.org/doi/10.1021/jacs.1c12203">this work</a> from the Doyle lab). But since often the people who would benefit most from a new technique are the people who might be most intimidated by wading though documentation, I thought I'd post some simple example code here that others can copy-and-paste and modify to suit their own ends.
</p>

<p>
There are lots of ways to approach plotting chemical space, but fundamentally all approaches must address two big questions:
</p>

<ol>
  <li>
    How do you convert molecules into some numeric representation?
  </li>
  <li>
    Once you have numeric representations of all your molecules, how do you plot this?
  </li>
</ol>

<p>
I chose a relatively simple approach to the first question: molecular fingerprints (if you don't know what these are, I liked <a href=https://towardsdatascience.com/a-practical-introduction-to-the-use-of-molecular-fingerprints-in-drug-discovery-7f15021be2b1>this introduction</a> from <i>Towards Data Science</i>).
Based on <a href="https://greglandrum.github.io/rdkit-blog/similarity/reference/2021/05/26/similarity-threshold-observations1.html">Greg Landrum's findings</a>, I used the RDKit7 fingerprint. RDKit is the premier cheminformatics package, and well worth a download for anyone interested in these concepts.
</p>

<p>
For the second question (dimensionality reduction), I used the <a href="https://arxiv.org/abs/1802.03426">UMAP</a> algorithm. There are other approaches to this, like tSNE or PCA, but in my opinion there are <a href="https://blog.reverielabs.com/mapping-chemical-space-with-umap/">relatively convincing reasons</a> to favor UMAP (although <a href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x">this paper</a> points out some limitations).
</p>

<p>
Without further ado, then, here's some example code to take a list of IUPAC-type names and generate a 2D representation:
</p>

<pre class=code-block>
from rdkit import Chem
from urllib.request import urlopen
import re, tqdm, sys, umap
import numpy as np
import matplotlib.pyplot as plt

# make matplotlib look good
plt.rc('font', size=11, family="serif")
plt.rc('axes', titlesize=12, labelsize=12)
plt.rc(['xtick', 'ytick'], labelsize=11)
plt.rc('legend', fontsize=12)
plt.rc('figure', titlesize=14)
%matplotlib inline
%config InlineBackend.figure_format='retina'

# function for turning names into SMILES strings, because I find writing SMILES by hand impossible
def smiles_from_name(name):
    try:
        url_name = re.sub(" ", "%20", name)
        url = 'http://cactus.nci.nih.gov/chemical/structure/' + url_name + '/smiles'
        smiles = urlopen(url, timeout=5).read().decode('utf8')
        return smiles
    except Exception as e:
        print(name + " failed SMILES conversion")

class THbC():
    """ A tetrahydrobetacarboline. """
    def __init__(self, group, substituent, color="grey"):
        self.name = f"2-benzyl-1-({group})-{substituent}2,3,4,9-tetrahydro-1H-pyrido[3,4-b]indole"
        self.smiles = smiles_from_name(self.name)
        self.mol = Chem.MolFromSmiles(self.smiles)
        self.fingerprint = None
        self.color = color

    def get_fingerprint(self):
        if self.fingerprint is None:
            self.fingerprint = Chem.RDKFingerprint(self.mol, maxPath=7, branchedPaths=False)
        return self.fingerprint

# I just wrote out a lot of aromatic groups...
groups = [
    "phenyl", "4-methylphenyl", "4-methoxyphenyl", "4-fluorophenyl", "4-chlorophenyl", "4-bromophenyl",
    "4-(trifluoromethyl)phenyl", "4-nitrophenyl", "4-cyanophenyl", "piperonyl", "dihydrobenzofuryl",
    "3-methylphenyl", "3-methoxyphenyl", "3-fluorophenyl", "3-chlorophenyl", "3-bromophenyl",
    "3-(trifluoromethyl)phenyl", "3-nitrophenyl", "3-cyanophenyl", "2-methylphenyl", "2-methoxyphenyl",
    "2-fluorophenyl", "2-chlorophenyl", "2-bromophenyl", "2-(trifluoromethyl)phenyl",
    "2-nitrophenyl", "2-cyanophenyl", "2-pyridyl", "3-pyridyl", "4-pyridyl", "2-thiophenyl", "3-thiophenyl",
    "2-furyl", "3-furyl", "2-quinolinyl", "3-quinolinyl","6-quinolinyl", "5-quinolinyl", "8-quinolinyl",
    "5-indolyl", "3-indolyl", "7-azaindol-3-yl", "2-pyrrolyl", "3-pyrrolyl", "2-thiazolyl", "4-thiazolyl",
    "5-thiazolyl", "5-phenylisoxazol-3-yl", "imidazol-2-yl" "5-pyrimidyl", "5-indazolyl", "3-pyrazolyl",
    "4-pyrazolyl", "4-imidazolyl"
]

# substituents on the indole ring, and corresponding colors
subs = ["", "6-methoxy", "6-chloro"]
colors = ["grey", "red", "green"]

# build THbC objects (this might take a minute or two)
mols = list()
for group in tqdm.tqdm(groups):
    for sub, c in zip(subs, colors):
        mols.append(THbC(group=group, substituent=sub, color=c))

# generate UMAP embedding
crds = umap.UMAP(n_components=2, n_neighbors=20, min_dist=0.1, metric="jaccard").fit_transform([m.get_fingerprint() for m in mols])

# plot the result
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))
plt.scatter(crds[:,0], crds[:,1], c=[m.color for m in mols], s=20, alpha=0.8)
ax.set_xticks([])
ax.set_yticks([])
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.tight_layout()
plt.show()
</pre>

<p>
This code generates the following image:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20220926_umap.png style="width:400px;" />
  <figcaption>
    A 2D plot of the molecules shown above. Colors represent different substitution on the indole ring.
  </figcaption>
</figure>

<p>
  Although this program is a little clunky (slow calls to the CACTUS web service), it works well enough and is easy to modify as needed (to label the individual molecules, or apply a clustering algorithm to pick out model substrates). I hope you find this useful!
</p>

<p>
<b>
  Update: as of November 2024, you can generate plots of chemical space like this through <a href="https://www.rowansci.com">Rowan</a>, my computational chemistry startup. You can read more about this feature on <a href="https://www.rowansci.com/tools/descriptors">our website</a>.
</b>
</p>
]]></description>
              <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Computational Report Cards</title>
              <link>public/blog/20220919_report_cards.html</link>
              <description><![CDATA[
<p>
In the course of preparing a literature meeting on post-Hartree–Fock computational methods last year, I found myself wishing that there was a quick and simple way to illustrate the relative error of different approximations on some familiar model reactions, like a "report card" for different levels of theory. I couldn't find any such graphic online, so I decided to make one (click image to view high-res PDF):
</p>

<figure>
  <a href="https://corinwagen.github.io/public/img/20220919_report_card.pdf">
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220919_report_card.png" style="width:700px;" />
  </a>
  <figcaption> Illustration of the accuracy of various computational methods for various model reactions</figcaption>
</figure>

<p>
All values are in kcal/mol, and the colors encode the error of the computed value: green values are within 10% or 1 kcal/mol of the truth, while yellow values are within 50% or 5 kcal/mol and red values are outside that range. (In each case the more restrictive cutoff was used.) Where possible, values have been benchmarked to experimental data; in the remaining cases, coupled-cluster calculations were employed.
</p>

<p>
While small relative to <a href="https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j">more professional benchmarks</a>, these data nicely illustrate a few important trends:
</p>

<ul>
<li>
    Dispersion corrections are badly needed for density-functional theory, but beyond that there isn't a clear "best" DFT method.
</li>
<li>
    Hartree–Fock theory consistently predicts electron-dense species to be less stable than they are, owing to the neglect of electron correlation: so anions are too basic/nucleophilic, bonds are too weak, and the aromatic Cope transition state is far too high in energy.
</li>
<li>
    Second-order Møller–Plesset perturbation theory (MP2) overcorrects the Hartree–Fock error, MP3 overcorrects the MP2 error, and MP4 overcorrects the MP3 error. The result is that HF and MP3 tend to "zig" where MP2 and MP4 tend to "zag." (The crude solution of just averaging MP2 and MP3 is <a href="https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/cphc.201200850">surprisingly effective</a>.) 
</li>
<li>
    <a href="https://pubs.acs.org/doi/10.1021/acs.jpca.9b05734">DLPNO-CCSD(T)</a>, from Neese and co-workers, really is a magical combination of speed and accuracy!
</li>
</ul>

<p>
  Hopefully this overview, while simple, helps to build intuition about how good or bad computational predictions at a given level of theory are.
</p>
]]></description>
              <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Zero to One</title>
              <link>public/blog/20220914_zero_to_one.html</link>
              <description><![CDATA[
<p>
Who is Peter Thiel? <a href="https://conversationswithtyler.com/episodes/peter-thiel/">Tyler Cowen</a> calls him one of the most important public intellectuals of our era. <a href="https://www.bloomberg.com/news/features/2021-09-15/peter-thiel-gamed-silicon-valley-tech-trump-taxes-and-politics">Bloomberg</a> called him responsible for the ideology of Silicon Valley “more than any other living Silicon Valley investor or entrepreneur.” Depending on who you ask, he’s either a shadowy plutocratic genius or a visionary forward-thinking genius: but everyone seems to at least agree that he’s a genius.
</p>
<p>
<i>Zero to One</i> is his book of business advice. Given that Thiel started two very successful businesses (PayPal and Palantir) and has been a key early investor in many others (Facebook, SpaceX, and Airbnb, to name a few), I reasoned that it had to be pretty good advice. What I didn’t anticipate is how surprising the advice would be. Given Thiel's vast influence on the tech world, I assumed that any wisdom would have filtered into mainstream awareness and become part of the established dogma. Instead, I had the opposite experience: I was surprised by much of the advice, and thought some parts were even in active conflict with other advice I’d read.
</p>

<p>
(Vox’s <a href="https://www.vox.com/2014/11/30/7300019/how-peter-thiel-repackaged-conventional-wisdom-as-bold-contrarianism">Timothy Lee</a> takes the other position, accusing Thiel of repackaging “conventional wisdom as bold contrarianism” and “contrasting his own views against caricatured positions that hardly anyone actually agrees with.” I’ll present some examples below that illustrate why I think this is wrong.)
</p>

<p>
<i>Zero to One</i> is ostensibly a regular book organized into chapters, but reading it felt more like reading a collection of essays loosely connected by a few <i>leitmotifs</i>. Accordingly, I’ll summarize what I consider to be the big ideas of the book below, without any attempt to mirror the actual order that they’re presented in.
</p>

<h2>Big Ideas:</h2>

<h3>1. Competition is bad.</h3> 
<p>
This seems counterintuitive: isn’t the whole point of the free market that competition is good? But Thiel argues that, from the perspective of businesses, competition is the ultimate evil. A competitive environment is one in which resources must be expended on staying ahead of other businesses, not in investing in the future. At the limit, perfect competition leads to perfect stagnation. In one of <i>Zero to One</i>’s many iconic quotes, Thiel quips:
</p>

<blockquote>
If you can recognize competition as a destructive force instead of a sign of value, you're already more sane than most.
</blockquote>

<p>
Viewing competition as bad has non-obvious implications. One implication is that you should only start a business when you have a clear path towards a competition-free market. This might be a new market, or a technological advance that obliterates the existing market. For examples of the latter, Thiel cites <i>inter alia</i> Google (for search) and Apple (for iPads). As a rough rule of thumb, a business needs a order-of-magnitude advantage over the competition to be free from competitive insecurity.
</p>

<p>
Thiel then goes on to criticize the push to maximize the total addressable market (TAM) of a startup. This is surprising, because every startup pitch I’ve seen tries to emphasize how big their TAM is. But if competition is bad, then the perfect place to start a company is a small, competition-free pond adjacent to a much larger ocean. 1% of a $10B market and 100% of a $100M market are the same number, but it’s much easier to grow a company in the latter.
(This whole point reminds me of <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy"><i>Blue Ocean Strategy</i></a>.)
</p>

<p>
(You might think that this principle is less true for businesses dependent on network effects, like Ebay or Facebook, but Thiel paradoxically asserts the exact opposite. Since network effects rely on almost complete saturation, these startups in particular need to start in a small, easily dominated area—like Harvard for Facebook.)
</p>

<p>
Perhaps a less obvious implication of viewing competition as bad is that thinking in terms of “disruption” is also bad. I was surprised to read this, because the Silicon Valley-adjacent people I’ve interacted with seem obsessed with disruption: everyone wants to disrupt healthcare, or education, or government. But Thiel is skeptical of this urge:
</p>

<blockquote>
If you think of yourself as an insurgent battling dark forces, it’s easy to become unduly fixated on the obstacles in your path. But if you truly want to make something new, the act of creation is far more important than the old industries that might not like what you create.
</blockquote>

<p>
Disruption is a way to frame innovation in negative terms, highlighting how new ideas can destroy existing systems. Thiel instead wants founders to be motivated by a desire to create new wonders. A critical reader might call this a glass-half-full/glass-half-empty rephrasing, but how we frame our own motivations often has a variety of subtle, downstream effects that are hard to appreciate in the moment.
</p>

<h3>2. Founders Need Definite Optimism.</h3> 
<p>
It’s pretty obvious that founders need to be optimistic, because convicted pessimists lack the desire to create anything new: a precondition for succeeding is believing that success is possible. But Thiel further bisets optimism into definite and indefinite halves. Indefinite optimists believe that things are going to get better, but lack a clear vision of how or why. In contrast, definite optimists have a positive vision for the future.
</p>

<p>
Thiel argues that indefinite optimism epitomizes the modern world (from c. 1970 to the present). Much of our economy is devoted to indefinite optimism:
</p>

<blockquote>
Finance epitomizes indefinite thinking because it’s the only way to make money when you have no idea how to create wealth… the fundamental tenet is that the market is random.
</blockquote>

<p>
Thiel also criticizes philosophy for succumbing to indefinite optimism. Previously, philosophers offered substantive visions of the good life; in the late 20th century, philosophers like Robert Nozick and John Rawls, although ideological adversaries, both focused on procedural theories of philosophy that emphasized the fairness of processes, not the nature of their outcomes.
</p>

<p>
Thiel proceeds to criticize government for indefinite optimism, focusing on entitlements and procedural fairness rather than centralized planning for the future, and biotech:
</p>

<blockquote>
Today it’s possible to wonder whether the genuine difficulty of biology has become an excuse for biotech startups’ indefinite approach to business in general. Most of the people involved expect some things to work eventually, but few want to commit to a specific company with the level of intensity necessary for success. It starts with the professors who often become part-time consultants instead of full-time employees—even for the biotech startups that begin from their own research. Then everyone else imitates the professors’ indefinite attitude. <u>It’s easy for libertarians to claim that heavy regulation holds biotech back—and it does—but indefinite optimism may pose an even greater challenge for the future of biotech.</u>
<i>(emphasis added)</i>
</blockquote>

<p>
The conclusion of all this is what you might expect: as a founder, you should have definite optimism. Just thinking the world is likely to change is not enough to make you start a good company, no matter how exciting the field: “No sector will ever be important enough that merely participating in it will be enough to build a great company.” Thiel concludes:
</p>

<blockquote>
Darwinism may be a fine theory in other contexts, but in startups, intelligent design works best…. A startup is the largest endeavor over which you can have definite mastery. You can have agency not just over your own life, but over a small and important part of the world.
</blockquote>

<p>
This seems like good advice. Nevertheless, I find it tough to square Thiel’s view that a company should have a clearly defined mission and purpose with the conventional wisdom that startups ought to be flexible in their early days. Paul Graham describes this in his <a href="http://www.paulgraham.com/startupmistakes.html">piece</a> on common startup mistakes:
</p>

<blockquote>
Don't get too attached to your original plan, because it's probably wrong. Most successful startups end up doing something different than they originally intended — often so different that it doesn't even seem like the same company. You have to be prepared to see the better idea when it arrives. And the hardest part of that is often discarding your old idea.
</blockquote>

<p>
And in another <a href="http://www.paulgraham.com/notnot.html">essay</a>, he describes how common it is for startups to change their goal:
</p>

<blockquote>
In the average Y Combinator startup, I'd guess 70% of the idea is new at the end of the first three months. Sometimes it's 100%.
</blockquote>

<p>
Do Graham and Thiel disagree with one another on this point, or are these essays arguing about something slightly different? One attempt to synthesize these two views might be the opinion that a startup should work towards a definite goal but be somewhat path-agnostic, especially early on. I’m not sure if this synthesis would satisfy either party.
</p>

<h3>3. A company’s culture proceeds from its mission, not vice versa.</h3>
<p>
Thiel is quick to criticize the increasingly lavish corporate perks used to lure top talent, claiming that putting culture ahead of mission ultimately dooms companies:
</p>

<blockquote>
No company <i>has</i> a culture; every company <i>is</i> a culture. A startup is a team of people on a mission, and a good culture is just what that looks like on the inside. <i>(emphasis original)</i>
</blockquote>

<p>
So, how do you build a team of people on a mission? The answer, Thiel claims, is to organize your company around a shared secret: a belief that the world can be changed for the better in a specific way. This is similar to the previous point about the importance of definite optimism. All founders should believe that they’ve discovered a way to change the world that other people haven’t realized—that they’ve discovered a secret insight. A company, then, is just the natural way to actualize and communicate that insight:
</p>

<blockquote>
The best entrepreneurs know this: every great company is built around a secret that’s hidden from the outside. A great company is a conspiracy to change the world; when you share your secret, the recipient becomes a fellow conspirator.
</blockquote>

<p>
Thiel uses this insight to drive various points about corporate structure. For instance, every member of the conspiracy needs to be fully invested in the mission, to prevent problems arising from imperfect alignment of individual goals: “You need good people who get along, but you also need a structure to help keep everyone aligned for the long term.” This is true for management, who should be mostly paid in stock, to encourage long-termism, and for regular employees:
</p>

<blockquote>
Everyone you involve with your company should be involved full-time… anyone who doesn’t own stock options or draw a regular salary from your company is fundamentally misaligned…. Even working remotely should be avoided.
</blockquote>

<p>
Having a good mission is also key to recruiting, in Thiel’s mind. The uncomfortable truth about startups is that any potential hire could get a better offer, with more benefits, from an established company. Why would anyone want to work for you? The answer is that they believe in your mission—that they agree with your vision of the world and want to change it accordingly. Without a vision you’re just bidding for mercenaries.
</p>

<h2>Smaller Points Which I Found Interesting:</h2>

<h3>4. Aspects of Governance </h3>
<p>
Thiel breaks corporate governance down into three parts, which I found a useful conceptual distinction:
</p>

<ul>
<li>Ownership (equity, who controls the stock)</li>
<li>Possession (who makes day-to-day decisions, typically the CEO)</li>
<li>Control (who formally governs, e.g. a board of directors)</li>
</ul>

<h3>5. The Importance of Sales</h3>
<p>
Perhaps one of the most explicitly esoteric ideas in the book is the claim that sales is omnipresent and rules every area of life, but is cloaked by the systematic attempt every salesman makes to disguise the nature of their art. In Thiel’s words:
</p>

<blockquote>
Whatever the career, sales ability distinguishes superstars from also-rans… Even university professors, who claim authority from scholarly achievement, are envious of the self-promoters who define their field. Academic ideas about history or English don’t just sell themselves on their intellectual merits. Even the agenda of fundamental physics and the future path of cancer research are results of persuasion. The most fundamental reason that even businesspeople underestimate the importance of sales is the systematic effort to hide it at every level of every field in a world secretly driven by it…. If you’ve invented something new but you haven’t invented an effective way to sell it, you have a bad business—no matter how good the product.
</blockquote>

<p>
I don’t think anyone currently in academia would dispute this characterization.
</p>

<h3>6. Technological Innovation</h3>
<p>
Thiel thinks that computers are better treated as assistants than replacements for humans:
</p>

<blockquote>
Computers are complements for humans, not substitutes. The most valuable businesses of coming decades will be built by entrepreneurs who seek to empower people rather than try to make them obsolete… We have let ourselves become enchanted by big data only because we exoticize technology.
</blockquote>

<p>
This makes sense, but also might be a bit tautological. Any sufficiently advanced tool will invariably become a complement for some human’s workflow: AlphaFold and DALL-E have automated some human tasks, and now everyone’s expectations have adjusted and we use these tools as complements for our own capabilities. What’s the difference, <i>a priori</i>, between seeking to empower people and trying to make them obsolete? Does AlphaFold empower medicinal chemists or make structural biologists obsolete? (The critical reader will respond “Neither, at the moment,” which is fair.)
</p>

<h2>Additional Memorable Quotes:</h2>
<ul>
<li>“The most contrarian thing of all is not to oppose the crowd but to think for yourself.”</li>
<li>“All Rhodes Scholars had a great future in their past.”</li>
<li>“If everything worth doing has already been done, you may as well feign an allergy to achievement and become a barista.”</li>
<li> “Recruiting is a core competency for any company. It should never be outsourced.” </li>
<li> “If you don’t see any salespeople, you’re the salespeople.” </li>
<li> “You can’t dominate a submarket if it’s fictional.” </li>
<li> “The best projects are likely to be overlooked, not trumpeted by a crowd; the best problems to work on are often the ones nobody else even tries to solve.” </li>
</ul>

<h2>Conclusions:</h2>
<p>
How can such a famous book remain countercultural almost a decade after it was published? One possibility is that Thiel presents only one side of a dialectic, while other authors present the natural opposing views. Since community consensus follows the synthesis of the two views, both extremes appear to be arguing against the norm even at equilibrium. This might be true; I’m not well-versed enough in the startup literature to know.
</p>

<p>
Another possibility is that Thiel’s advice is simply difficult to understand, or difficult to follow. It’s easy to read <a href="https://en.wikipedia.org/wiki/Wisdom_literature">wisdom literature</a> (like the Book of Proverbs) but hard to apply it to your life; simply reading something wise doesn’t automatically make you wise. Maybe <i>Zero to One</i> is like a modern-day Proverbs for founders—multitudes read it, but only a rare breed of person is able to successfully understand and actualize its insights.
</p>

<p>
Perhaps the biggest drawback of <i>Zero to One</i> is apparent just from the title. Thiel is primarily interested in businesses that aim to change the world, to build something completely new and revolutionary, to go not from 1 to N but from 0 to 1. And so much of the advice in the book is only applicable to businesses trying to go from 0 to 1: how does a restaurant gain an order-of-magnitude advantage over other restaurants and avoid competition? Is the French Laundry 10x better than Alinea? Is Chipotle 10x better than Qdoba?
</p>

<p>
Thiel’s answer to this question is probably “don’t start a restaurant”; in an <a href="https://conversationswithtyler.com/episodes/peter-thiel/">interview with Tyler Cowen</a> he said that the Straussian reading of <i>Zero to One</i> was “don’t start a business.” And indeed every piece of positive advice in the book can be inverted to the corresponding cautionary wisdom: don’t start a business unless (i) you think you’ve found a secret about the world and (ii) you have a concrete plan for implementing it without (iii) competing with existing businesses.
</p>

<p>
But it’s good that restaurants exist, and so at the very least we can conclude that <i>Zero to One</i> shouldn’t be read as a categorical imperative for all businesses, but advice only for a narrower subset thereof. Not every founder needs to start a “Zero to One” business. Not every business needs to be PayPal or Facebook.
</p>

<p>
What wisdom does <i>Zero to One</i> have for academia? The principle of avoiding competition is an obvious one: nobody in their right mind should start a new program in photoredox catalysis right now. In contrast, some research areas seem almost totally neglected at the moment (<a href="https://sites.google.com/view/the-harman-lab-uva/publications">W. Dean Harman</a> and his work on η<sub>2</sub>–arene complexes comes to mind).
</p>

<p>
Thiel’s point about definite optimism also translates readily to the research sphere. If our goal is to advance scientific progress, definite optimism starts with a vision of the future of our field and generates a roadmap of how to get there. This vision could be a lot of things—designing arbitrary enzymes <i>in silico</i>, perfect prediction of absolute binding constants in relevant environments, CO<sub>2</sub> reduction on megaton scale, fully automated synthesis of arbitrary natural products—but the correct research direction then becomes “whatever gets us closer to that goal.” In contrast, indefinite optimism is a belief that science will advance more or less through Brownian motion, by following random grants and research directions that lead to publications. Needless to say, the former is better than the latter.
</p>

<p>
(The point about misalignment of incentives is also excellent when applied to academia, but deserves a longer treatment than I can give here.)
</p>

<p>
Overall, I think <i>Zero to One</i> is an excellent book, and worth reading for anyone considering anything startup-related (including starting a research group).
</p>
]]></description>
              <pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Singleton Saturday: Nitration of Toluene</title>
              <link>public/blog/20220909_nitration_of_toluene.html</link>
              <description><![CDATA[
<p>
<i>
This is the second in what will hopefully become a series of blog posts (<a href="https://corinwagen.github.io/public/blog/20220805_singleton_saturday_decarboxylation.html">previously</a>) focusing on the fascinating work of Dan Singleton (professor at Texas A&amp;M). My goal is to provide concise and accessible summaries of his work and highlight conclusions relevant to the mechanistic or computational chemist. 
</i>
</p>

<p>
Today I want to discuss one of my favorite papers, a detailed study of the nitration of toluene by Singleton lab at Texas A&amp;M. 
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_exp_regiochem.png" style="width:450px;" />
<figcaption>
Regiochemistry of the nitration of toluene with nitronium tetrafluoroborate in dichloromethane.
</figcaption>
</figure>

<p>
This reaction, a textbook example of electrophilic aromatic substitution, has long puzzled organic chemists. The central paradox is that intermolecular selectivity is typically quite poor between different arenes, but positional selectivity is high—implying that NO<sub>2</sub><sup>+</sup> is unable to recognize more electron-rich arenes but somehow still able to recognize more electron-rich sites within a given arene. 
</p>

<p>
George Olah devoted considerable effort to studying this paradox:
</p>

<blockquote>
Kuhn and I have subsequently developed a new efficient nitration method by using stable nitronium salts (like tetrafluoroborate) as nitrating agents. Nitronium salt nitrations are also too fast to measure their absolute rates, but the use of the competition method showed in our work low substrate selectivity, e.g., k<sub>t</sub>/k<sub>b</sub> of 1-2. <i>[In other words, competition experiments show at most a 2-fold preference for toluene.]</i> <u>On the basis of the Brown selectivity rules, if these fast reactions followed σ-complex routes they would also have a predictably low positional selectivity (with high meta isomer content).</u> However, the observed low substrate selectivities were all accompanied by high discrimination between available positions (typical isomer distributions of nitrotoluenes were (%) ortho:meta:para = 66:3:31). Consequently, a meta position would seem to be sevenfold deactivated compared to a benzene position, giving a partial rate factor of m<sub>f</sub> = 0.14. These observations are inconsistent with any mechanism in which the individual nuclear positions compete for the reagent (in the σ-complex step).
<br>
<br>
In explanation, we suggested the formation of a π complex in the first step of the reactions followed by conversion into σ complexes (which are of course separate for the individual ortho, para, and meta positions), allowing discrimination in orientation of products. 
(<a href=https://pubs.acs.org/doi/pdf/10.1021/ar50043a002>ref</a>, emphasis added)
</blockquote>

<p>
His conclusion, summarized in the last sentence of the above quote, was that two different sets of complexes were involved: π complexes which controlled arene–arene selectivity, and σ complexes which controlled positional selectivity. Thus, the paradox could be resolved simply by invoking different ∆∆G<sup>‡</sup> values for the transition states leading to π- and σ-complex formation. The somewhat epicyclic nature of this proposal led to pushback from the community, and (as Singleton summarizes) no cogent explanation for this reactivity had yet been advanced at the time of writing. 
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_olah_mech.png" style="width:425px;" />
<figcaption>A summary of Olah’s proposed resolution, featuring π- and σ-complexes.</figcaption>
</figure>

<p>
The authors of this paper initiated their studies of this reaction by performing an extensive series of “traditional” DFT calculations in implicit solvent. M06-2X/6-311G(d) was chosen by benchmarking against coupled-cluster calculations, and the regiochemistry was examined with a variety of computational methods. 
</p>

<p>
In the absence of BF<sub>4</sub><sup>-</sup>, naïve calculations predict entirely the wrong result: the <i>para</i> product is predicted to be more favorable than the <i>ortho</i> product, and no <i>meta</i> product is predicted to form at all. However, closer examination of the transition states reveals post-transition-state bifurcation in each case: for instance, the “<i>para</i>” transition state actually leads to <i>para</i>/<i>meta</i> in a 89:11 ratio. When all possible bifurcations for all transition states are taken into account in a Boltzmann-weighted way, the results remain wrong: <i>para</i> is still incorrectly favored over <i>ortho</i>, and <i>meta</i> is now predicted to form in a much higher proportion than observed.
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_implicit.png" style="width:550px;" />
<figcaption>Illustration of transition states and post-TS bifurcations for traditional DFT calculations.</figcaption>
</figure>

<p>
The authors examine various potential resolutions of this problem, including inclusion of BF<sub>4</sub><sup>-</sup>, use of explicit solvent within an ONIOM scheme, and other nitration systems which might lead to more dissociated counterions. These methods lead to different, but equally wrong, conclusions. 
</p>

<p>
They then perform free energy calculations to determine the energetics of nitronium approach to toluene (in dichloromethane). Surprisingly, no barrier exists to NO<sub>2</sub><sup>+</sup> attack: once nitronium comes within 4.5 Å of the arene, it is “destined to form some isomer” of product (in the authors’ words). Singleton and Nieves-Quinones dryly note:
</p>

<blockquote>
…The apparent absence of transition states (more on this later) after formation of the encounter complex has never previously been suggested. This absence is in fact counter to basic ideas in all previous explanations of the selectivity.
</blockquote>

<p>
This observation explains one horn of the dilemma—why selectivity between different arenes is low—but leaves unanswered why positional selectivity is so high. To examine this question, the authors then directly run reactions <i>in silico</i> by using unconstrained <i>ab initio</i> molecular dynamics (AIMD) and observe the product ratio. The product ratio (45:2:53 <i>o</i>/<i>m</i>/<i>p</i>) they observe matches the experimental values (41:2:57 <i>o</i>/<i>m</i>/<i>p</i>) almost perfectly!
</p>

<p>
With this support for the validity of the computational model in hand, the authors then examine the productive trajectories in great detail. Surprisingly, they find that although no barrier exists to nitronium attack, the reaction is relatively slow to proceed, taking on average of 3.1 ps to form the product. Trajectories lacking either explicit solvent or tetrafluoroborate lack both this recalcitrance and the observed selectivity: instead, nitronium opportunistically reacts with the first carbon it approaches. This suggests that selectivity is only possible when the nitronium–toluene complex is sufficiently persistent.
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_explicit.png" style="width:550px;" />
<figcaption>
  Nitronium roams within the π complex, sampling different carbons before ultimately forming product.
</figcaption>
</figure>

<p>
The authors attribute the long life of the NO<sub>2</sub><sup>+</sup>—toluene complex to the fact that the explicit solvent cage must reorganize to stabilize formation of the Wheland intermediate. This requires both reorientation of the dichloromethane molecules and repositioning of the tetrafluoroborate anion, which both occur on the timescale of the trajectories (<a href=https://corinwagen.github.io/public/blog/20220719_timescales.html>ref</a>). Accordingly, the reaction is put on hold while cage reorganization occurs, giving NO<sub>2</sub><sup>+</sup> time to preferentially attack the <i>ortho</i> and <i>para</i> carbons. (I would be tempted to call the π complex a dynamic/entropic intermediate, in the language of either <a href="https://corinwagen.github.io/public/blog/20220805_singleton_saturday_decarboxylation.html">Singleton</a> or <a href="https://www.sciencedirect.com/science/article/abs/pii/S2589597419300103">Houk</a>.)
</p>

<p>
This computational picture thus accurately reproduces the experimental observations and explains the initial paradox we posed: selectivity does not arise through competing transition state energies, but through partitioning of a product-committed π complex which is prevented from reacting further by non-instantaneous solvent relaxation. Since similar π complexes can be formed under almost any nitration conditions, this proposal explains the often similar selectivities observed with other reagents or solvents.
</p>

<p>
More philosophically, this proposal explains experimental results without invoking any transition states whatsoever. In their introduction, the authors quote Gould as stating:
</p>

<blockquote>
If the configuration and energy of each of the intermediates and transition states through which a reacting system passes are known, it is not too much to say that the mechanism of the reaction is understood.
</blockquote>

<p>
While this may be true to a first approximation in most cases, Singleton and co-workers have demonstrated here that this is not true in every case. This is an important conceptual point. As our ability to study low-barrier processes and reactive intermediates grows, I expect that we will more clearly appreciate the limitations of transition-state theory, and have to develop new techniques to interpret experimental observations.  
</p>

<p>
But perhaps the reason I find this paper most exciting is simply the beautiful match between theory and experiment for such a complex and seemingly intractable system. This work not only predicts the observed reaction outcomes under realistic conditions, but also allows (through AIMD) the complete analysis of the entire reaction landscape in exquisite detail, from approach to post-bond-forming steps. In other words, this is a mechanistic chemist’s dream: a perfect movie of the entire reaction’s progress, from beginning to end. Go and do likewise!
</p>

<i>Thanks to Daniel Broere and Joe Gair for noticing that "electrophilic aromatic substitution" was erroneously written as "nucleophilic aromatic substitution." This embarassing oversight has been corrected.</i>

]]></description>
              <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Elegy for the MIT of Yesteryear</title>
              <link>public/blog/20220907_mit_elegy.html</link>
              <description><![CDATA[
<figure>
<img class=centered-img src=https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Old_drawing_of_MIT.JPG/800px-Old_drawing_of_MIT.JPG style="width:550px;" />
<figcaption>Old image of MIT, c/o <a href="https://commons.wikimedia.org/wiki/File:Old_drawing_of_MIT.JPG">Wikimedia Commons</a></figcaption>
</figure>

<p>
Over the past few weeks, I’ve been transfixed, and saddened, by Eric Gilliam’s three-part series about the history of MIT (my <i>alma mater</i>). I’ll post a few quotations and responses below, but if you’re interested you should just go read the original essays
(<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">1</a>, 
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">2</a>,
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-045">3</a>).
</p>

<h3>Why MIT Was Created</h3>

<blockquote>
Professors who are not steeped in hands-on industrial practice could not produce the kinds of workers that were immediately useful to industry. These schools were outputting the kind of men that [Thomas] Edison, and many others mentioned above, did not believe were meeting the needs of industry. And the technical know-how taught in trade schools was great, but an ideal institute of technology should also impart some higher engineering and scientific knowledge to students to enable them to be more innovative, intelligent problem-solvers.
<br>
<br>
So, MIT was founded to solve this problem. This school was not designed to be a place for purely lecturing and rote learning. A smattering of intelligent men from industry and university men with an applied bent to them made up the original faculty. Content was lectured as needed, but what differentiated MIT was its innovative use of the laboratory method. Instructors taught “through actual handling of the apparatus and by working on problems, shoulder to shoulder with the boys.” And the schedule, from 9-5 (with a lunch break) 5 days a week and additional class on Saturday was meant to simulate a normal work schedule and, thus, ease the eventual transition to life in the working world.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">(part 1)</a>
</blockquote>

<p>
This quote highlights how MIT was intended to be a counter-cultural university, founded on a distinctly different model than other institutions (like Harvard).
MIT was not meant to be a center of learning and theoretical research, but a school focusing on training the next generation of industrial leaders.
</p>

<h3>How MIT Supported Itself</h3>

<blockquote>
But [MIT President] Maclaurin had an idea: self-support. MIT would capitalize on its own assets and earn money by formally offering its services to industry on a larger scale. High numbers of industrial partners had been eager to engage in ad-hoc courses of research with MIT’s applied professors, often paid for by the company, anyway. Why not turn this into a much larger, more formal program that was facilitated by the Institute? The idea would grow into what was known as the Technology Plan.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<p>
MIT operated on a different funding model than other universities, relying on support from industry.
This is, in essence, what I proposed several weeks ago in my <a href=https://corinwagen.github.io/public/blog/20220728_consulting_as_grad_school.html>reflection</a> on the similarities between graduate school and consulting.
This was seen as important and honorable by its leaders at the time:
</p>

<blockquote>
“There could be no more legitimate way for a great scientific school to seek support than by being paid for the service it can render in supplying special knowledge where it is needed... Manufacturers may come to us with problems of every kind, be they scientific, simple, technical or foolish. We shall handle each seriously, giving the best the institute has at its disposal” - William Walker, head of the Division for Industrial Cooperation and Research
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<h3>Why MIT Changed Paths</h3>

<p>
The answer to this question is the subject of Gilliam's 
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-045">third post</a>.
It's a bit too complex to fully summarize here, but there were a few key factors:
</p>

<ul>
  <li>
    The massive increase in post-WWII funding for science allowed MIT to shift away from applied research and towards basic research.
  </li>
  <li>
    The growing importance of industrial research labs at <i>inter alia</i> Bell Labs, GE, and DuPont meant that the need for academic applied research was decreasing, and also that academically trained researchers were valuable assets to industry. (This was the case only in the post-war period, however, and by 1980 the era of large industrial research labs had mostly concluded.)
  </li>
  <li>
    A growing distaste for pursing "boundary work" that could also be done in industry, as opposed to pursuing a uniquely academic niche.
  </li>
</ul>

<p>
  Crucially, the first two factors are less true today than they were when MIT made this decision, implying that the niche filled by "Old MIT" could be occupied again today.
</p>

<h3>Why A School Like Old MIT Should Still Exist</h3>

<blockquote>
It seems clear, given MIT’s transition to a more university style of education, that we are left with a hole. We do not have an elite hybrid technical school/applied research institute like this that can draw top talent away from places like Harvard and Stanford to its more hands-on style of education. But, as a country where the manufacturing sector is shrinking (and median wages aren’t doing so well either), we may need a new MIT now more than ever.
<br>
<br>
There are plenty of individuals at top schools who COULD be swayed to attend a place like this. Speaking for Stanford, where I went to undergrad, there was a large population of people who majored in mechanical engineering and were disenchanted because they did almost exclusively problem set work and very little building of anything real. And I knew even more people majoring in other subjects who abandoned mechanical engineering and majors like it for this reason! “We’re training you to be mechanical engineering managers, not traditional mechanical engineers,” was a common line used in the department. And, while that is a fine goal for a program, it is not what many of the students seem to want. What if I just want to be a top-flight regular engineer who can build awesome stuff?
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">(part 1)</a>
</blockquote>

<blockquote>
There can and should be individuals who spend almost all of their time on pure research. But it is probably bad for future progress to allow too many of these individuals to work in an environment in which few of their peers are spending a substantial amount of time working on industrial applications and problems. No matter what, some basic research will always find a way of trickling its way down into practical industrial importance. But allowing pure researchers to be siloed from the acquaintance of those who work on industrial applications — and not just the need to work on those problems themselves — feels like it is setting the system up for inefficiency. When we look back on the era of explosive productivity in areas of basic research like physics and math in the early 1900s, even the purest of pure researchers at the time tended to have regular interactions either with industry or with researchers who did industry-related research — due to industry contracts themselves, close friends who did industry work regularly, or conscription to work on military.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<p>
  Gilliam's conclusions seem broadly correct to me. While MIT is still a great school, it's no longer pursuing a distinct model for education. The main factors distinguishing MIT from peer institutions are cultural, and even those are being <a href="https://www.theatlantic.com/education/archive/2017/06/the-fall-of-mits-counter-culture-dorm/532074/">actively</a> 
  <a href="https://thetech.com/2022/08/25/fortress-mit">suppressed</a> by the current administration.
  In total it took less than a century for the entrepreneurial mindset of MIT, a "startup university", to be replaced by the exact institutional conservatism it was founded to oppose. "You either die a hero or live long enough to see yourself become the villain."
</p>

<p>
More broadly, there's a broad sense today that innovation, especially in the physical world, is slowing (<a href="http://danwang.co/why-is-peter-thiel-pessimistic-about-technological-innovation/">Peter Thiel</a> may be the most notable proponent of this claim).
A century ago, Americans could build whole networks of subways with comparatively primitive technology; now, something as simple as <a href="https://www.vox.com/policy-and-politics/2017/1/1/14112776/new-york-second-avenue-subway-phase-2">building a single subway station</a> has become a Herculean task.
I don't mean to draw too direct of a causal connection between the end of Old MIT and the decline in real-world innovation, but perhaps a new school focused on unglamorous, nuts-and-bolts innovation rather than holistic education is exactly what the US needs now.
</p>
]]></description>
              <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Potential Energy Surfaces in Python</title>
              <link>public/blog/20220905_pypes.html</link>
              <description><![CDATA[
<p>
    Organic chemists often think in terms of <a href="https://corinwagen.github.io/public/blog/20220815_rate_determining_span.html">potential energy surfaces</a>, especially when plotting the results of a computational study.
    Unfortunately it is non-trivial to generate high-quality potential energy surfaces. It's not too difficult to sketch something crude in ChemDraw or Powerpoint, but getting the actual
    barrier heights correct and proportional has always seemed rather tedious to me.
</p>

<p>
    I've admired the smooth potential energy surfaces from the <a href="https://baik-laboratory.com/publications">Baik group</a> for years, and so several months ago I decided to try and write my own
    program to generate these diagrams. I initially envisioned this as a python package (with the dubiously clever name of <i>pypes</i>), but it turned out to be simpler than expected, such that I 
    haven't actually ever turned it into a library. It's easier to just copy and paste the code into various Jupyter notebooks as needed.
</p>

<p>
    Here's the code:
</p>

<pre class=code-block>
# get packages
import numpy as np
import scipy.interpolate as interp
import matplotlib.pyplot as plt

# make matplotlib look good
plt.rc('font', size=11, family="serif")
plt.rc('axes', titlesize=12, labelsize=12)
plt.rc(['xtick', 'ytick'], labelsize=11)
plt.rc('legend', fontsize=12)
plt.rc('figure', titlesize=14)

%matplotlib inline
%config InlineBackend.figure_format='retina'

# x and y positions. y in kcal/mol, if you want, and x in the range [0,1].
Y = [2.49, 3.5, 0, 20.2, 19, 21.5, 20, 20.3, -5]
X = [0, 0.15, 0.3, 0.48, 0.55, 0.63, 0.70, 0.78, 1]

# labels for points. False if you don't want a label
label = ["label1", False, "label2", "label3", "label4", "label5", "label6", "label7", "label8"]

#### shouldn't need to modify code below this point too much...

# autodetect which labels correspond to transition states
TS = []
for idx in range(len(Y)):
    if idx == 0 or idx == len(Y)-1:
        TS.append(False)
    else:
        TS.append((Y[idx] > Y[idx+1]) and (Y[idx] > Y[idx-1]))

# sanity checks
assert len(X) == len(Y), "need X and Y to match length"
assert len(X) == len(label), "need right number of labels"

# now we start building the figure, axes first
f = plt.figure(figsize=(8,8))
ax = f.gca()
xgrid = np.linspace(0, 1, 1000)
ax.spines[['right', 'bottom', 'top']].set_visible(False)

YMAX = 1.1*max(Y)-0.1*min(Y)
YMIN = 1.1*min(Y)-0.1*max(Y)

plt.xlim(-0.1, 1.1)
plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)
plt.ylim(bottom=YMIN, top=YMAX)
ax.plot(-0.1, YMAX,"^k", clip_on=False)

# label axes
plt.ylabel("Gibbs Free Energy (kcal/mol)")
plt.xlabel("Reaction Coordinate")

# plot the points
plt.plot(X, Y, "o", markersize=7, c="black")

# add labels
for i in range(len(X)):
    if label[i]:
        delta_y = 0.6 if TS[i] else -1.2
        plt.annotate(
            label[i],
            (X[i], Y[i]+delta_y),
            fontsize=12,
            fontweight="normal",
            ha="center",
        )

# add connecting lines
for i in range(len(X)-1):
    idxs = np.where(np.logical_and(xgrid&gt;=X[i], xgrid&lt;=X[i+1]))
    smoother = interp.BPoly.from_derivatives([X[i], X[i+1]], [[y, 0] for y in [Y[i], Y[i+1]]])
    plt.plot(xgrid[idxs], smoother(xgrid[idxs]), ls="-", c="black", lw=2)

# finish up!
plt.tight_layout()
plt.show()
</pre>

<p>
The output looks like this:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220905_pes.png" style="width:550px;"/>
  <figcaption>The potential energy surface generated by the above code.</figcaption>
</figure>

<p>
If you like how this looks, feel free to use this code; if not, modify it and make it better! I'm sure this isn't the last word in potential-energy-surface creation, but it's good enough for me.
</p>
]]></description>
              <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Screening for Generality: Reflections</title>
              <link>public/blog/20220901_screening_for_generality.html</link>
              <description><![CDATA[
<p>
Now that our work on screening for generality has finally been published in <a href="https://www.nature.com/articles/s41586-022-05263-2"><i>Nature</i></a>, I wanted to first share a few personal reflections and then highlight the big conclusions that I gleaned from this project. 
</p>

<p>
This project originated from conversations I had with Eugene Kwan back in February 2019, when I was still an undergraduate at MIT. Although at the time our skills were almost completely non-overlapping, we shared both an interest in “big data” and high-throughput experimentation and a conviction that organic chemistry could benefit from more careful thinking about optimization methods. 
</p>

<p>
After a few months of work, Eugene and I had settled on the idea of a “catalytic reaction atlas” (in analogy to the <a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga">cancer genome atlas</a>) where we would exhaustively investigate catalysts, conditions, substrates, etc. for a single asymmetric reaction and then (virtually) compare different optimization methods to see which algorithms led to the best hits. Even with fairly conservative assumptions, we estimated that this would take on the order of 10<sup>5</sup> reactions, or about a year of continuous HPLC time, meaning that some sort of analytical advance was needed.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220901_slide.png" style="width:550px;"/>
  <figcaption>A slide comparing different optimization strategies, from April 2019.<br>Multi-substrate screening is proposed as one of many different algorithms.</figcaption>
</figure>

<p>
When I proposed this project to Eric, he was interested but suggested we focus more narrowly on the question of generality, or how to discover reactions with broad substrate scope. In an excited phone call, Eugene and I had the insight that we could screen lots of substrates at once by using mass spectrometry, thus bypassing our analytical bottleneck and enabling us to access the “big data” regime without needing vast resources to do so.<sup><a href="#fn1">1</a></sup>
</p>

<p>
Getting the analytical technology to work took about two years of troubleshooting. We were lucky to be joined by Spencer, an incredible analytical chemist and SFC guru, and eventually were able to get reproducible and accurate data by a combination of experimental insights (running samples at high dilution) and computational tweaks (better peak models and fitting algorithms). To make sure that the method was working properly, we ran validation experiments both on a bunch of scalemic samples and on a varied set of complex pharmaceutical racemates.
</p>

<p>
Choosing the proper reaction took a bit of thought, but once we settled on a set of substrates and catalysts the actual experiments were a breeze. Almost all the screening for this project was done in November–December 2021: in only a few hours, I could easily run and analyze hundreds of reactions per week. 
</p>

<p>
I want to conclude by sharing three high-level conclusions that I’ve taken away from working on this project; for the precise scientific conclusions of this study, you can read the paper itself.
</p>

<h3> 1. Chemical space is big, so how you search matters</h3>

<p>
There are a ton of potential catalysts waiting to be discovered, and it seems likely that almost any hit can be optimized to 90% ee by sufficient graduate-student hours. Indeed, one of the reasons we selected the Pictet–Spengler reaction was the diversity of different catalyst structures capable of giving high enantioselectivity. But just because you can get 90% ee from a given catalyst family doesn’t mean you should: it might be terrible for other substrates, or a different class of catalysts might be much easier to optimize or much more reactive. 
</p>

<p>
Understanding how many catalysts are out there to be discovered should make us think more carefully about which hits we pursue, since our time is too valuable to waste performing needless catalyst optimizations. In this study, we showed that screening only one substrate can be misleading when the goal is substrate generality, but one might prefer to screen for other factors: low catalyst loading, tolerance of air or water, or recyclability all come to mind. In all cases, including these considerations in initial screens means that the hits generated are more likely to be relevant to the final goal. Just looking for 90% ee is almost certainly not the best way to find a good reaction.
</p>

<h3>2. Don’t ignore analytical chemistry</h3>

<p>
Although assay development is a normal part of many scientific fields, many organic chemists seem to barely consider analytical chemistry in their research. Any ingenuity is applied to developing new catalysts, while the analytical method remains essentially a constant factor in the background. This is true even in cases where the analytical workflow represents a large fraction of the project (e.g. having to remove toluene before NMR for every screen).
</p>

<p>
This shouldn’t be the case! Spending time towards the beginning of a project to develop a nice assay is an investment that can yield big returns: this can be as simple as making a GC calibration curve to determine yield from crude reaction mixtures, or as complex as what we undertook here. Time is too valuable to waste running endless columns.
</p>

<p>
More broadly, it seems like analytical advances (e.g. NMR and HPLC) have had a much bigger impact on the field than any individual chemical discoveries. Following this trend forward in time would imply that we should be making bigger investments in new analytical technologies now, to increase scientist productivity in the future.
</p>

<h3>3. A little computer science can go a long way</h3>

<p>
A key part of this project (mentioned only briefly in the paper) was developing our own peak-fitting software that allowed us to reliably fit overlapped peaks. This was computationally quite simple and relied almost entirely on existing libraries (e.g. <i>scipy</i> and <i>lmfit</i>), but took a certain amount of comfort with signal processing / data science.<sup><a href="#fn2">2</a></sup> We later ended up moving our software pipeline out of unwieldy Jupyter notebooks and into a little Streamlit web app that Eugene wrote, which allowed us to quickly and easily get ee values from larger screens. 
</p>

<p>
Neither of these two advances required significant coding skill; rather, just being able to apply some computer science techniques to our chemistry problem unlocked new scientific opportunities and massive time savings (a la <a href="https://en.wikipedia.org/wiki/Pareto_principle#In_computing">Pareto principle</a>). Moving forward, I expect that programming will become a more and more central tool in scientific research, much like Excel is today. Being fluent in both chemistry and CS is currently a rare and valuable combination, and will only grow in importance in the coming decades.
</p>

<i>Thanks to Eugene Kwan for reading a draft of this post.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    I'd like to propose the following principle: any sufficiently clever analytical technique inevitably depends on mass spectrometry. If you don't believe me, just look at the field of proteomics...
  </li>
  <li id="fn2">
    I heavily recommend <a href="https://terpconnect.umd.edu/~toh/spectrum/"><i>A Pragmatic Introduction to Signal Processing</i></a> by Tom O'Haver.
  </li>
</ol>
]]></description>
              <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Rate-Determining Span</title>
              <link>public/blog/20220815_rate_determining_span.html</link>
              <description><![CDATA[
<p>
One common misconception in mechanistic organic chemistry is that reactions are accelerated by speeding up the rate-determining step. 
This mistaken belief can lead to an almost monomaniacal focus on determining the nature of the rate-determining step.
In fact, it's more correct to think of reactions in terms of the <u>rate-determining span</u>: the difference between the resting state and the highest-energy transition state.
(I thank Eugene Kwan's <a href=https://ekwan.github.io/pdfs/chem106/28%20-%20First-Order%20Kinetics.pdf>notes</a> for introducing me to this idea.)
</p>

<p>
In this post, I hope to demonstrate the veracity of this concept by showing that, under certain idealized assumptions, the existence of a low-energy intermediate has no effect on rate. Consider the following system:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_scheme.png" style="width:375px;" />
  <figcaption> Concerted and stepwise mechanisms. </figcaption>
</figure>

<p>
We can imagine plotting these two mechanisms on a potential energy surface:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_rds.png" style="width:450px;" />
  <figcaption> Concerted and stepwise mechanisms, on a PES. </figcaption>
</figure>

<p>
In this example, <b>X</b> = <b>Y</b> + <b>Z</b>; the energy of the transition state and ground state are the same in both cases, and only the presence (or absence) of an intermediate differentiates the two potential energy surfaces. We will now compute the rate of product formation in both cases.
Using the <a href="https://en.wikipedia.org/wiki/Eyring_equation">Eyring–Polyani equation</a>, it's straightforward to arrive at an overall rate for the concerted reaction as a function of the barrier:
</p>

<br>
<p>
k = k<sub>B</sub>T/h * exp(-<b>X</b>/RT)
</p>
<p>
rate<sub>concerted</sub> = k * [SM]
</p>
<p>
rate<sub>concerted</sub> = k<sub>B</sub>T/h * exp(-<b>X</b>/RT) * [SM]
</p>
<br>

<p>
The stepwise case is only slightly more complicated. Assuming that the barrier to formation of the intermediate is much lower than the barrier to formation of the product, and that the intermediate is substantially lower in energy than the rate-limiting transition state, we can apply the <a href="https://www-jmg.ch.cam.ac.uk/tools/magnus/kineticnotes.html">pre-equilibrium approximation</a>:
</p>

<br>
<p>
rate<sub>stepwise</sub> = k<sub>2</sub> * [INT] 
</p>
<p>
k<sub>2</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT)
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT) * [INT]
</p>
<br>

<p>
Solving for [INT] is straightforward, and we can plug the result in to get our final answer:
</p>

<br>
<p>
<b>Y</b> = -RT * ln([INT]/[SM])
</p>
<p>
[INT] = exp(-<b>Y</b>/RT)*[SM]
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT) * exp(-<b>Y</b>/RT) * [SM] 
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>X</b>/RT) * [SM] = rate<sub>concerted</sub>
</p>
<br>

<p>
As promised, the rates are the same—where the preequilibrium approximation holds, the existence of an intermediate has no impact on rate.
All that matters is the relative energy of the transition state and the ground state.
</p>

<p>
This method of thinking is particularly useful for rationalizing tricky Hammett trends. For instance, it's known that electron-rich indoles <a href="https://pubs.acs.org/doi/abs/10.1021/jacs.7b06811">react much faster</a> in Brønsted-acid-catalyzed Pictet–Spengler reactions, even though these reactions proceed through rate-determining elimination from a carbocation. Since electron-poor carbocations are more acidic, simple analysis of the rate-determining step predicts the opposite trend.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_ps.png" style="width:650px;" />
  <figcaption> Brønsted-acid-catalyzed Pictet–Spengler reactions. </figcaption>
</figure>

<p>
However, if we ignore the intermediate, it's clear that the transition state contains much more carbocationic character than the ground state, and so electron-donating groups will stabilize the transition state relative to the ground state and thereby accelerate the reaction. Thinking about intermediates is a great way to get confused; to understand trends in reactivity, all you need to consider is the transition state and the ground state.
</p>

]]></description>
              <pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Combating Computational Nihilism</title>
              <link>public/blog/20220810_viewpoints_on_simulation.html</link>
              <description><![CDATA[
<p>
The growing accessibility of computational chemistry has, unfortunately, led to a preponderance of papers with bad computations. Organic chemists are all too familiar with the “DFT section” of an otherwise high-quality publication which typically contains a transition-state structure or two, some sort of enigmatic cartoon purporting to explain the observed selectivity, and perhaps an uninterpretable NCIPLOT cited as evidence for the preceding claims.<sup><a href="#fn1">1</a></sup>
</p>

<p>
Faced with this sort of landscape, experimental chemists typically adopt one of two faulty heuristics: excessive credulity or universal skepticism. Being too trusting is dangerous, as evidenced by <a href="https://pubs.acs.org/doi/abs/10.1021/acs.organomet.8b00456">work</a> showcasing the manifold ways that simulations can deceive the unwary scientist. Almost anyone who’s made a catalyst predicted to be better by computations knows this well (even when the computations are your own).
</p>

<p>
However, equally dangerous—and, in my view, less appreciated—is the creeping ennui that diminishes the entire field. This is exemplified by statements like “I don’t believe computations can ever be predictive,” “You can make DFT say anything you want to,” or, more delicately, “Computations are more for generating hypotheses, not being physically correct.” Although most people may be too polite to admit this to their computational collaborators, this nihilism is pervasive—just listen to the conversations as students walk back from a departmental seminar.
</p>

<p>
This viewpoint is wrong. The existence of bad computational models does not mean that all models are bad, nor does it imply that the task of creating models is inherently futile. Examples from other scientific fields, like orbital mechanics and fluid dynamics, indicate that computations can achieve impressive degrees of accuracy and become pivotal and trustworthy components of the scientific process. Closer to home, even the most skeptical chemists would admit that for e.g. calculating IR frequencies in the ground state, DFT shows impressive predictive accuracy (modulo the usual systematic error). There’s no intrinsic reason why accurately modeling chemical systems, even prospectively, ought to be impossible; chemistry is not a social science.
</p>

<p>
Why, then, is this variety of skepticism so common? Part of the problem comes from the bewildering milieu of options available to practitioners in the field. While a seasoned expert can quickly assess the relative merits of BYLP/MIDI! and DSD-PBEP86/def2-TZVP, to the layperson it’s tough to guess which might be superior. Without transparent heuristics by which to judge the quality of computational results, it’s no surprise that zeroth-order approximations (“DFT good” or “DFT bad”) have become so common among non-experts.<sup><a href="#fn2">2</a></sup>
</p>

<p>
Another issue is the generally optimistic demeanor of computational chemists towards their field. While the temptation to emphasize the potential upside of one’s research area is understandable, overestimating the capabilities of state-of-the-art technology inevitably leads to a reckoning when the truth becomes obvious. Except in certain circumscribed cases, we are still far from any predictive models of reactivity or selectivity for typical solution-phase reactions, various purported “breakthroughs” notwithstanding. Based on questions I’ve heard in talks, this uncomfortable truth is not universally understood by experimental audiences.
</p>

<p>
What, then, are the practical conclusions for computational chemists? Firstly, we should not be afraid to be our field’s own harshest critics. Allowing low-quality work into the literature erodes trust in our field; although raising our standards may be difficult and unpopular in the short term (kinetics), in the long run it will benefit the field (thermodynamics). You never get a second chance at a first impression; every bad paper published causes good papers to get that much less attention.
</p>

<p>
Secondly, we should work to develop consistent standards and workflows by which one can obtain reliable computational results. Just like there are accepted means by which new compounds are characterized (<sup>1</sup>H NMR, <sup>13</sup>C NMR, HRMS, IR), there ought to be transparent methods by which transition states can reliably be found and studied. The manifold diversity of parameters employed today is a sign of the field’s immaturity—in truly mature fields, there’s an accepted right way to do things.<sup><a href="#fn3">3</a></sup> The growing popularity of tools like <a href=https://xtb-docs.readthedocs.io/en/latest/crest.html><i>crest</i></a> is an important step in this direction, as is the ability to to use high-level post-Hartree–Fock wavefunction methods like DLPNO-CCSD(T) to refine single-point energies.
</p>

<p>
Finally, we must be honest about the limitations of our techniques and our results. So much about the chemical world remains mysterious and far beyond our understanding, let alone our ability to reproduce <i>in silico</i>. Far from being a failure for the field, however, this is something to be acknowledged and celebrated; science is only possible when there remain secrets to be found.
</p>

<p>
Between the Scylla of gullible credulity and the Charybdis of defensive nihilism, we must chart a middle way.
</p>

<i>
Thanks to Hayden Sharma for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    <a href="https://pubs.acs.org/doi/10.1021/ct100641a">NCIPLOT</a> is a program which allows one to visualize non-covalent interactions; although the output can be useful, for large molecules it's also very overwhelming.
  </li>
  <li id="fn2">
    A related idea is "<a href="https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/">epistemic learned helplessness</a>", where people unable to evaluate the quality of a certain kind of argument resolve not to be persuaded by this argument one way or the other.
  </li>
  <li id="fn3">
    I want to thank Frank Neese and coworkers for <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">publishing</a> their entire transition-state-finding workflow in detail, which I've found very useful and is certainly a step in the right direction.
  </li>
</ol>
]]></description>
              <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Singleton Saturday: Dynamics at the Border Between General- and Specific-Acid Catalysis</title>
              <link>public/blog/20220805_singleton_saturday_decarboxylation.html</link>
              <description><![CDATA[
<p>
<i>
This is the first in what will hopefully become a series of blog posts focusing on the fascinating work of Dan Singleton (professor at Texas A&amp;M). My goal is to provide concise and accessible summaries of his work and highlight conclusions relevant to the mechanistic or computational chemist. 
</i>
</p>

<p>
A central theme in mechanistic chemistry is the question of concertedness: if two steps occur simultaneously (“concerted”) or one occurs before the other (“stepwise”). One common way to visualize these possibilities is to plot the reaction coordinate of each step on two axes to form a 2D More O’Ferrall–Jencks (<a href="https://en.wikipedia.org/wiki/More_O%27Ferrall%E2%80%93Jencks_plot">MOJ</a>) plot. On an MOJ plot, a perfectly concerted reaction looks like a straight line, since the two steps occur together, while a stepwise reaction follows the border of the plot, with an intermediate located at one of the corners:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_moj.png" style="width:450px;" alt="generic MOJ plot" />
  <figcaption> Generic More O'Ferrall–Jencks plot for general- and specific-acid catalysis (Figure 1 in the paper) </figcaption>
</figure>

<p>
In the context of acid catalysis, where a Brønsted acid activates a substrate towards further transformations, the concerted mechanism is known as “general-acid catalysis” and the stepwise mechanism is known as “specific-acid catalysis.” This case is particularly interesting because the timescales of heavy-atom motion and proton motion are somewhat different, as can be seen by comparing typical O–H and C–O IR stretching frequencies:
</p>
<p>
1/(3500 cm<sup>-1</sup> * 3e10 cm/s) = 9.5 fs for O–H bond vibration
</p>
<p>
1/(1200 cm<sup>-1</sup> * 3e10 cm/s) = 28 fs for C–O bond vibration
</p>

<p>
Since these timescales are so different, it’s impossible for the two steps to proceed perfectly synchronously, since the proton transfer will be done before heavy-atom motion is even half complete; in other words, the slope of the reaction’s path on the MOJ diagram can’t be 1. <i>Ceteris paribus</i>, then, one might expect stepwise specific-acid mechanisms to be favored. In some cases, however, the putative intermediate would be so unstable that its lifetime ought to be roughly zero (an <a href="https://pubs.acs.org/doi/10.1021/ar50150a001">enforced concerted mechanism</a>, to paraphrase Jencks). 
</p>

<p>
In <a href="https://pubs.acs.org/doi/full/10.1021/jacs.7b02148">this week's paper</a>, Aziz and Singleton investigate the mechanism of one such example, the decarboxylation of benzoylacetic acid, which in the stepwise limit proceeds through a bizarre-looking zwitterion:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_mech.png" style="width:450px;" alt="mechanistic possibilities" />
  <figcaption> Concerted vs. stepwise mechanisms for decarboxylation of benzoylacetic acid (taken from the paper)</figcaption>
</figure>

<p>
Distinguishing concerted and stepwise mechanisms is, in general, a very tough question. In rare cases an intermediate can actually be observed spectroscopically, but inability to observe the intermediate proves nothing: the intermediate could be 10 kcal/mol above the ground state (leading to a vanishingly low concentration) or could persist only briefly before undergoing subsequent reactions. Accordingly, other techniques must be used to study the mechanisms of these reactions. 
</p>

<p>
In this case, the authors measured the <sup>12</sup>C/<sup>13</sup>C kinetic isotope effects using their group’s 
<a href="https://pubs.acs.org/doi/10.1021/ja00141a030">natural abundance method</a>. Heavy-atom kinetic isotope effects are one of the best ways to study these sorts of mechanistic questions because isotopic perturbation is at once <a href="https://pubs.acs.org/doi/full/10.1021/jacs.1c07351">extremely informative</a> and very gentle, causing minimal perturbation to the potential energy surface (unlike e.g. a Hammett study). The KIEs they found are shown below:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_kies.png" style="width:450px;"/>
  <figcaption> Kinetic isotope effects for decarboxylation of benzoylacetic acid (Figure 2 from the paper)</figcaption>
</figure>

<p>
These KIEs match the computed structure shown below nicely, which shows that proton transfer precedes C–C bond breaking: 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_ts.png" style="width:400px;"/>
  <figcaption> Transition state for decarboxylation of benzoylacetic acid (M06-2X/6-311+G**) (taken from the paper)</figcaption>
</figure>

<p>
To probe the stepwise/concerted nature of this reaction, the authors conducted quasiclassical <i>ab initio</i> molecular dynamics, propagating trajectories forwards and backwards from the transition state. Surprisingly, the dynamics show that proton transfer is complete before C–C bond scission occurs, forming an intermediate (<b>6</b>) which persists for, on average, 3.4 O–H bond vibrations despite not being a minimum on the PES. This reaction therefore inhabits the border between general- and specific-acid catalysis—proton transfer does occur before decarboxylation, but the intermediate species (in the nomenclature of <a href="https://www.pnas.org/doi/full/10.1073/pnas.1209316109">Houk and Doubleday</a>, a “dynamic intermediate”) is incredibly ephemeral.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_trajs.png" style="width:450px;"/>
  <figcaption> Results of quasiclassical trajectories showing the existence of a dynamic intermediate (<b>6</b>) before C–C bond breakage occurs (Figure 4a from the paper)</figcaption>
</figure>

<p>
This surprising scenario occurs because of the different timescales of the two elementary mechanistic steps, as discussed above. In the words of the authors:
</p>

<blockquote>
It is well understood in chemistry that concerted multibond reactions often involve highly asynchronous bonding changes. However, the normal understanding of asynchronous concerted reactions is that the bonding changes overlap. If otherwise, why should the reaction be concerted at all? This view fails to take into account the differing physics of the heavy-atom versus proton motions. <b>Because of the uneven contribution of the motions, their separation is arguably intrinsic and unavoidable whenever the reaction is highly asynchronous.</b> <i>(emphasis added)</i>
</blockquote>

<p>
Aziz and Singleton also observe a curious phenomenon in the quasiclassical trajectories, wherein some trajectories initiated backwards from the (late) transition state fully form the C–C bond before reverting to enol + CO<sub>2</sub>. This phenomenon, termed “deep recrossing,” occurs because the oxygen of the carboxylate is unable to receive the proton, stalling the reaction in the region of the unstable zwitterion; unable to progress forward, the species simply extrudes CO<sub>2</sub> and reverts back to the enol. Thus, even though the O–H bond is formed after the C–C bond (in the reverse direction) and little O–H movement occurs in the TS, inability to form the O–H bond prevents productive reaction, just like one might expect for a concerted TS.
</p>

<p>
The picture that emerges, then, is a reaction which “wants” to be concerted, owing to the absence of a stable intermediate along the reaction coordinate, but ends up proceeding through a stepwise mechanism because of the speed of proton transfer. Importantly, the dynamic intermediate “undergoes a series of relevant bond vibrations, as would any intermediate, and it can proceed from this structure in either forward or backward directions”: it is, in meaningful ways, an intermediate.
</p>

<p>
Given the ubiquity of proton transfer in organic chemistry, it is likely that many more reactions proceed through this sort of rapidly stepwise mechanism than is commonly appreciated. One case which I find particularly intriguing is “E<sub>2</sub>” reactions, which typically feature proton transfer to a strong base (e.g. potassium <i>tert</i>-butoxide) at the same time as C–Br or C–I bond dissociation. How do these reactions actually proceed on the femtosecond timescale? Is it possible that, <a href="https://pubs.acs.org/doi/pdf/10.1021/ar50059a003">as Bordwell proposed</a>, many E<sub>2</sub> reactions are actually stepwise? So much remains to be learned.
</p>

]]></description>
              <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Consulting as Private-Sector Graduate School</title>
              <link>public/blog/20220728_consulting_as_grad_school.html</link>
              <description><![CDATA[<p>
  As an undergraduate student in the sciences at MIT, contempt for management consulting was commonplace. 
  Consulting was the path for people who had ambition devoid of any real interests, 
  the “sellout road” where you made endless Powerpoints instead of providing any tangible improvement to the world. 
  In contrast, going to graduate school was a choice that showed commitment and integrity. 
  If you were willing to sacrifice your 20s in service to a scientific discipline, that showed true passion and an honorable commitment to the field. 
</p>

<p>
  I’m now midway through my PhD, and I’ve come to the conclusion that my previous impressions were mistaken and that consulting and graduate school are in fact more alike than they seem. This change has been spurred by making new friends from the world of management consulting and realizing not only that they enjoyed and benefited greatly from their experience, but also that their experience seemed broadly similar to mine. 
</p>

<p>
  This essay is an attempt to outline some similarities and differences between consulting and graduate school, speculate about why these differences exist, and finally determine if graduate schools can learn anything from the consulting model.
  My tentative conclusion is that, at the margin, research groups would benefit from acting more like consultants and directly solving industry-relevant problems for pay.
</p>

<p>
  <i>
    Epistemic status: moderate to low. 
    These thoughts are based mainly on my own experience as a chemistry PhD student, and likely do not translate to the humanities or social sciences. 
    I also have only a secondhand knowledge of management consulting and so probably suffer from myriad misconceptions.
  </i>
</p>

<h2>
  Similarities and Differences
</h2>

<p>
  At a superficial level, management consulting and graduate school both fill the same role: 
  a safe and prestigious opportunity for a new graduate to diversify his or her skills and accrue <a href="https://80000hours.org/articles/career-capital/">“career capital.”</a> 
  In consulting, much like in graduate school, learning is key: in the words of
  <a href="https://web.archive.org/web/20220720040016/https://www.theatlantic.com/politics/archive/2019/12/pete-buttigieg-mckinsey/603421/">Pete Buttigieg</a>, McKinsey was 
  “a place where I could learn as much as I could by working on interesting problems and challenges.” 
  Both occupations can segue smoothly into a variety of opportunities afterwards, in part due to the shared emphasis on connections, networking, and presentation skills,
  and as a result both professions attract a steady stream of bright, highly motivated people. 
  Easy access to human capital seems to be a shared prerequisite: 
  without a supply of new graduates willing to work long hours in a high-stress environment, neither BCG nor Harvard could survive.
</p>

<p>
  Given the plethora of interesting, well-paid opportunities available for high-achieving graduates, the popularity of these more grueling professions might be surprising. 
  In her essay <a href="https://palladiummag.com/2020/07/27/harvard-creates-managers-instead-of-elites/">“Harvard Creates Managers Instead of Elites,”</a> 
  Saffron Huang describes the thought process behind why so many of her Harvard classmates took “the decreasing returns of another NGO internship or McKinsey job” 
  over more inventive careers, concluding that “school-validated options” appeal to students who are “naïve and uncertain about [their] own futures.” 
  In other words, the safety of taking a job well-known to be prestigious is what makes consulting and similar options so appealing. 
</p>

<p>
  Although Huang doesn’t mention graduate school specifically, I would argue that staying within academia is the most “school-validated” of all choices. 
  Getting a PhD not only gives one a defensible claim to domain expertise and a chance at a higher-status job but also allows students to stay within the familiar academic system for longer. 
  Faced with all the manifold diversity of the private sector, the chance for a graduate to stay within the familiar confines of the university for a few more years 
  is a safe and socially acceptable way to delay one’s arrival into corporate America. 
  And the high status that professors have in the eyes of undergraduates only strengthens the appeal of graduate school: 
  if all one’s academic role models went down this path, surely it can’t be a bad choice.
</p>

<p>
  From the perspective of the student, one obvious difference is the pay: a typical starting consulting salary is 
  <a href="https://managementconsulted.com/consultant-salary/">$100k</a>, 
  while my Harvard graduate student stipend is currently <a href="https://chemistry.harvard.edu/financial-support">$43k</a>. 
  Given that a first-year consultant and a first-year graduate student have essentially the same skills (i.e. what you’d expect from an undergraduate education and not much more), 
  this difference is surprising. Based on anecdotal reports from consulting, my intuition is that this difference is not limited to salaries: 
  the consulting world is flush with cash, while the academic world often runs on the verge of bankrupcy.<sup><a href="#fn1">1</a></sup>
</p>

<p>
  (I’m intentionally avoiding questions around the ethics of consulting because I think it’s not particularly relevant to this piece, 
  and because I don’t think I have any unique insights on this topic.<sup><a href="#fn2">2</a></sup>)
</p>

<h2>
  Academia’s Unique Niche
</h2>

<p>
  Why does consulting have so much more money than academia? 
  One simple model of academia is as follows: discoveries that provide “present value” can easily be funded by companies, because there’s a quick return-on-investment. 
  On the other hand, discoveries that provide “future value” are hard to fund through the private sector, because there’s no guarantee that the real-world value will be captured by the funder. 
  Accordingly, the government sponsors research into interesting problems with uncertain timeframes to do what the free market cannot.<sup><a href="#fn3">3</a></sup>
  This comports with what Vannevar Bush wrote in his landmark 1945 work <a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm#ch1.1"><i>Science, The Endless Frontier</i></a>:
</p>

<blockquote>
New impetus must be given to research in our country. Such impetus can come promptly only from the Government…. Further, we cannot expect industry adequately to fill the gap. Industry will fully rise to the challenge of applying new knowledge to new products. The commercial incentive can be relied upon for that. But basic research is essentially noncommercial in nature. It will not receive the attention it requires if left to industry.
</blockquote>

<p>
  Viewed within this model, we might hypothesize that consulting is lucrative because it’s easier to finance providing present value than providing future value 
  (or because the free market is more efficient than the NIH/NSF). 
  But this picture is oversimplified. Much current chemistry research at least ostensibly addresses present problems in the chemical industry, 
  and research groups frequently collaborate with (and receive money from) chemical companies. Why, then, is consulting better at capturing returns on present value than academia?
</p>

<p>
  Structural factors disincentivize academic labs from acting as consultants.<sup><a href="#fn4">4</a></sup> 
  Harvard’s stated <a href="https://osp.finance.harvard.edu/consulting-or-related-service-agreements">policy</a> on academic–industrial collaborations involving specific deliverables is that they are discouraged, 
  allowed “only if the activity in question advances a core academic mission of the faculty member’s school and either provides a significant institutional benefit or a public benefit that is consistent with the University’s mission and charitable status.” 
  This matches my experience collaborating with Merck, a pharmaceutical company; it was clear that we were not accepting money for rendering Merck a service, but instead simply working together because our intellectual interests aligned. Although we did receive some money, it was a fraction of what our total costs in salary, materials, etc were for the project.
</p>

<p>
Policies like this prevent companies from hiring research labs on a purely transactional basis, forcing academics to decouple their incentives from those of industry. Even if an academic lab is running out of money, it must find some way to justify its collaborations beyond pure economic necessity: research groups cannot simply remake their interests to suit whichever employer they want to attract. Viewed within the above model, this is good! Academia is supposed to focus on problems that can’t be solved by industry, not act as a contractor in service of corporate profits. 
</p>

<p>
Yet the preponderance of academic–industrial collaborations suggests that academia’s ostensible focus on long-term projects is not as strong as it could be. In a world where funding for basic research seems to be <a href="https://www.science.org/content/article/data-check-us-government-share-basic-research-funding-falls-below-50">declining</a> on a per-lab basis, it is perhaps unsurprising that professors turn to alternate sources of funding to keep their labs afloat; moving forward, we can expect this trend only to intensify. 
</p>

<p>
Perhaps the biggest omission from the above discussion is another key role of academia: training students. Graduate school, after all, seeks not only to advance the frontiers of human knowledge but also to train students in this pursuit. But from the perspective of the typical graduate student, it strikes me as unlikely that the specific nature of the problems under study (i.e. purely academic versus industrially relevant) has a massive impact on the student’s learning. Indeed, many students might be better prepared for their careers by having more encounters with industrial problems and techniques. The existence of current <a href="https://www.science.org/content/article/industrial-postdocs-road-less-traveled">industrial postdoctoral positions</a> suggests that gaining scientific experience through industry-relevant problems can be a successful strategy.
</p>

<h2>
  Conclusions
</h2>

<p>
Although the idealized model of the university—a place dedicated to advancing long-term human flourishing through the pursuit of knowledge 
<a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm#ch3.3">“without thought of practical ends”</a>—is indeed utopian, the present problems with academic funding suggest that a more pragmatic outlook may be needed in the short term. In particular, finding new ways to efficiently fund scientific research and education is a pressing challenge for the field (absent major changes to the funding ecosystem) which remains, from my point of view, unsolved.
</p>

<p>
Accordingly, the consulting model presents an interesting alternative to the current system. Consulting firms sustain themselves solely by providing solutions to current problems in industry, training their “students” without any need for external subsidies. Is it possible for research groups to support part-time basic research by consulting the rest of the time? At the margin, should graduate schools be more like consulting firms? This approach would require reducing the stigma around research groups acting as contractors, and in so doing perhaps run the risk of lessening the prestige of the university. On the other hand, directly applying university knowledge to solving practical problems might raise public appreciation for science. 
</p>

<p>
We may see the results of this experiment sooner rather than later. As acquiring scientific funding continues to grow more difficult, I expect that smaller, more poorly funded departments will begin to pursue money from industry more aggressively to keep themselves afloat, moving more and more towards the consulting model out of necessity. Time will tell whether this proves to be an alternate, or even superior, model for funding research, or a negative development that undermines what makes universities distinctive.  
</p>

<p>
If forced to guess, my tentative prediction would be that these changes will be good. The present funding model seems wasteful and unsustainable, a relic of 
<a href="https://newscience.org/nih/#the-boom-decade">massive growth </a> in federal science funding over the past 100 years. A correction is coming, and it will be brutal when it does. Finding new ways to fund research beyond just federal grants, then, is important for the future of research in the US; it’s been done before, and it can be done again. In fact, some of the greatest scientific discoveries have originated not from universities but from <a href="https://en.wikipedia.org/wiki/Bell_Labs">the corporate sphere!</a> Disrupting our institutions of science will be painful, but I think the potential upside is high—that is, if academic researchers can accept corporate money while still preserving some ability to pursue basic science.
</p>

<p>
Another conclusion from this area of thinking is that federally funded scientists ought, as much as possible, to focus on their area of comparative advantage—long-term research with uncertain payoffs, “essentially noncommercial” in nature. At least in organic chemistry, most funding applications that I’ve seen are very careful to point out how their discoveries could lead to immediate deliverables with practical impact.<sup><a href="fn5">5</a></sup> If these claims are really true, then these discoveries should be funded by the private sector, not by federal money. These assertions may be part of what a competitive grant application today requires, but their existence seem to point to a fundamental disconnect between what academic research is and what it should be. 
</p>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    It's tough to find sources on this, but anecdotally even at top universities most research labs seem strapped for cash. For structural discussions, see 
    <a href="https://www.npr.org/2014/09/10/347305805/built-in-better-times-university-labs-now-lack-research-funding">this NPR article</a> and 
    <a href="https://newscience.org/nih/#the-boom-decade">this New Science report</a>.
  </li>
  <li id="fn2">
    This issue has been discussed a lot: one particularly influential piece in this area is Daniel Markovits's 
    <a href="https://web.archive.org/web/20220604022838/https://www.theatlantic.com/ideas/archive/2020/02/how-mckinsey-destroyed-middle-class/605878/">“How McKinsey Destroyed The Middle Class”</a>.
  </li>
  <li id="fn3">
    This is equivalent to saying that scientific progress is a public good. There are more ways that things could be public goods than just long timeframes, but without loss of generality we’ll elide these considerations here.
  </li>
  <li id="fn4">
    Many professors do serve as consultants for industry, but they generally do this apart from the university, without involving their students, and the money goes to them personally, not to their research groups.
  </li>
  <li id="fn5">
    The <a href="https://newscience.org/nih/#aversion-to-high-risk-research">New Science NIH report</a> discusses this phenomenon at length: over the past 20 years, the NIH has changed its standards, such that now new grants are expected to have “clear research goals with obvious practical applications.”
  </li>
</ol>
]]></description>
              <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Timescales</title>
              <link>public/blog/20220719_timescales.html</link>
              <description><![CDATA[<p>
  For many organic chemists, it’s hard to grasp the vast difference between various “fast” units of time. 
  For instance, if a reactive intermediate has a lifetime of microseconds, does that mean it can escape the solvent shell and react with a substrate? 
  What about a lifetime of nanoseconds, picoseconds, or femtoseconds?
</p>

<p>
  To help answer these questions for myself, I made the following graphic about a year ago, which compares the timescale of various molecular processes on a logarithmic axis.
  Although someone skilled in Adobe Illustrator could doubtless make a prettier version, 
  I've still found this to be a useful reference, and frequently use it as a slide in talks or group meetings:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220719_timescales.png" style="width:650px;" alt="illustration of chemical timescales" />
  <figcaption> Illustration of timescales of chemical processes—open in new tab for higher resolution</figcaption>
</figure>

<p>
  Based on this graphic, it becomes easier to think about the interplay between competing fast processes. 
  Species that persist for less than ~5 ps, for instance, are effectively “frozen” in the solvent configuration they’re formed in, 
  whereas molecules that persist for longer can sample different solvent configurations. 
  If a species can persist for 10-100 ps, it can begin to sample the nearby conformational landscape through low-barrier processes (e.g. single-bond rotation), 
  although larger conformational changes might still be inaccessible.
</p>

<p>
  As lifetimes stretch into the nanosecond regime, diffusion and solvent-cage escape become more realistic possibilities: 
  based on <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.200503273>Mayr’s value</a> for the “diffusion limit” (2–4e9 M<sup>-1</sup>s<sup>-1</sup>), 
  we can estimate that bimolecular association with a 1.0 M reactant will take 200-400 ps, while association with a 100 mM reactant will take 2-4 ns. 
  On the far right of the graph, being able to distinguish different species by NMR (e.g. amide methyl groups in DMF) means that these species have a very long lifetime indeed.
</p>

<p>
  Framed in these terms, then, it becomes obvious why the 10-100 ps timescales currently accessible by <i>ab initio</i> molecular dynamics (AIMD) are unable to model many important 
  molecular processes.  Indeed, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00833>work from Grossman and coworkers</a> 
  has shown that the results of AIMD simulations can be very dependent on the pre-AIMD equilibration method used, 
  since the actual solvent environment is unable to fully relax over the timescale of the simulation. 
  For AIMD to become a truly useful alternative to forcefield molecular dynamics, much faster <i>ab initio</i> methods will be needed!
<p>

<h3>
  Sources:
</h3>
<ul>
  <li>
    intramolecular vibrational redistribution: <a href=http://www.cchem.berkeley.edu/millergrp/pdf/237.pdf>ref</a>
  </li>
  <li>
    hydrogen bonding: <a href=https://web.stanford.edu/group/fayer/articles/351.pdf>ref</a>
  </li>
  <li>
    NMR: <a href= http://www.satyensaha.com/pdf%20files/literature/SS-NMR-timescale.pdf>ref</a>
  </li>
  <li>
    bond rotation: <a href=https://pubs.rsc.org/en/content/articlelanding/2015/an/c5an00558b>ref</a>, 
    <a href=https://web.stanford.edu/group/fayer/articles/350.pdf>ref</a>
  </li>
  <li>
    carbocation lifetimes: <a href=https://catalogimages.wiley.com/images/db/pdf/0471233242.01.pdf>ref</a>, 
    <a href=https://www.sciencedirect.com/science/article/abs/pii/S0301010408000050?via%3Dihub>ref</a>
  </li>
  <li>
    excited state lifetimes: <a href=https://pubs.acs.org/doi/10.1021/acs.chemrev.6b00057>ref</a>, 
    <a href=https://pubs.rsc.org/en/content/articlelanding/2019/cc/c9cc01047e>ref</a>, 
    <a href=https://pubs.acs.org/doi/10.1021/cr300503r>ref</a>, 
    <a href=https://pubs.rsc.org/en/content/articlelanding/2016/cp/c6cp01635a>ref</a>
  </li>
</ul>

<i>Thanks to Richard Liu and Eugene Kwan for feedback on this figure.</i>

<h3>Corrections (updated 7/25/2022):</h3>
  <ul>
    <li>
      The lifetime given for benzophenone is for the singlet excited state, not the triplet excited state 
      (which has a much longer lifetime, explaining why benzophenone can be used as a triplet sensitizer).
      Thanks to <a href="https://twitter.com/DanielEFalvey/status/1549868458673901568">Daniel Falvey</a> for pointing this out.
    </li>
    <li>
      The illustration of "NMR timescale" refers only to the timescale in which different peaks can be distinguished in a 1D NMR experiment.
      More sophisticated NMR experiments can probe much faster processes, such as molecular tumbling, so the use of the phrase "NMR timescale" is misleading.
      Thanks to <a href="https://twitter.com/DominikJKubicki/status/1549809467247689728">Dominik Kubicki</a> for pointing this out.
    </li>
  </ul>
]]></description>
              <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Apotheosis of 2D Infrared Spectroscopy</title>
              <link>public/blog/20220708_apotheosis_of_2d_ir.html</link>
              <description><![CDATA[
<p>
  While IR spectroscopy is still taught in introductory organic chemistry classes, it has been almost completely replaced by NMR spectroscopy and mass spectrometry for 
  routine structural assignments. Still, IR spectroscopy offers unique advantages to the mechanistic chemist: the short timescale of IR allows for the 
  observation of transient molecular interactions even when slower techniques like NMR only yield time-averaged data, 
  and IR absorbances can easily be perturbed by isotopic substitution while leaving the underlying potential-energy surface unchanged. 
</p>

<p>
  These advantages are nicely illustrated in the IR spectrum of a mixture of phenol, benzene, and CCl<sub>4</sub>, 
  which shows two peaks corresponding to free phenol (2665 cm<sup>-1</sup>) and phenol complexed to benzene (2631 cm<sup>-1</sup>). 
  (The phenolic proton was replaced by deuterium, moving the O–D stretch away from C–H stretches and into a vacant region of the spectrum.) 
  From the standpoint of assessing purity, it might be upsetting that a pure compound shows two peaks; 
  from the standpoint of a mechanistic chemist, the ability to distinguish two different solvation environments experimentally is incredible.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_1d_ir.png" style="width:400px;" alt="IR spectrum of phenol in benzene/CCl4" />
  <figcaption> IR spectrum of phenol in benzene/CCl<sub>4</sub> (Fayer Figure 2). </figcaption>
</figure>

<p>
  Unfortunately, measuring this spectrum tells us about the thermodynamics of the equilibrium, but not the kinetics; 
  there’s not a good way to determine how fast these two species are exchanging from these data.<sup><a href="#fn1">1</a></sup> 
  In 2005, <a href=https://www.science.org/doi/10.1126/science.1116213>Fayer and coworkers</a> developed a pump–probe infrared spectroscopy method called “2D IR” to tackle this problem. 
  In 2D IR, the system is excited, allowed to evolve for a variable length of time <i>T<sub>w</sub></i>, 
  and then triggered to emit a vibrational “echo” (in analogy to spin-echo NMR experiments) 
  which still contains phase information from the original excitation. 
  (There are a lot of <a href=https://pubs.acs.org/doi/pdf/10.1021/ar068010d>non-trivial spectroscopic details</a> here which I don’t really understand.) 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_2d_ir.png" style="width:320px;" alt="2D IR spectrum of phenol in benzene/CCl4" />
  <figcaption> 2D IR spectrum of phenol in benzene/CCl<sub>4</sub> (Fayer Figure 3). </figcaption>
</figure>

<p>
  The net result of this is a two-dimensional plot showing initial and final frequencies, in which cross-peaks represent molecules which have moved between one state and another during 
  <i>T<sub>w</sub></i>. 
  By surveying a range of <i>T<sub>w</sub></i> values, the kinetics of exchange can be quantitatively determined: in this case, the time constant τ was found to be 8 ± 2 ps. 
  This result might not seem thrilling (“fast exchange is fast”), but this experiment can be used to measure rates of phenol dissociation from electronically-varied aromatic rings, 
  or <a href=https://web.stanford.edu/group/fayer/articles/351.pdf>compared to results from molecular dynamics simulations</a> for benchmarking purposes. 
</p>

<p>
  While many groups are now using 2D IR, <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c00154>this recent paper</a> 
  from Tokmakoff and coworkers studying superconcentrated electrolytes stood out to me as particularly exceptional. 
  In superconcentrated solutions like those found in batteries (e.g. 15 M LiTFSI in acetonitrile), the extreme salt concentration leads to high viscosity and substantial aggregation, 
  leading to questions about how charge transport in batteries occurs. 
  Some simulations seem to suggest that, rather than “vehicular diffusion” wherein a cation diffuses along with its solvent shell, 
  charge transport occurs through “structural diffusion” involving breaking/reforming of cation–solvent interactions. 
  (This is analogous to the Grotthuss mechanism of proton transport in water.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_transport_mechs.png" style="width:450px;" alt="different transport mechanisms" />
  <figcaption> Illustration of different transport mechanisms (Tokmakoff Figure 8). </figcaption>
</figure>

<p>
  Since distinct C≡N stretches are visible for cation-bound and free acetonitrile, it might seem straightforward to simply measure time evolution of the cross-peaks 
  and thereby determine the rate of solvent exchange. 
  Unfortunately studying exchange in the bulk solvent is complicated by the fact that direct vibrational energy transfer can occur through collisions, 
  meaning that cross-peaks are observed even in the absence of exchange. 
  The authors solve this problem by using a mixture of D<sub>3</sub>CCN and D<sub>3</sub>C<sup>13</sup>CN: 
  while cross-peaks between the heavy and light isotopologues can only occur through energy transfer, 
  cross-peaks between the same isotopologue can also occur through chemical exchange.<sup><a href="#fn2">2</a></sup> 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_2d_ir_tokmakoff.png" style="width:600px;" alt="2D IR of electrolyte" />
  <figcaption> 2D IR measurement of 1.9 M LiTFSI in acetonitrile (Tokmakoff Figure 5). </figcaption>
</figure>

<p>
  They find that the time evolution of all cross-peaks is identical under all conditions, 
  indicating that solvent exchange must be slower than energy transfer (~20 ps) for any cation or concentration studied. 
  This suggests that, contrary to a variety of theoretical studies, structural-diffusion mechanisms for cation transport are quite slow and unlikely to be relevant for these electrolytes. 
</p>

<p>
  This study is a beautiful example of designing a cutting-edge spectroscopic experiment to solve a key scientific problem, 
  and reminds me how much we still don’t know about “simple” systems like ionic solutions. 
  I would love to see techniques like this applied to study reactive intermediates in the ground state, e.g. Olah-style superacid solutions! 
  More broadly, it’s exciting to see how 2D IR can advance in less than two decades from being limited to simple model systems to 
  now being used to tackle the biggest open questions in chemistry. What new techniques being developed today will rise to prominence in the coming decades?
</p>

<i>Thanks to Joe Gair for reading a draft of this.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    For a different way to extract rates from vibrational spectra, see <a href=https://pubs.acs.org/doi/abs/10.1021/ja00882a048>this work</a> on line-broadening analysis.
  </li>
  <li id="fn2">
    Deuterated acetonitrile is used owing to <a href=https://pubs.acs.org/doi/10.1021/acs.jpcb.1c09572>complications with Fermi resonances</a> in protio-acetonitrile.
  </li>
</ol>
]]></description>
              <pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Four Handy Bash One-Liners</title>
              <link>public/blog/20220706_bash_oneliners.html</link>
              <description><![CDATA[
<p>
  In my experience, most computational chemists only know a handful of basic Bash commands, which is a shame because Bash is incredibly powerful.
  Although I'm far from an expert, here are a few commands I frequently find myself using:
</p>

<h3> 1. <i>sed</i> For Find-and-Replace.</h3>
<p class=code-block>
  $ sed -i “s/b3lyp/m062x/” *.gjf
</p>
<p>
  If you want to resubmit a bunch of transition states at a different level of theory, don't use a complex package like <a href=https://github.com/ekwan/cctk>cctk</a>! 
  You can easily find and replace text using <a href=https://www.gnu.org/software/sed/manual/sed.html>sed</a>, which runs almost instantly even for hundreds of files. 
  (Note that the syntax for modifying in-place is <a href=https://stackoverflow.com/questions/4247068/sed-command-with-i-option-failing-on-mac-but-works-on-linux>slightly different</a> on macOS.)
</p>

<h3> 2. Renaming Lots of Files </h3>
<p class=code-block>
  $ for f in *.gjf; do mv $f ${f/.gjf/_resubmit.gjf}; done
</p>
<p>
  Unfortunately, you can't rename lots of files with a single command in Bash, but using a <span class=code>for; do; done</span> loop is almost as easy.
  Here, we simply use <a href=https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html>parameter expansion</a> to replace the end of the filename, 
  but the possibilities are endless.
</p>

<h3> 3. Counting Occurrences in a File</h3>
<p class=code-block>
  $ for f in */*.out; do echo $f; grep "SCF Done" $f | wc -l; done
</p>
<p>
  Here we again use a <span class=code>for</span> loop, but in this case we use <i>grep</i> to search for the string "SCF Done". 
  We then pipe the output of this search to the <i>wc -l</i> command, which counts the number of lines. 
  Since <i>grep</i> returns each result on a new line, this prints the number of optimization steps completed.
</p>

<h3> 4. Cancelling Jobs By Matching Name </h3>
<p class=code-block>
  $ squeue -u cwagen | grep "g16_ts_scan" | awk '{print $1}' | xargs -n 1 scancel
</p>
<p>
  Although the <i>slurm</i> workload manager allows one to cancel jobs by partition or by user, to my knowledge there isn't a way to cancel jobs that match a certain name.
  This is a problem if, for instance, you're working on two projects at once and want to resubmit only one set of jobs.
  Here, we use <i>squeue</i> to get a list of job names, search for the names that match, extract the job number using <a href=https://www.gnu.org/software/gawk/manual/gawk.html>awk</a>,
  and finally cancel each job by building the <i>scancel</i> commands with <a href=https://www.man7.org/linux/man-pages/man1/xargs.1.html>xargs</a>.
  (This should be easily modifiable for other workload managers.)
</p>
]]></description>
              <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Plenty of Room at the Bottom</title>
              <link>public/blog/20220628_plenty_of_room_at_the_bottom.html</link>
              <description><![CDATA[
<p>
  <a href=https://edoc.ub.uni-muenchen.de/15211/1/Sailer_Christian.pdf>This thesis</a>, from Christian Sailer at Ludwig Maximilian University in Munich, 
  is one of the most exciting studies I’ve read this year. 
</p>

<p>
  Sailer and coworkers are able to generate benzhydryl carbocations from photolysis of the corresponding phosphonium salts, and can monitor their 
  formation and lifetime via femtosecond transient absorption spectroscopy. 
  (There are some technical challenges which I’ll omit here.) 
  They then use this platform to study the addition of alcohols to these cations and obtain nice kinetic data on some shockingly fast reactions:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220628_electrophile.png" style="width:400px;" alt="rates of reactions with electrophiles" />
  <figcaption> Rates of reaction between methanol and various cations (page 77). </figcaption>
</figure>

<p>
  Although these species are extremely reactive, substituent effects are still paramount: 
  adding two methyl groups to stabilize the benzhydryl carbocation extends its lifetime by 6.3x, whereas adding two electron-withdrawing fluorines shortens its lifetime by 5.6x. 
  The most electrophilic species studied, (dfp)<sub>2</sub>CH<sup>+</sup>, reacts with methanol in only 2.6 ps! 
  These findings demonstrate how even an extremely reactive carbocation like Ph<sub>2</sub>CH<sup>+</sup> doesn’t react at the rate of diffusion with nucleophiles; 
  although this reaction is almost instant on an absolute scale, there are still tens or hundreds of unproductive alcohol–carbocation interactions before product is finally formed.
</p>

<p>
  In contrast to Sailer’s results on the electrophile, which align nicely with results from more conventional kinetic measurements, different alcohols behave very differently:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220628_nucleophile.png" style="width:350px;" alt="rates of reactions with nucleophiles" />
  <figcaption> Rates of reaction between mfp(Ph)CH<sup>+</sup> and various alcohols (page 75). </figcaption>
</figure>

<p>
  This rate difference is surprising, since Mayr’s measurements demonstrate that there’s no conventional difference in nucleophilicity between these species—and intuitively, 
  one wouldn’t expect adding additional carbons to substantially hinder approach to the oxygen. 
  However, as Sailer notes, larger molecules rotate and reorient much more slowly than smaller molecules. 
  This is evident from a number of different physical properties: larger alcohols form more viscous liquids and have slower dielectric relaxation times 
  (how long it takes them to reorient in response to a new charge).
</p>

<table>
  <tr>
    <th> alcohol </th> 
    <th> viscosity (<a href=http://murov.info/orgsolvents.htm>ref</a>) </th>
    <th> dielectric relaxation time (<a href=https://pubs.acs.org/doi/pdf/10.1021/j100048a004>ref</a>) </th>
    <th> reaction time (above) </th>
  </tr>
  <tr>
    <td> methanol </td>
    <td> 0.54 cP </td>
    <td> 5.0 ps </td>
    <td> 22 ps </td>
  </tr>
  <tr>
    <td> ethanol </td>
    <td> 1.08 cP </td>
    <td> 16 ps </td>
    <td> 41 ps </td>
  </tr>
  <tr>
    <td> <i>n</i>-propanol </td>
    <td> 1.95 cP </td>
    <td> 26 ps </td>
    <td> 62 ps </td>
  </tr>
  <tr>
    <td> <i>iso</i>-propanol </td>
    <td> 2.07 cP </td>
    <td> </td>
    <td> 106 ps</td>
  </tr>
  </tr>
</table>

<p>
  Sailer thus concludes that, as the timescale of reaction approaches the timescale of molecular motion, nucleophile reorientation becomes rate-limiting. 
  This is a really cool conclusion, and highlights how the environment that reacting molecules “see” differs from our macroscopic chemical intuition: 
  although we think of reorientation as barrierless, for very fast reactions reorientation is actually slower than bond formation.
  (Philosophically related, in my mind, is Dan Singleton’s <a href=https://pubs.acs.org/doi/10.1021/jacs.0c06295>work on non-instantaneous solvent relaxation</a>, 
  and how this influences the stability of reactive intermediates.)
</p>

<p>
  Thus, this work simultaneously highlights both similarities and differences between ultrafast reactions and their benchtop congeners. 
  My hope is that future studies can find ways to move beyond just benzhydryl cations and study elementary questions of selectivity with different nucleophiles (e.g. addition/elimination). 
  The ability to observe what is typically the “invisible” step in S<sub>N</sub>1 is incredibly powerful, and I think we’ve barely scratched the surface with what we can learn from measurements like these.
</p>

]]></description>
              <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Site-Selective Glycosylation: Reflections</title>
              <link>public/blog/20220620_glycosylation.html</link>
              <description><![CDATA[
<p>
  Now that our lab’s site-selective glycosylation has been <a href="https://www.nature.com/articles/s41586-022-04958-w">published</a>,
  I wanted to share some reflections from the computational portion of the work.
</p>

<p>
  As one might expect, finding transition states for these large and flexible catalysts was a substantial challenge.
  We ended up using an ONIOM-type scheme to model most of the catalyst using PM7, 
which meant only the thioureas and the “northern” amide (which acts as a general base) needed to be computed using DFT. 
  Even with this approximation, it took three months to find the first glycosylation transition state. 
  Although we ended reoptimizing everything with all-atom DFT for publication, 
  using PM7 for non-essential atoms was crucial in accelerating the initial transition-state search.
</p>

<p>
  Working on this project also gave me a new appreciation for all of the work done to develop linear-scaling DFT methods. 
  The size of the full system meant that even routine Hessian calculations with a double-zeta basis set took most of a week; 
  without all the work that’s been done to speed up calculations on large systems (e.g. the <a href="https://www.science.org/doi/10.1126/science.271.5245.51">fast multipole method</a>), 
  these computations would not even have been possible. 
  Hopefully, calculations on systems of this size will become routine in the coming decade.
</p>

<p>
  After finding the (1,2) and (1,3) transition states, we were surprised (and disappointed) to find that the predicted selectivity was completely backwards from that observed experimentally. 
  Closer observation of the transition states showed that the hydrogen-bonding network in the unprotected acceptor was quite different between the two structures, 
  leading us to suspect that this energetic difference might be an artifact of implicit solvation. Since the reaction is run in diisopropyl ether, a hydrogen-bond acceptor, 
  we reasoned that the unprotected hydroxyls would be able to form hydrogen bonds with solvent ether molecules and not with the donor.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220620_presto.png" style="width:450px;" alt="picture of a solvated complex" />
  <figcaption> CYLView visualization of the solvated complex </figcaption>
</figure>

<p>
  To test this idea, we decided to attempt explicit solvent calculations. Although a full <i>ab initio</i> molecular dynamics study of this system was clearly out of the question, 
  we were able to run molecular dynamics using Grimme’s GFN2-xtb method for the catalyst, donor, and acceptor and the GFN-FF polarizable forcefield for the solvent. 
  Examination of the pre-(1,3) complex shows that the C4 hydroxyl is indeed solvated by a diisopropyl ether, meaning that the donor–acceptor hydrogen bond predicted by DFT is just wrong. 
  (As a bonus, we found that no (1,2) preference exists in the ground state, in agreement with the experimental observation that <i>K<sub>M</sub></i> is not lower for more selective catalysts.)
</p>

<p>
  Although the coolest solution would have been to do free energy perturbation in explicit solvent to get an accurate ∆∆G between the (1,2) and (1,3) transition states, 
  technical barriers meant we had to settle for modeling the C4-protected acceptor, which indeed favored the correct product. 
  Still, I think this approach demonstrates the computational insight that explicit solvent calculations can give even when a full, high-level treatment of the system is unreasonable. 
  We’ve been developing <a href="https://github.com/corinwagen/presto">software</a> to make this sort of molecular dynamics more routine—if you’re interested in using this in your research, 
  please contact me!
</p>
]]></description>
              <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
          </item>
      </channel>
  </rss>