  <rss version="2.0">
      <channel>
          <description>My personal blog, mainly focusing on issues of chemistry and metascience, unified by trying to answer the question &#34;how can we make science better&#34;?</description>
          <link>https://corinwagen.github.io</link>
          <title>Corin Wagen</title>
          <item>
              <title>How Common Are Different Functional Groups?</title>
              <link>public/blog/20230804_abundance.html</link>
              <description><![CDATA[
<p>
Since the ostensible purpose of organic methodology is to develop reactions that are useful in the real world, the utility of a method is in large part dictated by the accessibility of the starting materials. If a compound is difficult to synthesize or hazardous to work with, then it’s difficult to convince people to use it in a reaction (e.g. most diazoalkanes). Organic chemists are pragmatic, and would usually prefer to run a reaction that starts from a commercial and bench-stable starting material. 
</p>

<p>
For instance, this explains the immense popularity of the Suzuki reaction: although the Neigishi reaction (using organozinc nucleophiles) usually works better for the same substrates, you can buy lots of the organoboron nucleophiles needed to run a Suzuki and leave them lying around without taking any precautions. In contrast, organozinc compounds usually have to be made from the corresponding organolithium/Grignard reagent and used freshly, which is considerably more annoying.
</p>

<p>
The ideal starting material, then, is one which is commercially available and cheap. In recent years, it’s become popular to advertise new synthetic methods by showing that they work on exceptionally cheap and common functional groups, and in particular to compare the abundance of different functional groups to demonstrate that one starting material is more common than another. To pick just one of many examples, Dave MacMillan used this plot to show why cross-coupling reactions of alcohols were important (<a href=https://macmillan.princeton.edu/wp-content/uploads/Zhe.pdf>ref</a>):
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230804_macmillan.png style="width:500px;" />
  <figcaption>
  This visual works really well.
  </figcaption>
</figure>

<p>
When I saw MacMillan’s talk at MIT last year, I was curious what it would take to make additional graphics like this. The “number of reactions” plot can be made pretty easily from Reaxys, but I’ve always been uncertain how the “number of commercial sources” plots are made: I haven’t seen references listed for these numbers, nor is anything usually found in the Supporting Information. 
</p>

<p>
I decided to take a swing at getting this data myself by analyzing the <a href=https://mcule.com/database/>Mcule</a> "building blocks" database, which contains about 3.5 million compounds. Although Mcule doesn't define what a building block is (at least, not that I can find), it’s likely that their definition is similar to that of ZINC, which defines building blocks as “those catalogs of compounds available in preparative quantities, typically 250 mg or more” (<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559>ref</a>). This seems like a reasonable proxy for the sorts of compounds synthetic chemists might use in reactions. I defined patterns to match a bunch of functional groups using SMARTS/SMILES, and then used RDKit to find matches in the Mcule building blocks database. The code can be found on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule>Github</a>, along with the patterns I used.
</p>

<p>
The results are shown below. As expected, ethers, amines, amides, and alcohols are quite common. Surprisingly, aryl chlorides aren't that much more common than aryl bromides—and, except for aliphatic fluorides, all aliphatic halides are quite rare. Allenes, carbodiimides, and SF5 groups are virtually unheard of (&lt;100 examples).
</p>

<table class=left-aligned-table>
  <tr>
    <th>Functional Group</th>
    <th>Number</th>
    <th>Percent</th>
  </tr>
	<tr>
		<td>acid chloride</td>
		<td>6913</td>
		<td>0.19</td>
	</tr>
	<tr>
		<td>alcohol</td>
		<td>1022229</td>
		<td>28.60</td>
	</tr>
	<tr>
		<td>aliphatic bromide</td>
		<td>42018</td>
		<td>1.18</td>
	</tr>
	<tr>
		<td>aliphatic chloride</td>
		<td>70410</td>
		<td>1.97</td>
	</tr>
	<tr>
		<td>aliphatic fluoride</td>
		<td>650576</td>
		<td>18.20</td>
	</tr>
	<tr>
		<td>aliphatic iodide</td>
		<td>3159</td>
		<td>0.09</td>
	</tr>
	<tr>
		<td>alkene</td>
		<td>176484</td>
		<td>4.94</td>
	</tr>
	<tr>
		<td>alkyne</td>
		<td>35577</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>allene</td>
		<td>99</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>amide</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td>anhydride</td>
		<td>1279</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>aryl bromide</td>
		<td>451451</td>
		<td>12.63</td>
	</tr>
	<tr>
		<td>aryl chloride</td>
		<td>661591</td>
		<td>18.51</td>
	</tr>
	<tr>
		<td>aryl fluoride</td>
		<td>618620</td>
		<td>17.31</td>
	</tr>
	<tr>
		<td>aryl iodide</td>
		<td>216723</td>
		<td>6.06</td>
	</tr>
	<tr>
		<td>azide</td>
		<td>5164</td>
		<td>0.14</td>
	</tr>
	<tr>
		<td>aziridine</td>
		<td>748</td>
		<td>0.02</td>
	</tr>
	<tr>
		<td>carbamate</td>
		<td>127103</td>
		<td>3.56</td>
	</tr>
	<tr>
		<td>carbodiimide</td>
		<td>28</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>carbonate</td>
		<td>1231</td>
		<td>0.03</td>
	</tr>
	<tr>
		<td>carboxylic acid</td>
		<td>410860</td>
		<td>11.49</td>
	</tr>
	<tr>
		<td>chloroformate</td>
		<td>250</td>
		<td>0.01</td>
	</tr>
	<tr>
		<td>cyclobutane</td>
		<td>195728</td>
		<td>5.48</td>
	</tr>
	<tr>
		<td>cyclopropane</td>
		<td>349455</td>
		<td>9.78</td>
	</tr>
	<tr>
		<td>diene</td>
		<td>10188</td>
		<td>0.29</td>
	</tr>
	<tr>
		<td>difluoromethyl</td>
		<td>163395</td>
		<td>4.57</td>
	</tr>
	<tr>
		<td>epoxide</td>
		<td>5859</td>
		<td>0.16</td>
	</tr>
	<tr>
		<td>ester</td>
		<td>422715</td>
		<td>11.83</td>
	</tr>
	<tr>
		<td>ether</td>
		<td>1434485</td>
		<td>40.13</td>
	</tr>
	<tr>
		<td>isocyanate</td>
		<td>1440</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>isothiocyanate</td>
		<td>1389</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>nitrile</td>
		<td>209183</td>
		<td>5.85</td>
	</tr>
	<tr>
		<td>nitro</td>
		<td>126200</td>
		<td>3.53</td>
	</tr>
	<tr>
		<td>pentafluorosulfanyl</td>
		<td>18</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>primary amine</td>
		<td>904118</td>
		<td>25.29</td>
	</tr>
	<tr>
		<td>secondary amine</td>
		<td>857290</td>
		<td>23.98</td>
	</tr>
	<tr>
		<td>tertiary amine</td>
		<td>609261</td>
		<td>17.04</td>
	</tr>
	<tr>
		<td>trifluoromethoxy</td>
		<td>18567</td>
		<td>0.52</td>
	</tr>
	<tr>
		<td>trifluoromethyl</td>
		<td>455348</td>
		<td>12.74</td>
	</tr>
	<tr>
		<td>urea</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td><i>Total</i></td>
		<td><i>3574611</i></td>
		<td><i>100.00</i></td>
	</tr>
</table>

<p>
(Fair warning: I’ve spotchecked a number of the SMILES files generated (also on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule/output>Github</a>), but I haven’t looked through every molecule, so it’s possible that there are some faulty matches. I wouldn’t consider these publication-quality numbers yet.)
</p>

<p>
An obvious caveat: there are lots of commercially “rare” functional groups which are easily accessible from more abundant functional groups. For instance, acid chlorides seem uncommon in the above table, but can usually be made from ubiquitous carboxylic acids with e.g. SOCl2. So these data shouldn’t be taken as a proxy for a more holistic measure of synthetic accessibility—they measure commercial availability, that’s all. 
</p>

<p>
What conclusions can we draw from this?
</p>

<ul>
<li>
The most common functional groups are the milquetoast ones: alcohols, amines, esters, etc. Perhaps this explains <a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jmedchem.5b01409>where all the new reactions have gone</a>: unless your new method works on alcohols or amines, it will struggle to get traction in most of chemical space relative to e.g. Williamson ether synthesis or reductive amination. (Kudos to MacMillan for identifying this; <i>vide supra</i>.)
</li>
<li>
Ureas are much more common than you’d expect from academic methods papers. This I think speaks to the difference between what methodologists want and what medicinal chemists want. Ureas are a bit annoying to work with: they’re pretty polar by the standards of academia, they’re not always soluble in organic solvents, and they have a tendency to stick to transition metal catalysts or get deprotonated by strong bases. But they’re easy to make in libraries, since the isocyanate/amine disconnection is so robust, and they’re excellent hydrogen-bond donors and acceptors.
<b>CORRECTION: There's a SMARTS error, so the match for "ureas" actually matches amides—disregard this section. Thanks to <a href=https://twitter.com/wmdhn>@wmdhn</a> for catching this.</a></b>
</li>
<li>
Uncommon functional groups, like SF<sub>5</sub> and allenes, are very uncommon. If you want to introduce an SF<sub>5</sub> group, you are in for a rough time: there aren’t great ways to add it to molecules (although there have been some steps forward <a href=https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/chem.202201491>in recent years</a>), and there are only 18 commercial examples. So people can write as many <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7728329/>papers</a> as they want about how cool SF<sub>5</sub> groups are: I still doubt we’ll see them used very much in the near future.
</li>
<li>
But also, the abundance of a given functional group is very elastic in the long run. Trifluoromethyl groups used to be extremely rare—they’re not found in nature!—but now 1 in 8 molecules has a CF<sub>3</sub> group. CF<sub>3</sub> just turns out to be a very good handle for a lot of molecular design tasks, and so people found ways to introduce it all over the place, and now it’s not hard to get molecules that have trifluoromethyl groups. Synthetic chemists should feel good about this.
</li>
</ul>

<p>
The functional-group-specific SMILES files are in the previously mentioned Github repo, so anyone who wants to e.g. look through all the commercially available alkenes and perform further cheminformatics analyses can do so. I hope the attached code and data helps other chemists perform similar, and better, studies, and that this sort of thinking can be useful for those who are currently engaged in reaction discovery. 
</p>

<i>
Thanks to Eric Jacobsen for helpful conversations about these data.
</i>

]]></description>
              <pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Two Cultures in Atomistic Simulation</title>
              <link>public/blog/20230728_two_cultures.html</link>
              <description><![CDATA[<br>
<br>
<i>
TW: stereotypes about molecular dynamics.
</i>

<p>
In his fantastic essay “<a href=https://en.wikipedia.org/wiki/The_Two_Cultures>The Two Cultures</a>,” C. P. Snow observed that there was (in 1950s England) a growing divide between the academic cultures of science and the humanities:
</p>

<blockquote>
Literary intellectuals at one pole—at the other scientists, and as the most representative, the physical scientists. Between the two a gulf of mutual incomprehension—sometimes (particularly among the young) hostility and dislike, but most of all lack of understanding. They have a curious distorted image of each other. Their attitudes are so different that, even on the level of emotion, they can't find much common ground.
</blockquote>

<p>
He reflects on the origins of this phenomenon, which he contends is new to the 20th century, and argues that it ought to be opposed:
</p>

<blockquote>
This polarisation is sheer loss to us all. To us as people, and to our society. It is at the same time practical and intellectual and creative loss, and I repeat that it is false to imagine that those three considerations are clearly separable. But for a moment I want to concentrate on the intellectual loss.
</blockquote>

<p>
Snow’s essay is wonderful: his portrait of a vanishing cultural intellectual unity should inspire us all, scientists and otherwise, to improve ourselves, and the elegiac prose reminds the reader that even the best cultural institutions are fragile and fleeting things.
</p>

<p>
I want to make an analogous—but much less powerful—observation about the two cultures present in atomistic simulation. I’ll call these the “QM tribe” and the “MD tribe” for convenience: crudely, “people who use Gaussian/ORCA/Psi4 for their research” and “people who use Schrodinger/AMBER/OpenMM/LAMMPS for their research,” respectively. Although this dichotomy is crude, I contend there are real differences between these two groups, and that their disunity hurts scientific progress.
</p>

<h2>
The Nature of Energy Surfaces
</h2>

<p>
The most fundamental disagreement between these two cultures is in how they think about energy surfaces, I think. Most QM-tribe people think in terms of optimizing to discrete critical points on the potential energy surface: one can perform some sort of gradient-informed optimization to a ground state, or follow negative eigenvalues to a transition state.
</p>

<p>
Implicit to this assumption is that there exist well-defined critical points on the PES, and that finding such critical points is meaningful and productive. Conformers exist, and many people now compute properties as Boltzmann-weighted averages over conformational ensembles, but this is usually done for 10–100 conformers, not thousands or millions. Entropy and solvation, if they’re considered at all, are viewed as corrections, not key factors: since QM is so frequently used to study high-barrier bond-breaking processes where enthalpic factors dominate, one can often get reasonable results with cartoonish treatments of entropy.
</p>

<p>
In contrast, MD-tribe scientists generally don’t think about transition states as specific configurations of atoms—rather, a transition state can emerge from some sort of simulation involving biased sampling, but it’s just a position along some abstract reaction coordinate, rather than a structure which can be visualized in CYLView. Any information gleaned is statistical, rather than concretely visual (e.g. “what is the mean number of hydrogen bonds to this oxygen near this transition state”).
</p>

<p>
Unlike the QM tribe, MD-tribe scientists generally cannot study bond-breaking processes, and so focus on conformational processes (protein folding, aggregation, nucleation, transport) where entropy and solvation are of critical importance: as such, free energy is almost always taken into consideration by MD-tribe scientists, and the underlying PES itself is rarely (to my knowledge) viewed as a worthy topic of study in and of itself.
</p>

<h2>
Molecular Representations
</h2>

<p>
This divide also affects how the two cultures view the task of molecular representation. QM-tribe scientists generally view a list of coordinates and atomic numbers as the most logical representation of a molecule (perhaps with charge and multiplicity information). To the QM tribe, a minimum on the PES represents a structure, and different minima naturally ought to have different representations. Bonding and bond order are not specified, because QM methods can figure that out without assistance (and it’s not uncommon for bonds to change in a QM simulation anyway).
</p>

<p>
In contrast, people in the MD tribe generally want a molecular representation that’s independent of conformation, since many different conformations will intrinsically be considered. (See Connor Coley’s <a href=https://drive.google.com/file/d/1PMZ8GHvvJv_jhy8t6fH73wv4e9ZjHwUN/view>presentation</a> from a recent MolSSI workshop for a discussion of this.) Thus, it’s common to represent molecules through their topology, where connectivity and bond order are explicitly specified. This allows for some pretty wild <a href=https://pubs.acs.org/doi/10.1021/ja00241a001>simulations</a> of species that would be reactive in a QM simulation, but also means that e.g. tautomers can be a massive problem in MD (<a href=https://link.springer.com/article/10.1007/s10822-016-9920-5>ref</a>), since protons can’t equilibrate freely.
</p>

<p>
For property prediction, an uneasy compromise can be reached wherein one takes a SMILES string, performs a conformational search, and then Boltzmann-averages properties over all different conformers, to return a set of values which are associated only with the SMILES string and not any individual conformation. (Matt Sigman does this, as does Bobby Paton <a href=https://nova.chem.colostate.edu/cascade/predict/>for NMR prediction.</a>) This is a lot of work, though.
</p>

<h2>
“A Gulf Of Mutual Incomprehension”
</h2>

<p>
These differences also become apparent when comparing the software packages that different tribes use. Take, for instance, the task of predicting partial charges for a given small molecule. A QM-tribe scientist would expect these charges to be a function of the geometry, whereas an MD-tribe scientist would want the results to be explicitly geometry-independent (<a href=https://www.cell.com/biophysj/fulltext/S0006-3495(22)01326-1>e.g.</a>) so that they can be used for subsequent MD simulations.
</p>

<p>
The assumptions implicit to these worldviews mean that it’s often quite difficult to go from QM-tribe software packages to MD-tribe software packages or vice versa. I’ve been <a href=https://github.com/openforcefield/openff-toolkit/issues/611>stymied</a> before by trying to get OpenMM and openforcefield to work on organic molecules for which I had a list of coordinates and not e.g. a SMILES string—although obviously coordinates will at some point be needed in the MD simulation, most workflows expect you to start from a topology and not an xyz file.
</p>

<p>
Similarly, it’s <a href=https://github.com/nglviewer/nglview/issues/589#issuecomment-468339971>very difficult</a> to get the graphics package NGLView to illustrate the process of bonds breaking and forming—NGLView is typically used for MD, and expects that the system’s topology will be defined at the start of the simulation and never changed. (There are kludgy workarounds, like defining a new object for every frame, but it’s nevertheless true that NGLView is not made for QM-tribe people.)
</p>

<p>
(I’m sure that MD-tribe people are very frustrated by QM software as well, but I don’t have as much experience going in this direction. In general, MD tooling seems quite a bit more advanced than QM-tribe tooling; most MD people I’ve talked to seem to interact with QM software as little as possible, and I can’t say I blame them.)
</p>

<h2>
“A Curious Distorted Image of Each Other”
</h2>

<p>
There are also cultural factors to consider here. The questions that QM-tribe scientists think about are different than those that MD-tribe scientists think about: a somewhat famous QM expert once told me that they were “stuck on an ivory tower where people hold their nose when it comes to DFT, forget anything more approximate,” whereas MD-tribe scientists often seem alarmingly unconcerned about forcefield error but are obsessed with proper sampling and simulation convergence.
</p>

<p>
It seems that most people have only a vague sense of what their congeners in the other tribe actually work on. I don’t think most QM-tribe scientists I know have ever run or analyzed a regular molecular dynamics simulation using e.g. AMBER or OpenMM, nor do I expect that most MD-tribe scientists have tried to find a transition state in Gaussian or ORCA. In theory, coursework could remedy this, but education for QM alone already seems chaotic and ad hoc—trying to cram in MD, statistical mechanics, etc is probably ill-advised at the present.
</p>

<p>
Social considerations also play a role. There’s limited crosstalk between the two fields, especially at the trainee level. How many QM people even know who Prayush Tiwary is, or Michael Shirts, or Jay Ponder? How many MD graduate students have heard of Frank Neese or Troy Van Voorhis? As always, generational talent manages to transcend narrow boundaries—but rank-and-file scientists would benefit immensely from increased contact with the other tribe.
</p>

<h2>
“Unite Them”
</h2>

<p>
I’m not an expert on the history of chemistry, but my understanding is that the two fields were not always so different: Martin Karplus, Arieh Warshel, and Bill Jorgensen, key figures in the development of modern MD, were also formidable quantum chemists. (If any famous chemists who read this blog care to share their thoughts on this history, please email me: you know who you are!)
</p>

<p>
And as the two fields advance, I think they will come closer together once more. As QM becomes capable of tackling larger and larger systems, QM-tribe scientists will be forced to deal with more and more complicated conformational landscapes: modern enantioselective catalysts routinely have hundreds of ground-state complexes to consider (<a href=https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc02274e>ref</a>), and QSimulate and Amgen recently reported full DFT calculations on protein–ligand complexes (<a href=https://core.ac.uk/download/pdf/345075772.pdf>ref</a>).
</p>

<p>
Similarly, the increase in computing power means that many MD use cases (like FEP) are now limited not by insufficient sampling but by the poor energetics of the forcefields they employ. This is difficult to prove unequivocally, but I’ve heard this in interviews with industry folks, and there are certainly plenty of references complaining about poor forcefield accuracy (<a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00801>1</a>, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c01081>2</a>): a Psivant <a href=https://pubs.acs.org/doi/10.1021/acs.jcim.7b00564>review</a> dryly notes that “historically solvation energy errors on the order of 2–3 kcal/mol have been considered to be accurate,” which is hardly encouraging.
</p>

<p>
Many QM-tribe professors now work on dynamics: Dean Tantillo and Todd Martinez (who have long been voices “crying out in the wilderness” for dynamics) perhaps most prominently, but also Steven Lopez, Daniel Ess, Fernanda Duarte, Peng Liu, etc. And MD-tribe professors seem more and more interested in using ML mimics of QM to replace forcefields (<a href=https://www.biorxiv.org/content/10.1101/2020.07.29.227959v1>e.g.</a>), which will inevitably lead them down the speed–accuracy rabbit hole that is quantum chemistry. So it seems likely to me that the two fields will increasingly reunite, and that being a good 21st-century computational chemist will require competency in both areas.
</p>

<p>
If this is true, the conclusions for individual computational chemists are obvious: learn techniques outside your specialty, before you get forcibly dragged along by the current of scientific progress! There’s plenty to learn from the other culture of people that deals with more-or-less the same scientific problems you do, and no reason to wait.
</p>

<i>
As a denizen of quantum chemistry myself, I apologize for any misrepresentations or harmful stereotypes about practitioners of molecular dynamics, for whom I have only love and respect. I would be happy to hear any corrections over email.
</i>

]]></description>
              <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Machine Learning for Explicit Solvent Molecular Dynamics</title>
              <link>public/blog/20230720_ml_aimd.html</link>
              <description><![CDATA[
<p>
An important problem with simulating chemical reactions is that reactions generally take place in solvent, but most simulations are run without solvent molecules. This is a big deal, since much of the inaccuracy associated with simulation actually stems from poor treatment of solvation: when gas phase experimental data is compared to computations, the results are often quite good. 
</p>

<p>
Why don’t computational chemists include solvent molecules in their models? It takes a lot of solvent molecules to accurately mimic bulk solvent (enough to cover the system with a few different layers, usually ~10<sup>3</sup>).<sup><a href="#fn1">1</a></sup> Since most quantum chemical methods scale in practice as <i>O</i>(N<sup>2</sup>)–<i>O</i>(N<sup>3</sup>), adding hundreds of additional atoms has a catastrophic effect on the speed of the simulation.
</p>

<p>
To make matters worse, the additional degrees of freedom introduced by the solvent molecules are very “flat”—solvent molecules don’t usually have well-defined positions about the substrate, meaning that the number of energetically accessible conformations goes to infinity (with attendant consequences for entropy). This necessitates a fundamental change in how calculations are performed: instead of finding well-defined extrema on the electronic potential energy surface (ground states or transition states), molecular dynamics (MD) or Monte Carlo simulations must be used to sample from an underlying distribution of structures and reconstruct the free energy surface. Sufficient sampling usually requires consideration of 10<sup>4</sup>–10<sup>6</sup> individual structures,<sup><a href="#fn2">2</sup></a> meaning that each individual computation must be very fast (which is challenging for quantum chemical methods).
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230720_jorgensen.png style="width:550px;" />
  <figcaption>
  The title of this paper makes me so sad, because these techniques are still ignored by most organic chemists.
  </figcaption>
</figure>

<p>
Given the complexity this introduces, it’s not surprising that most computational organic chemists try to avoid explicit solvent at all costs. The typical workaround is to use “implicit solvent” models, which “reduce the complexity of individual solvent−solute interactions such as hydrogen-bond, dipole−dipole, and van der Waals interactions into a fictitious surface potential... scaled to reproduce the experimental solvation free energies” (<a href=https://pubs.acs.org/doi/abs/10.1021/acs.organomet.8b00456?src=recsys>Baik</a>). This preserves the well-defined potential energy surfaces that organic chemists are accustomed to, so you can still find transition states by eigenvector following, etc.
</p>

<p>
Implicit solvent models like PCM, COSMO, or SMD are better than nothing, but are known to struggle for charged species. In particular, they don’t really describe explicit inner-sphere solvent–solute interactions (like hydrogen bonding), meaning that they’ll behave poorly when these interactions are important. Dan Singleton’s paper on <a href=https://pubs.acs.org/doi/10.1021/ja5111392>the Baylis–Hillman reaction</a> is a nice case study of how badly implicit solvent can fail: even high-level quantum chemical methods are useless when solvation free energies are 10 kcal/mol off from experiment!
</p>

<p>
This issue is well-known. To quote from <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201709943>Schreiner and Grimme</a>:
</p>

<blockquote>
An even more important but still open issue is solvation. In the opinion of the authors it is a ‘scandal’ that in 2018 no routine free energy solvation method is available beyond (moderately successful) continuum theories such as COSMO-RS and SMD and classical FF/MD-based explicit treatments.
</blockquote>

<p>
When computational studies have been performed in explicit solvent, the results have often been promising: Singleton has studied <a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c06295>diene hydrochlorination</a> and <a href=https://pubs.acs.org/doi/10.1021/jacs.6b07328>nitration of toluene</a>, and Peng Liu has recently conducted a nice <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.0c12096>study of chemical glycosylation</a>. Nevertheless, these studies all require heroic levels of effort: quantum chemistry is still very slow, and so a single free energy surface might take months and months to compute.<sup><a href="#fn3">3</a></sup>
</p>

<p>
One promising workaround is using machine learning to accelerate quantum chemistry. Since these MD-type studies look at the same exact system over and over again, we could imagine first training some sort of ML model based on high-level quantum chemistry data, and then employing this model over and over again for the actual MD run. As long as (1) the ML model is faster than the QM method used to train it and (2) it takes less data to train the ML model than it would to run the simulation, this will save time: in most cases, a lot of time. 
</p>

<p>
(This is a somewhat different use case than e.g. <a href=https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a>ANI</a>-type models, which aim to achieve decent accuracy for any organic molecule. Here, we already know what system we want to study, and we’re willing to do some training up front.)
</p>

<p>
A lot of people are working in this field right now, but today I want to highlight some work that I liked from Fernanda Duarte and co-workers. Last year, they published a <a href=https://pubs.rsc.org/en/content/articlelanding/2022/cp/d2cp02978b#cit23>paper</a> comparing a few different ML methods for studying quasiclassical dynamics (in the gas phase), and found that atomic cluster expansion (ACE) performed better than Gaussian approximation potentials while training faster than NequIP. They then went on to show that ACE models could be trained automatically through active learning, and used the models to successfully predict product ratios for cycloadditions with post-TS bifurcations.
</p>

<p>
Their new <a href=https://chemrxiv.org/engage/chemrxiv/article-details/64a8085fba3e99daefab8f89>paper</a>, posted on ChemRxiv yesterday, applies the same ACE/active learning approach to studying reactions in explicit solvent, with the reaction of cyclopentadiene and methyl vinyl ketone chosen as a model system. This is more challenging than their previous work, because the ML model now not only has to recapitulate the solute reactivity but also the solute–solvent and solvent–solvent interactions. To try and capture all the different interactions efficiently, the authors ended up using four different sets of training data: substrates only, substrates with 2 solvent molecules, substrates with 33 solvent molecules, and clusters of solvent only. 
</p>

<p>
Previously, the authors used an energy-based selector to determine if a structure should be added to the training set: they predicted the energy with the model, ran a QM calculation, and selected the structure if the difference between the two values was big enough. This approach makes a lot of sense, but has the unfortunate downside that a lot of QM calculations are needed, which is exactly what this ML-based approach is trying to avoid. Here, the authors found that they could use similarity-based descriptors to select data points to add to the training set: these descriptors are both more efficient (needing fewer structures to converge) and faster to compute, making them overall a much better choice. (This approach is reminiscent of the metadynamics-based approach previously <a href=https://arxiv.org/abs/1712.07240>reported</a> by John Parkhill and co-workers.)
</p>

<p>
With a properly trained model in hand, the authors went on to study the reaction with biased sampling MD. They find that the reaction is indeed accelerated in explicit water, and that the free energy surface begins to look stepwise, as opposed to the concerted mechanism predicted in implicit solvent. (Singleton has observed similar behavior <a href=https://pubs.acs.org/doi/10.1021/ja208779k>before</a>, and <a href=https://chemrxiv.org/engage/chemrxiv/article-details/6362b49531107263acfa50e6>I’ve seen this too.</a>) They do some other interesting studies: they look at the difference between methanol and water as solvents, argue that Houk is wrong about the role of water in the TS,<sup><a href="#fn4">4</a></sup> and suggest that the hydrophobic effect drives solvent-induced rate acceleration.<sup><a href="#fn5">5</a></sup>
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230720_pes.png style="width:450px;" />
  <figcaption>
  Figure 4B from the paper, showing the change in the PES.
  </figcaption>
</figure>

<p>
The results they find for this particular system are interesting, but more exciting is the promise that these techniques may soon become accessible to “regular” computational chemists. Duarte and co-workers have shown that ML can be used to solve an age-old problem in chemical simulation; if explicit solvent ML/MD simulations of organic reactions become easy enough for non-experts to run, I have no doubt that they will become a valued and essential part of the physical organic chemistry toolbox. Much work is needed to get to that point—new software packages, further validation on new systems, new ways to assess quality and check robustness of simulations, and much more—but the vision behind this paper is powerful, and I can’t wait until it comes to fruition.
</p>

<i>
Thanks to Croix Laconsay for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    <a href=https://www.youtube.com/watch?v=caxpxBT8JsM>This video</a> from Chris Cramer makes the point nicely.
  </li>
  <li id="fn2">
    This obviously depends on the system in question, and what processes are being studied. But in general insufficient sampling is a big issue in molecular dynamics, which I think is underappreciated by organic chemists wading into the area. Jeff Grossman has a <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.0c00833>nice paper on this</a>.
  </li>
  <li id="fn3">
    If you look carefully, many people who claim to be doing big <i>ab initio</i> molecular dynamics studies are actually doing semiempirical molecular dynamics. This isn’t dishonest per se, but it’s a little underwhelming to a computational chemist, especially when it’s only mentioned in the SI. Things get even more confusing when plane wave DFT is employed: in theory, plane wave DFT can be just as accurate as regular DFT, but in practice there are some sneaky approximations that often get introduced.
  </li>
  <li id="fn4">
  This argument hinges on whether uphill dynamics (starting from reactants, going to transition state) or downhill dynamics (starting from transition state, going to reactants) are more appropriate. The authors argue that "uphill dynamics allow the solvent sufficient time to reorganise [<i>sic</i>] before the trajectory passes the free energy barrier, providing a more realistic view of solvent behaviour [<i>sic</i>] during the reaction." I'm not fully convinced by this—isn't the idea that the system reorganizes to minimize the energy of the transition state a basic precept of transition state theory? But I'm not convinced I understand these issues deeply enough to have an opinion; I will leave this to the experts.
  </li>
  <li id="fn5">
    This argument hearkens back to some old-school computational organic chemistry I love from Bill Jorgensen, studying the <a href=https://pubs.aip.org/aip/jcp/article-abstract/77/11/5757/783263/Monte-Carlo-simulation-of-n-butane-in-water?redirectedFrom=fulltext>hydrophobic effect on conformational preferences of butane</a>. We usually think of the hydrophobic effect as associated with macromolecules (ligands binding to proteins, etc), but it can still matter in tiny systems!
  </li>
</ol>
]]></description>
              <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>For-Profit Micro Focused Research Organizations: A Proposal</title>
              <link>public/blog/20230717_fmufros.html</link>
              <description><![CDATA[
<p>
<i>
TW: sarcasm.
</i>
</p>

<p>
Today, most research is done by academic labs funded mainly by the government. Many articles have been written on the shortcomings with academic research: Sam Rodriques recently had a nice <a href=https://www.sam-rodriques.com/post/academia-is-an-educational-institution>post</a> about how academia is ultimately an educational institution, and how this limits the quality of academic research. (It’s worth a read; I’ve written about these issues from a <a href=https://corinwagen.github.io/public/blog/20221026_structural_diversity.html>few</a> <a href=https://corinwagen.github.io/public/blog/20230502_startups.html>angles</a>, and will probably write more at a later date.)
</p>

<p>
The major alternative to academic research that people put forward is <a href=https://www.nature.com/articles/d41586-022-00018-5>focused research organizations</a> (FROs): large, non-profit research organizations capable of tackling big unsolved problems. These organizations, similar in scope and ambition to e.g. CERN or LIGO, are envisioned to operate with a budget of $20–100M over five years, making them substantially larger and more expensive than a single academic lab. This model is still being tested, but it seems likely that some version of FROs will prove effective for appropriately sized problems. 
</p>

<p>
But FROs have some disadvantages, too: they represent a significant investment on the part of funders, and so it’s important to choose projects where there’s a high likelihood of impact in the given area. (In contrast, it’s expected that most new academic labs will focus on high-risk projects, and pivot if things don’t work out in a few years.) In this piece, I propose a new form of scientific organization that combines aspects of both FROs and academic labs: <u>for-profit micro focused research organizations (FPµFROs)</u>.
</p>

<p>
The key insight behind FPµFROs is that existing financial markets could be used to fund scientific research when there is a realistic possibility for profit as a result of the research. This means that FPµFROs need not be funded by the government or philanthropic spending, but could instead raise capital from e.g. venture capitalists or angel investors, who have access to substantially more money and are used to making high-risk, high-reward investments. 
</p>

<p>
FPµFROs would also be smaller and more nimble than full-fledged FROs, able to tackle high-risk problems just like academia. But unlike academic labs, FPµFROs would be able to spend more freely and hire more aggressively, thus circumventing the human capital issues that plague academic research. While most academic labs are staffed entirely with inexperienced trainees (as Rodriques notes above), FPµFROs could hire experienced scientists, engineers, and programmers, thus accelerating the rate of scientific progress.
</p>

<p>
One limitation of the FPµFRO model is that research would need to be profitable within a reasonable time frame. But this limitation might actually be a blessing in disguise: the need for profitability means that FPµFROs would be incentivized to provide real value to firms, thus preventing useless research through the magic of Adam Smith’s invisible hand. 
</p>

<p>
Another disadvantage of FPµFROs is that they must be able to achieve success with relatively little funding (probably around $10M; big for academia, but small compared to a FRO). This means that their projects would have to be modest in scope. I think this is probably a blessing in disguise, though. Consider the following advice <a href=http://www.paulgraham.com/ambitious.html>from Paul Graham</a>:
</p>

<blockquote>
Empirically, the way to do really big things seems to be to start with deceptively small things.… Maybe it's a bad idea to have really big ambitions initially, because the bigger your ambition, the longer it's going to take, and the further you project into the future, the more likely you'll get it wrong.
</blockquote>

<p>
Thus, the need for FPµFROs to focus on getting a single “minimal viable product” right might be very helpful, and could even lead to more impactful firms later on.
</p>

<p>
In conclusion, FPµFROs could combine the best qualities of academic labs and FROs: they would be agile and risk-tolerant, like academic labs, but properly incentivized to produce useful research instead of publishing papers, like FROs. This novel model should be investigated further as a mechanism for generating new scientific discoveries at scale with immediate short-term utility.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Hopefully it’s clear by now that this is a joke: an FPµFRO is just a startup. 
</p>

<p>
The point of this piece isn’t to criticize FROs or academia: both have their unique advantages relative to startups, and much has been written about the relative advantages and disadvantages of different sorts of research institutions (<a href=https://www.convergentresearch.org/about-fros>e.g.</a>).
</p>

<p>
Rather, I want to remind people that startups can do really good scientific work, something that many people seem to forget. It’s true that basic research can be a public good, and something that’s difficult to monetize within a reasonable timeframe. But most research today isn’t quite this basic, which leads me to suspect that many activities today confined to academic labs could be profitably conducted in startups. 
</p>

<p>
Academics are generally very skeptical of organizations motivated by profit. But all incentives are imperfect, and the drive to achieve profitability pushes companies to provide value to real customers, which is more than many academics motivated by publication or prestige ever manage to achieve. It seems likely that for organizations focused on applied research, profit is the least bad incentive. 
</p>

<p>
I’ll close with a quote from Eric Gilliam’s recent <a href=https://www.freaktakes.com/p/an-alternative-approach-to-deep-tech>essay</a> on a new model for “deep tech” startups:
</p>

<blockquote>
Our corporate R&amp;D labs in most industries have taken a step back in how “basic” their research is. Meanwhile, what universities call ‘applied’ research has become much less applied than it used to be. This ‘middle’ of the deep tech pipeline has been hollowed out.
</blockquote>

<p>
What Eric proposes in his piece, and what I’m arguing here, is that scientific startups can help fill this void: not by replacing FROs and academic research, but by complementing them. 
</p>

<i>Thanks to Ari Wagen for reading a draft of this piece.</i>
]]></description>
              <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>pKa and Nonpolar Media</title>
              <link>public/blog/20230710_pka.html</link>
              <description><![CDATA[
<p>
The concept of p<i>K</i><sub>a</sub> is introduced so early in the organic chemistry curriculum that it’s easy to overlook what a remarkable idea it is.
</p>

<p>
<i>
Briefly, for the non-chemists reading this: p</i>K<i><sub>a</sub> is defined as the negative base-10 logarithm of the acidity constant of a given acid H–A:
</i>
</p>

<p>
p<i>K</i><sub>a</sub> := -log<sub>10</sub>([HA]/[A-][H+]) 
</p>

<p>
<i>
Unlike pH, which describes the acidity of a bulk solution, p</i>K<i><sub>a</sub> describes the intrinsic proclivity of a molecule to shed a proton—a given molecule in a given solvent will always have the same p</i>K<i><sub>a</sub>, no matter the pH. This makes p</i>K<i><sub>a</sub> a very useful tool for ranking molecules by their acidity (e.g. <a href=http://ccc.chem.pitt.edu/wipf/MechOMs/evans_pKa_table.pdf>the Evans p</i>K<i><sub>a</sub> table</a>).
</i>
</p>

<p>
The claim implicit in the definition of p<i>K</i><sub>a</sub> is that a single parameter suffices to describe the acidity of each molecule.<sup><a href="#fn1">1</a></sup> In general, this isn’t true in chemistry—there’s no single “reactivity” parameter which describes how reactive a given molecule is. For various regions of chemical space a two-parameter model can <a href=https://www.cup.lmu.de/oc/mayr/reaktionsdatenbank/>work</a>, but in general we don’t expect to be able to evaluate the efficacy of a given reaction by looking up the reactivity values of the reactants and seeing if they’re close enough. 
</p>

<p>
Instead, structure and reactivity interact with each other in complex, high-dimensional ways. A diene will react with an electron-poor alkene and not an alcohol, while acetyl chloride doesn’t react with alkenes but will readily acetylate alcohols, and a free radical might ignore both the alkene and the alcohol and abstract a hydrogen from somewhere else. Making sense of this confusing morass of different behaviors is, on some level, what organic chemistry is all about. The fact that the reactivity of different functional groups depends on reaction conditions is key to most forms of synthesis!
</p>

<p>
But p<i>K</i><sub>a</sub> isn’t so complicated. If I want to know whether acetic acid will protonate pyridine in a given solvent, all I have to do is look up the p<i>K</i><sub>a</sub> values for acetic acid and pyridinium (pyridine’s conjugate acid). If pyridinium has a higher p<i>K</i><sub>a</sub>, protonation will be favored; otherwise, it’ll be disfavored. More generally, one can predict the equilibrium distribution of protons amongst <i>N</i> different sites from a list of the corresponding p<i>K</i><sub>a</sub>s. 
</p>

<p>
Why is p<i>K</i><sub>a</sub> so well-behaved? The key assumption underlying the above definition is that ions are free and do not interact with one another. This allows us to neglect any specific ion–ion interactions, and makes the scale universal: if the pyridinium cation and the acetate anion never interact, then I can learn everything I need to about pyridinium acetate just by measuring the p<i>K</i><sub>a</sub>s of pyridine and acetic acid in isolation. 
</p>

<p>
This assumption is quite good in solvents like water or DMSO, which excel at stabilizing charged species, but progressively breaks down as one travels to the realm of nonpolar solvents. As ions start to pair with themselves, specific molecule–molecule interactions become important. The relative size of the anions can matter: in a nonpolar solvent, a small anion will be better stabilized by a small cation than by a large, diffuse cation, meaning that e.g. acetate will appear more acidic when protonating smaller molecules. Other more quotidian intermolecular interactions, like hydrogen bonding and π-stacking, can also play a role.
</p>

<p>
And the ions aren’t the only thing that can stick together: aggregation of acids is often observed in nonpolar solvents. Benzenesulfonic acid <a href=https://link.springer.com/article/10.2116/analsci.15.303>forms a trimer</a> in benzonitrile solution, which is still pretty polar, and alcohols and carboxylic acids are known to aggregate under a variety of conditions as well.<sup><a href="#fn2">2</a></sup> Even seemingly innocuous species like tetrabutylammonium chloride will aggregate at high concentrations (<a href=https://pubs.acs.org/doi/pdf/10.1021/ja9723139>ref</a>, <a href=https://pubs.rsc.org/en/content/articlelanding/1995/c3/c39950002513>ref</a>).
</p>

<p>
To reliably extend p<i>K</i><sub>a</sub> scales to nonpolar solvents, one must thus deliberately choose compounds which resist aggregation. As the dielectric constant drops, so does the number of such compounds. The clearest demonstration of this I’ve found is a series of papers (<a href=https://pubs.acs.org/doi/10.1021/jo9713013>1</a>, <a href=https://pubs.acs.org/doi/10.1021/jo0343477>2</a>) by p<i>K</i><sub>a</sub> guru Ivo Leito measuring the acidity of a series of fluorinated compounds in heptane:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230710_leito.png style="width:350px;" />
  <figcaption>
  A portion of the scale, showing the compounds employed.
  </figcaption>
</figure>

<p>
This effort, while heroic, demonstrates the futility of measuring p<i>K</i><sub>a</sub> in nonpolar media from the standpoint of the synthetic chemist. If only weird fluoroalkanes engineered not to aggregate can have p<i>K</i><sub>a</sub> values, then the scale may be analytically robust, but it’s hardly useful for designing reactions! 
</p>

<p>
The key point here is that the difficulty of measuring p<i>K</i><sub>a</sub> in nonpolar media is not an analytical barrier which can be surmounted by new and improved technologies, but rather a fundamental breakdown in the idea of p<i>K</i><sub>a</sub> itself. Even the best p<i>K</i><sub>a</sub> measurement tool in the world can’t determine the p<i>K</i><sub>a</sub> of HCl in hexanes, because no such value exists—the concept itself is borderline nonsensical. Chloride will ion-pair with everything in hexanes, hydrogen chloride will aggregate with itself, chloride will stick to hydrogen chloride, and so forth. Asking for a p<i>K</i><sub>a</sub> in this context just doesn't make much sense.<sup><a href="#fn3">3</a></sup>
</p>

<p>
It’s important to remember, however, that just because the p<i>K</i><sub>a</sub> scale no longer functions in nonpolar solvents doesn’t mean that acids don’t have different acidities. Triflic acid in toluene will still protonate just about <a href=https://pubs.acs.org/doi/abs/10.1021/ol061174+>everything</a>, whereas acetic acid will not. Instead, chemists wishing to think about acidity in nonpolar media have to accept that no one-dimensional scale will be forthcoming. The idealized world of p<i>K</i><sub>a</sub> we’re accustomed to may no longer function in nonpolar solvents, but chemistry itself still works just fine.
</p>

<i>
Thanks to Ivo Leito for discussing these topics with me over Zoom, and to Joe Gair for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    Really, each proton.
  </li>
  <li id="fn2">
   Ivo Leito called carboxylic acids "the world champions of aggregation" when I asked him about these issues.
  </li>
  <li id="fn3">
    Even experimentally nonsensical p<i>K</i><sub>a</sub>s can be simulated, though: Jorgensen <a href=https://pubs.acs.org/doi/10.1021/ja00256a053>famously</a> used free energy perturbation to compute the p<i>K</i><sub>a</sub> of ethane in water.
  </li>
</ol>
]]></description>
              <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Opinionated Advice for Incoming Graduate Students</title>
              <link>public/blog/20230703_advice.html</link>
              <description><![CDATA[
<ol>
<li>
You are a scientist, not a lab monkey. You ought not to view your degree as “six years of hard labor in the chemistry mines.” Always make time to go to interesting seminars, talk with other people about their research, and read the literature. Otherwise, what’s the point of being a scientist?
</li>
<li>
Only one person is really looking out for your best interests: you. Your advisor, your classmates, and your collaborators all have their own distinct incentives and interests, which will often roughly align with yours, but will never align perfectly.
</li>
<li>
Your research interests will also not match up perfectly with those of your advisor. Your job as a graduate student is to find the intersection between what the two of you care about, and work there. Otherwise, one of you will be unhappy.
</li>
<li>
The more non-chemists in your life, the better your mental health will be. (h/t <a href=https://www.theatlantic.com/family/archive/2021/04/deep-friendships-aristotle/618529/>Arthur Brooks</a>)
</li>
<li>
Try to maximize the ratio of “thinking”/publishable work to mindless SI work. Any project will take some grinding, but the thinking work is (ideally) what you’ve been recruited for, what you’ll present on, and what’ll get you a job. If your advisor views you only as a set of hands… be very worried.
</li>
<li>
Cold emails to scientists work much better than it seems they ought to. Most professors spend their entire career trying to get people to care about their work—if you’re interested, they’ll usually talk your ear off.
</li>
<li>
Read your PI’s old papers, as many as possible. Too often students are totally ignorant of the work that occurred a decade before they joined the lab, and end up repeating it or falling into the same pitfalls time and time again.
</li>
<li>
Learn to code; please stop performing curve fits in Excel. (cf. <a href=https://a16z.com/2011/08/20/why-software-is-eating-the-world/>pmarca</a>)
</li>
<li>
Be outcome focused. Each day, ask yourself “What is the biggest problem I’m facing in my research?” If whatever you’re doing isn’t addressing that problem, you’re wasting your time. Sometimes this means stopping all experiments and reading papers for a few weeks; sometimes this looks like running reactions; sometimes this looks like just sitting at your desk and writing. (h/t Brian Liau for giving me this advice when I started graduate school)
</li>
<li>
Be outcome focused on the big scale, too. Figure out what success in graduate school looks like—what your ideal job is, and what it takes to get that sort of job—and then pursue those outcomes relentlessly. Perhaps in an ideal world we could all follow our natural curiosity to our heart’s content, but that’s not real life, not in science as I’ve known it.
</li>
<li>
Success in graduate school may be necessary for your life goals, but it won’t be sufficient. At the end of the day, science is just a job, and your molecules will never love you. So don’t work so hard that you put the rest of your life on hold; don’t make science the highest good in your worldview. (cf. <a href=https://www3.dbu.edu/naugle/pdf/disordered_love.pdf>Augustine</a>)
</li>
</ol>

<i>
Thanks to Joe Gair for reading a draft of this piece.
</i>
]]></description>
              <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Peer Review, Imperfect Feedback</title>
              <link>public/blog/20230626_feedback.html</link>
              <description><![CDATA[
<p>
I’ve been pretty critical of peer review in the past, arguing that it <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>doesn’t accomplish much</a>, <a href=https://corinwagen.github.io/public/blog/20230320_domain_arbitrage.html>contributes to status quo bias</a>, etc. But a few recent experiences remind me of the value that peer review provides: <u>in today’s scientific culture, peer review is essentially the only time that scientists get honest and unbiased feedback on their work.</u> 
</p>

<p>
How can this be true? In experimental science, scientists typically work alongside other students and postdocs under the supervision of a professor. This body of people forms a lab, also known as a research group, and it’s to these people that you present most frequently. Your lab generally knows the techniques and methods that you employ very well: so if you’ve misinterpreted a piece of data or designed an experiment poorly, group meeting is a great place to get feedback. 
</p>

<p>
But a lab is also biased in certain ways. People are attracted to a lab because they think the science is exciting and shows promise, and so they’re likely to be credulous about positive results. Certain labs also develop beliefs or dogmas about how to conduct science: the best ways to perform a mechanistic study, or the most useful reaction conditions. To some extent, every lab is a paradigm unto itself. This means that paradigm-shifting criticism is hard to find among one’s coworkers, even if it’s common in the outside world.
<p>

<p>
Here are some examples of controversial-in-the-field statements that are unlikely to be controversial within given labs:
</p>
<ul>
<li>
Palladium-based catalysis is going to become obsolete due to the scarcity of Pd and must be replaced. <a href=https://www.dal.ca/sites/stradiotto/research.html>Some labs</a> build their research program around this; others think Pd will always be relevant.
</li>
<li>
Water is the greenest possible solvent. Some scientists believe this <a href=https://lipshutz.chem.ucsb.edu/research>wholeheartedly</a>, while others think it’s stupid (for instance, a 2021 <a href=https://link.springer.com/article/10.1007/s10098-021-02188-8>review</a> states that “to meet regulations concerning the discharge of waste water into rivers and other natural waters… [water] often requires very extensive treatment prior to discharge, making the use of water as a reaction medium much less attractive.”)
</li>
<li>
More broadly, the belief that machine learning and generative AI are the future of chemical discovery: while many computational chemists believe this, plenty of experimentalists (even young ones) are pretty skeptical.
</li>
</ul>

<p>
In each of these cases, it’s unlikely that criticism along these lines is available internally: people who’ve chosen to do their PhDs studying ML in chemistry aren’t likely to criticize your paper for overemphasizing the importance of ML in chemistry!
</p>

<p>
More generally, internal criticism works best when a lab serves as a shared repository of expertise, i.e. when everyone in the lab has roughly the same skillset. Some labs focus instead on a single overarching goal and employ many different tools to get to that point: a given chemical biology group might have a synthetic chemist, a MS specialist, a genomics guru, a mechanistic enzymologist, and someone specializing in cell culture. If this is the case, your techniques are opaque to your coworkers: what advice can someone who does cell culture give about improving Q-TOF signal-to-noise?
</p>

<p>
Ideally, one’s professor is well-versed enough in each of the techniques employed that he or she can dispense criticism as needed. But professors are often busy, aren’t always operational experts at each of the techniques they oversee, and suffer from the same viewpoint biases that their students do (perhaps even more so).
</p>

<p>
So, it’s important to solicit feedback from external sources. Unfortunately, at least in my experience most external feedback is too positive: “great talk,” “nice job,” etc. Our scientific culture tries so hard to be supportive that I almost never get any meaningful criticism from people outside my group, either publicly or privately. (Ideally one’s committee would help, but I never really got to present research results to my committee, and this doesn’t help postdocs anyhow.) 
</p>

<p>
Peer review, then, serves as the last bastion against low-quality science: reviewers are outside the lab, have no incentive to be nice, and are tasked specifically with poking holes in your argument or pointing out extra experiments that would improve it. Peer review has improved each one of my papers, and I’m grateful for it.<sup><a href="fn1">1</a></sup>
</p>

<p>
What’s a little sad is that the excellent feedback that reviewers give only comes at the bitter end of a project, which for me has often meant that the results are more than a year old and my collaborators have moved on. Much more useful would be critical feedback delivered early on in a project, when my own thinking is more flexible and the barrier to running additional experiments is lower. And more useful still would be high-quality criticism available at every step of the project, given not anonymously but by people whom you can talk to and learn from.
</p>

<p>
What might this practically look like? 
</p>
<ul>
<li>
A culture of “red teaming,” where students are incentivized to find flaws in others’ projects in somewhat adversarial ways. This would need to be done within a supportive and collegial atmosphere, lest it degenerate into bullying: red teaming need not be red in tooth and claw.
</li>
<li>
Similarly, PIs could invite other professors to come to group meetings and (constructively) criticize the projects, particularly professors from adjacent subfields who might have different perspectives.
</li>
<li>
Poster presentations or talks (at conferences), although often used to present finished projects, can also be used to present unfinished work. I presented some unfinished work at <a href=http://jiwu.chem.uh.edu/tpoc.html>TPOC</a> a few years ago, and got really helpful suggestions from Ken Houk and Dean Tantillo that fundamentally changed how we approached the rest of the project. Maybe this is something that we should encourage more, although finished projects will probably always look more impressive than unfinished projects.
</li>
<li>
Decentralized peer review solutions like <a href=https://www.theseedsofscience.org/><i>Seeds of Science</i></a> or <a href=https://pubpeer.com/><i>PubPeer</i></a> might also help here, but my sense is that it’s unlikely that qualified experts will just spend their time investigating something that they found online; they need to be solicited somehow.
</li>
</ul>

<p>
I don’t know what the right solution looks like here: the burden of peer review is already substantial, and I don’t mean to suggest that this work ought to be arbitrarily multiplied for free. But I do worry that eliminating peer review, absent other changes, would simply mean that one of the only meaningful chances to get unfiltered feedback on one’s science would be eliminated, and that this would be bad.
<p>

<i>
Thanks to Croix Laconsay and Lucas Karas for helpful feedback on this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  It's also true that the threat of peer review increases paper quality. I agree with <a href=https://twitter.com/dasingleton/status/1528877848093954048?s=20>Singleton</a> that this is important today, but am less convinced that this is necessary from an institutional design perspective: if peer review didn't exist, I think some other system of norms or regulations would spring forth to protect good-quality science. See the "Anarchic Preprint Lake" discussion in <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>my piece on journals</a>. (h/t Croix Laconsay for raising this point)
  </li>
</ol>

]]></description>
              <pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>A Farewell to Lab</title>
              <link>public/blog/20230622_farewell.html</link>
              <description><![CDATA[
<p>
I first encountered organic chemistry on Wikipedia, my freshman year of high school. The complexity and arcanity of the field instantly attracted me: here was something interesting that I didn’t know about and which didn’t require years of mathematical training to approach (unlike most of physics).
</p>

<p>
I soon started reading about organic chemistry more and more, albeit with no rhyme or reason to my study. I didn’t know what the good textbooks were, what order to study things in or which concepts ought to be understood in depth before progressing further. Organic chemistry was just a “<a href=https://en.wikipedia.org/wiki/The_Glass_Bead_Game>glass bead game</a>” to me, an art of symbols devoid of any real-world representations. But enthusiasm can sometimes suffice where wisdom is lacking.
</p>

<p>
With the support of a teacher and a half-dozen friends who also wanted to learn more organic chemistry, we started a little independent study. We met in a closet and read a textbook, worked through the problems, and our teacher wrote us tests. We eventually managed to get through all of Paula Bruice, although various misconceptions (and mispronunciations) stayed with me until I took Movassaghi’s course at MIT.
</p>


<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_iodolact.jpg style="width:500px;" />
  <figcaption>
    Here’s a picture of some work from high school, which does not in fact make very much sense. (I think this is from the Evans problem set library.)
  </figcaption>
</figure>

<p>
But in high school we had no lab, and thus no practical knowledge. We split into two groups and tried to come up with experiments for ourselves, but the results were dismal. Here’s the procedure (copied verbatim) for the one and only reaction we ran, synthesis of nitrobenzene via nitration of benzene, which was to be the first step in a multistep synthesis of Kevlar:
</p>

<blockquote>
In a 250 mL beaker, dissolve benzene in a solution of concentrated H<sub>2</sub>SO<sub>4</sub> of twice its volume. Use an ice-salt bath to bring this solution to 0 °C or below, and use a Pasteur pipette to slowly add a 1:1 solution of HNO<sub>3</sub> and H<sub>2</sub>SO<sub>4</sub> (be sure to keep the solution below 15 degrees Celsius at all times, as the reaction is strongly exothermic). Once all the solution has been added, warm it to room temperature and allow it to sit for 15 minutes.
<br><br>
Pour the solution over 50 g of crushed ice in a 250 mL beaker. Once the ice has melted, isolate the product via vacuum filtration via a Buchner funnel and rinse it twice with water and twice with methanol. Recrystallize in a solution of methanol.
</blockquote>

<p>
The astute observer will notice that there aren’t very many details here. How much benzene? How much nitric acid? We didn’t have the glassware mentioned above—neither a beaker nor a Buchner funnel. And, perhaps most damningly, the procedure calls for isolation by filtration, challenging since nitrobenzene is a liquid at room temperature.
</p>

<p>
Despite these problems, we successfully ran this reaction (open to air, not in a fume hood), and obtained the product. I vividly recall the yellow bubbles of nitrobenzene floating to the top of the vial, and the smell of cherries that filled the room, a smell that returns to me in Proustian fashion from time to time when using certain reagents. We didn’t have a separatory funnel (or we didn’t know how to use it if we did), so we fished some of the nitrobenzene out with a Pasteur pipette and threw the rest away.
</p>

<p>
A little bit of knowledge would have served us well, as would have gloves and a fume hood. Here’s <a href=https://en.wikipedia.org/wiki/Nitrobenzene>Wikipedia</a>:
</p>

<blockquote>
Prolonged exposure [to nitrobenzene] may cause serious damage to the central nervous system, impair vision, cause liver or kidney damage, anemia and lung irritation. Inhalation of vapors may induce headache, nausea, fatigue, dizziness, cyanosis, weakness in the arms and legs, and in rare cases may be fatal…
<br><br>
Nitrobenzene is considered a likely human carcinogen by the United States Environmental Protection Agency, and is classified by the IARC as a Group 2B carcinogen which is "possibly carcinogenic to humans".
</blockquote>

<p>
Indeed, a coworker of mine would later be sent to the hospital after spilling nitrobenzene on herself. Surprisingly, my group’s experiment still ended up being the safer one—the lab portion of our course was disbanded the following day after my classmates caused an explosion with thionyl chloride.
</p>

<p>
I was fortunate enough to land a summer research internship in the Sessler lab at UT the following summer, and I started studying chemistry in earnest: column chromatography, Anslyn/Dougherty, NMR spectroscopy, and all the rest. I remember the first reaction I ran at UT: retro-Friedel–Crafts dealkylation of a calix[4]arene, using about 10 g each of phenol and aluminum(III) chloride. The brutal physicality of lab work was a nice contrast to the gnosticism of software (where I’d worked previously), and I was hooked.
</p>

<p>
I’ll fast-forward through the more recent parts of my chemical career: I went to MIT and joined the Imperiali lab to work on essentially a medicinal chemistry project: hit-to-lead optimization, featuring lots of de novo heterocycle synthesis. I got to cook reactions in molten urea, quench 500 mL of phosphorus oxychloride at a time, and even design new routes (with a little oversight from my postdoc). It was awesome.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_imperiali.jpg style="width:500px;" />
  <figcaption>
    My hood in the Imperiali Group. Although this hood appears to have a Schlenk line, do not be fooled—it’s purely decorative.
  </figcaption>
</figure>

<p>
After three semesters, I got tired of my cross couplings mysteriously failing and joined the Buchwald lab, where I learned how to do chemistry more carefully: handling air-sensitive materials with a Schlenk line or in the glovebox, not “as fast as possible.” My tenure in the Buchwald lab also introduced me to the importance of computations, which became a key part of my doctoral work, particularly when we were sent home in March of my first year. COVID gave me the opportunity to pursue some software engineering projects that I wouldn’t have had time to work on otherwise (like <i><a href=https://github.com/ekwan/cctk>cctk</a></i> and <i><a href=https://github.com/corinwagen/presto>presto</a></i>), and simulation started to take up more and more of my time and intellectual bandwidth.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_benzyne.jpg style="width:500px;" />
  <figcaption>
    A large-scale biaryl synthesis proceeding through a benzyne intermediate, from my time in the Buchwald Group.
  </figcaption>
</figure>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230622_hpc.jpg style="width:400px;" />
  <figcaption>
    One of my first Gaussian jobs ever. I didn't get cluster access for a little while, so I used to run Gaussian on the printer computers in my dorm's basement and just hope that nobody needed the computer overnight.
  </figcaption>
</figure>

<p>
I defended my dissertation on June 5th, and cleaned out my hood last week. For the foreseeable future, I’ll be a purely computational chemist—computation is advancing quickly, and I think that’s where I have the most to offer the field right now. But I’ll miss the sights and sounds of the lab. There’s a satisfaction to making a new molecule and holding the final product in your hands: the knowledge that you’ve reshaped this little corner of reality through your own actions, and that this particular arrangement of matter has never existed before.
</p>

<p>
And simulation is, at the end of the day, only useful insofar as it helps us make real molecules. There may be people who wish to model reactions purely for the sake of modeling them, but I am not one of them. What drew me to simulation initially, and what still attracts me to the field today, is its potential to help experimental scientists do their work faster and better. It was easy for me to ensure that this was true when I was both doing the experiments and running the simulations; my incentives were aligned properly. It will be harder in the future.
</p>

<p>
There’s a seductive appeal in leaving lab work behind altogether, too, and one that’s dangerous. Any experimentalist who’s worked with a computational collaborator knows that nothing ever works precisely as modeled. There are untold depths of chemical behavior still inaccessible to the idealized world of simulation, and it’s all too easy for computational chemists just to look the other way. Life is easier when you don’t have to deal with sludgy workups or poorly soluble intermediates, but they don’t go away just because you can’t model them by DFT—reality has a way of keeping us honest that simulations frequently lack.
</p>

<p>
So, although I bid lab farewell at this point in my career, it’s a bittersweet parting. I hope to return someday; only time will tell.
</p>

<i>Thanks to Jacob Thackston for reading a draft of this piece.</i>
]]></description>
              <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>A Year In The Blog</title>
              <link>public/blog/20230620_year.html</link>
              <description><![CDATA[
<p>
I started this blog one year ago today, with a post on site-selective glycosylation. According to Google Analytics, there have been 24,035 views since then.
</p>

<p>
What have the top posts been? 
</p>

<ol>
<li>
<a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>Against Carbon NMR</a> (3201 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220928_talent.html>Book Review: Talent</a> (1588 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20230616_varda.html>Crystallization in Microgravity</a> (1467 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20230203_poy.html>2022 Paper(s) of the Year</a> (721 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220810_viewpoints_on_simulation.html>Combating Computational Nihilism</a> (694 views)
</li>
</ol>

<p>
The only of these that really surprises me is #5: the <sup>13</sup>C NMR post made a lot of organic chemists really angry, the <i>Talent</i> review was reposted on <i>Marginal Revolution</i>, and Delian Asparouhov (CEO of Varda) retweeted my post about Varda’s crystallization ideas. And everyone loves to share a ranking of the year’s papers, especially when their own work is highlighted. 
</p>

<p>
The least-viewed posts?
</p>

<ol>
<li>
<a href=https://corinwagen.github.io/public/blog/20230413_new_ways.html>New Ways To Read The Blog: RSS and Substack</a> (26 views) 
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221128_business_card_explained.html>Business Card Lennard–Jones Simulation, Explained</a> (32 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20220620_glycosylation.html>Site-Selective Glycosylation: Reflections</a> (34 views; my first post)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221121_business_card_lennard_jones.html>Business Card Lennard–Jones Simulation</a> (43 views)
</li>
<li>
<a href=https://corinwagen.github.io/public/blog/20221231_books.html>Books from 2022</a> (45 views)
</li>
</ol>

<p>
Twitter downranked the Substack post pretty heavily, so it’s not surprising that nobody saw it. The Lennard–Jones posts are more unexpected. Whenever I write about anything computational or coding-related, it seems to attract much less engagement, which is perhaps a reflection of the fact that most of my followers are experimental chemists who don’t really care about obfuscated C code. 
</p>

<p>
A year in, writing blog posts has gotten much easier. The following advice from <a href=https://guzey.com/personal/why-have-a-blog/#writing-helps-you-think-better>Alexey Guzey</a> didn’t seem true when I started, but it does seem true now:
</p>

<blockquote>
Writing not only helps you to understand what’s going on and to crystallize your thoughts, it actually makes you think of new ideas and come up with solutions to your problems.
</blockquote>

<p>
I’ve fallen into a 1x/week update schedule, which seems to work pretty well: enough to keep the routine up, but not so much that it’s a serious distraction from my actual job. I hope to maintain this schedule for the foreseeable future, and recommend it to other bivocational bloggers.
</p>

<p>
Anyhow, thanks for reading!
</p>

<p>
(Also, today in off-blog content: I appeared on my first podcast, <i>Forbidden Conversations</i>, hosted by Harry Wetherall. We talk about why people don’t have kids earlier, how I reconcile being a Christian with being a scientist, the concept of “cope,” and more: you can check it out on <a href=https://podcasts.apple.com/us/podcast/forbidden-conversations/id1676705756>Apple Podcasts</a> or <a href=https://open.spotify.com/episode/38AK3AdSf52gwNAkyA9Lpv>Spotify</a>.)
</p>




]]></description>
              <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Crystallization in Microgravity</title>
              <link>public/blog/20230616_varda.html</link>
              <description><![CDATA[
<p>
<i>Not Boring</i> recently published a <a href=https://www.notboring.co/p/varda-the-space-drug-factory>panegyric</a> about <a href=https://varda.com/>Varda</a>, a startup that’s trying to create “space factories making drugs in orbit.” When I first read this description, alarm bells went off in my head—why would anyone try to make drugs in space? Nevertheless, there’s more to this idea than I initially thought. In this piece, I want to dig a little deeper into the chemistry behind Varda, and discuss some potential advantages and challenges of the approach they’re exploring.
</p>

<p>
Much of my confusion was quickly resolved by realizing that Varda is not actually “making drugs in orbit,” or not in the way that an organic chemist would interpret that sentence. Varda’s proposal is actually much more specific: they aim to crystallize active pharmaceutical ingredients (APIs, i.e. finished drug molecules) in microgravity, allowing them to access crystal forms and particle size distributions which can’t be made under terrestrial conditions. To quote from <a href=https://varda.com/wp-content/uploads/varda-info-sheet-mar23_pharmaceutical.pdf>their website</a>:
</p>

<blockquote>
Varda’s microgravity platform is grounded in decades of proven research conducted on NASA’s space stations. By eliminating factors such as natural convection and sedimentation, processing in a microgravity environment provides a unique path to formulating small molecules and biologics that traditional manufacturing processes cannot address. The resulting tunable particle size distributions, more ordered crystals, and new forms can lead to improved bioavailability, extended shelf-life, new intellectual property, and new routes of administration. 
</blockquote>

<p>
Crystallization is an excellent target for a new and expensive manufacturing process because it’s at once very important and very hard to control. The goal of crystallization is to grow crystals of a given compound, fitting the component molecules together into a perfect lattice that excludes impurities and can be easily handled. To the best of my knowledge, almost every small-molecule API is crystallized at one stage or another; it’s the best way to ensure that the material is extremely pure. 
</p>

<p>
(Crystallizing proteins for administration is less common, since proteins are really difficult to crystallize, but it’s not unheard of—insulin is often administered subcutaneously as a <a href=https://onlinelibrary.wiley.com/doi/10.1002/prot.22213>microcrystalline suspension</a>, which allows higher concentrations to be accessed without excessive viscosity.)
</p>

<p>
But crystallization is also something we can’t really control. We can’t physically put molecules into a lattice or force them to adopt ordered configurations; all we can do is dissolve them in some mixture, tweak the conditions a little bit, and hope that crystals form. Thus, finding good crystallization conditions basically amounts to randomly screening solvents and additives, leaving the solutions for a long time, and checking to see if crystals grow. In the words of <a href=https://www.science.org/content/blog-post/voodoo-nominations>Derek Lowe</a>:
</p>

<blockquote>
I'd like to open up the floor for nominations for the Blackest Art in All of Chemistry. And my candidate is a strong, strong contender: crystallization. When you go into a protein crystallography lab and see stack after stack after stack of plastic trays, each containing scores of different little wells, each with a slight variation on the conditions, you realize that you're looking at something that we just don't understand very well.
</blockquote>

<p>
Why does microgravity matter for crystallization? <i>Not Boring</i> says that crystallization occurs “at the mesoscopic scale, the length scale on which objects are larger than nanoscale (on the order of atoms and molecules) but still small enough that quantum mechanical or other non-trivial ‘microscopic’ behavior becomes apparent.” I found this answer a little confusing—doesn’t crystallization begin on the nanoscale and end on the macroscopic scale? 
</p>

<p>
Clearer to me was the explanation from a 2001 <a href=https://pubs.acs.org/doi/10.1021/cg005511b>review</a> by Kundrot and co-workers:
</p>

<blockquote>
In zero gravity, a crystal is subject to Brownian motion as on the ground, but unlike the ground case, there is no acceleration inducing it to sediment <i>[fall out of solution]</i>. A growing crystal in zero gravity will move very little with respect to the surrounding fluid. Moreover, as growth units leave solution and are added to the crystal, a region of solution depleted in protein is formed. Usually this solution has a lower density than the bulk solution and will convect upward in a 1g field as seen by Schlerien photography (Figure 1). In zero gravity, the bouyant <i>[sic]</i> force is eliminated and no bouyancy-driven convection occurs. Because the positions of the crystal and its depletion zone are stable, the crystal can grow under conditions where its growing surface is in contact with a solution that is only slightly supersaturated. In contrast, the sedimentation and convection that occur under 1g place the growing crystal surface in contact with bulk solution that is typically several times supersaturated. Lower supersaturation at the growing crystal surface allows more high-energy misincorporated growth units to disassociate from the crystal before becoming trapped in the crystal by the addition of other growth units… 
<br><br>
In short, promotion of a stable depletion zone in microgravity is postulated to provide a better ordered crystal lattice and benefit the crystal growth process.
</blockquote>

<p>
To summarize, microgravity serves to immobilize the crystal with respect to the surrounding solution, preventing convection or sedimentation from bringing highly concentrated solutions into contact with the crystal. This slows crystal growth, which might sound bad but is actually really good: in general, the slower a crystal grows, the higher its purity. (See also <a href=https://www.nature.com/articles/npjmgrav201510>this 2015 article</a> for further discussion.)
</p>

<p>
What practical impact does this have? In most cases, crystals grown in space are better than their terrestrial congeners by <a href=https://pubs.acs.org/doi/10.1021/acs.cgd.2c01056>a variety of metrics</a>: larger, structurally better, and more uniform. To quote from the <i>Not Boring</i> piece:
</p>

<blockquote>
Doing crystallization in space is like adding a gravity knob to your instrument—it opens up regions of process design space that would otherwise be inaccessible. Importantly, after the crystallization occurs in space, the drug retains its solid state upon re-entry. Manufacture in space; use on earth. 
<br><br>
This is why pharma is going to space to experiment with a wide range of medicines. Formulations made in microgravity could open the door to improvements in drug shelf life, bioavailability, IP expansion, and even better approaches to drug delivery…
<br><br>
To date, there has been a major disconnect between microgravity research and manufacturing. While it’s been possible to hitch a ride to the ISS and collaborate with NASA on PCG experiments, there is no existing commercial offering to actually manufacture drugs in space. Merck used their research results on Keytruda® crystallization to tinker with their terrestrial approaches to formulation. What if they could actually just manufacture the crystals they discovered in microgravity at commercial scale?
<br><br>
This is Varda’s mission—to make widespread research and manufacturing in microgravity a reality.
</blockquote>

<p>
One concern I have is that to date, the vast majority of space-based crystallization has been aimed at structural biology (elucidating the structure of a protein via crystallography), which only takes one crystal, one time. What Varda is aiming to do is preparative crystallography: crystallizing proteins and small molecules to isolate large quantities of them. Both processes obviously involve growing crystals, but otherwise they’re pretty different: in structural biology, all you care about is isolating a single large and very pure crystal, while uniformity and reproducibility are paramount in preparative crystallography. 
</p>

<p>
There’s some precedent for preparative protein crystallization in microgravity: a 1996 Schering-Plough paper studied crystallization of zinc interferon-α2b on the Space Shuttle. The results are excellent: over 95% of the protein crystallized, and the resulting suspension showed good stability and improved serum half-life in <i>Cynomolgus</i> monkeys (<a href=https://pubs.aip.org/aip/acp/article-abstract/361/1/139/609196/Macroscale-production-and-analysis-of-crystalline?redirectedFrom=fulltext>ref</a>, <a href=https://pubs.aip.org/aip/acp/article-abstract/387/1/671/811857/Protein-crystal-growth-in-microgravity-review-of?redirectedFrom=fulltext>ref</a>). The difference in crystal quality is huge:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230616_interferon.png style="width:450px;" />
  <figcaption>
  Space-grown crystals are much, much larger than crystals grown on Earth.
  </figcaption>
</figure>

<p>
More recently, scientists from Merck found that crystallization of pembrolizumab (Keytruda) in microgravity reproducibly formed a monomodal particle size distribution, as opposed to the bimodal particle size distribution formed under conventional conditions (<a href=https://www.nature.com/articles/s41526-019-0090-3>ref</a>), although the crystals didn’t seem any larger:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230616_keytruda.png style="width:450px;" />
  <figcaption>
  Comparison of pembrolizumab crystal size distribution under different conditions.
  </figcaption>
</figure>

<p>
Of course, until recently access to space was very limited and there wasn’t much reason to study preparative protein crystallography in microgravity, so the lack of studies is hardly surprising. For all the reasons discussed above, it seems very likely that preparative crystallography will be generally better in microgravity, and that the resulting crystals will be more homogenous and more pure than the crystals you could grow on Earth. But it’s not 100% certain, and that’s something Varda will have to establish. The word “can” is doing a lot of work in this text from their website:
</p>

<blockquote>
By eliminating factors such as natural convection and sedimentation, processing in a microgravity environment provides a unique path to formulating small molecules and biologics that traditional manufacturing processes cannot address. The resulting tunable particle size distributions, more ordered crystals, and new forms <b>can</b> lead to improved bioavailability, extended shelf-life, new intellectual property, and new routes of administration <i>(emphasis added)</i>
</blockquote>

<p>
Another concern is that little work (that I’m aware of) has been done on small molecule crystallization in microgravity—the very task Varda intends to start with. Small molecules, in general, are much easier to crystallize than proteins, and there are more parameters for the experimental scientist to tune. While there are certainly cases where small molecule crystals can display unexpected or problematic behaviors (like the famous case of <a href=https://link.springer.com/article/10.1023/A:1011052932607>ritonavir</a>, the molecule Varda is investigating first), in general crystallization of small molecules seems like an easier problem, and one for which there are better state-of-the-art workarounds.
</p>

<p>
The most likely failure mode to me, though, is just that microgravity crystallization is better than crystallization on Earth, but not that much better. Shooting APIs into space, waiting for them to crystallize, and then launching them back to Earth is going to be really expensive, and Varda will have to demonstrate extraordinary results to justify the added hassle—particularly for a technique that they hope to make a key part of the pharmaceutical manufacturing process. Talk about supply chain risk! (Is this all going to be <a href=https://www.fda.gov/drugs/pharmaceutical-quality-resources/facts-about-current-good-manufacturing-practices-cgmp>GMP</a>?)
</p>

<p>
And it’s worth pointing out a fairly obvious consideration too: what Varda is proposing to do in space is only one part of a vast, multi-step operation. No synthesis will take place in space, so all the manufacturing of either proteins or small molecules will take place on Earth. The purified products will then be launched into space, crystallized, and then the crystal-containing solution will undergo reentry and then be formulated into whatever final form the drug needs to be administered in.
</p>

<p>
So, although “there are a number of small molecules that fetch hundreds of thousands or millions of dollars per kg” (<i>Not Boring</i>), Varda can address only a small—albeit important—part of the manufacturing process. I doubt this is likely to change. On average, it takes 100–1000 kg of raw materials to manufacture 1 kg of a small molecule drug (<a href=https://pubs.acs.org/doi/10.1021/acs.oprd.1c00477>ref</a>), so shipping everything to orbit would massively raise costs, without any obvious advantages that I can think of. The TAM for Varda might be large enough to break even, but it’s not going to replace conventional pharmaceutical manufacturing anytime soon.
</p>

<p>
Varda’s pitch is perfect for venture capital: ambitious, risky, and potentially game-changing if it succeeds. And I wish them luck in their quest, since new and better ways to approach formulation would be awesome. But I can’t shake the nagging doubt that they’re so excited about the image of space-based manufacturing that they’re trying to invent a problem that their aerospace engineers have a solution for. We’ll find out soon enough if they’ve succeeded.
</p>

]]></description>
              <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Seven Degrees of Screening for Generality</title>
              <link>public/blog/20230607_degrees.html</link>
              <description><![CDATA[
<p>
<i>
(with apologies to <a href=https://www.chabad.org/library/article_cdo/aid/45907/jewish/Eight-Levels-of-Charity.htm>Maimonides</a> and <a href=https://rintintin.colorado.edu/~vancecd/phil215/Nozick.pdf>Nozick</a>)
</i>
</p>

<ol type="I">
<li>
Screening on only one substrate before assessing the substrate scope. This is the “ordinary means” in methods development. 
</li>

<li>
Screening on one substrate, but choosing a substrate that worked poorly in a previous study (<a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.201705525>e.g.</a>). This can be thought of as serial multi-substrate screening, where each substrate is a separate project, but the body of work achieves greater generality over time.
</li>

<li>
Screening on one substrate at a time, but rescreening catalysts when you find problematic substrates (<a href=https://www.nature.com/articles/s41557-022-00895-3>e.g.</a>). This amounts to serial multi-substrate screening within a single project.
</li>

<li>
Intentionally choosing a variety of catalysts up front and screening this set of catalysts for each new substrate class (<a href=https://pubs.acs.org/doi/10.1021/jacs.3c03182>e.g.</a>), thus achieving a high degree of generality with a family of catalysts, but without attempting to systematically quantify the generality of each catalyst in this set.
</li>

<li>
Choosing a handful of model substrates instead of just one, but otherwise doing everything the same as one would normally (<a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c06139>e.g.</a>, pages S24–S29).
</li>

<li>
Intentionally choosing a large, diverse panel of substrates and screening against this panel to quantify catalyst generality over a given region of chemical space. This is essentially what <a href=https://www.nature.com/articles/s41586-022-05263-2>we</a> and <a href=https://www.science.org/doi/10.1126/science.adf6177>the Miller group</a> did recently (and others, etc). 
</li>

<li>
The same, but incorporating robotics, fancy analytical methods, generative AI, or whatever else.
</li>
</ol>

<p>
When I present the “screening for generality” work, I often get the response “this is cool, but my reaction doesn’t work in 96-well plates/I don’t have an SFC-MS/my substrates are hard to make.” The point of this taxonomy is to illustrate that there are a lot of ways to move towards “screening for generality” that don’t involve 96-well plates. 
</p>

<p>
If you have the time and resources for robotics or SFC-MS, that’s great—you’ll be able to screen more quickly and cover more ground. But you can still start to consider more than a single model substrate even without any specialized equipment. It’s a mindset, not a recipe.
</p>

]]></description>
              <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Data Are Getting Cheaper</title>
              <link>public/blog/20230602_data_cheaper.html</link>
              <description><![CDATA[
<p>
Much ink has been spilled on whether scientific progress is slowing down or not (<a href=https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/>e.g.</a>). I don’t want to wade into that debate today—instead, I want to argue that, regardless of the rate of new discoveries, acquiring scientific data is easier now than it ever has been.
</p>

<p>
There are a lot of ways one could try to defend this point; I’ve chosen some representative anecdotes from my own field (organic chemistry), but I’m sure scientists in other fields could find examples closer to home. 
</p>

<h3>
NMR Spectroscopy
</h3>

<p>
    NMR spectroscopy is now the primary method for characterization and structural study of organic molecules, but it wasn’t always very good. The last half-century has seen a <a href=https://content.iospress.com/download/biomedical-spectroscopy-and-imaging/bsi055?id=biomedical-spectroscopy-and-imaging%2Fbsi055>steady increase</a> in the quality of NMR spectrometers (principally driven by the development of more powerful magnetic fields), meaning that even a relatively lackluster NMR facility today has equipment beyond the wildest dreams of a scientist in the 1980s:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_nmr.png style="width:400px;" />
  <figcaption>
  Most powerful NMR magnet over time (Campbell, Figure 1). Broad adoption lags these records, but the trend is comparable.
  </figcaption>
</figure>

<h3>
Commercially Available Compounds
</h3>

<p>
The number of compounds available for purchase has markedly increased in recent years. In the 1950s, the Aldrich Chemical Company’s listings could fit on a single page, and even by the 1970s Sigma-Aldrich only sold 40,000 or so chemicals (<a href=https://www.encyclopedia.com/books/politics-and-business-magazines/sigma-aldrich>source</a>). 
</p>

<p>
But things have changed. Nowadays new reagents are available for purchase within weeks or months of being reported (<a href=https://enamine.net/news-press-releases/press-releases/1286-enamine-brings-levin-nitrogen-deleting-reagent-to-the-market>e.g.</a>), and companies like Enamine spend all their time devising new and quirky structures for medicinal chemists to buy.
</p>

One way to quantify how many compounds are for sale today is through <a href=https://en.wikipedia.org/wiki/ZINC_database>the ZINC database</a>, aimed at collecting compounds for virtual screening, which is updated every few years or so. The first iteration of ZINC, in 2005, had fewer than a million compounds: now there are almost 40 billion:

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_zinc.png style="width:400px;" />
  <figcaption>
  Number of compounds in the ZINC database over time (graphic made by me).
  </figcaption>
</figure>

<p>
(Most compounds in the ZINC database aren’t available on synthesis scale, so it’s not like you can order a gram of all 40 billion compounds—there’s probably more like <a href=https://mcule.com/database/>3 million</a> “building blocks” today, which is still a lot more than 40,000.)
</p>

<h3>
Chromatography 
</h3>

<p>
Chromatography, the workhorse of synthetic and analytical chemistry, has also gotten a lot better. Separations that took almost an hour can now be performed in <a href=https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/jssc.201500270>well under a minute</a>, accelerating purification and analysis of any new material (this paper focuses on chiral stationary phase chromatography, but many of the advances translate to other forms of chromatography too).
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_chromatography.jpg style="width:450px;" />
  <figcaption>
    Fastest chiral separation for a given compound by year (Regalado, Figure 1). 
  </figcaption>
</figure>

<h3>
Computational Chemistry
</h3>

<p>
Moore’s Law is powerful. In the 1970s, using semiempirical methods and minimal basis set Hartree–Fock to investigate an 8-atom system was cutting-edge, as demonstrated by <a href=https://pubs.acs.org/doi/abs/10.1021/ja00455a056>this paper from Ken Houk</a>:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_houk.png style="width:400px;" />
  <figcaption>
    Figure 3 from the 1977 Houk paper.
  </figcaption>
</figure>

<p>
Now, that calculation would probably take only a few seconds on my laptop computer, and it’s becoming increasingly routine to perform full density-functional theory or post-Hartree–Fock studies on 200+ atom systems. <a href=https://www.science.org/doi/10.1126/science.ade5320>A recent paper</a>, also from Ken Houk, illustrates this nicely: 
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20230602_houk2.png style="width:400px;" />
  <figcaption>
    Figure 3E from the 2022 Houk paper.
  </figcaption>
</figure>

<p>
The fact that it’s now routine to perform conformational searches and high-accuracy quantum chemical calculations on catalyst•substrate complexes with this degree of flexibility would astonish anyone from the past. (To be fair to computational chemists, it’s not all Moore’s Law—advances in QM tooling also play a big role.)
</p>

<h3>
What Now?
</h3>

<p>
There are lots of advances that I haven’t even covered, like the general improvement in synthetic methodology and the rise of robotics. Nevertheless, I think the trend is clear: it’s easier to acquire data than it’s ever been. 
</p>

<p>
What, then, do we do with all this data? Most of the time, the answer seems to be “not much.” <a href=https://pubs.acs.org/doi/10.1021/acs.orglett.2c03246>A recent editorial</a> by Marisa Kozlowski observes that the average number of substrates in <i>Organic Letters</i> has increased from 17 in 2009 to 40 in 2021, even as the information contained in these papers has largely remained constant. Filling a substrate scope with almost identical compounds is a boring way to use more data; we can do better.
</p>

<p>
The availability of cheap data means that scientists can—and must—start thinking about new ways to approach experimental design. Lots of academic scientists still labor under the delusion that “hypothesis-driven science” is somehow superior to HTE, when in fact the two are ideal complements to one another. “Thinking in 96-well plates” is already common in industry, and should become more common in academia; why run a single reaction when you can run a whole screen?
</p>

<p>
New tools are needed to design panels, run reactions, and analyze the resultant data. One nice entry into this space is Tim Cernak’s <a href=https://chemrxiv.org/engage/chemrxiv/article-details/60c75166702a9bcd6918bf39><i>phactor</i></a>, a software package for high-throughput experimentation, and I’m sure lots more tools will spring up in the years to come. (I’m also optimistic that multi-substrate screening, which we and others have used in “screening for generality,” will become more widely adopted as HTE becomes more commonplace.)
</p>

<p>
The real worry, though, is that we will refuse to change our paradigms and just use all these improvements to publish old-style papers faster.  All the technological breakthroughs in the world won’t do anything to accelerate scientific progress if we refuse to open our eyes to the possibilities they create. If present trends continue, it may be possible in 5–10 years to screen for a hit one week, optimize reaction conditions the next week, and run the substrate scope on the third week. Do we really want a world in which every methodology graduate student is expected to produce 10–12 low-quality papers per year? 
</p>

]]></description>
              <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Chaos Monkeys</title>
              <link>public/blog/20230530_chaos_monkeys.html</link>
              <description><![CDATA[
<p>
Recently, I wrote about <a href=https://corinwagen.github.io/public/blog/20230502_startups.html>how scientists could stand to learn a lot from the tech industry.</a> In that spirit, today I want to share a book review of <i>Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley</i>, Antonio García Martínez’s best-selling memoir about his time in tech and “a guide to the spirit of Silicon Valley” (<a href=https://www.nytimes.com/2016/06/29/business/dealbook/review-chaos-monkeys-is-a-guide-to-the-spirit-of-silicon-valley.html>NYT</a>).
</p>

<p>
<i>Chaos Monkeys</i> is one of the most literary memoirs I’ve read. The book itself is a clear homage to Hunter S. Thompson’s <i>Fear and Loathing in Las Vegas</i>; Martínez writes in the “gonzo journalism” style, blending larger-than-life personal exploits with frank accounting of the facts. But Antonio García Martínez’s writing, replete with GRE-level words and classical epigraphs, invites further literary comparisons.
</p>

<h2>
<i>Chaos Monkeys</i> as <i>The Odyssey</i>
</h2>

<p>
The first comparison that springs to mind is <i>The Odyssey</i>. Odysseus, the protagonist of The Odyssey, is frequently described as <i>polytropos</i> (lit. “many turns”), which denotes his wily and cunning nature. Antonio García Martínez (or, per the tech fondness for acronyms, “AGM”) certainly deserves the same epithet.
</p>

<p>
<i>Chaos Monkeys</i> is structured as a recounting of AGM’s escapades in Silicon Valley. In order, he (1) leaves his job at Goldman Sachs and joins an ad-tech company, (2) quits and founds his own company, AdGrok, (3) gets accepted to Y Combinator and survives a lengthy legal battle with his former employer, (4) sells AdGrok simultaneously to both Twitter and Facebook, eventually sending the other employees to Twitter and himself to Facebook, (5) becomes a PM at Facebook and engineers a scheme to fix their ad platform and make them profitable, (6) succeeds, sorta, but pisses people off and gets fired, and then (7) turns around and sells his expertise to Twitter.
</p>

<p>
His circuitous journey around the Bay Area has the rough form of an ancient epic: at each company, he’s faced with new challenges and new adversaries, and his fractious relationships with his superiors mean that he’s often at the mercy of capricious higher powers, not unlike Odysseus. Nevertheless, through a mixture of cunning and hard work he manages to escape with his skin intact every time, ready for the next episode. (And, best of all, he literally lives on a boat while working at Facebook.)
</p>

<p>
(His escapades have only continued since this book was published: he got hired at Apple, unceremoniously fired a few weeks later [for passages in <i>Chaos Monkeys</i>], made it on <a href=https://open.spotify.com/episode/2JMZ0yOuTaGU3bXLymlWYT>Joe Rogan</a>, and has now founded <a href=https://www.spindl.xyz/>spindl</a>, a web3 ad-tech startup, while simultaneously <a href=https://www.thepullrequest.com/p/why-judaism>converting to Judaism</a>.)
</p>

<h2>
<i>Chaos Monkeys</i> as <i>Moby-Dick</i>
</h2>

<p>
<i>Chaos Monkeys</i> bears a structural resemblance to <i>Moby-Dick</i>. Narrative passages alternate with lengthy technical discussions about the minutiae of Silicon Valley: one chapter, you’re reading about how AGM flooded Mark Zuckerberg’s office trying to brew beer inside Facebook, while the next chapter is devoted to a discussion of how demand-side advertising platforms work.
</p>

<p>
The similarities run deeper, though. Venture capitalism, the funding model that dominates Silicon Valley, was originally developed to fund whaling expeditions in the 1800s (<a href=https://newrepublic.com/article/154490/small-world-vc>ref</a>, <a href=https://www.hbs.edu/faculty/Pages/item.aspx?num=43322>ref</a>). While once venture capitalists listened to prospective whaling captains advertising the quality of their crews in a New England tavern, today VCs hear pitches from thousands of startups hoping to develop <s>the next killer app</s> the best ChatGPT frontend and make millions.
</p>

<p>
This isn’t a coincidence: whaling expeditions and tech startups are both high-risk, high-reward enterprises that require an incredible amount of skill and hard work, but also a healthy dose of luck. Both operations have returns dominated by outliers, making picking winners much more important than making safe bets, and in both cases the investment remains illiquid for a long time, demanding trust from the investor.
</p>

<p>
Much like Ishmael in <i>Moby-Dick</i>, AGM’s adventures see him join forces with a motley crew of high-performing misfits from around the globe. And just as Ahab’s quest for the whale is foreshadowed to be a ruinous one, so too does the reader quickly come to understand that AGM’s tenure in Silicon Valley will not, ultimately, end well. A fatalistic haze hangs over the book, coloring his various hijinks with a sense of impending loss.
</p>

<p>
(And did I mention he lives on a boat?)
</p>

<h2>
<i>Chaos Monkeys</i> as <i>The Great Gatsby</i>
</h2>

<p>
The emotional tone of the book, however, is best compared to that favorite of high-school English classes, <i>The Great Gatsby</i>. AGM, like Nick Carraway, is an outsider in the world of the nouveau riche—opulent parties, high-speed road races, conspicuous consumption—and, over the course of the book, is alternately infatuated with and disgusted by his surroundings. When at last AGM retires to a quiet life on <s>Ithaca</s> the San Juan islands, it’s with feelings of disillusionment, betrayal, and frustration, not unlike Carraway withdrawing to the Midwest. As AGM writes in the penultimate paragraph of <i>Chaos Monkeys</i>’s acknowledgements:
</p>

<blockquote>
To Paul Graham, Jessica Livingston, Sam Altman, and the rest of the Y Combinator partners and founders involved in the AdGrok saga. In a Valley world awash with mammoth greed and opportunism masquerading as beneficent innovation, you were the only real loyalty and idealism I ever encountered.
</blockquote>

<p>
But <i>Chaos Monkeys</i> isn’t solely an indictment of Silicon Valley’s worst excesses. Not unlike <i>The Wire</i>, <i>Chaos Monkeys</i> manages to simultaneously portray the positive and negative aspects of its subject matter, refusing to be reduced to “tech good” or “tech bad.” The panoply of grifters, Dilbert-tier bosses, and Machiavellian sociopaths lambasted by AGM can exist only because of the immense value that their enterprises provide to society—and his faith in the ability of tech to create wonders persists even as his own efforts to do so are undermined.
</p>

<p>
So the underlying message of <i>Chaos Monkeys</i>, ultimately, is one of hope for tech. If the excesses of tech are worse than other industries, it's only because the underlying field itself is so much more fertile. Far from condemning it, the depths of the decadence spawned by Silicon Valley bear witness to the immense value it creates. Imitation is the highest form of flattery; grift is the surest sign of productivity.
</p>

<p>
Overall, <i>Chaos Monkeys</i> is an exhilarating and hilarious read, a gentle introduction to the world of term sheets, product managers, and non-competes, and a book replete with anecdotes sure to fulfill the stereotypes of tech-lovers and tech-haters alike.
</p>


]]></description>
              <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Editorial Overreach and Scientific Authority</title>
              <link>public/blog/20230525_jcim.html</link>
              <description><![CDATA[
<p>
Previously, I wrote about various <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>potential future roles for journals</a>. Several of the scenarios I discussed involved journals taking a much bigger role as editors and custodians of science, using their power to shape the way that science is conducted and exerting control over the scientific process.
</p>

<p>
I was thus intrigued when, last week, <i>The Journal of Chemical Information and Modeling</i> (<i>JCIM</i>; an ACS journal) released <a href=https://pubs.acs.org/doi/10.1021/acs.jcim.3c00599>a set of guidelines</a> for molecular dynamics simulations that future publications must comply with. These guidelines <a href=https://twitter.com/LindorffLarsen/status/1659889905843990529>provoked a reaction from the community</a>: various provisions (like the requirement that all simulations be performed in triplicate) were alleged to be <a href=https://twitter.com/PabloGalazDavis/status/1659940766330781696>arbitrary</a> or <a href=https://twitter.com/PiiaBartos/status/1659976208015130626>unscientific</a>, and the fact that these standards were imposed by editors and not determined by the community also <a href=https://twitter.com/daviddesancho/status/1660184730862583808>drew criticism</a>.
</p>

<p>
The authors <a href=https://twitter.com/RommieAmaro/status/1660050832157716481>say</a> that the editorial “is *not* intended to instruct on how to run MD”, but this defense rings hollow to me. See, for instance, the section about choosing force fields:
</p>

<blockquote>
JCIM will not accept simulations with old force field versions unless a clear justification is provided. Specialized force fields should be used when available (e.g., for intrinsically disordered proteins). In the case of the reparametrization or development of new parameters compatible with a given force field, please provide benchmark data to support the actual need for reparameterization, proper validation of novel parameters against experimental or high-level QM data…
</blockquote>

<p>
I’m not a molecular dynamics expert, so I’m happy to stay out of the scientific side of things (although the editorial’s claim that “MD simulations are not suitable to sample events occurring between free energy barriers” seems clearly false for sufficiently low-barrier processes). Nor do I wish to overstate the size of the community’s reaction: a few people complaining on Twitter doesn’t really matter.
</p>

<p>
Rather, I want to use this vignette to reflect on the nature of scientific authority, and return to a piece I’ve cited <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>before</a>: Geoff Anders’ <a href=https://www.palladiummag.com/2022/10/10/the-transformations-of-science/>“The Transformations of Science.”</a> Anders describes how the enterprise of science, initially intended to be free from authority, has evolved into a magisterium of knowledge that governments, corporations, and laypeople rely upon:
</p>

<blockquote>
The original ideal of <i>nullius in verba</i> sometimes leads people to say that science is a never-ending exploration, never certain, and hence antithetical to claims on the basis of authority. This emphasizes one aspect of science, and indeed in theory any part of the scientific corpus could be overturned by further observations.
<br><br>
There is, however, another part of science—<i>settled science.</i> Settled science is safe to rely on, at least for now. Calling it into question should not be at the top of our priorities, and grant committees, for example, should typically not give money to researchers who want to question it again.
</blockquote>

<p>
While each of these forms of science is fine on its own, they ought not to be conflated:
</p>

<blockquote>
When scientists are meant to be authoritative, they’re supposed to know the answer. When they’re exploring, it’s okay if they don’t. Hence, encouraging scientists to reach authoritative conclusions prematurely may undermine their ability to explore—thereby yielding scientific slowdown. Such a dynamic may be difficult to detect, since the people who are supposed to detect it might themselves be wrapped up in a premature authoritative consensus.
</blockquote>

<p>
This is tough, because scientists like settled science. We write grant applications describing how our research will bring clarity to long-misunderstood areas of reality, and fancy ourselves explorers of unknown intellectual realms. How disappointing, then, that so often science can only be relied upon when it settles, long after the original discoveries have been made! An intriguing experimental result might provoke further study, but it’s still “in beta” (to borrow the software expression) for years or decades, possibly even forever.
</p>

<p>
Applying the settled/unsettled framework of science to the <i>JCIM</i> question brings some clarity. I don’t think anyone would complain about settled science being used in editorial guidelines: I wouldn’t want to open <i>JACS</i> and read a paper that questioned the existence of electrons, any more than I want <i>The Economist</i> to publish articles suggesting that Switzerland is an elaborate hoax.
</p>

<p>
Scientific areas of active inquiry, however, are a different matter. Molecular dynamics might be a decades-old field, but the very existence of journals like <i>JCIM</i> and <i>JCTC</i> points to its unsettled nature—and <a href=https://twitter.com/LindorffLarsen/status/1659931209659457539>AlphaFold2</a>, discussed in the editorial, is barely older than my toddler. There are whole hosts of people trying to figure out how to run the best MD simulations, and editors giving them additional guidelines is unlikely to accelerate this process. (This is separate from mandating they report what they actually did, which is fair for a journal to require.)
</p>

<p>
Scientists, especially editors confronted with an unending torrent of low-quality work, want to combat bad science. This is a good instinct. And I’m sympathetic to the idea that journals need to become something more than a neutral forum in the Internet age—the editorial aspect of journals, at present, seems underutilized. But prematurely trying to dictate rules for exploring the frontier of human knowledge is, in my opinion, the wrong way to do this. What if the rules are wrong?
</p>

<p>
There may be a time when it’s prudent for editors to make controversial or unpopular decisions: demanding pre-registration in psychology, for instance, or mandating external validation of a new synthetic method. But I’m not sure that “how many replicates MD simulations need” is the hill I would choose to die on. In an age of declining journal relevance, wise editorial decisions might be able to set journals apart from the “anarchic preprint lake”—but poor decisions may simply hasten their decline into irrelevance.
</p>

]]></description>
              <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: The Art of Doing Science and Engineering</title>
              <link>public/blog/20230516_hamming.html</link>
              <description><![CDATA[
<br>
<br>
<p class=epigraph>
“They performed his signs among them, and miracles in the land of Ham.”
</p>
<p class=epigraph-byline>
—Psalm 105:27
</p>

<p>
Who was Richard Hamming, and why should you read his book?
</p>

<p>
If you’ve taken computer science courses or messed around enough with <i>scipy</i>, you might recognize his name in a few different places—<a href=https://en.wikipedia.org/wiki/Hamming_code>Hamming error-correction codes</a>, the <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hamming.html>Hamming window function</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_distance>Hamming distance</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_bound>Hamming bound</a>, etc. I had heard of some of these concepts, but didn’t know anything concrete about him before I started reading this book.
</p>

<p>
His brief biography is as follows: Richard Hamming (1915–1998) studied mathematics at the University of Chicago, earned a PhD in three years from Illinois, and then started as a professor at the University of Louisville… in 1944. He was almost immediately recruited for the Manhattan Project, where he worked on large-scale simulations of imploding spherical shells of explosives, and generally acted as a “computational janitor” for various projects.
</p>

<p>
In 1946, he moved to Bell Telephone Laboratories, <a href=https://www.theverge.com/2012/3/21/2887206/jon-gertner-idea-factory-bell-labs-great-american-age-innovation-book-review>“arguably the most innovative research institution in the country,”</a> and worked there until 1976. During his time at Bell Labs, he was involved in “nearly all of the laboratories' most prominent achievements” (<a href=https://en.wikipedia.org/wiki/Richard_Hamming>Wikipedia</a>), and was rewarded with such accolades as the third-ever Turing Award (essentially the Nobel Prize for computer science) and the IEEE Richard W. Hamming medal. (You know you’re successful when someone else names an award after you!)
</p>

<p>
After he retired from Bell Labs, Hamming went on to go teach classes at the Naval Postgraduate School. This book—published in 1996, just before his death—is based on the capstone engineering class he taught there, which attempted to prepare students for their technical future by conveying the “style of thinking” necessary to be a great scientist and engineer. Stripe Press calls it “a provocative challenge to anyone who wants to build something great” and “a manual of style for how to get there,” while the foreword calls it a “tour of scientific greatness” which prepares “the next generation for even greater greatness” while challenging them to serve the public good.
</p>

<p>
I was excited to read this book because:
</p>

<ol>
<li>
Hamming was present at arguably the two most successful scientific institutions of the 20th century—the Manhattan Project and Bell Labs—and so was witness to more innovative scientific discoveries than almost anyone today. So if anyone has a shot at teaching how to be an effective scientist, it’s Hamming.
</li>

<li>
The interface between academic science and real-world advances seems to be one of the most broken elements of the modern research ecosystem, and I was interested to see what Hamming, as a self-proclaimed scientist and engineer, had to say on the subject.
</li>

<li>
It’s a first-hand account of the creation of the most important field of the past century—computer science—and thus a way to witness, albeit second-hand, the important process of <a href=https://freaktakes.substack.com/p/when-do-ideas-get-easier-to-find>scientific branch-creation</a>.
</li>

<li>
The book is a chance to see computing, and computer science, as it was before the implications and importance of the field became obvious, and thus a way to understand what a promising area of research looks like without the benefit of hindsight.
</li>

<li>
This is a chance to read frank and honest reflections from someone who both was a great scientist and was adjacent to a huge number of other great scientists, and thus learn about the culture of successful science (especially successful science in the mid-20th century, which might be different than the science of today).
</li>
</ol>

<h2>
1. What’s In The Book?
</h2>

<p>
<i>The Art of Doing Science and Engineering</i> is not a conventional textbook, but neither is it a self-help book, a memoir, or a guide to personal strategy. The two books that remind me of it most are <i>Godel, Escher, Bach</i>, by Douglas Hofstadter, and <i>Zero To One</i>, by Peter Thiel. All three books are composed of various pseudo-independent chapters which, in isolation, could likely function as essays, but which echo certain themes over and over again in a way that makes the whole greater than the sum of its parts in a hard-to-summarize way.
</p>

<p>
Hamming’s goal in structuring the book this way is clear, and explicitly stated: he doesn’t want to teach object-level facts, because the facts needed to succeed in any discipline will inevitably change over the course of time. Instead he seeks to teach the patterns of thought which will enable anyone to succeed in an evolving technical landscape. To do so, he uses a mix of first-person stories, historical reflections, mathematical proofs, and graphs, all with the goal of teaching something that’s essentially incommunicable: style. I quote:
</p>

<blockquote>
I will show you my style as best I can, but, again, you must take those elements of it which seem to fit you, and you must finally create your own style. Either you will be a leader or a follower, and my goal for you is to be a leader. You cannot adopt every trait I discuss in what I have observed in myself and others; you must select and adapt, and make them your own if the course is to be effective.
</blockquote>

<p>
Despite Hamming’s insistence that there is “really no technical content” in the book and that any mathematics is only “window dressing,” I found that my lack of background knowledge made several chapters—particularly those on digital filters—quite difficult to understand.
</p>

<h2>
2. Representative Anecdotes
</h2>

<p>
In keeping with Hamming’s desire to convey the “style of thinking” rather than actual object-level scientific ideas, I’ll share a few anecdotes, insights, and quotes in an attempt to reproduce the style of his writing.
</p>

<h3>
2.1 The Second Mouse Gets The Cheese
</h3>

<p>
Hamming recounts the invention of interpreters and compilers and then reflects on how hard it was for early computer pioneers to think of computers as “symbol manipulators and not just number crunchers,” observing that often the first people in a field do not understand it as well as those who come after them. Why is this?
</p>

<blockquote>
The reason this happens is that the creators have to fight through so many dark difficulties, and wade through so much misunderstanding and confusion, they cannot see the light as others can, now the door is open and the path made easy…. in time the same will probably be true of you.
</blockquote>

<p>
This is a nice observation, and perhaps explains the value of startups: incumbents in a market can be inefficient not only for bureaucratic reasons, but also because they’re intellectually less suited to see new opportunities—the young can see the status quo more clearly than those who’ve had to create it. This also explains why “really new ideas seldom arise from the experts in the field”—experts always bring their expertise when looking at something new, which makes them more likely to be correct, but also disincentivizes new ways of thinking and thus creates a sort of status quo bias.
</p>

<h3>
2.2 Order-of-Magnitude Changes
</h3>

<p>
Many people, faced with early computers, dismissed them as simply a faster way to do rote calculations—which seems silly in hindsight, but was a real barrier for early computer pioneers to overcome. Hamming argues “a single order of magnitude change (a factor of ten) produces fundamentally new effects” in any piece of technology, and immediately thereafter reflects on how it’s difficult for people to accept something new:
</p>

<blockquote>
People always want to think that something new is just like the past—they like to be comfortable in their minds as well as their bodies—and hence they prevent themselves from making any significant contribution to the new field being created under their noses. Not everything which is said to be new really is new, and it is hard to decide in some cases when something is new, yet the all too common reaction of “it’s nothing new” is stupid.
</blockquote>

<p>
I had previously attributed the idea that ten-fold improvement creates a fundamentally new product to Peter Thiel (<i>Zero To One</i>, pp. 48–49), but it seems Hamming (as usual) got there first.
</p>

<h3>
2.3 Intuition In High Dimensions
</h3>

<p>
Since most complex problems occur in high-dimensional spaces, Hamming illustrates a few ways that our 2D or 3D intuition can fail us.
</p>

<p>
One particularly nice thought experiment is this: take a square with side length 4 and divide it into four squares, each containing a unit circle. Now draw a circle in the middle of the square, such that the circle just touches each of the four unit circles. Realizing that the distance from the center of this circle to the center of each unit circle must be √2, and that the radius of the unit circle is, of course, 1, we can see that the radius of the inner circle must be √2 - 1 ≈ 0.414.
</p>

<p>
Now, we can perform the same mental exercise for a cube with side length four, and find that the analogous inner sphere has side length √3 - 1 ≈ 0.732. More generally, as we extend this exercise to higher dimensions, we find that the radius of the inner <i>n</i>-dimensional hypersphere is √<i>n</i> - 1, which is bizarre! For instance, in ten dimensions, the inner sphere has radius √10 - 1 ≈ 2.162, meaning that it reaches outside of the surrounding cube:
</p>

<blockquote>
Yes, the sphere is convex, yes it touches each of the 1,024 packed spheres on the inside, yet it reaches outside the cube! So much for your raw intuition about n-dimensional space, but remember the n-dimensional space is where the design of complex objects generally takes place.
</blockquote>

<p>
This sort of intuition is difficult to obtain, but Hamming gives a few examples. One that stuck with me was the claim that, in high-dimensional shapes, almost all of the volume is on the surface—so “almost surely the optimal design will be on the surface and will not be inside, as you might think from taking the calculus and doing optimizations in that course.”
</p>

<h3>
2.4 The Fast Fourier Transform
</h3>

<p>
Here’s a memorable story from Hamming’s life:
</p>

<blockquote>
You have all heard about the fast Fourier transform [FFT] and the Tukey-Cooley paper. It is sometimes called the <i>Tukey-Cooley</i> transform or algorithm. Tukey had suggested to me, sort of, the basic ideas of the FFT. I had at the time an IBM Card Programmed Calculator (CPC) and the “butterfly” operation meant it was completely impractical to do with the equipment I had. Some years later I had an internally programmed IBM 650 and he remarked on it again. All I remembered was that it was one of Tukey’s few bad ideas; I completely forgot why it was bad—namely because of the equipment I had at the time. So I did not do the FFT, though a book I had already published (1961) shows I knew all the facts necessary, and could have done it easily!
<br><br>
Moral: when you know something cannot be done, also remember the essential reason why, so later, when the circumstances have changed, you will not say “It can’t be done.” Think of my error! How much more stupid can anyone be?
</blockquote>

<p>
Later in the book, Hamming puts forward the following “old statement” about experts:
</p>

<blockquote>
If an expert says something can be done he is probably correct, but if he says it is impossible then consider getting another opinion.
</blockquote>

<h3>
2.5 The Mixed Blessings of Jargon
</h3>

<p>
Hamming emphasizes the importance, as an interdisciplinary scientist, of mastering the language of the field in which you’re working, but warns against embracing jargon too much. Why? Jargon serves “to facilitate communication over a restricted area of things or events… [but] also blocks thinking outside the original area it was designed to cover.” So jargon makes intra-domain communication easier, but makes inter-domain communication harder.
</p>

<p>
More philosophically, jargon is a consequence of how “we have been mainly selected by evolution to resent outsiders,” and thus the “instinctive use of jargon” is a base instinct that must be consciously resisted.
</p>

<h3>
2.6 Optimal Components, Suboptimal Systems
</h3>

<p>
Hamming discusses the field of systems engineering, which he defines as “the attempt to keep at all times the larger goals in mind and to translate local actions into global results” (emphasis original), and coins the first rule of systems engineering:
</p>

<blockquote>
If you optimize the components, you will probably ruin the system performance.
</blockquote>

<p>
This point is illustrated with a few examples. One of these examples is the progressive optimization of calculus and linear algebra classes in college, where “we have stripped out anything not immediately relevant to each course,” leading to “large parts of any mathematical education being omitted in the urge to optimize the individual courses.” Only when the proper goal of a mathematical education is taken into account—producing well-trained students with a firm grasp of math and the ability to apply it to important problems—can the constituent courses sanely be optimized.
</p>

<p>
I found this idea pretty insightful, and have thought about it a lot since reading this book. For instance, one can see many researchers as over-optimizing for “publishing papers” or “winning grants” rather than working towards maximizing total scientific progress. (Alex Danco has a great piece discussing the same ideas in the context of the Canadian tech ecosystem, which I wrote about <a href=https://corinwagen.github.io/public/blog/20221206_definite_games_indefinite_optimism.html>here</a>.)
</p>

<h3>
2.7 Learning Should Be Difficult
</h3>

<p>
I like this story so much I’ll just reproduce it in its entirety:
</p>

<blockquote>
When I first came to the Naval Postgraduate School in 1976 there was a nice dean of the extension division concerned with education. In some hot discussions on education we differed. One day I came into his office and said I was teaching a weightlifting class (which he knew I was not). I went on to say that graduation was lifting 250 pounds, and I had found many students got discouraged and dropped out, some repeated the course, and a very few graduated. I went on to say thinking this over last night I decided the problem could be easily cured by simply cutting the weights in half—the student in order to graduate, would lift 125 pounds, set them down, and then lift the other 125 pounds, thus lifting the 250 pounds.
<br><br>
I waited a moment while he smiled (as you probably have) and I then observed that when I found a simpler proof for a theorem in mathematics and used it in class, was I or was I not cutting the weights in half? What is your answer? Is there not an element of truth in the observation that the easier we make the learning for the student, the more we are cutting the weights in half?
</blockquote>

<p>
This story reflects a key belief of Hamming’s: that creativity and talent in technical disciplines are not innate traits given only to rare geniuses, but trainable skills which anyone can hope to acquire and improve at, given the appropriate training regimen. In Hamming’s worldview, staring at a confusing math problem is not a sign that you’re a failure, but the process by which you become successful.
</p>

<h3>
2.8 The Importance of Presentation
</h3>

<p>
Hamming emphasizes that being able to “sell” one’s ideas is a key part of being a scientist:
</p>

<blockquote>
All [methods of conveying ideas] are essential—you must learn to sell your ideas, not by propaganda, but by force of clear presentation. I am sorry to have to point this out; many scientists and others think good ideas will win out automatically and need not be carefully presented. They are wrong; many a good idea has had to be rediscovered because it was not well presented the first time, years before! New ideas are automatically resisted by the establishment, and to some extent justly. The organization cannot be in a continual state of ferment and change, but it should respond to significant changes.
</blockquote>

<p>
In this view, a certain degree of institutional conservatism is necessary to avoid being swept up by any new fad (in machine learning terms, we might say that organizations need to limit their learning rate), and so you alone must convince your peers that your insights are the real deal and deserve to be taken seriously.
</p>

<p>
Hamming then extends this idea to needing to sell your abilities more broadly:
</p>

<blockquote>
You do not hire a plumber to learn plumbing while trying to fix your trouble; you expect he is already an expert. Similarly, only when you have developed your abilities will you generally get the freedom to practice your expertise, whatever you choose to make it, including the expertise of “universality,” as I did.
</blockquote>

<p>
My experience within science is that most people are a bit allergic to the idea of “selling” themselves or their research—with the exception of a few people who become almost addicted to it. Hamming (who never shies away from quoting a Greek philosopher) would probably think that there’s an Aristotelian mean between these two extremes: the ideal scientist/engineer recognizes that self-promotion is a necessary means to an end, but never engages in self-promotion absent a higher goal.
</p>

<h2>
3. Overall Themes
</h2>

<p>
I would summarize Hamming’s key themes—those leitmotifs which pop up time and time again in the book—as the following:
</p>

<h3>
3.1 Fundamentals Are Key
</h3>

<p>
Wherever possible, try to understand the intellectual underpinnings of a field as well as possible, rather than the surface-level results. If you do so, you will not only understand the field better than most of its practitioners, but also be better at transferring knowledge between fields. Perceiving “the essential unity of all knowledge rather than the fragments which appear as the individual topics are taught” allows one to quickly access relevant information to the problem at hand, no matter the field of origin.
</p>

<p>
Hamming frequently points out the failings of domain experts to perceive the fundamental underpinnings of their knowledge:
</p>

<blockquote>
Lo and behold, the famous <i>transfer function</i> is exactly the <i>eigenvalues</i> of the corresponding eigenfunctions! Upon asking various electrical engineers what the transfer function was, no one has ever told me that! Yes, when pointed out to them that it is the same idea they have to agree, but the fact it is the same idea never seemed to have crossed their minds! The same, simple idea, in two or more different disguises in their minds, and they knew of no connection between them! Get down to the basics every time! <i>(emphasis original)</i>
</blockquote>

<p>
Not only is a good grasp of fundamentals important for understanding your own domain, it also helps with creativity. Hamming argues that creative insights come from the subconscious, and that “flexible access to pieces of knowledge” is the most important way to give the subconscious the tools it needs to solve a problem. This flexible access arises from “looking at knowledge while you are acquiring it from many different angles,” making sure to capture its key features rather than simply memorizing the aspect relevant to the task at hand.
</p>

<p>
(The idea that creativity comes from the subconscious is pretty common—see, for instance, Nisbett and Wallace’s article <a href=https://home.csulb.edu/~cwallis/382/readings/482/nisbett%20saying%20more.pdf>“Telling More Than We Can Know,”</a> which argues that basically all higher order cognitive processes are subconscious.)
</p>

<h3>
3.2 Gain Insight Where Possible
</h3>

<p>
No matter the task at hand, Hamming argues that the correct immediate step is always to gain insight about the situation, and then go from there. He uses the example of Planck and the “ultraviolet catastrophe” to illustrate how crucial insight can be:
</p>

<blockquote>
Max Planck (1858–1947) fit the black-body radiation experimental data with an empirical curve, and it fit so well he “knew” it was “the right formula.” He set out to derive it, but had trouble. Finally he used a standard method of breaking up the energy into finite sizes and then going to the limit… Fortunately for Planck the formula fit only so long as he avoided the limit, and no matter how he took the limit the formula disappeared. He finally, being a very good, honest physicist, decided he had to stop short of the limit, and that is what defines Planck’s constant!
<br><br>
<i>[another historical paragraph omitted]</i>
<br><br>
Before going on, let me discuss how this piece of history has affected my behavior in science. Clearly Planck was led to create the theory because the approximating curve fit so well, and had the proper form. I reasoned, therefore, if I were to help anyone do a similar thing, I had better represent things in terms of functions they believed would be proper for their field rather than in the standard polynomials. I therefore abandoned the standard polynomial approach to approximation, which numerical analysts and statisticians among others use most of the time, for the harder approach of finding which class of functions I should use.
</blockquote>

<p>
This episode demonstrates how insight can arise from a simulation, and enable future work (like, in this case, all of quantum mechanics), and also illustrates how the manner in which one performs simulations can make it either easier or harder to obtain underlying insights about the problem. Hamming emphasizes this point with a few stories from his time at Bell Labs, and argues that there are times where more computational power is actually counterproductive:
</p>

<blockquote>
I have often wondered what would have happened [in the Nike guided missile project] if I had had a modern, high-speed computer. Would I ever have acquired the feeling for the missile, upon which so much depended in the final design? I often doubt hundreds more trajectories would have taught me as much—I simply do not know. But that is why <i>I am suspicious, to this day, of getting too many solutions and not doing enough very careful thinking about what you have seen.</i> Volume output seems to me to be a poor substitute for acquiring an intimate feeling for the situation being simulated… doing simple simulations at the early stages lets you get insights into the whole system which would be disguised in any full-scale simulation. <i>(emphasis original)</i>
</blockquote>

<p>
This point—that simulation is not the same as understanding—is not unique to Hamming (see <i>inter alia</i> Roald Hoffmann on the subject: <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201902527>1</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910283>2</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910285>3</a>), but hearing this from the father of scientific simulation certainly drives the message home!
</p>

<h3>
3.3 Vision Matters
</h3>

<p>
Hamming goes to great lengths to emphasize the importance of having a vision for your life:
</p>

<blockquote>
It is well known the drunken sailor who staggers to the left or right with n independent random steps will, on the average, end up about √<i>n</i> steps from the origin. But if there is a pretty girl in one direction, then his steps will tend to go in that direction and he will go a distance proportional to <i>n</i>. In a lifetime of many, many independent choices, small and large, a career with vision will get you a distance proportional to <i>n</i>, while no vision will get you only the distance √<i>n</i>. In a sense, the main difference between those who go far and those who do not is some people have a vision and the others do not and therefore can only react to the current events as they happen… having a vision is what tends to separate the leaders from the followers.
</blockquote>

<p>
This theme permeates his discussion of systems engineering: a successful systems engineer must, at all times, keep the overall vision and purpose of the system in mind, so as to optimize in the right direction. It also motivates what problems you must choose:
</p>

<blockquote>
If you do not work on important problems, how can you expect to do important work? Yet direct observation and direct questioning of people show most scientists spend most of their time working on things they believe are not important and are not likely to lead to important things.
</blockquote>

<p>
In Hamming’s view, it seems the precise vision one follows is less important than the simple act of having a vision at all. Forcing oneself to decide on goals and then strive to fulfill them will naturally lead you to excellence, even if the goals you choose aren’t the same as the one Hamming would have chosen:
</p>

<blockquote>
The chief gain is in the effort to change yourself, in the struggle with yourself, and it is less in the winning than you might expect. Yes, it is nice to end up where you wanted to be, but the person you are when you get there is far more important. I believe a life in which you do not try to extend yourself regularly is not worth living—but it is up to you to pick the goals you believe are worth striving for.
</blockquote>

<h2>
4. Should You Read This Book?
</h2>

<p>
I began this book review by claiming that <i>The Art of Doing Science and Engineering</i> isn’t a textbook, or a self-help book, or a memoir, but failed to offer a positive vision of what it was. I now reveal my true opinion: <i>The Art of Doing Science and Engineering</i> is best viewed as a modern example of <a href=https://en.wikipedia.org/wiki/Wisdom_literature>“wisdom literature,”</a> in the style of ancient scriptures.
</p>

<p>
Why is this? Wisdom literature frequently has the curious property that it’s accessible only to the wise. For instance, the book of Proverbs is ostensibly written to convey wisdom to those who seek it, but this hardly seems compatible with the following passage (Proverbs 26:7–9):
</p>

<blockquote>
Like a lame man's legs, which hang useless, is a proverb in the mouth of fools.<br>
Like one who binds the stone in the sling is one who gives honor to a fool.<br>
Like a thorn that goes up into the hand of a drunkard is a proverb in the mouth of fools.<br>
</blockquote>

<p>
If proverbs are useless—or worse than useless, a thorn in the hand of a drunkard—to those without wisdom, then what is the point of proverbs? If only the wise can understand your book of wisdom, why bother writing it at all?
</p>

<p>
There are several ways to resolve this tension (one being “the book of Proverbs is stupid”), but I think the right answer goes something like this: wisdom is accessible to those who seek it, but simply reading through a book of wisdom isn’t sufficient to make one wise. Rather, the search for wisdom requires discipline and vigilance—one must meditate on wise sayings, appreciate the underlying principles, and learn to discern what’s right even in complicated circumstances. So, wisdom literature can help us on the journey to wisdom, but ultimately we will have to take the intellectual burden upon ourselves if we hope to get anywhere. (This is roughly how <a href=https://www.desiringgod.org/articles/the-best-discoveries-begin-as-problems>John Piper interprets Proverbs</a>.)
</p>

<p>
Viewing Hamming’s book as essentially modern wisdom literature makes sense of his focus on the “style” of thinking, his insistence that the reader must rediscover much of what he’s saying for themselves, and his admonitions not to accept what he’s saying blindly but to think it over at length:
</p>

<blockquote>
You the reader should take your own opinions and try first to express them clearly, and then examine them with counterarguments, back and forth, until <i>you are fairly clear as to what you believe and why you believe it</i>. It is none of the author’s business in this matter what you believe, but it is the author’s business to get you to think and articulate your position clearly. <i>(emphasis original)</i>
</blockquote>

<p>
If we view this book—Hamming’s guide to future scientists and engineers, his <i>magnum opus</i> as a teacher and mentor—as wisdom literature, it implies that wisdom, not any specific technical skill, is rate-limiting for technical progress. This is very encouraging, because wisdom, unlike innate intelligence, is an acquired trait, and one which we can all cultivate in ourselves. We can’t all be Ramanujan or von Neumann, but (at least as Hamming tells it) we can all be Hamming.
</p>

<p>
Thus far, I’ve mostly given reasons why you should read this book. Why shouldn’t you read this book? One reason is that this book is aimed at scientists and engineers, and furthermore it seems primarily aimed at people with an interest in the “hard” sciences—much of the advice assumes some contact with simulation, math, or physics. So a reader without at least a glancing interest in these topics might struggle to find some of the content relevant. (But maybe the act of extending his advice to other domains would prompt deeper consideration of the fundamental principles at play, and thus serve to cultivate wisdom!)
</p>

<p>
Another reason you shouldn’t read this book is that it’s very much framed as a personal guide—it addresses the needs of an individual scientist, not ideas for how science writ large could be improved. So aspiring metascientists might be disappointed by Hamming’s perspective; he dedicates a lot of time to thinking about how one can navigate imperfect organizations, and much less time to thinking about what a perfect organization would look like.
</p>

<p>
The strongest reason for reading this book, though, is that the world Hamming hopes to write for is almost exactly our world today. Hamming anticipates “the inevitable and extensive computerization of your organization and society generally,” a world in which scientists are frequently overwhelmed by the “rate of innovation and change of the dominant paradigm,” and a world where there is “not time to allow us the leisure which comes from separating the two fields” of science and engineering. From my perspective, this almost perfectly captures the feeling of working in science or tech today.
</p>

<p>
And so Hamming’s message—the vision of scientists “trying to achieve excellence” through making “significant contributions to humanity” on important problems—seems more relevant today than ever. If you yourself work in a scientific field, and want to know how to have the greatest positive impact on your own character and on society, then Hamming’s wisdom is for you: but not without some struggle
</p>

<i>
A transcript of Hamming’s talk “You and Your Research” (a shorter exposition of some of the ideas discussed above) is available <a href=https://www.cs.virginia.edu/~robins/YouAndYourResearch.html>here</a>, and </i>The Art of Doing Science and Engineering<i> can be purchased from Stripe Press <a href=https://press.stripe.com/the-art-of-doing-science-and-engineering>here</a>.
</i>
<br>
<br>
<i>Thanks to Michael Tartre for giving me this book originally, and to Jacob Thackston and Ari Wagen for extensive edits.</i>
]]></description>
              <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>What Happened to IRMS?</title>
              <link>public/blog/20230512_irms.html</link>
              <description><![CDATA[
<p>
A few days ago, <a href=https://corinwagen.github.io/public/blog/20230508_aldehyde_eie.html>I wrote</a> about kinetic isotope effects (KIEs), probably my favorite way to study the mechanism of organic reactions. To summarize at a high level: if the bonding around a given atom changes over the course of a reaction, then different isotopes of that atom will react at different rates. The exact magnitude of the effect depends on the vibrational modes involved, but is often quite different for different mechanisms, meaning that you can computationally predict isotope effects for a lot of mechanisms and then use KIE measurements to figure out which one is actually happening.
</p>

<p>
The trouble is that the magnitude of the effect depends on the difference in the mass of the two isotopologues. <sup>1</sup>H/<sup>2</sup>H isotope effects are quite large: H reacts up to 7x faster than D (more for mechanisms that involve quantum tunneling), meaning that it’s not too hard to measure the value accurately. But as the atom gets heavier, the effects get smaller. For the next most common pair of isotopomers,<sup>12</sup>C/<sup>13</sup>C, the effect is usually 5% or less.
</p>

<p>
Small KIEs are usually measured by one-pot competition experiments: a mixture of the two isomers is reacted to partial conversion, and then the isotopic composition of either the starting material or the product is determined. The product will be enriched in the isotope that reacts more quickly, and the starting material will be enriched in the isotope that reacts more slowly. If you know the starting ratio of isotopes, the conversion, and the ratio of isotopes at partial conversion, then you can use the Bigeleisen−Mayer equation to figure out the KIE.
(<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.1c07351>This</a> is a really good review on isotope effects in general, if you want more than this cursory summary.)
</p>

<p>
The accuracy of the KIE measurement is thus limited by (1) how accurately you can determine conversion and (2) how accurately you can measure the isotopic composition of a sample. Although conversion can be annoying, the second is the more serious limitation—<i>a priori</i> it’s not obvious how to figure out what the relative abundance of various isotopes is.
</p>

<p>
Today, most people use approaches based on NMR spectroscopy: since <sup>1</sup>H and <sup>13</sup>C are both NMR-active nuclei, you can just integrate the peak of interest against another peak to figure out how much there is. (Quantitative <sup>13</sup>C NMR is super slow, so <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>various</a> <a href=https://www.nature.com/articles/s41557-018-0079-7>tricks</a> can be employed to speed things up.)
</p>

<p>
But there was an age before the advent of accurate NMR spectroscopy where people measured isotope effects differently. I was awestruck by <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00834a064>this 1975 paper</a> from Cromartie and Swain reporting the measurement of a <sup>35</sup>Cl/<sup>37</sup>Cl isotope effect in the cyclization of 4-chlorobutanol: they report an isotope effect of 1.000757 ± 0.00015 using hydroxide as base, which they differentiate from an isotope effect of 1.000796 ± 0.00013 using water as base by Student’s <i>t</i> test. These numbers are way, way smaller and more precise than any isotope effect I’ve seen measured in the last few decades.
</p>

<p>
Digging a little deeper reveals a <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00418a038>whole</a> <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00426a047>wealth</a> of papers using <sup>35</sup>Cl/<sup>37</sup>Cl isotope effects to study various mechanistic phenomena. The instrument Swain and others use (described <a href=https://pubs.acs.org/doi/pdf/10.1021/jo00972a016>here</a>) is an isotope-ratio mass spectrometer, which as the name implies is a special sort of mass spectrometer designed specifically to measure isotopic composition. These instruments, although a little obscure from my point of view, are <a href=https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry/isotope-ratio-mass-spectrometry-irms.html>commercial</a>!
</p>

<p>
So, why isn’t IRMS used more frequently in organic chemistry today? I think it’s for a few reasons. IRMS, at least historically, only works on gases, meaning that you have to either use gaseous reactants or convert your analytes to gases, both of which are pretty annoying. In the Swain work, they (i) incinerate their samples with nitric acid, (ii) precipitate out silver chloride by adding silver nitrate, and then (iii) convert silver chloride to gaseous methyl chloride by <a href=https://pubs.acs.org/doi/abs/10.1021/ja00873a025>heating with methyl iodide</a> in a sealed tube. This is certainly a lot of hassle to put up with for a single measurement—and you generally want to get a good number of replicates.
</p>

<p>
(There are some <a href=https://www.thermofisher.com/order/catalog/product/11206175>all-in-one solutions</a> available for sale, which automatically combust samples à la elemental analysis, but they don’t seem to work on non-standard isotopes like chlorine.)
</p>

<p>
Another reason why IRMS might have fallen out of favor is that it requires a dedicated instrument, whereas NMR-based methods can be done using the NMR spectrometers that any university already has. Most labs only have budgets for a handful of instruments—is an IRMS really worth the investment? (Owing to the typical aura of secrecy around instrument prices, I’m not sure how much one costs, but I’m guessing it’s a few hundred thousand dollars or so.)
</p>

<p>
These downsides notwithstanding, I think there is a lot of good science that could be done if a mechanistic group decided to make IRMS a core part of their program. In particular, <sup>35</sup>Cl/<sup>37</sup>Cl KIEs seem really powerful: there are a growing number of organometallic reactions which involve chlorine atoms in the key step(s), and for which Cl KIEs might be complementary or superior to more conventional KIEs. I’m envisioning studying <a href=https://www.science.org/doi/10.1126/science.aad6981>transmetallation from Pd(II) chlorides</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.1c13333>chlorine radical-mediated C–H activation</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.2c01356>photolysis of Ni(II) chlorides</a>.
</p>

<p>
(And why stop at Cl? According to ThermoFisher, thermal ionization mass spectrometry lets you analyze the isotopic composition of metals with really high accuracy [five decimal places, per their <a href=https://assets.thermofisher.com/TFS-Assets/CMD/brochures/br-30537-triton-xt-tims-br30537-en.pdf>brochure</a>]. Would a <sup>58</sup>Ni/<sup>60</sup>Ni isotope effect be possible to measure? This might provide a handle on some mechanistically ambiguous Ni(III) scenarios, like those reported <a href=https://pubs.acs.org/doi/10.1021/jacs.5b13211>here</a>: is radical trapping or reductive elimination rate- and enantioselectivity-determining?)
</p>

<p>
It doesn’t seem like it’s that easy to start a purely mechanistic research group these days, so maybe this is an unfundable idea. But it seems sad that a technique as powerful for physical (in)organic chemists as IRMS could just fade into obscurity, and I hope somebody finds the time and resources to apply it to modern mechanistic problems.
</p>

]]></description>
              <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Isotope Effects in Aldehyde Protonation</title>
              <link>public/blog/20230508_aldehyde_eie.html</link>
              <description><![CDATA[
<p>
I’m writing my dissertation right now, and as a result I’m going back through a lot of old slides and references to fill in details that I left out for publication.
</p>

<p>
One interesting question that I’m revisiting is the following: when protonating benzaldehyde, what is the H/D equilibrium isotope effect at the aldehyde proton? This question was relevant for the H/D KIE experiments we conducted in <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c06688>our study of the asymmetric Prins cyclization</a>. (The paper hasn’t gotten much attention, but it’s probably the most “classic” organic chemistry paper I’ve worked on, with a minimum of weird computational details or bizarre analytical techniques.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230508_aldehyde_nores.png" style="width:375px;" />
</figure>

<p>
Since the H/D bond isn’t involved in the reaction, we won’t see a primary effect; so we know we have to be thinking in terms of secondary effects. The most common reason to observe a secondary isotope effect is changes in hybridization: sp<sup>3</sup> to sp<sup>2</sup> gives a normal effect, whereas sp<sup>2</sup> to sp<sup>3</sup> gives an inverse effect. From this perspective, it looks like the effect should be unity, since the carbon in question is sp<sup>2</sup> in both structures.
</p>

<p>
Reality, however, disagrees. <a href=https://www.sciencedirect.com/science/article/abs/pii/0040403976800573>Hall and Milosevich</a> report a EIE of 0.94 for benzaldehyde in aq. sulfuric acid, and <a href=https://pubs.acs.org/doi/pdf/10.1021/ja982504r>Gajewski and co-authors</a> compute an EIE of 0.83 for acetaldehyde at the MP2/6-31G(d,p) level of theory. I performed my own calculations at the M06-2X/jun-cc-pVTZ level of theory and obtained an EIE of 0.851 with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>, qualitatively consistent with the above results.
</p>

<p>
Where does this EIE come from? It’s helpful to think of benzaldehyde as possessing multiple resonance forms:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230508_aldehyde_res.png" style="width:400px;" />
</figure>

<p>
We typically think of the neutral resonance form on the top left, but you can also imagine putting a positive charge on carbon and a negative charge on oxygen to create a zwitterion with a C–O single bond (bottom left). In neutral benzaldehyde, this resonance form is substantially disfavored, but in protonated benzaldehyde it doesn’t look any worse than the “normal” top resonance form!
</p>

<p>
If this is true, we’d expect the C–O bond order to decrease from 2 in neutral benzaldehyde to ~1.5 in protonated benzaldehyde. Indeed, in my calculations the bond length increases from 1.20 Å to 1.28 Å upon protonation—so it seems the double bond character is decreasing! It’s not quite the same as going from sp<sup>2</sup> to sp<sup>3</sup>, but the inverse KIE begins to make sense.
</p>

<p>
(This is purely guesswork, but my guess would be that the differences between the two structures are attenuated in a polar solvent like water. The zwitterionic resonance form of the neutral structure will be stabilized and thus the neutral aldehyde will be more polar, making the change to the oxocarbenium less drastic. This might explain why the measured EIE in water is smaller—although this might also be due to counterion effects, or something completely unrelated.)
</p>

<p>
Let’s go a level deeper. According to <a href=https://pubs.acs.org/doi/abs/10.1021/ja01542a075>Streitwieser</a>, secondary KIEs associated with hyperconjugation originate from the creation or destruction of the c. 800 cm<sup>-1</sup> out-of-plane bending vibrations of Csp<sup>2</sup>–H hydrogens, which are markedly lower in frequency than the c. 1350 cm<sup>-1</sup> bending vibrations associated with Csp<sup>3</sup>–H hydrogens.
</p>

<p>
Raising the frequency of a mode increases the energy required to inhabit the ground vibrational state (the “zero-point energy”)—but deuterium is heavier and vibrates more slowly, meaning that it possesses less ZPE and is less affected by these changes. So when an 800 cm<sup>-1</sup> sp<sup>2</sup> mode transforms to a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode, the ZPE increases, but less for D than for H, so D is favored. Conversely, when a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode transforms to a 800 cm<sup>-1</sup> sp<sup>2</sup> mode, the ZPE decreases, but less for D than for H, so H is favored. (For a more complete explanation, see <a href=https://macmillan.princeton.edu/wp-content/uploads/RRK-KIE.pdf>this presentation by Rob Knowles</a>.)
</p>

<p>
This effect is complicated for benzaldehyde by the fact that the out-of-plane bend of the aldehyde couples to the out-of-plane bend of the phenyl ring, so there are several modes involving out-of-plane vibration of the aldehyde proton. When I compared the out-of-plane bend of the aldehyde H in both structures, I saw only minimal differences: 771, 963, 1040, and 1051 cm<sup>-1</sup> for the neutral species, as compared to 790, 1003, and 1061 cm<sup>-1</sup> for the protonated species. These small differences can’t be responsible for the observed effect.
</p>

<p>
In contrast, the in-plane C–H bend shows a big change—1430 cm<sup>-1</sup> for benzaldehyde, but 1644 cm<sup>-1</sup> for the oxocarbenium (it seems to couple to the C–O stretch; the reduced mass increases from 1.26 amu to 3.52 amu). Applying Streitweiser’s formula for estimating the isotope effect for a specific mode gives a pretty good match:
</p>

<p>
kH/kD ≈ exp(0.187/T * ∆ν) = exp(0.187/298 * (-214)) = 0.87
</p>

<p>
I don’t understand this area well enough to comment on why there’s a change in the in-plane vibrational frequency and not the out-of-plane vibrational frequency, nor do I understand how to deconvolute the effects of mode-to-mode coupling. Nevertheless, this provides a tentative physical rationale for the observation.
</p>

<p>
On a more abstract level, this case study illustrates why isotope effects are such a good tool. Any transformation that perturbs the vibrational frequencies of a given molecule can, in principle, be monitored by isotope effects without affecting the electronic energy surface at all. So, although the precise nature and magnitude of the effect might be hard to predict <i>a priori</i>, it’s not surprising that a transformation as dramatic as protonating a functional group produces a sizable isotope effect.
</p>
]]></description>
              <pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Tech As Control Group For Science</title>
              <link>public/blog/20230502_startups.html</link>
              <description><![CDATA[
<p>
I frequently wonder what the error bars on my life choices are. What are the chances I ended up a chemist? A scientist of any type? Having two children in graduate school? 
</p>

<p>
If I had the ability, I would want to restart the World Simulator from the time I started high school, run a bunch of replicates, and see what happened to me in different simulations. And this wouldn’t just be useful for me personally—there are lots of things in the world that are just as contingent and path-dependent as one’s life choices. What would have happened if Charles the Bold hadn’t died in 1477 and Burgundy had preserved its independence? If the 1787 convention were rerun several times, how might the US Constitution differ? 
</p>

<p>
Sadly, we’ll never know the answer to these questions. But what we can do instead is find cases in which analogous institutions evolved in parallel, and try to learn from the similarities and differences between them. It’s an imperfect substitute for rerunning the World Simulator, but it’s still pretty cool. (This is far from an original idea: see for instance <a href=https://www.amazon.com/Legal-Systems-Very-Different-Ours/dp/1793386722/><i>Legal Systems Very Different From Ours</i></a>.)
</p>

<p>
Lately, I’ve come to think about the tech/startup world as somewhat parallel to academic science in this manner. Why? For one, both tech and academia deal with hard problems that demand obscure/arcane domain-specific knowledge inaccessible to non-experts. (It’s true that the problems are typically scientific in academia and engineering-related in tech, but I’ve argued <a href=https://corinwagen.github.io/public/blog/20230215_science_engineering.html>previously</a> that this distinction is flimsier than it seems.) And in both fields, a few high performers <a href=https://nadia.xyz/top-talent>vastly outperform</a> the rest of the field, be it a “10x engineer” or a Nobel laureate.
</p>

<p>
Startups, like academic labs, are small and agile institutions which face the task of raising money, building a team, selecting a hard yet solvable problem, and finding a solution all within a few years. In both cases, too, there are nonlinear returns to success: moderate success is not much better than failure, pushing founders/assistant professors to be as ambitious as possible.
</p>

<p>
If we accept these two fields as vaguely analogous, what interesting differences can we observe? 
</p>

<h3>
Startups Have Multiple Founders
</h3>

<p>
I’ll quote from <a href=http://www.paulgraham.com/startupmistakes.html>an essay by Paul Graham</a>, founder of Y Combinator and noted startup sage:
</p>

<blockquote>
Have you ever noticed how few successful startups were founded by just one person? Even companies you think of as having one founder, like Oracle, usually turn out to have more. It seems unlikely this is a coincidence.
<br><br>
What's wrong with having one founder? To start with, it's a vote of no confidence. It probably means the founder couldn't talk any of his friends into starting the company with him. That's pretty alarming, because his friends are the ones who know him best.
<br><br>
But even if the founder's friends were all wrong and the company is a good bet, he's still at a disadvantage. Starting a startup is too hard for one person. Even if you could do all the work yourself, you need colleagues to brainstorm with, to talk you out of stupid decisions, and to cheer you up when things go wrong.
</blockquote>

<p>
Ever since I read this, I’ve wondered why no labs ever have multiple PIs. I guess this would mess with the semi-feudal organization of university bureaucracy, but it doesn’t seem intrinsically bad—after all, lots of startups seem to do just fine.
</p>

<h3>
Startup Winners Can’t Be Picked <i>Ex Ante</i>
</h3>

<p>
The VC strategy, as I understand it, is basically “fund a bunch of companies, and one or two of them will make it all worth our while.” This is a little bit different than how universities approach hiring assistant professors: each university will typically hire a small number of professors each year, after much deliberation, and they have a pretty high likelihood of giving them tenure, at least relative to the likelihood of any given startup succeeding. (Basically, <a href=https://www2.nau.edu/lrm22/lessons/r_and_k_selection/r_and_k.html>startups are r-selected, whereas academic labs are K-selected.</a>)
</p>

<p>
There are a lot of reasons why this might be. For one, faculty members aren’t just trying to pick a winner but also their future colleague, so personal considerations probably matter more. Failure in science seems more cruel, too: while a failed startup founder can often negotiate the “sale” of their company and parlay that into new jobs and the constant churn of tech means that there are always new openings for talented ex-startup employees, a lab that doesn’t get tenure takes a toll on professor and students alike. 
</p>

<p>
A hypothesis for why the success rate for new labs is so much higher than the success rate for new businesses is that many labs only succeed a little bit. They don’t actually achieve what they dreamed about in their initial proposals, but they pivot and accrue enough publications and cachet to earn tenure nevertheless. In business, it seems harder to succeed a little bit—the market is a harsher critic than one’s peers.
</p>

<h3>
Founders Should Be Focused
</h3>

<p>
Paul Graham again, this time talking about <a href=http://www.paulgraham.com/ramenprofitable.html>the dangers of fundraising</a>:
</p>

<blockquote>
Raising money is terribly distracting. You're lucky if your productivity is a third of what it was before. And it can last for months.
<br><br>
I didn't understand (or rather, remember) precisely why raising money was so distracting till earlier this year. I'd noticed that startups we funded would usually grind to a halt when they switched to raising money, but I didn't remember exactly why till YC raised money itself. We had a comparatively easy time of it; the first people I asked said yes; but it took months to work out the details, and during that time I got hardly any real work done. Why? Because I thought about it all the time.
</blockquote>

<p>
The broader conclusion, from this and other essays, is that any distractions from the core mission of the startup are very dangerous, and should be avoided at all costs. This is very different from the lifestyle of new PIs, who are typically juggling departmental responsibilities, writing a curriculum, lecturing for the first time, and writing grants all while trying to get their lab up and running.
</p>

<h3>
Talent Acquisition Is Crucial
</h3>

<p>
In tech, people obsess about recruiting the best people possible—I reviewed <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>a whole book about this</a> last year. Hiring bad programmers is #6 on PG’s <a href=http://www.paulgraham.com/startupmistakes.html>list of mistakes that kill startups</a>, and there seems to be a general consensus that a great company takes great engineers, no matter what. 
</p>

<p>
In contrast, professors don’t have full control over whom they hire (for graduate students), making recruiting much harder. Graduate students are selected through a complex two-stage system involving admission to a school and then a subsequent group-joining process (and new assistant professors sometimes aren’t even around for the first of these stages). You can obviously try to coax talented students to work for you, but the pool of accepted students interested in your subfield might be tiny, and they might all prefer to work for an established group… 
</p>

<p>
(Plus, there’s not a good way to reward top performers in academia. All graduate students are equal, at least on paper—you can’t give someone a year-end bonus, or a promotion.)
</p>

<p>
A nice concrete example of this is how professors <a href=https://www.jefftk.com/p/hiring-programmers-in-academia>struggle to hire competent programmers</a>, even as research scientists—they aren’t allowed to pay enough to match market rates, even when the expense would be well worth the money. To quote <a href=https://acoup.blog/2023/04/28/collections-academic-ranks-explained-or-what-on-earth-is-an-adjunct/?s=03#easy-footnote-bottom-8-18391>Bret Devereaux</a>: “academic hiring, to be frank, is not conducted seriously” (he’s discussing the humanities, but the point stands). 
</p>

<h3>
Successful Startups Grow
</h3>

<p>
As a startup succeeds, it grows: while a seed-stage startup typically has &lt;15 people, startups at Series A often have 20–40, and startups at Series B–C might have as many as 300 employees (<a href=https://www.quora.com/How-many-employees-do-early-stage-startups-have>one ref</a>; rough numbers broadly consistent with other sources). Good companies grow, while bad ones die. 
</p>

<p>
In contrast, it’s rare for even the most successful US academic labs to grow past 30 people (although <a href=https://go.gale.com/ps/i.do?id=GALE%7CA195680544&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=00280836&p=HRCA&sw=w&userGroupName=mlin_oweb&isGeoAuthType=true>it occasionally happens</a>), limiting the reach of top-performing professors. While a huge proportion of tech employees work for the best companies (Google, Meta, Amazon, etc), only a very small number of students work for the best professors. 
</p>

<h3>
Concluding Thoughts
</h3>

<p>
The imperfect nature of the analogy means that some of these points might not be useful in a normative sense: universities are not really optimized to produce research as efficiently as possible, and maybe that’s fine. Likewise, startups aren’t optimized to produce unprofitable research or train future scientists, even if these activities may in the long run be beneficial. (This is <a href=https://corinwagen.github.io/public/blog/20220728_consulting_as_grad_school.html#:~:text=Accordingly%2C%20the%20government%20sponsors%20research%20into%20interesting%20problems%20with%20uncertain%20timeframes%20to%20do%20what%20the%20free%20market%20cannot>why basic science is considered a public good</a>, and why the government funds it at all!)
</p>

<p>
Nevertheless, I think there’s a lot that scientists can learn from startups. There is a whole army of people working to solve challenging technical problems in the most efficient way, and it’d be prudent to study the wisdom that emerges. 
</p>

<i>
Thanks to Ari Wagen and Jacob Thackston for reading drafts of this piece. 
</i>

]]></description>
              <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Do We Still Need Journals?</title>
              <link>public/blog/20230427_journals.html</link>
              <description><![CDATA[
<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_compass.png" style="width:550px;" />
  <figcaption> 
    This image inspired by the rightly famous <a href=https://images.are.na/eyJidWNrZXQiOiJhcmVuYV9pbWFnZXMiLCJrZXkiOiIxMDI4NDUyNy9vcmlnaW5hbF80ZmNlOTY3ODYxZTFiYWEwZjkwODMxYzlkZTNhNjhhZS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjE4MDAsImhlaWdodCI6MTgwMCwiZml0IjoiaW5zaWRlIiwid2l0aG91dEVubGFyZ2VtZW50Ijp0cnVlfSwid2VicCI6eyJxdWFsaXR5Ijo5MH0sImpwZWciOnsicXVhbGl0eSI6OTB9LCJyb3RhdGUiOm51bGx9fQ==?bc=0>original</a>.
  </figcaption>
</figure>

<p>
One of the most distinctive parts of science, relative to other fields, is the practice of communicating findings through peer-reviewed journal publications. Why do scientists communicate in this way? As I see it, scientific journals provide three important services to the community: 
</p>

<ol>
  <li>
    Journals help scientists communicate; they disseminate scientific results to a broad audience, both within one’s community and to a broader scientific audience.
  </li>
  <li>
    Through the peer review process, journals ensure scientific correctness and keep standards high. You never know which of your scientific adversaries might be scrutinizing your work for flaws, so you’re incentivized to do the best job possible.
  </li>
  <li>
    Journals, and peer review, help scientists to read high-quality, impactful work by filtering out low-impact papers (even those which are scientifically correct). This makes journals a somewhat “fair” way to score the importance of publications without being a subject-matter expert; I might not know what’s happening these days with topological quantum materials, but if I see three <i>Science</i>/<i>Nature</i> papers on a CV, I’ll certainly pay attention! 
  </li>
</ol>

<p>
(There are certainly other services that journals provide, like DOIs and typesetting, but these seem much less important to me.)
</p>

<p>
In this post, I want to (1) discuss the problems with scientific journals today, (2) briefly summarize the history of journals and explain how they came to be the way they are today, and (3) imagine how journals might evolve in the coming decades to adapt to the changing landscape of science. My central claim is that <u>the scientific journal, as defined by the above criteria, has only existed since about the 1970s, and will probably not exist for very much longer—and that’s ok.</u> (I’ll also try and explain the esoteric meme at the top.)
</p>

<h2>
1. Journals Present
</h2>

<p>
Many people are upset about scientific journals today, and for many different reasons. 
</p>

<h3>
1.1 Profits and Paywalls
</h3>

<p>
The business model of scientific journals is, to put it lightly, unusual. <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Writing for <i>The Guardian</i></a>, Stephen Buranyi describes how “scientific publishers manage to duck most of the actual costs” that normal magazines incur by outsourcing their editorial duties to scientists: the very people who both write and read the articles:
</p>

<blockquote>
It is as if the New Yorker or the Economist demanded that journalists write and edit each other’s work for free, and asked the government to foot the bill. Outside observers tend to fall into a sort of stunned disbelief when describing this setup. A 2004 parliamentary science and technology committee report on the industry drily observed that “in a traditional market suppliers are paid for the goods they provide”. A 2005 Deutsche Bank report referred to it as a “bizarre” “triple-pay” system, in which “the state funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product”.
</blockquote>

<p>
And this cost-dodging is very successful: scientific journals are a huge moneymaker, with Elsevier (one of the largest publishers) having a margin <a href=https://tidsskriftet.no/en/2020/08/kronikk/money-behind-academic-publishing>in excess of 30%</a>, and ACS’s “information services” (publication) division close behind, <a href=https://www.acs.org/content/dam/acsorg/about/aboutacs/financial/2022-audited-financial-statements.pdf>with a profit margin of 27%</a>.
</p>

<p>
The exorbitant fees charged by journals, and the concomitantly huge profits they earn, have led to increasing pushback against the paywall-based status quo. The Biden administration <a href=https://www.nature.com/articles/d41586-022-02351-1>has called</a> for all government-funded research to be free-to-read without any embargo by 2025, and other countries have been pursuing similar policies <a href=https://www.chemistryworld.com/news/eu-and-uk-bitten-by-the-open-access-bug/5236.article>for some time</a>. Similarly, <a href=https://news.mit.edu/2020/guided-by-open-access-principles-mit-ends-elsevier-negotiations-0611>MIT</a> and <a href=https://www.universityofcalifornia.edu/news/why-uc-split-publishing-giant-elsevier>the UC system</a> recently terminated their subscriptions to Elsevier over open-access issues. (And the rise of SciHub means that, even without a subscription, most scientists can still read almost any article they want—threatening to completely destroy the closed-access model.) 
</p>

<p>
In response to this pressure, journals have begun offering open-access alternatives, where the journal’s fees are paid by the submitting author rather than the reader. While in theory this is a solution to this problem, in practice the fees for authors are so high that it’s not a very good solution. The board of editors of NeuroImage <a href=https://www.nature.com/articles/d41586-023-01391-5>recently resigned</a> over their journal’s high open-access fees—and they’re <a href=https://www.nature.com/articles/d41586-019-00135-8>not the first</a> board of editors to do this. As <a href=https://www.vox.com/the-highlight/2019/6/3/18271538/open-access-elsevier-california-sci-hub-academic-paywalls>a 2019 <i>Vox</i> summary</a> put it: “Publishers are still going to get paid. Open access just means the paychecks come at the front end.” 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_oa_tweet.png" style="width:475px;" />
</figure>

<h3>
1.2 Efficacy of Peer Review
</h3>

<p>
In parallel, the <a href=https://en.wikipedia.org/wiki/Replication_crisis>“replication crisis”</a> has led to growing skepticism about the value of peer review. In his article <a href=https://www.experimental-history.com/p/the-rise-and-fall-of-peer-review>“The Rise and Fall of Peer Review,”</a> Adam Mastroianni describes how experiments to measure its value have yielded dismal outcomes:
</p>

<blockquote>
Scientists have run studies where they deliberately add errors to papers, send them out to reviewers, and simply count how many errors the reviewers catch. Reviewers are pretty awful at this. In <a href=https://www.sciencedirect.com/science/article/pii/S019606449870006X?casa_token=D5MklJnYP0MAAAAA:CzyYl8VLg-M-bvKIHxl7vWCIh8AVrkTUizQ9LZPZWh_eVT5jLf3WJlGaJQzYCsHMraF5Fh8mqw>this study</a> reviewers caught 30% of the major flaws, in <a href=https://jamanetwork.com/journals/jama/article-abstract/187748>this study</a> they caught 25%, and in <a href=https://journals.sagepub.com/doi/full/10.1258/jrsm.2008.080062>this study</a> they caught 29%. These were critical issues, like “the paper claims to be a randomized controlled trial but it isn’t” and “when you look at the graphs, it’s pretty clear there’s no effect” and “the authors draw conclusions that are totally unsupported by the data.” Reviewers mostly didn’t notice.
</blockquote>

<p>
While the worst of the replication crisis seems to be contained to the social sciences, my own field—chemistry—is by no means exempt. As I wrote <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>previously</a>, “<a href=https://pubs.acs.org/doi/10.1021/acscentsci.2c00325>elemental analysis doesn’t work</a>, <a href=https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e>integration grids cause problems</a>, and even <a href=http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html>reactions from famous labs can’t be replicated</a>.” There are a lot of bad results in the scientific literature, even in top journals—I don’t think many people in the field actually believe that a generic peer-reviewed publication is guaranteed to be correct.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_pr_tweet.png" style="width:475px;" />
</figure>

<p>
And the process of soliciting peer reviews is by no means trivial: prominent professors are commonly asked to peer review tons of articles as an unpaid service to the community, which isn’t really rewarded in any way. As the number of journals and publications grows faster than the number of qualified peer reviewers, burnout can result:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_bo_tweet.png" style="width:475px;" />
</figure>

<h3>
1.3 Preprint Servers
</h3>

<p>
The rise of preprint servers like arXiv, BioRxiv, and ChemRxiv also means journals aren’t necessary for communication of scientific results. More and more, preprints dominate discussions of cutting-edge science, while actual peer-reviewed publications lag months to years behind. 
</p>

<p>
While in theory preprints aren’t supposed to be viewed as scientifically authoritative—since they haven’t been reviewed—in practice most preprints are qualitatively identical to the peer-reviewed papers that they give rise to. <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001285>A retrospective analysis</a> of early COVID preprints found that the vast majority of preprints survived peer review without any substantive changes to their conclusions (although this might be biased by the fact that the worst pre-prints will never be accepted at all.)
</p>

<p>
If this is the case, why bother with journals at all? To a growing degree this seems to be the norm in CS and CS-adjacent fields: the landmark Google transformer paper from 2017, <a href=https://arxiv.org/pdf/1706.03762.pdf>“Attention Is All You Need,”</a> is still just a PDF on arXiv six years later, despite being potentially the most impactful discovery of the 2010s. Similarly, UMAP, <a href=https://corinwagen.github.io/public/blog/20230417_dimensionality_reduction.html>which I discussed last week,</a> is also just <a href=https://arxiv.org/abs/1802.03426>hanging out on arXiv</a>, no peer-reviewed publication in sight. Still, in chemistry and other sciences we’re expected to publish in “real journals” if we want to graduate or get jobs. 
</p>

<h3>
1.4 Impact-Neutral Reviewing
</h3>

<p>
An implicit assumption of the scientific journal is that high-impact publications can be distinguished from low-impact publications without the benefit of hindsight. Yet many of the most impactful scientific discoveries—like the Krebs cycle, the weak interaction, lasers, continental drift, and CRISPR—<a href=https://nintil.com/discoveries-ignored>were rejected</a> when first submitted to journals. How is this possible? 
</p>

<p>
I’d argue that peer review creates a bias towards incrementalism. It’s easy to see how an improvement over something already known is significant; it’s perhaps harder to appreciate the impact of a field-defining discovery, or to believe that such a result could even be possible. To quote Antonio Garcia Martinez on startups: “If your idea is any good, it won’t get stolen, you’ll have to jam it down people’s throats instead.” True zero-to-one thinking can provoke a strong reaction from the establishment, and rarely a positive one. 
</p>

<p>
(It’s worth noting that some of the highest profile organic chemistry papers from 2022 were new takes on old, established, reactions: <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c05648>Parasram</a> and <a href=https://www.nature.com/articles/s41586-022-05211-0>Leonori’s</a> “ozonolysis without ozone” and <a href=https://www.science.org/doi/10.1126/science.abo6443>Nagib’s</a> “carbenes without diazoalkanes.” I love both papers—but I also think it’s easier for audiences to appreciate why “ozonolysis without ozone” is a big deal than to process an entirely new idea.)
</p>

<p>
Even for more quotidian scientific results, the value of impact-based peer review is limited. Matt Clancy at <a href=https://www.newthingsunderthesun.com/pub/nc5341ua/release/3><i>New Things Under the Sun</i></a> writes that, for preprints, paper acceptance is indeed correlated with number of eventual citations, but that the correlation is weak: reviewers seem to be doing better than random chance, but worse than we might hope. (Similar results emerge when studying the efficacy of peer review for grants.) On the aggregate, it does seem true that the average <i>JACS</i> paper is better than the average <i>JOC</i> paper, but the trend is far from monotonic.
</p>

<p>
These concerns aren’t just mine; indeed, a growing number of scientists seek to reject impact-based refereeing altogether. The <a href=http://proteinsandwavefunctions.blogspot.com/2016/01/writing-impact-neutral-review.html?m=1&s=03>“impact-neutral reviewing”</a> movement thinks that papers should be evaluated only on the basis of their scientific correctness, not their perceived potential impact. Although I wouldn’t say this is a mainstream idea, journals like <a href=https://journals.plos.org/plosone/s/journal-information><i>PLOS One</i></a>, <a href=https://blog.frontiersin.org/2015/12/21/4782/><i>Frontiers</i></a>, and <a href=https://elifesciences.org/inside-elife/54d63486/elife-s-new-model-changing-the-way-you-share-your-research><i>eLife</i></a> have adopted versions of it, and perhaps more journals will follow in the years to come.
</p>

<p>
Taken together, these anecdotes demonstrate that all three pillars of the modern scientific journal—communication, peer review, and impact-based sorting—are threatened today. 
</p>

<h2>
2. Journals Past
</h2>

<p>
How did we get here? 
</p>

<p>
The importance of journals as a filter for low-quality work is a modern phenomenon. Of course, editors have always had discretion over what to publish, but until fairly recently <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>the total volume of papers was much lower</a>, meaning that it wasn’t so vital to separate the wheat from the chaff. In fact, <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Stephen Buranyi</a> attributes the modern obsession with impact factor and prestige to the founding of <i>Cell</i> in 1974:
</p>

<blockquote>
[<i>Cell</i>] was edited by a young biologist named Ben Lewin, who approached his work with an intense, almost literary bent. Lewin prized long, rigorous papers that answered big questions – often representing years of research that would have yielded multiple papers in other venues – and, breaking with the idea that journals were passive instruments to communicate science, he rejected far more papers than he published….
<br><br>
Suddenly, where you published became immensely important. Other editors took a similarly activist approach in the hopes of replicating <i>Cell</i>’s success. Publishers also adopted a metric called “impact factor,” invented in the 1960s by Eugene Garfield, a librarian and linguist, as a rough calculation of how often papers in a given journal are cited in other papers. For publishers, it became a way to rank and advertise the scientific reach of their products. The new-look journals, with their emphasis on big results, shot to the top of these new rankings, and scientists who published in “high-impact” journals were rewarded with jobs and funding. Almost overnight, a new currency of prestige had been created in the scientific world.
</blockquote>

<p>
As Buranyi reports, the changes induced by <i>Cell</i> rippled across the journal ecosystem. The acceptance rate at <i>Nature</i> <a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>dropped from 35% to 13%</a> over the following decade-and-a-half (coincidentally also the years when peer review was introduced), making journal editors the “kingmakers of science” (Buranyi). 
</p>

<p>
Peer review is also a modern addition. In <a href=https://physicstoday.scitation.org/doi/10.1063/PT.3.3463><i>Physics Today</i></a>, Melissa Baldwin recounts how peer review only became ubiquitous following a series of contentious House subcommittee hearings in 1974 that questioned the value of NSF-funded science:
</p>

<blockquote>
Spending on both basic and applied research had increased dramatically in the 1950s and 1960s—but when doubts began to creep in about the public value of the work that money had funded, scientists were faced with the prospect of losing both public trust and access to research funding. Legislators wanted publicly funded science to be accountable; scientists wanted decisions about science to be left in expert hands. Trusting peer review to ensure that only the best and most essential science received funding seemed a way to split the difference.
</blockquote>

<p>
Our expectation that journals ought to validate the correctness of the work they publish, too, is quite modern. Baldwin again:
</p>

<blockquote>
It also seems significant that refereeing procedures were not initially developed to detect fraud or to ensure the accuracy of scientific claims…. Authors, not referees, were responsible for the contents of their papers. It was not until the 20th century that anyone thought a referee should be responsible for the quality of the scientific literature, and not until the Cold War that something had to be peer-reviewed to be seen as scientifically legitimate.
</blockquote>

<p>
If journals didn’t do peer review and they didn’t do (as much) impact-based filtering before the 1970s, what <i>did</i> they do? The answer is simple: communication. Scientists started communicating in journals because writing books was too slow, and it was important that they be able to share results and get feedback on their ideas quickly. This was a founding aim of <i>Nature</i>:
</p>

<blockquote>
…to aid Scientific men themselves, by giving early information of all advances made in any branch of Natural knowledge throughout the world, and by affording them an opportunity of discussing the various Scientific questions which arise from time to time.
</blockquote>

<p>
Although perhaps underwhelming to a modern audience, this makes sense. Scientists back in the day didn’t have preprints, Twitter, or Zoom—so they published journal articles because it was “one of the fastest ways to bring a scientific issue or idea to their fellow researchers’ attention” (<a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>ref</a>), not because it would look good on their CV. Journals became “the place to discuss big science questions” among researchers, and frequently featured acrimonious and public disputes between researchers—far from celebrated storehouses of truth, journals were simply the social media of pre-communication age scientists. 
</p>

<h2>
3. Journals’ Future?
</h2>

<p>
So, is the solution “reject modernity, embrace tradition”? Should we go back to the way things used to be and stop worrying about whether published articles are correct or impactful?
</p>

<p>
Anyone who’s close to the scientific publishing process knows that this would be ridiculous and suicidal. We’ve come a long way from the intimate scientific community of 18th-century England, where scientists had reputations to uphold and weren’t incentivized to crank out a bunch of <i>Tet. Lett.</i> papers. Like it or not, today’s scientists have been trained to think of their own productivity in terms of publications, and the editorial standards we have today are just barely keeping a sea of low-quality crap at bay (cf. <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>Goodhart’s Law</a>). Sometimes it feels like peer reviewers are the only people who are willing to give me honest criticism about my work—if we get rid of them, what then? 
</p>

<p>
We can understand the changes in journals by borrowing some thinking from economics: as the scale of communities increases, the norms and institutions of the community must progress from informal to formal. This process <a href=https://www.cambridge.org/core/journals/journal-of-economic-history/article/abs/development-of-property-rights-on-frontiers-endowments-norms-and-politics/23F4D5DB23AE6EC415ADF049F6CD0B54>has been documented nicely</a> for the development of property rights on frontiers: at first, land is abundant, and no property rights are necessary. Later on, inhabitants develop a <i>de facto</i> system of informal property rights to mediate disputes—and still later, these <i>de facto</i> property rights are transformed into <i>de jure</i> property rights, raising them to the status of law. Communities with 10,000 people need more formal institutions than communities with 100 people. 
</p>

<p>
If we revisit the history of scientific journals, we can see an analogous process taking place. Centuries ago there were relatively few scientists, and so journals could simply serve as a bulletin board for whatever these scientists were up to. As the scale and scope of science expanded in the late 20th century, peer review became a way to deal with the rising number of scientific publications, sorting the good from the bad and providing feedback. Today, as the scale of science continues to increase and the communication revolution renders many of the historical functions of journals moot, it seems that journals will have to change again, to adapt to the new needs of the community.
</p>

<p>
To the extent that this post has a key prediction, it’s this: <u>scientific journals are going to change a lot in the decade or two to come.</u> If you’re a scientist today—even a relatively venerable one—you’ve lived your whole career in the post-peer review era, and so I think people have gotten used to the status quo. Submitting papers to journals, getting referee reviews, etc are part of what we’re taught “being a scientist” means. But this hasn’t always been true, and it may not be true within your lifetime! 
</p>

<p>
Sadly, I don’t really have a specific high-confidence prediction for how journals will change, or how they should change. Instead, I want to sketch out nine little vignettes of what could happen to journals, good or bad. These options are neither mutually exclusive nor collectively exhaustive; it’s meant simply as an exercise in creativity, and to provide a little basis set with which to imagine the future. 
</p>

<p>
I’ll repost the initial image of the post here, for ambiance, and then walk through the possibilities. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230427_compass.png" style="width:550px;" />
</figure>

<h3>
3.1 The Journal as Bastion of Verified Truth
</h3>

<p>
One scenario is that journals, no longer being needed to distribute results widely, will double down on their role as defenders of scientific correctness. To a much greater degree, journals will focus on only publishing truly correct work, and thus make peer review their key “value add.” This is already being done post-replication crisis in some fields; Michael Nielsen and Kanjun Qiu describe the rise of “Registered Reports” in their <a href=https://scienceplusplus.org/metascience/>essay on metascience</a>:
</p>

<blockquote>
The idea [behind Registered Reports] is for scientists to design their study in advance: exactly what data is to be taken, exactly what analyses are to be run, what questions asked. That study design is then pre-registered publicly, and before data is taken the design is refereed at the journal. The referees can't know whether the results are "interesting", since no data has yet been taken. There are (as yet) no results! Rather, they're looking to see if the design is sound, and if the questions being asked are interesting – which is quite different to whether the answers are interesting! If the paper passes this round of peer review, only then are the experiments done, and the paper completed.
</blockquote>

<p>
This makes more sense for medicine or psychology than it does for more exploratory sciences—if you’re blundering around synthesizing novel low-valent Bi complexes, it’s tough to know what you’ll find or what experiments you’ll want to run! But there are other ways we could make science more rigorous, if we wanted to.
</p>

<p>
A start would be requiring original datafiles (e.g. for NMR spectra) instead of just providing a PDF with images, and having reviewers examine these data. ACS has made some moves in this direction (<a href=https://publish.acs.org/publish/data_policy>e.g.</a>), although to my knowledge no ACS journal yet requires original data. One could also imagine requiring all figures to be linked to the underlying data, with code supplied by the submitting group (like a Jupyter notebook). A more drastic step would be to require all results to be independently reproduced by another research group, <a href=http://www.orgsyn.org/about.aspx>like <i>Organic Syntheses</i> does</a>. 
</p>

<p>
These efforts would certainly make the scientific literature more accurate, but at what cost? Preparing publications already consumes an excessive amount of time and energy, and making peer review stricter might just exacerbate this problem. Marc Edwards and Siddhartha Roy discuss this in a nice <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5206685/>perspective</a> on perverse incentives in modern science:
</p>

<blockquote>
Assuming that the goal of the scientific enterprise is to maximize true scientific progress, a process that overemphasizes quality might require triple or quadruple blinded studies, mandatory replication of results by independent parties, and peer-review of all data and statistics before publication—such a system would minimize mistakes, but would produce very few results due to overcaution.
</blockquote>

<p>
It seems good that there are some “overcautious” journals, like <i>Org. Syn.</i>, but it also seems unlikely that all of science will adopt this model. In fact, a move in this direction might create a two-tiered system: some journals would adopt stringent policies, but there’s a huge incentive for some journals to defect and avoid these policies, since authors are lazy and would prefer not to do extra work. It seems unlikely that all of science could realistically be moved to a “bastion of truth” model in the near future, although perhaps we could push the needle in that direction.
</p>

<h3>
3.2 The Journal as Guild-Approved Periodical
</h3>

<p>
If peer review is so vital, why not make it a real career? Imagine a world in which journals like <i>Nature</i> and <i>Science</i> have their own in-house experts, recruited to serve as professional overseers and custodians of science. Instead of your manuscript getting sent to some random editor, and thence to whomever deigns to respond to that editor’s request for reviewers, your manuscript would be scrutinized by a team of hand-picked domain specialists. This would certainly cost money, but journals seem to have a bit of extra cash to spare.
</p>

<p>
I call this scenario the “guild-approved periodical” because the professionals who determined which papers got published would essentially be managers, or leaders, of science—they would have a good amount of power over other scientists, to a degree that seems uncommon today. Thus, this model would amount to a centralization of science: if <i>Nature</i> says you have to do genomics a certain way, you have to do it that way or <i>Nature</i> journals won’t publish your work! I’m not sure whether this would be good or bad. 
</p>

<p>
(It is a little funny that the editors of high-tier journals—arguably the most powerful people in their field—are chosen without the knowledge or consent of the field, through processes that are completely opaque to rank-and-file scientists. To the extent that this proposal allows scientists to choose their own governance, it might be good.)	
</p>

<h3>
3.3 The Journal as Living Knowledge Aggregator
</h3>

<p>
This scenario envisions a world in which “publications” are freed from the tyranny of needing to be complete at a certain point. While that was true in the days when you actually got a published physical issue in the mail, it’s not necessary in the Internet age! Instead, one can imagine a dynamic process of publishing, where a journal article is continually updated in response to new data. 
</p>

<p>
A 2020 article in <a href=https://www.facetsjournal.com/doi/10.1139/facets-2019-0012><i>FACETS</i></a> proposes exactly this model:
</p>

<blockquote>
The paper of the future may be a digital-only document that is not only discussed openly after the peer-review process but also regularly updated with time-stamped versions and complementary research by different authors… Living systematic reviews are another valuable way to keep research up to date in rapidly evolving fields. The papers of the future that take the form of living reviews can help our understanding of a topic keep pace with the research but will also add complexities. <i>(citations removed from passage)</i>
</blockquote>

<p>
The idea of the living systematic review is being tried out by the <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031><i>Living Journal of Computational Molecular Science</i></a>, which (among other things) has published a 60-page <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031>review of enhanced sampling methods in MD</a>, which will continue being updated as the field evolves.
</p>

<p>
These ideas are cool, but I wonder what would happen if more research became “living.” Disputes and acrimony are part of the collective process of scientific truth-seeking. What will happen if bitter rivals start working on the same “living” publications—who will adjudicate their disputes? 
</p>

<p>
Wikipedia manages to solve this problem through a plethora of editors, who can even lock down particularly controversial pages, and perhaps editors of living journals will assume analogous roles. But the ability of our collective scientific intelligence to simultaneously believe contradictory ideas seems like a virtue, not a vice, and I worry that living journals will squash this. 
</p>

<p>
An even thornier question is who adjudicates questions of impact. The enhanced sampling review linked above has over 400 references, making it a formidable tome for a non-expert like myself. There’s a lot of merit in a non-comprehensive and opinionated introduction to the field, which takes some subjective editorial liberties, but it’s not clear to me how that would work in a collaborative living journal. What’s to stop me from linking to my own papers everywhere? 
</p>

<p>
(I’m sure that there are clever organizational and administrative solutions to these problems; I just don’t know what they are.)
</p>

<h3>
3.4 The Journal as Curated Scientific Vision
</h3>

<p>
If “objective impact” is so hard to determine fairly, why not just accept that we’re basically just subjectively scoring publications based on how much we like them, and abandon the pretense of objectivity? One can imagine the rise of a new kind of figure: the editor with authorial license, who has a specific vision for what they think science should look like and publishes work in keeping with that vision. The role is as much aesthetic as it is analytic. 
</p>

<p>
There’s some historical precedent for this idea—Eric Gilliam’s written about how Warren Weaver, a grant director for the Rockefeller Foundation, essentially <a href=https://freaktakes.substack.com/p/a-report-on-scientific-branch-creation>created the field of molecular biology</a> <i>ex nihilo</i> by following an opinionated thesis about what work ought to be funded. Likewise, one can envision starting a journal as an act of community-building, essentially creating a Schelling point for like-minded scientists to collaborate, share results, and develop a common approach to science.
</p>

<p>
We can see hints of this today: newsletters like <a href=https://pubs.acs.org/doi/10.1021/acs.oprd.3c00060>“Some Items of Interest to Process Chemists”</a> or Elliot Hershberg’s <a href=https://centuryofbio.substack.com/about><i>Century of Bio</i> Substack</a> highlight a particular vision of science, although they haven’t quite advanced to the stage of formally publishing papers themselves. But perhaps it will happen soon; new movements, like molecular editing or digital chemistry, might benefit from forming more tightly-knit communities. 
</p>

<h3>
3.5 The Journal as Post Hoc Impact Archive
</h3>

<p>
If preprints take over every field of science as thoroughly as they have computer science, journals may find themselves almost completely divorced from the day-to-day practice of science, for better or for worse. Papers might still be submitted to journals, and the status of the journal might still mean something, but it wouldn’t be a guess anymore—journals could simply accept the advances already proven to be impactful and basically just publish a nicely formatted “version of record,” like a scientific Library of Congress. 
</p>

<p>
This is essentially equivalent to the “publish first, curate second” proposal of <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000116>Stern and O’Shea</a>—preprints eliminate the need for journals to move quickly, so we can just see what work the community finds to be best and collect that into journals. The value of journals for specialists, who already need to be reading a large fraction of the papers in their area, would be much lower—journals would mainly be summarizing a field’s achievements for those out-of-field. In this scenario, “many specialized journals that currently curate a large fraction of the literature will become obsolete.”
</p>

<p>
(This already happens sometimes; I remember chuckling at the 2020 Numpy <i>Nature</i> <a href=https://www.nature.com/articles/s41586-020-2649-2>paper</a>. Numpy isn’t successful because it was published in <i>Nature</i>; Numpy got into <i>Nature</i> because it was already successful.)
</p>

<h3>
3.6 The Journal as Antediluvian Status Symbol
</h3>

<p>
Pessimistically, one can imagine a world in which journal publications still carry weight with the “old guard” and certain sentimental types, but the scientific community has almost completely moved to preprints for day-to-day communication. In this scenario, one might still have to publish journal articles to get a job, but it’s just a formality, like a dissertation: the active work of science is done through preprints. Like Blockbuster, journals might limp along for some time, but their fate is pretty much sealed.
</p>

<h3>
3.7 The Journal as Philanthropic Pravda
</h3>

<p>
Another reason why journals might persist in a world driven by preprints is the desire of philanthropic agencies to appear beneficent. If a certain organization, public or private, is giving tens of millions of dollars to support scientific progress, the only real reward it can reap in the short term is the prestige of having its name associated with a given discovery. Why not go one step further and control the means of publication?
</p>

<p>
In this <i>Infinite Jest</i>-like vision, funding a certain project buys you the right to publish its results in your own journal. We can imagine <i>J. Pfizer-Funded Research</i> and <i>J. Navy Research</i> competing to fund and publish the most exciting work in a given area, since no one wants to sponsor a loser. (Why stop there? Why not name things after corporate sponsors? We could have the Red Bull–Higgs Boson, or the Wittig–Genentech olefination.)
</p>

<p>
As discussed at the beginning of this article, the government “funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product.” There’s a certain simplicity in a funding agency just taking over the whole process, but  I doubt this would be good for scientists. Unifying the roles of funder, publisher, and editor would probably lower the agency of actual researchers to an untenably low level. 
</p>

<h3>
3.8 The Journal as Rent-Seeking Data Troll
</h3>

<p>
Another depressing scenario is one in which journals cease contributing to the positive progress of science, and start essentially just trying to monetize their existing intellectual property. As ML and AI become more important, legal ownership of data rights will presumably increase in economic value, and one can easily imagine the Elseviers of the world vacuuming up any smaller journals they can and then charging exorbitant fees for access to their data. (Goodbye, Reaxys…)
</p>

<p>
I hope this doesn’t happen. 
</p>

<h3>
3.9 No Journals; Just an Anarchic Preprint Lake
</h3>

<p>
The obvious alternative to these increasingly far-fetched scenarios is also the simplest; we get rid of journals all together, and—just like in the 1700s—rely solely on communication-style preprints on arXiv, bioRxiv, ChemRxiv, etc. This has been termed a “preprint lake,” in analogy to <a href=https://en.wikipedia.org/wiki/Data_lake>“data lakes.”</a> 
</p>

<p>
To help scientists make sense of the lake, one can envision some sort of preprint aggregator: a Reddit or Hacker News for science, which sorts papers by audience feedback and permits <a href=https://en.wikipedia.org/wiki/PubPeer>PubPeer</a>-type public comments on the merits and shortcomings of each publication. The home page of Reddit-for-papers could serve as the equivalent to <i>Science</i>; the chemistry-specific subpage, the equivalent to <i>JACS</i>. Peer review could happen in a decentralized fashion, and reviews would be public for all to see.
</p>

<p>
There’s an anarchic appeal to this proposal, but it has potential drawbacks too:
</p>

<ol type="i">
  <li>
  For those not immersed in a given field, it’s very difficult to know what’s good research and what isn’t. This is doubly true for non-scientists—what will become of high-school students trying to write papers?
  </li>

  <li>
    Existing status symbols will become more important absent journal status. To quote <a href=https://luispedro.substack.com/p/against-publication-lakes-glam-journals?s=03>Luis Pedro Coelho</a>:

    <blockquote>
    I mostly read preprints by people whose names I already recognize. When thousands of papers are thrown into the “level playing field” of biorxiv, pre-existing markers of prestige end up taking an even greater role.
    </blockquote>

    This presumably will disadvantage up-and-coming scientists, or scientists without access to existing networks of prestige. That being said, one might make the same arguments for the Internet, and the real effect seems to have been exactly the opposite! So I’m not quite sure how to think about this.
  </li>

  <li>
    What “goes viral” may not always be what’s the best science. Rarely do thoughtful or contemplative ideas rise to popularity out of the unstructured morass of the Internet, and I find it naïve to expect that scientists would be any different here. That being said, the wisdom of crowds might be the lesser of two evils, given that our current system is basically “ask three random rivals in the field.”
  </li>

  <li>
    There are also just so many papers out there today that it might just become overwhelming, even to specialists. I miss papers in “my areas” constantly, and I try pretty hard to keep up with the literature! (Some have proposed that <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/leap.1514>AI might help us sift through things</a>, but AI might also help people write more papers faster—tough to say who will win.)
  </li>

  <li>
Without the implicit threat of peer review, standards might ebb across the board. I think this is a real concern, but it’s possible that the same community norms enforced through today’s peer review might also be enforced through whatever decentralized review process replaces it. <a href=https://link.springer.com/article/10.1007/s10657-013-9420-1>There’s some evidence</a> that, in knowledge industries with high information asymmetry (like science), communities tend to spontaneously develop strong systems of self-regulation.
  </li>
</ol>

<h2>
4. Conclusion: Archipelagic Multiverse
</h2>

<p>
The most likely scenario, to me, is that all of this sorta happens simultaneously. Most cutting-edge scientific discussion will move to the anarchic world of preprints, but there will still be plenty of room for more traditional journals: some journals will have very high standards and represent the <a href=https://www.palladiummag.com/2022/10/10/the-transformations-of-science/>magisterium of scientific authority</a>, while other journals will act as living repositories of knowledge and still others will become subjectively curated editorial statements. 
</p>

<p>
We can see journals moving in different directions even today: some journals are indicating that they’ll start requiring original data and implement more aggressive fraud detection, while others are moving away from impact-based reviewing. And I can’t help but notice that it seems to be increasingly acceptable to cite preprints in publications, suggesting that the needle might be moving towards the “anarchic preprint lake” scenario ever so slightly. 
</p>

<p>
For my part, I plan to continue writing and submitting papers as necessary, reviewing papers when asked, and so forth—but I’m excited for the future, and to see how the new world order compares to the old. 
</p>

<i>
Thanks to Melanie Blackburn, Jonathan Wong, Joe Gair, and my wife for helpful discussions, and Ari Wagen, Taylor Wagen, and Eugene Kwan for reading drafts of this piece.
</i>

]]></description>
              <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Dimensionality Reduction in Cheminformatics</title>
              <link>public/blog/20230417_dimensionality_reduction.html</link>
              <description><![CDATA[
<p>
In many applications, including cheminformatics, it’s common to have datasets that have too many dimensions to analyze conveniently. For instance, <a href=https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf>chemical fingerprints</a> are typically 2048-length binary vectors, meaning that “chemical space” as encoded by fingerprints is 2048-dimensional. 
</p>

<p>
To more easily handle these complex datasets (and to bypass the <a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality>“curse of dimensionality”</a>), it’s common practice to use a dimensionality reduction algorithm to convert the data to a low-dimensional space. In this post I want to compare and contrast three approaches to dimensionality reduction, and discuss the challenges with low-dimensional embeddings in general.
</p>

<h2>
Dimensionality Reduction Algorithms
</h2>

<p>
There are many approaches to dimensionality reduction, but I’m only going to talk about three here: PCA, tSNE, and UMAP.
</p>

<p>
<a href=https://en.wikipedia.org/wiki/Principal_component_analysis>Principal component analysis</a> (PCA) is perhaps the most famous dimensionality reduction algorithm, and is commonly used in a variety of scientific fields. PCA works by transforming the data into a new set of coordinates such that the first coordinate vector explains the largest amount of the variance, the second coordinate vector the next most variance, and so on and so forth. It’s pretty common for the first 5–20 dimensions to capture &gt;99% of the variance, meaning that the subsequent dimensions can essentially be discarded wholesale. 
</p>

<p>
<a href=https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding>tSNE</a> (t-distributed stochastic neighbor embedding) and <a href=https://umap-learn.readthedocs.io/en/latest/how_umap_works.html>UMAP</a> (uniform manifold approximation and projection) are alternative dimensionality reduction approaches, based on much more complex algorithms. To quote Wikipedia: 
</p>

<blockquote>
The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map.
</blockquote>

<p>
UMAP, at a high level, works in a very similar way, but uses some fancy topology to construct a “fuzzy simplicial complex” representation of the data in high-dimensional space, and then projects this representation down into a lower dimension (<a href=https://pair-code.github.io/understanding-umap/>more detailed explanation</a>). Practically, UMAP is a lot faster than tSNE, and is becoming the algorithm of choice for most cheminformatics applications. (Although, in fairness, there are <a href=https://arxiv.org/abs/1712.09005>ways to make tSNE faster</a>.)
</p>

<h2>
Data Visualization 
</h2>

<p>
For the purposes of this post, I chose to study Abbie Doyle’s set of <a href=https://pubs.acs.org/doi/10.1021/jacs.1c12203>2683 aryl bromides</a> (obtained from Reaxys, with various filters applied). I used the RDKIT7 fingerprint to generate a 2048-bit encoding of each aryl bromide, computed a distance matrix using Tanimoto/Jaccard distance, and then used each dimensionality reduction technique to generate a 2-dimensional embedding.
</p>

<p>
Let’s look at PCA first:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_pca_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using PCA.
  </figcaption>
</figure>

<p>
PCA generally creates fuzzy-looking blobs, which sometimes show some amount of meaningful structure but don’t really display many sharp boundaries.
</p>

<p>
Now, let’s compare to tSNE:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_tsne_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using tSNE (perplexity 20).
  </figcaption>
</figure>

<p>
tSNE creates “blob-of-blob” plots which show many tight clusters arranged together in some sort of vague pattern. The size and position of the clusters can be tuned by changing the “perplexity” hyperparameter (see <a href=https://stats.stackexchange.com/questions/399868/why-does-larger-perplexity-tend-to-produce-clearer-clusters-in-t-sne>this StackOverflow post</a> for more discussion, and <a href=https://distill.pub/2016/misread-tsne/?s=03>this excellent post</a> for demonstrations of how tSNE can be misleading).
</p>

<p>
What about UMAP?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_umap_plot.png" style="width:400px;" />
  <figcaption> 
  2D plot of aryl bromide chemical space using UMAP (30 neighbors, 0.1 minimum distance).
  </figcaption>
</figure>

<p>
UMAP also creates tight tSNE-like clusters, but UMAP plots generally have a much more variable overall shape—the clusters themselves are tighter and scattered across more space. (These considerations are complicated by the fact that UMAP has multiple tunable hyperparameters, meaning that the exact appearance of the plot is substantially up to the end user.)
</p>

<p>
The debate between tSNE and UMAP is spirited (<a href=https://www.biorxiv.org/content/10.1101/2019.12.19.877522v1.full.pdf>e.g.</a>), but for whatever reason people in chemistry almost exclusively use UMAP. (See, for instance, pretty much every paper I taked about <a href=https://corinwagen.github.io/public/blog/20230118_meta_optimization.html>in this post</a>.)
</p>

<p>
An important thing that I’m not showing here, but which bears mentioning, is that the clusters in all three plots are actually chemically meaningful. For instance, each cluster in the tSNE plot generally corresponds to a different functional group: carboxylic acids, alkynes, etc. So the graphs do in some real sense correspond to the intuition we have about molecular similarity, which is good! (You can use <a href=https://github.com/wjm41/molplotly>molplotly</a> to visualize these plots very easily.)
</p>

<h2>
Distance Preservation
</h2>

<p>
How well are distances from the high-dimensional space preserved in the 2D embedding? Obviously the distances won’t all be the same, but ideally the mapping would be monotonic: if distance A is greater than distance B in the high-dimensional space, we would like distance A to also be greater than distance B in the low-dimensional space. 
</p>

<p>
We can measure this with <a href=https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>Spearman correlation</a>, which is like a Pearson correlation (AKA “r-squared”) but without the assumption of linearity. A Spearman correlation coefficient of 1 indicates a perfect monotonic relationship, while a coefficient of 0 indicates no relationship. Let’s plot the pairwise distances from each embedding against the true distances and compare the Spearman coefficients:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_spearman_plots.png" style="width:300px;" />
  <figcaption> 
  Comparison of distances in high-dimensional space against distances in embedding space, and associated Spearman coefficients. (Only one in every hundred points is plotted, but all points are used for the Spearman coefficient calculation.)
  </figcaption>
</figure>

<p>
In each case, the trend is in the right direction (i.e. increased distance in high-dimensional space is correlated with increased distance in low-dimensional space), but the relationship is far from monotonic. It’s clear that there will be plenty of cases where two points will be close in low-dimensional space and far in high-dimensional space. 
</p>

<p>
Does this mean that UMAP, tSNE, and PCA are all failing? To understand this better, let’s plot a histogram of all the distances in each space:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_dist_hist.png" style="width:400px;" />
  <figcaption> 
  Histogram of all distances in each space. Distances have been scaled to the range [0,1] to match distances obtained with the Jaccard metric.
  </figcaption>
</figure>

<p>
We can see that the 2048-dimensional space has a very distinct histogram. Most of the compounds are pretty different from one another, and—crucially—most of the distances are about the same (0.8 or so). In chemical terms, this means that most of the fingerprints share a few epitopes in common, but otherwise are substantially different, which is unsurprising since fingerprints in general are quite sparse. 
</p>

<p>
Unfortunately, “lots of equidistant points” is an extremely tough pattern to recapitulate in a low-dimensional space. We can see why with a toy example: in 2D space, we can only have 3 equidistant points (an equilateral triangle), and in 3D space, we can only have 4 equidistant points (a tetrahedron). More generally, if we want <i>N</i> equidistant points, we need to be in <b>R</b><sup><i>N</i>-1</sup> (<i>N</i>-1 dimensional Euclidean space). We can <a href=https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma>relax this requirement</a> a little bit if we’re willing to accept approximate equidistance, but the general principle still holds: it’s hard to recapitulate lots of equidistant points in a low-dimensional space. 
</p>

<p>
As expected, then, we can see that the histogram of each of our algorithms looks very different from the ideal distance histogram.
</p>

<h2>
Local Structure
</h2>

<p>
Both tSNE and UMAP take the nearest neighbors of each point explicitly into account, and claim to preserve the local structure of the points as much as possible. To put these claims to the test, I looked at the closest 30 neighbors of each point in high-dimensional space, and then checked how many of those neighbors made it into the closest 30 neighbors in low-dimensional space. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_neighbor_hist.png" style="width:400px;" />
  <figcaption> 
  Histogram of how many of the closest 30 neighbors of each point are recapitulated after dimensionality reduction.
  </figcaption>
</figure>

<p>
We can see that PCA only preserves about 30–40% of each point’s neighbors, whereas PCA and UMAP generally preserve 60% of the neighbors: not perfect, but much better.
</p>

<p>
I chose to look at 30 neighbors somewhat arbitrarily: what happens if we change this number?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_neighborhood_scan.png" style="width:400px;" />
  <figcaption> 
  The percent of neighbors recapitulated correctly, as neighborhood size increases.
  </figcaption>
</figure>

<p>
We can see that UMAP and tSNE both preserve about 60% of the neighbors across a wide range of neighborhood sizes, while PCA gets better as we zoom out more. (At the limit where we consider all 2683 points as neighbors, every method will trivially achieve perfect accuracy.) tSNE does much better than UMAP for small neighborhoods; I’m not sure why!
</p>

<p>
Another way to think about this is in terms of the <a href=https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html>precision–recall tradeoff</a>. In classification, “precision” refers to a classifier’s ability to avoid false positives, while “recall” refers to a classifier’s ability to avoid false negatives. What does this mean in the context of embedding? 
</p>

<p>
Imagine looking at all points in the neighborhood of our central point in high-dimensional space, and then comparing to the points within a certain radius of our point in low-dimensional space. As we increase the radius, we expect to see more of the correct neighbor points in low-dimensional space, but we also expect to see more “incorrect neighbors” that aren’t really there in the high-dimensional space. (<a href=https://jmlr.org/papers/volume11/venna10a/venna10a.pdf>This paper</a> discusses these issues nicely, as does <a href=https://coursepages2.tuni.fi/mttts17/wp-content/uploads/sites/136/2020/03/drv_2020_lecture_7.pdf>this presentation</a>.)
</p>

<p>
So low radii lead to high precision (most of the points are really neighbors) but low recall (we’re not finding most of the neighbors), while high radii lead to low precision and high recall. We can thus study the performance of our embedding by graphing the precision–recall curve for various neighborhood sizes. The better the embedding, the closer the curve will come to the top right:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_precision_recall.png" style="width:400px;" />
  <figcaption> 
  Precision–recall tradeoff for all three methods.
  </figcaption>
</figure>

<p>
We can see that tSNE does better in the high precision/low recall area of the curve (as we saw in the previous graph), but otherwise tSNE and UMAP are quite comparable. In contrast, PCA is just abysmal.
</p>

<p>
The big conclusion of this section is that, if you’re doing something that depends on the local structure of the data, you should avoid PCA. 
</p>

<h2>
Do Higher Dimensions Help Things? 
</h2>

<p>
Since the root of our issues here is trying to represent a 2048-dimensional distance matrix in 2 dimensions, one might wonder if we could do better by expanding to 3, 4, or more dimensions. This would make visualization tricky, but might still be suitable for other operations (like clustering). 
</p>

<p>
tSNE gets very, very slow in higher dimensions, so I focused on PCA and UMAP for this study. I started out by comparing the Spearman correlation for PCA and UMAP up to 20 dimensions:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_spearman_dim.png" style="width:400px;" />
  <figcaption> 
  Precision–recall tradeoff for all three methods.
  </figcaption>
</figure>

<p>
Surprisingly, UMAP doesn’t seem to get any better in high dimensions, but PCA does. (Changing the number of neighbors didn’t help UMAP at all.)
</p>

<p>
How do our other metrics look with high-dimensional PCA?
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_dist_hist.png" style="width:400px;" />
  <figcaption> 
  Distance histogram for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
As we increase the number of dimensions, the distance histogram starts to approach the correct distribution. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_neighbor_hist.png" style="width:400px;" />
  <figcaption> 
  Neighbor histogram for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
We also start to do a better job capturing the local structure of the graph, although we’re still not as good as tSNE or UMAP even at 10 dimensions. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230417_10dim_precision_recall.png" style="width:400px;" />
  <figcaption> 
  Precision–recall curve for PCA with 2–10 dimensions.
  </figcaption>
</figure>

<p>
And our precision–recall graph is still pretty dismal when compared to tSNE or UMAP. So, it seems like if distances are what matters, then high-dimensional PCA is an appealing choice—but if local structure is what matters, tSNE or UMAP is still superior.
</p>

<h2>
Conclusions
</h2>

<p>
My big takeaway from all of this is: dimensionality reduction is a lossy process, and one where you always have to make tradeoffs. You’re fundamentally throwing away information, and that always has a cost: there’s no such thing as a free lunch. As such, if you don’t have to perform dimensionality reduction, then my inclination would be to avoid it. (People in single-cell genomics seem to have come to <a href=https://www.biorxiv.org/content/10.1101/2021.08.25.457696v4.full.pdf>a similar conclusion</a>.)
</p>

<p>
If you really need your data to be in a low-dimensional space (e.g. for plotting), then keep in mind what you’re trying to study! PCA seems to do a slightly better job with distances (although I’m sure there are more sophisticated strategies for distance-preserving dimensionality reduction), while tSNE and UMAP seem to do much, much better with local structure. 
</p>

<i>
Thanks to Michael Tartre for helpful conversations, and the students in Carnegie Mellon’s “Digital Molecular Design Studio” class for their thought-provoking questions on these topics.  
</i>

]]></description>
              <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>New Ways To Read The Blog: RSS and Substack</title>
              <link>public/blog/20230413_new_ways.html</link>
              <description><![CDATA[
<p>
<i>
(This is more of a housekeeping post than an actual post with content; apologies.)
</i>
</p>

<p>
Up until now, my blogging strategy has been to write new posts about once a week and publicize them on Twitter, which works great for people who are on Twitter but (obviously) fails for people who aren’t on Twitter. I’m frequently asked if there are non-Twitter ways to subscribe to the blog updates: given that I myself don’t love relying on Twitter to bring me content, and that Twitter itself feels increasingly dicey, I feel bad saying no every time.
</p>

<p>
I’m happy to announce that there are now two additional ways to read the blog: RSS and Substack. 
</p>

<h2>
RSS
</h2>

<p>
RSS is a lovely way to get updates from sites, which is sadly limited by the fact that nobody uses it anymore. (Half the people I talk to these days don’t even know what it is.) You can use an RSS aggregator like <a href=https://feedly.com/>Feedly</a>, and simply subscribe to various sites, so that they’ll dependably show up in your feed. This is the main way I get <a href=https://corinwagen.github.io/public/blog/20230329_literature.html>journal updates</a> and my news. 
</p>

<p>
So, if you like using RSS, you can simply search “corinwagen.github.io” in Feedly, and the blog will come up:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230413_feedly.jpg" style="width:300px;" />
  <figcaption> 
  What it looks like on Feedly. The Twitter preview images sadly don't display.
  </figcaption>
</figure>

<h2>
Substack
</h2>

<p>
Substack is a platform that helps people write and manage newsletters. It essentially solves the problem of “how do I create an email list”/“how do I manage subscriptions” for people who would rather not take care of hosting a web service and handling payments themselves, like me.
</p>

<p>
I initially didn’t want to use Substack because (1) I wanted the blog to be part of my website, (2) I liked being able to control every aspect of the design, and (3) I wasn’t sure if anyone would read the blog, and there’s nothing sadder than an empty Substack. As things stand, (3) is a non-issue, so the question is whether the added convenience of Substack outweighs my own personal design and website preferences.
I suspect that it may, so I’ve capitulated and copied all existing posts over to <a href=https://cwagen.substack.com>my new Substack</a>. (There are a few formatting issues in old posts, but otherwise things copied pretty well.)
</p>

<p>
For now, I plan to continue posting everything on the blog, and manually copying each post over to Substack (I write in plain HTML so this is not too hard). If Substack ends up totally outperforming the blog in terms of views, then I’ll probably switch to Substack entirely for blogging and just leave my website up as a sort of virtual CV. 
</p>

<p>
(I have no plans to enable subscriptions at this point; that being said, if for some bizarre reason there’s sufficient demand I’ll probably try to think of something to reward subscribers.)
</p>

<p>
If you’d like to receive updates on Substack, you can subscribe below: 
</p>

<br>
<iframe src="https://cwagen.substack.com/embed" width="480" height="320" style="display:block; border:1px solid #EEE; background:white; margin:auto;" frameborder="0" scrolling="no"></iframe>

]]></description>
              <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Why New Ventures Are So Important</title>
              <link>public/blog/20230411_newcomers.html</link>
              <description><![CDATA[
<p>
This Easter week, I’ve been thinking about why new ventures are so important. Whether in private industry, where startups are the most consistent source of innovative ideas, or in academia, where new assistant professors are hired each year, newcomers are often the most consistent source of innovation. Why is this?
</p>

<p>
One explanation is the <a href=https://www.nber.org/system/files/chapters/c2144/c2144.pdf>Arrow replacement effect</a> (named after <a href=https://en.wikipedia.org/wiki/Kenneth_Arrow>Kenneth Arrow</a>), which states that “preinvention monopoly power acts as a strong disincentive to further innovation.” Arrow’s argument goes like this: suppose there’s an organization that is earning profit <i>P<sub>old</sub></i>, and there is some innovation that will increase profit to <i>P<sub>new</sub></i> (<i>P<sub>new</sub></i> &gt; <i>P<sub>old</sub></i>). If the existing organization pursues the innovation, their profits will thus increase by <i>∆P</i> := <i>P<sub>new</sub></i> - <i>P<sub>old</sub></i>. But a new organization will see its profits increase by <i>P<sub>new</sub></i>: since the startup has no existing profit to replace, the rewards to innovation are higher. Thus innovation is more appealing for those without any economic stake in the status quo.<sup><a href="#fn1">1</a></sup>
</p>

<p>
We can see this play out today in the dynamic between Google and OpenAI/Microsoft: Google already has a virtual monopoly in search, and so is hesitant to replace what they have, whereas Microsoft has already been losing in search and so is eager to replace Bing with <s>Sydney</s> an AI-powered alternative. (It’s to Apple’s credit that they so eagerly pursued the iPhone when it meant effectively destroying the iPod, one of their top money-makers.<sup><a href="#fn2">2</a></sup>)
</p>

<p>
One can also see this scenario in academia—plenty of established labs have programs built up around studying specific systems, and are thus disincentivized to study areas which might obviate projects they’ve spent decades working on. For instance, labs dedicated to “conventional” synthetic methodology might be slower to turn to biocatalysis than a new assistant professor with nothing to lose; labs that have spent decades studying protein folding might be slower to turn to AlphaFold than they ought to.
</p>

<p>
Another reason is that new entrants often have an advantage in understanding the status quo. In <i>The Art of Doing Science and Engineering</i> (book review coming, eventually), computing legend <a href=https://en.wikipedia.org/wiki/Richard_Hamming>Richard Hamming</a> discusses how there’s often a disadvantage to being a pioneer in a field. Hamming’s argument, essentially, is that those who’ve had to invent something new never understand it as intuitively as those who have simply learned to take it for granted:
</p>

<blockquote>
The reason this happens is that the creators have to fight through so many dark difficulties, and wade through so much misunderstanding and confusion, they cannot see the light as others can, now the door is open and the path made easy…. in time the same will probably be true of you.
</blockquote>

<p>
In Hamming’s view, it’s the latecomers to a field who can see more clearly the new possibilities opened by various innovations, and take the next steps towards previously unimaginable frontiers. There’s a deep irony in this: the very act of inventing something new makes you less able to see the innovations enabled by your own work. The process of invention thus acts like a relay race, where newer generations continually take the baton and push things forward before in turn dropping back.
</p>

<p>
I’ve heard these ideas discussed in terms of naïvete before—the idea being that innovation requires a sort of “beginner’s luck,” a blind optimism about what’s possible that the experienced lack—but I think that’s wrong. A belief in naïvete as the key driver of innovation implies that excessive knowledge is detrimental: that it’s possible to “know too much” and cripple oneself. If anything, the opposite is true in my experience. The most creative and productive people I’ve met are those with an utter mastery of the knowledge in their domain.
</p>

<p>
Hamming’s proposal, which is more cognitive/subconscious, is thus complementary to the more calculated logic of the Arrow replacement theorem: existing organizations are both less incentivized to innovate and less able to see potential innovations. These ideas should be encouraging to anyone at the beginning of their career: you are uniquely poised to discover and exploit new opportunities! So consider this an exhortation to go out and do so now (rather than waiting until you are older and more secure in your field).
</p>

<i>Credit to <a href="https://marginalrevolution.com/marginalrevolution/2023/04/the-arrow-replacement-effect-and-the-dynamics-of-us-inventors.html?utm_source=feedly&utm_medium=rss&utm_campaign=the-arrow-replacement-effect-and-the-dynamics-of-us-inventors">Alex Tabarrok</a> for introducing me to the Arrow replacement effect, and ChatGPT for some edits.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  This is a bit of a cartoonish depiction of the Arrow replacement theorem—the original paper (linked above) is quite readable, and performs a more sophisticated analysis. See the heading “Competition, Monopoly, and the Incentive to Innovate” on page 12 of the PDF (journal page 619).
  </li>
  <li id="fn2">
  Tony Fadell discusses this in <a href=https://www.amazon.com/Build-Unorthodox-Guide-Making-Things/dp/0063046067><i>Build</i></a>: suffice it to say this was not an internally popular decision at Apple.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Industry Research Seems Underrated</title>
              <link>public/blog/20230403_industry.html</link>
              <description><![CDATA[
<p>
While scientific companies frequently publish their research in academic journals, it seems broadly true that publication is not incentivized for companies the same way it is for academic groups. Professors need publications to get tenure, graduate students need publications to graduate, postdocs need publications to get jobs, and research groups need publications to win grants. So the incentives of everyone in the academic system are aligned towards publishing papers, and lots of papers get published.
</p>

<p>
In contrast, the success or failure of a private company is—to a first approximation—unrelated to its publication record. Indeed, publication might even be harmful for companies, insofar as time spent preparing manuscripts and acquiring data <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>only needed for publication</a> is time that could be spent on more mission-critical activities.
</p>

<p>
That’s why I generally believe industry publications, especially those where no academic co-authors are involved, are underrated, and are probably better than the journal they’re in might indicate. Getting a publication into a prestigious journal like <i>Science</i> or <i>Nature</i> is pretty random, requires a lot of effort, and frequently has a slow turnaround time, whereas lower-tier journals are likely to accept your work, and typically review and publish papers much, much faster. (In particular, ACS is <a href=https://scholarlykitchen.sspnet.org/2022/11/08/guest-post-publishing-fast-and-slow-a-review-of-publishing-speed-in-the-last-decade/>among the fastest of all scientific publishers</a>, and is generally a pleasure to work with.)
</p>

<p>
The above reflections were prompted by reading <a href=https://pubs.acs.org/doi/full/10.1021/acs.jmedchem.0c00452>an absolute gem of a paper</a> in <i>J. Med. Chem.</i>, a collaboration between X-Chem, ZebiAI, and Google Research. The paper is entitled “Machine Learning on DNA-Encoded Libraries: A New Paradigm for Hit Finding” and describes how data from DNA-encoded libraries (DELs) can be used to train ML models to predict commercially available compounds with activity against a given target. This is a really, really big deal. As the authors put it in their conclusion:
</p>

<blockquote>
[Our approach] avoids the time-consuming and expensive process of building new chemical matter into a DEL library and performing new selections or incorporating new molecules into a HTS screening library. This ability to consider compounds outside of the DEL is the biggest advantage of our approach; notably, this approach can be used at a fraction of the cost of a traditional DEL screening follow-up, driven primarily by the large difference in synthesis cost.
</blockquote>

<p>
Now, the precise impact of this discovery will of course be determined in the years to come; Derek Lowe raises some fair concerns <a href=https://www.science.org/content/blog-post/machine-learning-top-dna-encoded-libraries>on his blog</a>, pointing out that the targets chosen are relatively easy to drug, and so probably wouldn’t be the subject of a high-tech DEL screen anyway, and it’s entirely possible that there will be other unforeseen complications with this technology that are only revealed in the context of a real-world discovery pipeline. (Given that Relay <a href=https://www.biopharmadive.com/news/relay-acquire-zebiai-ai-drug-discovery/598550/>acquired ZebiAI</a> for $85M in 2021 essentially on the strength of this paper alone, I’m guessing plenty of real-world testing is already underway.)
</p>

<p>
The point I want to make is that if this paper had come from an academic group, I would be very, very surprised to see it in <i>J. Med Chem</i>. This project has everything that one expects in a <i>Science</i> paper: a flashy new piece of technology, a problem that’s understandable to a broad audience, clear clincal relevance, even a domain arbitrage angle. Yet this paper is not in <i>Science</i>, nor <i>ACS Central Science</i>, nor even <i>JACS</i>, but in <i>J. Med. Chem.</i>, a journal I don’t even read regularly.
</p>

<p>
My conclusions from this are (1) to remember that not everyone is incentivized to market their own findings as strongly as academics are and (2) to try and look out for less-hyped industry results that I might neglect otherwise.
</p>

]]></description>
              <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>You Should Read The Literature More</title>
              <link>public/blog/20230329_literature.html</link>
              <description><![CDATA[
<p>
If you are a scientist, odds are you should be reading the literature more. This might not be true in every case—one can certainly imagine someone who reads the literature too much and never does any actual work—but as a heuristic, my experience has been that most people would benefit from reading more than they do, and often much more. Despite the somewhat aggressive title, my hope in writing this is to encourage you to read the literature more: to make you excited about reading the literature, not to guilt you into it or provoke anxiety.
<p>

<h2>
Why You Should Read The Literature More
</h2>

<p>
You should read the literature because you are a scientist, and your business is ideas. The literature is the vast, messy, primal chaos that contains centuries of scientific ideas. If you are an ideas worker, this is your raw material—this is what you work with. Not reading the literature as an ideas worker is like not going to new restaurants as a new chef, or not looking at other people’s art as an artist, or not listening to music as a composer. Maybe the rare person has an internal creativity so deep that they don’t need any external sources of inspiration, but I’m not sure I know anyone like that.
</p>

<p>
If you buy the concept of <a href=https://corinwagen.github.io/public/blog/20230320_domain_arbitrage.html>“domain arbitrage”</a> I outlined last week, then reading the literature becomes doubly important for up-and-coming arbitrageurs. Not only do you need to stay on top of research in your own field, but you also need to keep an eye on other fields, to look for unexpected connections. It was only after months of reading physical chemistry papers about various IR spectroscopy techniques, with no direct goal in mind, that I realized I could use <i>in situ</i> IR to <a href=https://pubs.acs.org/doi/10.1021/acs.orglett.2c03622>pin down the structure of ethereal HCl</a>; simply reading organic chemistry papers would not have given me that insight.
</p>

<h2>
How You Can Read The Literature More
</h2>

<p>
If you don’t read the literature at all—like me, when I started undergrad—then you should start small. I usually recommend <i>JACS</i> to chemists. Just try to read every paper in your subfield in <i>JACS</i> for a few months; I began by trying to read every organic chemistry paper in <i>JACS</i>. At the beginning, probably only 10–20% will make sense. But if you push through and keep trying to make sense of things, eventually it will get easier. You’ll start to see the same experiments repeated, understand the structure of different types of papers, and even recognize certain turns of phrase. (This happened to me after about a year and a half of reading the literature.)
</p>

<p>
Reading more papers makes you a faster reader. Here’s <a href=https://marginalrevolution.com/marginalrevolution/2006/12/how_to_read_fas.html>Tyler Cowen</a> on how he reads so quickly (not papers specifically, but still applicable):
</p>

<blockquote>
The best way to read quickly is to read lots. And lots. And to have started a long time ago. Then maybe you know what is coming in the current book. Reading quickly is often, in a margin-relevant way, close to not reading much at all.
<br><br>
Note that when you add up the time costs of reading lots, quick readers don’t consume information as efficiently as you might think. They’ve chosen a path with high upfront costs and low marginal costs. "It took me 44 years to read this book" is not a bad answer to many questions about reading speed.
</blockquote>

<p>
All of Tyler’s advice applies doubly to scientific writing, which is often jargon-filled and ordered in arcane ways. After 7ish years of reading the scientific literature, I can “skim” a <i>JACS</i> paper pretty quickly and determine what, if anything, is likely to be novel or interesting to me, which makes staying on top of the literature much easier than it used to be.
</p>

<p>
Once you are good with a single journal, you can expand to multiple journals. A good starting set for organic chemistry is <i>JACS</i>, <i>Science</i>, <i>Nature</i>, <i>Nature Chemistry</i>, and <i>Angewandte</i>. If you already know how to read papers quickly, it will not be very hard to read more and more papers. But expanding to new journals brings challenges: how do you keep up with all of them at once? Lots of people use an RSS feed to aggregate different journals—I use <a href=https://feedly.com/>Feedly</a>, as do several of my coworkers. (You can also get this blog on Feedly!)
</p>

<p>
I typically check Feedly many times a day on my phone; I can look at the TOC graphic, the abstract, and the title, and then if I like how the paper looks I’ll email it to myself. Every other day or so, I sit down at my computer with a cup of coffee and read through the papers I’ve emailed to myself. This is separate from my “pursuing ideas from my research”/”doing a literature dive for group meeting” way of reading the literature—this is just to keep up with all the cool stuff that I wouldn’t otherwise hear about.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230329_email.png" style="width:550px;" />
  <figcaption> 
  “Inbox Zero” often proves elusive.
  </figcaption>
</figure>

<p>
(I also use Google Scholar Alerts to email me when new labs publish results—I have probably 20-30 labs set up this way, just to make sure I don’t miss results that might be important just because they’re not in a high-profile journal.)
</p>

<p>
Keeping track of papers you actually like and want to remember is another challenge. For the past two years, I’ve put the URLs into a Google Sheet, along with a one-sentence summary of the paper, which helps me look through my “most liked” papers when I want to find something. Sadly, I didn’t do this earlier, so I’m often tormented by papers I dimly remember but can no longer locate.
</p>

<h2>
What Literature You Should Read
</h2>

<p>
This obviously depends on what you’re doing, but I tend to think about literature results in three categories:
</p>

<ol>
<li>Things every scientist should know about</li>
<li>Things I am supposed to be an expert on</li>
<li>Things I’m not supposed to be an expert on, but would still like to know about</li>
</ol>

<p>
Category 1 basically covers the highest profile results (<i>Science</i> and <i>Nature</i>), and these days Twitter makes that pretty easy.
</p>

<p>
Category 2 covers things “in-field” or directly related to my projects—anything it would be somewhat embarrassing not to know about. For me, this means <i>JACS</i>, <i>Angewandte</i>, <i>ACS Catalysis</i>, <i>Org. Lett.</i>, <i>OPRD</i>, <i>Organometallics</i>, <i>J. Org. Chem.</i>, and <i>Chem. Sci.</i> (I also follow <i>Chem. Rev.</i> and <i>Chem. Soc. Rev.</i>, because review articles are nice.)
</p>

<p>
Category 3 covers things that I am excited to learn about. Right now, that’s <i>JCTC</i> and <i>J. Phys. Chem. A–C</i>. In the past, that’s included <i>ACS Chem. Bio.</i>, <i>Nature Chem. Bio.</i>, and <i>Inorganic Chemistry</i>. (Writing this piece made me realize I should follow <i>JCIM</i> and <i>J. Chem. Phys.</i>, so I just added them to Feedly.)
</p>

<h2>
Conclusion
</h2>

<p>
Reading the literature is—in the short term—pointless, sometimes frustrating, and just a waste of time. It’s rare that the article you read today will lead to an insight on the problem you’re currently facing! But the gains to knowledge compound over time, so spending time reading the literature today will make you a much better scientist in the long run.
</p>

<i>
Thanks to Ari Wagen and Joe Gair for reading drafts of this post.
</i>

]]></description>
              <pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Domain Arbitrage</title>
              <link>public/blog/20230320_domain_arbitrage.html</link>
              <description><![CDATA[
<p>
It’s a truth well-established that interdisciplinary research is good, and we all should be doing more of it (e.g. <a href=https://beta.nsf.gov/funding/learn/research-types/learn-about-interdisciplinary-research>this NSF page</a>). I’ve always found this to be a bit uninspiring, though. “Interdisciplinary research” brings to mind a fashion collaboration, where the project is going to end up being some strange chimera, with goals and methods taken at random from two unrelated fields.
</p>

<p>
Rather, I prefer the idea of “domain arbitrage.”<sup><a href="#fn1">1</a></sup> Arbitrage, in economics, is taking advantage of price differences in different markets: if bread costs $7 in Cambridge but $4 in Boston, I can buy in Boston, sell in Cambridge, and pocket the difference. Since this is easy and requires very little from the arbitrageur, physical markets typically lack opportunities for substantial arbitrage. In this case, the efficient market hypothesis works well.
</p>

<p>
Knowledge markets, however, are much less efficient than physical markets—many skills which are cheap in a certain domain are expensive in other domains. For instance, fields that employ organic synthesis, like chemical biology or polymer chemistry, have much less synthetic expertise than actual organic synthesis groups. The knowledge of how to use a Schlenk line properly is cheap within organic chemistry but expensive everywhere else. And organic chemists certainly don’t have a monopoly on scarce skills: trained computer scientists are very scarce in most scientific fields, as are statisticians, despite the growing importance of software and statistics to almost every area of research.
</p>

<p>
Domain arbitrage, then, is taking knowledge that’s cheap in one domain to a domain where it’s expensive, and profiting from the difference. I like this term better because it doesn’t imply that the goal of the research has to be interdisciplinary—instead, you’re solving problems that people have always wanted to solve, just now with innovative methods. And the concept of arbitrage highlights how this can be beneficial for the practitioner. You’re bringing new insights to your field so you can help your own research and make cool discoveries, not because you’ve been told that interdisciplinary work is good in an abstract way.
</p>

<p>
There are many examples of domain arbitrage,<sup><a href="#fn2">2</a></sup> but perhaps my favorite is the recent black hole image, which was largely due to work by Katie Bouman (formerly a graduate student at MIT, <a href=http://users.cms.caltech.edu/~klbouman/>now a professor at Caltech</a>):
</p>


<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230320_black_hole.jpg" style="width:350px;" />
  <figcaption> 
  The black hole picture, extracted from noisy radio telescope data by Bouman’s new algorithms.
  </figcaption>
</figure>

<p>
What’s surprising is that Bouman didn’t have a background in astronomy at all: she “hardly knew what a black hole was” (<a href=https://www.pbs.org/newshour/science/katie-bouman-hardly-knew-what-a-black-hole-was-her-algorithm-helped-us-see-one>in her words</a>) when she started working on the project. Instead, Bouman’s work drew on her background in computer vision, adapting statistical image models to the task of reconstructing astronomical images. In <a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bouman_Computational_Imaging_for_CVPR_2016_paper.pdf>a 2016 paper</a>, she explicitly credits computer vision with the insights that would later lead to the black hole image, and concludes by stating that “astronomical imaging will benefit from the crossfertilization of ideas with the computer vision community.”
</p>

<p>
If we accept that domain arbitrage is important, why doesn’t it happen more often? I can think of a few reasons: some fundamental, some structural, and some cultural. On the fundamental level, domain arbitrage requires knowledge of two fields of research at a more-than-superficial level. This is relatively common for adjacent fields of research (like organic chemistry and inorganic chemistry), but becomes increasingly rare as the distance between the two fields grows. It’s not enough to just try and read journals from other fields occasionally—without the proper context, other fields are simply unintelligible. Given how hard achieving competence in a single area of study can be, we should not be surprised that those with a working knowledge of multiple fields are so scarce.
</p>

<p>
The structure of our research institutions also makes domain arbitrage harder. In theory, a 15-person research group could house scientists from a variety of backgrounds: chemists, biologists, mathematicians, engineers, and so forth, all focused on a common research goal. In practice, the high rate of turnover in academic positions makes this challenging. Graduate students are only around for 5–6 years, postdocs for fewer, and both positions are typically filled by people hoping to learn things, not by already competent researchers. Thus, senior lab members must constantly train newer members in various techniques, skills, and ways of thinking so that institutional knowledge can be preserved.
</p>

<p>
This is hard but doable for a single area of research, but quickly becomes untenable as the number of fields increases. A lab working in two fields has to pass down twice as much knowledge, with the same rate of personnel turnover. In practice, this often means that students end up deficient in one (or both) fields. As Derek Lowe put it <a href=https://www.science.org/content/blog-post/pipeline-1060>when discussing chemical biology</a> in 2007:
</p>

<blockquote>
I find a lot of [chemical biology] very interesting (though not invariably), and some of it looks like it could lead to useful and important things. My worry, though, is: what happens to the grad students who do this stuff? They run the risk of spending too much time on biology to be completely competent chemists, and vice versa.
</blockquote>

<p>
To me, this seems like a case in which the two goals of the research university—to teach students and to produce research—are at odds. It’s easier to teach students in single-domain labs, but the research that comes from multiple domains is superior. It’s not easy to think about how to address this without fundamental change to the structure of universities (although perhaps others have more creative proposals than I).<sup><a href="#fn3">3</a></sup>
</p>

<p>
But, perhaps most frustratingly, cultural factors also contribute to the rarity of domain arbitrage. Many scientific disciplines today define themselves not by the questions they’re trying to solve but by the methods they employ, which disincentivizes developing innovative methods. For example, many organic chemists feel that biocatalysis shouldn’t be considered organic synthesis, since it employs enzymes and cofactors instead of more traditional catalysts and reagents, even though organic synthesis and biocatalysis both address the same goal: making molecules. While it’s somewhat inevitable that years of lab work leaves one with a certain affection for the methods one employs, it’s also irrational.
</p>

<p>
Now, one might reasonably argue that precisely delimiting where one scientific field begins and another ends is a pointless exercise. Who’s to say whether biocatalysis is better viewed as the domain of organic chemistry or biochemistry? While this is fair, it’s also true that the scientific field one formally belongs to matters a great deal. If society deems me an organic chemist, then overwhelmingly it is other organic chemists who will decide if I get a PhD, if I obtain tenure as a professor, and if my proposals are funded.<sup><a href="#fn4">4</a></sup>
</p>

<p>
Given that the success or failure of my scientific career thus depends on the opinion of other organic chemists, it starts to become apparent why domain arbitrage is difficult. If I attempt to solve problems in organic chemistry by introducing techniques from another field, it’s likely that my peers will be confused or skeptical by my work, and hesitate to accept it as “real” organic chemistry (see, for instance, the biocatalysis scenario above). Conversely, if I attempt to solve problems in other domains with the tools of organic chemistry, my peers will likely be uninterested in the outcome of the research, even if they approve of the methods employed. So from either angle domain arbitrage is disfavored.
</p>

<p>
The factors discussed here don’t serve to completely halt domain arbitrage, as successful arbitrageurs like Katie Bouman or Frances Arnold demonstrate, but they do act to inhibit it. If we accept the claim that domain arbitrage is good, and we should be working to make it more common, what then should we do to address these problems? One could envision a number of structural solutions, which I won’t get into here, but on a personal level the conclusion is obvious: if you care about performing cutting-edge research, it’s important to learn things outside the narrow area that you specialize in and not silo yourself within a single discipline.
</p>

<i>
Thanks to Shlomo Klapper and Darren Zhu for helpful discussions. Thanks also to Ari Wagen, Eric Gilliam, and Joe Gair for editing drafts of this post; in particular, Eric Gilliam pointed out the cultural factors discussed in the conclusion of the post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  I did not coin this term—credit goes to Shlomo Klapper. I do think this is the first time it's been used in writing, however.
  </li>
  <li id="fn2">
  I'll actually go a step farther and propose the Strong Theorem of Domain Arbitrage: All non-incremental scientific discoveries arise either from domain arbitrage or random chance. I don't want to defend it here, but I think there's a reasonable chance that this is true.
  </li>
  <li id="fn3">
  Collaborations between differently skilled labs help with this problem, but the logistical and practical challenges involved in collaboration make this an inefficient solution. Plus, the same cultural challenges still confront the individual contributors. 
  </li>
  <li id="fn4">
  <a href=https://twitter.com/Stephen_Curry/status/1637496308935057409>This tweet</a>, quoting the Nielsen/Qiu metascience essay <a href=https://corinwagen.github.io/public/blog/20221026_structural_diversity.html>which I wrote about before</a>, seems relevant.
  </li>
</ol>
]]></description>
              <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Computational NMR Prediction: A Microreview</title>
              <link>public/blog/20230314_nmr_microreview.html</link>
              <description><![CDATA[
<p>
Recently, I’ve been working to assign the relative configuration of some tricky diastereomers, which has led me to do a bit of a deep dive into the world of computational NMR prediction. Having spent the last week or so researching the current state-of-the-art in simulating experimental <sup>1</sup>H NMR spectra, I’m excited to share some of my findings.
</p>

<p>
My main resource in this quest has been <a href=https://www.mdpi.com/1420-3049/28/6/2449>a new NMR benchmarking paper</a>, published on March 7th by authors from Merck (and a few other places). Why this paper in particular? Although there have been many NMR benchmarks, not all of these papers are as useful as they seem. Broadly speaking, there are two ways to benchmark NMR shifts: (1) against high-level computed results or (2) against experimental NMR shifts.
</p>

<p>
The first strategy seems to be popular with theoretical chemists: NMR shifts at a very high level of theory are presumably very accurate, and so if we can just reproduce those values with a cheap method, we will have solved the NMR prediction problem. Of course, effects due to solvation and vibrational motion will be ignored, but these effects can always be corrected for later. In contrast, the second strategy is more useful for experimental chemists: if the calculation is going to be compared to experimental NMR spectra in CDCl<sub>3</sub> solution, the match with experiment is much more important than the gas-phase accuracy of the functional employed.
</p>

<p>
Not only are these two approaches different in theory, they yield vastly different results in practice, as is nicely illustrated by the case of the double-hybrid functional DSD-PBEP86. DSD-PBEP86 was <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.8b00624>first reported in 2018</a> by Frank Neese and coworkers, who found it to be much superior to regular DFT methods or MP2-type wavefunction methods at reproducing CCSD(T) reference data.<sup><a href=#fn1>1</a></sup> <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.1c00919>A subsequent benchmark</a> by Kaupp and coworkers looked at a much larger set of compounds and confirmed that DSD-PBEP86 was indeed superior at reproducing CCSD(T) data, with a mean absolute error (MAE) for <sup>1</sup>H of 0.06 ppm. In contrast, <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.1c00604>de Oliveira and coworkers</a> found that DSD-PBEP86 and related double-hybrid methods were much worse at predicting experimental <sup>1</sup>H NMR shifts, with a MAE of 0.20 ppm, making them no better than conventional DFT approaches.
</p>

</p>
The difference between these two mindsets is nicely demonstrated by Kaupp’s paper, which dismisses de Oliveira’s work as suffering from “methodological inadequacies” and states:
</p>

<blockquote>
[Benchmarking] can be done by comparing approximative calculations to experimental data or to data computed using high-level ab initio methodologies. The latter helps to eliminate a number of factors that often complicate the direct comparison against experiment, such as environmental, ro-vibrational, or thermal contributions (possibly also relativistic effects, see below).
</blockquote>

<p>
While Kaupp is correct that using gas-phase CCSD(T) data does eliminate “environmental” effects (e.g. from solvent), it’s not clear that these effects always ought to be eliminated! Although directly optimizing a computational method to reproduce a bunch of ill-defined environmental effects is perhaps inelegant, it’s certainly pragmatic.
</p>

<p>
The authors of the 2023 benchmark create a new set of well-behaved reference compounds that avoid troublesome heavy-atom effects (poorly handled by most conventional calculations) or low-lying conformational equilibria, and re-acquire experimental spectra (in chloroform) for every compound in the set. They then score a wide variety of computational methods against this dataset: functionals, basis sets, implicit solvent methods, and more.
</p>

<p>
In the end, <a href=https://pubs.acs.org/doi/10.1021/ct6001016>Cramer’s WP04 functional</a> is found to be best, which is perhaps unsurprising given that it was specifically optimized for the prediction of <sup>1</sup>H shifts in chloroform.<sup><a href="#fn2">2</a></sup> The WP04/6-311++G(2d,p)/PCM(chloroform) level of theory is optima, giving an MAE of 0.08 ppm against experiment, but WP04/jul-CC-PVDZ/PCM(chloroform) is cheaper and not much worse. B3LYP-D3/6-31G(d) works fine for geometry optimization, as do wB97X-D/6-31G(d) and M06-2X/6-31G(d).
</p>

<p>
Based on these results, my final workflow for predicting experimental proton spectra is:
</p>

<ol>
<li><a href=https://corinwagen.github.io/public/blog/20221219_low_code_csearch.html>Run a conformational search using <i>crest</i></a>.</li>
<li>Optimize each conformer using B3LYP-D3BJ/6-31G(d).</li>
<li>Remove duplicate conformers with <span class=code>cctk.ConformationalEnsemble.eliminate_redundant()</span>.</li>
<li>Predict NMR shifts for each conformer using WP04/6-311++G(2d,p)/PCM(chloroform).</li>
<li>Combine conformer predictions through <a href=https://corinwagen.github.io/public/blog/20221228_boltzmann_error.html>Boltzmann weighting</a>, and apply a linear correction.</li>
</ol>

<p>
For small molecules, this workflow runs extremely quickly (just a few hours from start to finish), and has produced good-quality results that solved the problem I was trying to solve.
</p>

<p>
Nevertheless, the theoreticians have a point—although WP04 can account for a lot of environmental effects (essentially by overfitting to experimental data), there are plenty of systems for which this pragmatic approach cannot succeed. For instance, the DELTA50 dataset intentionally excludes molecules which might exhibit concentration-dependent aggregation behavior, which includes basically anything capable of hydrogen bonding or π–π stacking! If we hope to get beyond a certain level of accuracy, it seems likely that physically correct models of NMR shieldings, solvent effects, and aggregation will be necessary.
</p>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  CCSD(T) NMR shifts have to be computed in <a href=https://cfour.uni-mainz.de/cfour/>CFOUR</a>.
  </li>
  <li id="fn2">
  The WP04 functional is not technically in Gaussian, but can be employed with the following route card: <span class=code>#p nmr=giao BLYP IOp(3/ 76=1000001189,3/77=0961409999,3/78=0000109999) 6-311++G(2d,p) scrf=(pcm,solvent=chloroform)</span>.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Optimizing Python</title>
              <link>public/blog/20230309_optimizing_python.html</link>
              <description><![CDATA[
<p>
Python is an easy language to write, but it’s also <a href=https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html>very</a> <a href=https://programming-language-benchmarks.vercel.app/problem/nbody>slow</a>. Since it’s a dynamically typed and interpreted language, every Python operation is much slower than the corresponding operation would be in C or FORTRAN—every line of Python must be interpreted, type checked, and so forth (see this <a href=https://www.codingdojo.com/blog/interpreters-run-python-code>little overview</a> of what the Python interpreter does).
</p>

<p>
Fortunately for those of us who like programming in Python, there are a number of different ways to make Python code faster. The simplest way is just to use NumPy, the <i>de facto</i> standard for any sort of array-based computation in Python; NumPy functions are written in C/C++, and so are much faster than the corresponding native Python functions.
</p>

<p>
Another strategy is to use a just-in-time compiler to accelerate Python code, like Jax or Numba. This approach incurs a substantial <i>O</i>(1) cost (compilation) but makes all subsequent calls orders of magnitude faster. Unfortunately, these libraries don’t support all possible Python functions or external libraries, meaning that sometimes it’s difficult to write JIT-compilable code.
</p>

<p>
How do these strategies fare on a real-world problem? I selected pairwise distance calculations for a list of points as a test case; this problem is pretty common in a lot of scientific contexts, including calculating electrostatic interactions in molecular dynamics or quantum mechanics.
</p>

<p>
We can start by importing the necessary libraries and writing two functions. The first function is the “naïve” Python approach, and the second uses <span class=code>scipy.spatial.distance.cdist</span>, one of the most <a href=https://github.com/ekwan/cctk/blob/master/cctk/molecule.py#L170>overpowered</a> functions I’ve encountered in any Python library.
</p>

<pre class=code-block>
import numpy as np
import numba
import cctk
import scipy

mol = cctk.XYZFile.read_file("30_dcm.xyz").get_molecule()
points = mol.geometry.view(np.ndarray)

def naive_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i, A in enumerate(points):
        for j, B in enumerate(points):
            distances[i,j] = np.linalg.norm(A-B)
    return distances

def scipy_get_distance(points):
    return scipy.spatial.distance.cdist(points,points)
</pre>

<p>
If we score these functions in Jupyter, we can see that <span class=code>cdist</span> is almost 2000 times faster than the pure Python function!
</p>

<pre class=code-block>
%%timeit
naive_get_distance(points)

103 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre>

<pre class=code-block>
%%timeit
scipy_get_distance(points)

55.2 µs ± 2.57 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</pre>

<p>
In this case, it’s pretty obvious that we should just use <span class=code>cdist</span>. But what if there wasn’t a magic built-in function for this task—how close can we get to the performance of <span class=code>cdist</span> with other performance optimizations?
</p>

<p>
The first and most obvious optimization is simply to take advantage of the symmetry of the matrix, and not compute entries below the diagonal. (Note that this is sort of cheating, since <span class=code>cdist</span> doesn’t know that both arguments are the same.)
</p>

<pre class=code-block>
def triangle_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i in range(N):
        for j in range(i,N):
            distances[i,j] = np.linalg.norm(points[i]-points[j])
            distances[j,i] = distances[i,j]
    return distances
</pre>

<p>
As expected, this roughly halves our time:
</p>

<pre class=code-block>
%%timeit
triangle_get_distance(points)

57.6 ms ± 409 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre>

<p>
Next, we can use Numba to compile this function. This yields roughly a 10-fold speedup, bringing us to about two orders of magnitude slower than <span class=code>cdist</span>.
</p>

<pre class=code-block>
numba_triangle_get_distance = numba.njit(triangle_get_distance)
</pre>

<pre class=code-block>
%%timeit
numba_triangle_get_distance(points)

5.74 ms ± 36.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre>

<p>
Defining our own norm with Numba, instead of using <span class=code>np.linalg.norm</span>, gives us another nice boost:
</p>

<pre class=code-block>
def custom_norm(AB):
    return np.sqrt(AB[0]*AB[0] + AB[1]*AB[1] + AB[2]*AB[2])

numba_custom_norm = numba.njit(custom_norm)

def cn_triangle_get_distance(points):
    N = points.shape[0]
    distances = np.zeros(shape=(N,N))
    for i in range(N):
        for j in range(i,N):
            distances[i,j] = numba_custom_norm(points[i] - points[j])
            distances[j,i] = distances[i,j]
    return distances

numba_cn_triangle_get_distance = numba.njit(cn_triangle_get_distance)
</pre>

<pre class=code-block>
%%timeit
numba_cn_triangle_get_distance(points)

1.35 ms ± 21.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
What about trying to write this program using only vectorized NumPy functions? This takes a bit more creativity; I came up with the following function, which is a bit memory-inefficient but still runs quite quickly:
</p>

<pre class=code-block>
def numpy_get_distance(points):
    N = points.shape[0]

    points_row = np.repeat(np.expand_dims(points,1), N, axis=1)
    points_col = np.repeat(np.expand_dims(points,0), N, axis=0)

    sq_diff = np.square(np.subtract(points_row, points_col))
    return np.sqrt(np.sum(sq_diff, axis=2))
</pre>

<pre class=code-block>
%%timeit
numpy_get_distance(points)

426 µs ± 6.34 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
Unfortunately, calling <span class=code>np.repeat</span> with arguments <a href=https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html>isn’t supported by Numba</a>, meaning that I had to get a bit more creative to write a Numba-compilable version of the previous program. The best solution that I found involved a few array reshaping operations, which are (presumably) pretty inefficient, and the final code only runs a little bit faster than the Numpy-only version.
</p>

<pre class=code-block>
def numpy_get_distance2(points):
    N = points.shape[0]

    points_row = np.swapaxes(points.repeat(N).reshape((N,3,N)),1,2)
    points_col = np.swapaxes(points_row,0,1)

    sq_diff = np.square(np.subtract(points_row, points_col))
    return np.sqrt(np.sum(sq_diff, axis=2))

numba_np_get_distance2 = numba.njit(numpy_get_distance2)
</pre>

<pre class=code-block>
%%timeit
numba_np_get_distance2(points)

338 µs ± 4.11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</pre>

<p>
I tried a few other approaches, but ultimately wasn’t able to find anything better; in theory, splitting the loops into chunks could improve cache utilization, but in practice anything clever I tried just made things slower.
</p>

<p>
In the end, we were able to accelerate our code about 250x by using a combination of NumPy and Numba, but were unable to match the speed of an optimized low-level implementation. Maybe in a future post I’ll drop into C or C++ and see how close I can get to the reference—until then, I hope you found this useful.
</p>

<p>
(I’m sure that there are ways that even this Python version could be improved; I did not even look at any other libraries, like Jax, Cython, or PyPy. Let me know if you think of anything clever!)
</p>
]]></description>
              <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Gilliam and Girard on Scientific Innovation</title>
              <link>public/blog/20230228_gilliam_and_girard.html</link>
              <description><![CDATA[
<p>
Eric Gilliam, whose work on the history of MIT I highlighted before, has <a href=https://freaktakes.substack.com/p/irving-langmuir-the-general-electric>a nice piece</a> looking at Irving Langmuir’s time at the General Electric Research Laboratory and how applied science can lead to advances in basic research.
</p>

<p>
Gilliam recounts how Langmuir started working on a question of incredible economic significance to GE—how to make lightbulbs last longer without burning out—and after embarking on a years-long study of high-temperature metals under vacuum, not only managed to solve the lightbulb problem (by adding an inert gas to decrease evaporation and coiling the filament to prevent heat loss), but also starting working on the problems he would later become famous for studying. In <a href=https://www.jstor.org/stable/16349>Langmuir’s own words</a>:
</p>

<blockquote>
The work with tungsten filaments and gases done prior to 1915 [at the GE laboratory] had led me to recognize the importance of single layers of atoms on the surface of tungsten filaments in determining the properties of these filaments.
</blockquote>

<p>
Indeed, Langmuir was awarded the <a href=https://www.nobelprize.org/prizes/chemistry/1932/summary/>1932 Nobel Prize in Chemistry</a> “for his discoveries and investigations in surface chemistry.”
</p>

<figure>
  <img class="centered-img" src=https://upload.wikimedia.org/wikipedia/commons/4/4d/Irving_Langmuir_and_Guglielmo_Marconi_in_lab.jpg style="width:350px;" />
  <figcaption> 
  Langmuir in the GE Research Laboratory.
  </figcaption>
</figure>

<p>
Nor were lightbulbs the only thing Langmuir studied at GE: he invented a greatly improved form of vacuum pump, invented a hydrogen welding process used to construct vacuum-tight seals, and employed thin films of molecules on water to determine accurate molecular sizes with unprecedented accuracy. Gilliam argues that this tremendous productivity can in part be attributed to the fact that Langmuir’s work was in constant contact with practical problems, which served as a source of scientific inspiration:
</p>

<blockquote>
In a developed world that is not exactly beset by scarcity and hardship anymore, it is hard to come up with the best areas to explore out of thin air. Pain points are not often obvious. Fundamental researchers can benefit massively from going to a lab mostly dedicated to making practical improvements to things like light bulbs and pumps and observing/asking questions. It is, frankly, odd that we normalized a system in which so many of our fundamental STEM researchers are allowed to grow so disjoint from the applied aspects of their field in the first place.
</blockquote>

<p>
And <a href=https://www.jstor.org/stable/16349>Langmuir himself</a> seems to agree:
</p>

<blockquote>
As [the GE] laboratory developed it was soon recognized that it was not practicable nor desirable that such a laboratory should be engaged wholly in fundamental scientific research. It was found that at least 75 per cent of the laboratory must be devoted to the development of the practical applications. It is stimulating to the men engaged in fundamental science to be in contact with those primarily interested in the practical applications.
</blockquote>

<p>
Let’s bring in our second thinker. Last weekend, I had the privilege of attending a lecture by Johnathan Bi on Rene Girard and the philosophy of innovation, which discussed (among other things) how a desire for “disruption” above all else actually makes innovation more difficult. To quote Girard’s <a href=https://www.jstor.org/stable/3684663>“Innovation and Repetition,”</a> which Bi discussed at length:
</p>

<blockquote> 
The main prerequisite for real innovation is a minimal respect for the past and the mastery of its achievements, i.e., mimesis. To expect novelty to cleanse itself of imitation is to expect a plant to grow with its roots up in the air. In the long run, the obligation always to rebel may be more destructive of novelty than the obligation never to rebel.
</blockquote>

<p>
What does this mean? Girard is describing two ways in which innovation can fail. The first is quite intuitive—if we hew to tradition too much, if we have an excessive respect for the past and not a “minimal respect,” we’ll be afraid to innovate. This is the oft-derided state of stagnation.
</p>

<p>
The second way in which we can fail to innovate, however, is a bit more subtle. Girard is saying that innovation also requires a mastery of the past’s achievements; we can’t simply ignore tradition, we have to understand what exists before we can innovate on top of it. Otherwise we will be, in Girard’s words, like a plant “with its roots up in the air.” All innovation has to occur within its proper context—to quote Tyler Cowen, <a href=https://marginalrevolution.com/marginalrevolution/2022/02/context-is-that-which-is-scarce-2.html>“context is that which is scarce.”</a>
</p>

<p>
This might seem a little silly. Innovation, “the introduction of new things, ideas or ways of doing something” (<a href=https://www.oxfordlearnersdictionaries.com/us/definition/english/innovation>Oxford</a>), at first inspection seems not to depend on tradition at all. But novelty with no hope of improvement over the status quo is simply a cry for attention; wearing one’s shoes on the wrong feet may be unusual, but is unlikely to win one renown as a great innovator. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230228_tweet.png" style="width:500px;" />
  <figcaption> 
  When innovation, devoid of context, becomes the highest virtue... (I added this to the post a few hours late, sorry.)
  </figcaption>
</figure>

<p>
What does this mean for scientific innovation, and how does this connect to Gilliam’s thoughts about Langmuir and the GE Research Laboratory? I’d argue that much of our fundamental research today, even that which is novel, lacks the context necessary to be transformatively innovative. Often the most impactful discoveries aren’t those which self-consciously aim to be <i>Science</i> or <i>Nature</i> papers, but those which simply aim to address outstanding problems or investigate anomalies. For instance, our own lab’s interest in hydrogen-bond-donor organocatalysis <a href=https://pubs.acs.org/doi/pdf/10.1021/ja980139y>was initiated</a> by the unexpected discovery that omitting the metal from an ostensibly metal-catalyzed Strecker reaction increased the enantioselectivity. Girard again:
</p>

<blockquote>
The principle of originality at all costs leads to paralysis. The more we celebrate "creative and enriching" innovations, the fewer of them there are.
</blockquote>

<p>
Langmuir’s example shows us a different path towards innovation. If we set out to investigate and address real-world problems of known practical import, without innovation in mind, Gilliam and Girard argue that we’ll be more innovative than if we make innovation our explicit goal. I don’t have a concrete policy recommendation to share here, but some of my other blog posts on <a href=https://corinwagen.github.io/public/blog/20220907_mit_elegy.html>applied research at MIT</a> and <a href=https://corinwagen.github.io/public/blog/20230215_science_engineering.html>the importance of engineering</a> perhaps hint at what positive change might look like.
</p>

<em>
In accordance with the themes of this piece, my interpretation of Girard pretty much comes straight from Johnathan Bi. He has a lecture on Youtube where he discusses these ideas: <a href=https://youtu.be/qdWHcBBCaww?t=3906>here’s a link</a> to the relevant segment, which is the only part I’ve watched.
</em>
]]></description>
              <pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Models and Oracles</title>
              <link>public/blog/20230221_models_and_oracles.html</link>
              <description><![CDATA[
<p>
When thinking about science, I find it helpful to divide computations into two categories: models and oracles.
</p>

<p>
In this dichotomy, models are calculations which act like classic ball-and-stick molecular models. They illustrate that something is geometrically possible—that the atoms can literally be arranged in the proper orientation—but not much more. No alternative hypotheses have been ruled out, and no unexpected insights have emerged. A model has no intelligence of its own, and only reflects the thought that the user puts into it.
</p>

<p>
This isn’t bad! Perhaps the most striking example of the utility of models is <a href=https://pubs.acs.org/doi/abs/10.1021/ja00713a007>Tolman’s original cone angle report</a>, where he literally made a wooden model of different phosphine ligands and measured the cone angle with a ruler. The results are excellent!
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_tolman.png" style="width:350px;" />
  <figcaption> 
  Figure 1, from Tolman’s paper.
  </figcaption>
</figure>

<p>
In contrast, an oracle bestows new insights or ideas upon a petitioner—think <a href=https://en.wikipedia.org/wiki/Pythia>the Oracle at Delphi</a>. This is what a lot of people imagine when they think of computation: we want the computer to predict totally unprecedented catalysts, or figure out the mechanism without any human input. We bring our problems to the superintelligence, and it solves them for us. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_miola.jpeg" style="width:450px;" />
  <figcaption> 
  <i>The Oracle</i>, by Camille Miola (1880). Picture from Wikimedia Commons.
  </figcaption>
</figure>


<p>
In reality, every simulation is somewhere between these two limiting extremes. No matter how hard you try, a properly executed DFT calculation will not predict formation of a methyl cation to be a low-barrier process—the computational method used understands enough chemistry to rule this out, even if the user does not. On the flip side, even the most sophisticated calculations all involve some form of human insight or intuition, either explicitly or implicitly. We’re still very far away from the point where we can ask the computer to generate the structures of new catalysts (or medicines) and expect reasonable, trustworthy results. But that’s ok; there’s a lot to be gained from lesser calculations! There’s no shame in generating computational models instead of oracles.
<p>

<p>
What’s crucial, though, is to make sure that everyone—practitioners, experimentalists, and readers—understands where a given calculation falls on the model–oracle continuum. An expert might understand that a semiempirical AIMD study of reaction dynamics is likely to be only qualitatively correct (if that), but does the casual reader? I’ve talked to an unfortunate number of experimental chemists who think a single DFT picture means that we can “predict better catalysts,” as if that were a button in GaussView. The appeal of oracles is seductive, and we have to be clear when we’re presenting models instead. (This ties into <a href=https://corinwagen.github.io/public/blog/20220810_viewpoints_on_simulation.html>my piece about computational nihilism.</a>)
<p>

<p>
Finally, this piece would be incomplete if I didn’t highlight Jan Jensen and co-workers’ <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.202218565>recent work</a> on automated design of catalysts for the Morita–Baylis–Hillman reaction. The authors use a generative model to discover tertiary amines with lower DFT-computed barriers than DABCO (the usual catalyst), and then experimentally validate one of their candidates, finding that it is indeed almost an order-of-magnitude faster than DABCO. It’s difficult to underscore how groundbreaking this result is; as the authors dryly note, “We believe this is the first experimentally verified <i>de novo</i> discovery of an efficient catalyst using a generative model.” On the spectrum discussed above, this is getting pretty close to “oracle.”
<p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230221_jensen.png" style="width:450px;" />
  <figcaption> 
  Figure 3 from the paper, illustrating discovery of new catalysts.
  </figcaption>
</figure>

<p>
Nevertheless, the choice of model system illustrates how far the field still has to go. The MBH reaction is among the best-studied reactions in organic chemistry, as illustrated by <a href=https://pubs.acs.org/doi/10.1021/ja5111392>Singleton’s 2015 mechanistic <i>tour de force</i></a> (and references therein, and subsequent work), so Jensen and co-workers could have good confidence that the transition state they were studying was correct and relevant. Furthermore, as I understand it, the MBH reaction can be catalyzed by just about any tertiary amine—there aren’t the sort of harsh activity cliffs or arcane structural requirements that characterize many other catalytic reactions. Without either of these factors—well-studied mechanism or friendly catalyst SAR—I doubt this work would be possible. 
<p>

<p>
This point might seem discouraging, but I mean it in quite the opposite way. <i>De novo</i> catalyst design isn’t impossible for mysterious and opaque reasons, but for quite intelligible reasons—mechanisms are complicated, catalysts are hard to design, and we just don’t understand enough about what we’re doing, experimentally or computationally. What Jensen has shown us is that, if we can address these issues, we can expect to start converging on oracular results. I find this very exciting!
<p>

<em>
Jan Jensen was kind enough to <a href=https://twitter.com/janhjensen/status/1628063110572507136>reply to this post on Twitter</a> with a few thoughts and clarifications, which are worth reading.
</em>

]]></description>
              <pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Science and Engineering</title>
              <link>public/blog/20230215_science_engineering.html</link>
              <description><![CDATA[
<blockquote><i>
In science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it.
</i></blockquote>

<p style="text-align:right;">
—Richard Hamming
</p>

<p>
What’s the difference between science and engineering? 
</p>

<p>
Five years ago, I would have said something along the lines of “engineers study known unknowns, scientists study unknown unknowns” (with apologies to <a href=https://en.wikipedia.org/wiki/There_are_unknown_unknowns>Donald Rumsfeld</a>), or made a distinction between expanding the frontiers of knowledge (science) and settling already-explored territory (engineering).
</p>

<p>
These thoughts seem broadly consistent with what others think. Boston University’s <a href=https://www.bu.edu/eng/about-eng/meet-the-dean/engineering-is-not-science/>College of Engineering</a> says:
</p>

<blockquote>
Engineers are not a sub-category of scientists. So often the two terms are used interchangeably, but they are separate, albeit related, disciplines. Scientists explore the natural world and show us how and why it is as it is. Discovery is the essence of science. Engineers innovate solutions to real-world challenges in society. While it is true that engineering without science could be haphazard; without engineering, scientific discovery would be a merely an academic pursuit.
</blockquote>

<p>
And the <a href=https://www.nspe.org/resources/press-room/resources/frequently-asked-questions-about-engineering>National Society of Professional Engineers</a> says:
<p>

<blockquote>
Science is knowledge based on observed facts and tested truths arranged in an orderly system that can be validated and communicated to other people. Engineering is the creative application of scientific principles used to plan, build, direct, guide, manage, or work on systems to maintain and improve our daily lives.
</blockquote>

<p>
As I’ve started thinking more about the structure of the modern research system, and what its proper scope and purpose should be, I’ve grown increasingly skeptical of these distinctions. The claim I want to make in this post is that, following the above definitions of engineering, <u>most chemistry is engineering</u>. I don’t think this is bad! In fact, I think that many chemists could benefit from borrowing from an engineering mindset, and should consider incorporating this perspective in their self-conception.
<p>

<h2>
Much of Organic Chemistry is Engineering
</h2>

<p>
I want to return to the BU and NSPE definitions, because I think they’re concise and well-written, and take as gospel that scientists “explore the natural world and show us how and why it is as it is,” while engineers “innovate solutions to real-world challenges in society” (we’ll revisit issues of definition later). In short, developing something you want other people to use makes you an engineer. Which branches of modern organic chemistry are science, and which are engineering? 
<p>

<p>
Method development—one of my core interests—seems like a good candidate for “engineering.” Researchers in this space identify unsolved problems in organic synthesis, develop methods or catalysts to solve these problems, and then (in many cases) advertise, license, &amp; sell their solutions to consumers! (If you don’t believe me, just look at the <a href=https://www.sigmaaldrich.com/US/en/collections/professor-product-portal>“Organic Synthesis” tab</a> of the Sigma-Aldrich Professor Product Portal.) If these products weren’t molecules and were instead mechanical gadgets, nothing about this would be obviously scientific.
<p>

<p>
And the problems chosen are identified almost purely on the basis of what might be useful to potential users. There’s no clearer illustration of this than the recent gold rush to identify synthetic routes to bicyclic arene bioisosteres, which are useful in medicinal chemistry. Five years ago, I can’t think of a single paper making these compounds; now, I can find nine in high-profile journals just from the past year or so (<a href=https://www.nature.com/articles/s41586-022-05290-z.pdf>1</a>,
<a href=https://www.nature.com/articles/s41557-022-00979-0.pdf>2</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09733>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c11501>4</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acscatal.2c03498>5</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.1c03681>6</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03606>7</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202205103>8</a>,
<a href=https://www.nature.com/articles/s41557-023-01135-y>9</a>).
<p>

<p>
Mechanistic and physical organic chemistry—another love of mine—present a tougher case, since in most cases the outcome of the studies is only knowledge. But I’m still not sure that this makes this field scientific! Let me illustrate why with an example. 
<p>

<p>
An automotive engineer may be confused by why a given transmission is not working. He/she may formulate a hypothesis, take the transmission apart, perform various experiments on its constituent pieces, validate or disprove the initial hypothesis, and generally conduct a thorough mechanistic investigation to understand the origin of the problem. But does that make him/her a scientist?
<p>

<p>
The answer, I think, is no. The subject matter is not scientific, so no amount of scientific thinking can make the work science. Similarly, I’d argue that investigating the mechanism of a system invented and developed by humans—like a Pd-catalyzed cross-coupling reaction—doesn’t count as science. (Does application of the scientific method automatically make one a scientist? See below for a continued discussion.)
<p>

<p>
In contrast, something that I think is a truly scientific area of investigation is the study of enzyme structure and function. Despite extensive study and many Nobel prizes, we’re still learning about how enzymes operate and how they achieve such extraordinary reactivity and selectivity. (For an example of this sort of work, see Arieh Warshel’s <a href=https://pubs.acs.org/doi/full/10.1021/cr0503106>review</a> on electrostatic effects in enzyme catalysis, and references therein.)
<p>

<p>
I don’t think I understand all areas of chemistry well enough to fairly judge whether they’re better understood as engineering or science, so I’ll leave this as an exercise to the reader: What motivates your research? Are you mainly driven by a desire to understand the natural order of things, or do you aim to develop technology to make the world better? Both are important, and neither answer is bad—but if your main goal is inventing a new molecule, material, algorithm, or medicine, you might consider thinking of yourself as more of an engineer than a scientist.
<p>

<h2>
Do Different Definitions Clarify Matters?
</h2>

<p>
Since the claim that “most chemistry is engineering” is weird, we might consider alternative definitions to solve this problem.
<p>

<p>
One appealing definition: “a scientist is anyone who uses the scientific method.” As I discussed above, in the case of the automotive engineer, lots of people use the scientific method who clearly aren’t scientists: engineers, yes, but also detectives, doctors, and many other people. Indeed, according to this definition almost anyone who acquires data to shed light on a problem is “doing science.” So I don’t think this is a very good definition.
<p>

<p>
Another definition might be: “if you’re closely involved with science, you’re a scientist, even if the work you’re doing isn’t literally pushing the frontiers of knowledge forward.” I’m sympathetic to this definition, but I still find it hard to separate scientists and engineers here. What makes an engineer optimizing a new flow reactor less of a scientist than the chemist optimizing a new catalyst? Are they both scientists? What about people who work on chip design and fabrication, or people who design analytical instruments, or people who use them? I can’t find any clean way to divide scientists from engineers that recapitulates the conventional usage of the terms.
<p>

<h2>
Why Does This Matter?
</h2>

<p>
I think the root of this confusion is that <u>the nature of scientific fields has changed over the past half-century</u>. Organic chemistry hasn’t always been largely engineering; a century ago, the structure of natural products and the nature of the chemical bond were mysteries, truly the domain of science, and these issues were studied by chemists. As we’ve grown to understand our field better and better, our work has shifted from science to engineering—the true mysteries in chemistry are now few and far between, and the challenge facing today’s chemists is how to use centuries of accumulated knowledge to better society. But because of our lineage, we think of ourselves as scientists, and have managed to disguise the true nature of our work so well that we’ve deceived even ourselves.
</p>

<p>
By this point, it should be obvious that I don’t think science is superior to engineering. In fact, I’m glad to work in an area that’s largely engineering! But the way that an engineer ought to approach their work is different from the way a scientist ought to approach their work. In writing this piece, I came across a 1996 <a href=https://dl.acm.org/doi/10.1145/227234.227243>article</a> by Frederick Brooks, who argued that computer science was better understood as an engineering discipline and defended the importance of this claim: 
</p>

<blockquote>
If our discipline has been misnamed, so what? Surely computer science is a harmless conceit. What’s in a name? Much. Our self-misnaming hastens various unhappy trends.
<br><br>
First, it implies that we accept a perceived pecking order that respects natural scientists highly and engineers less so, and that we seek to appropriate the higher station for ourselves. That is a self-serving gambit, hence dubious….
<br><br>
Second, sciences legitimately take the discovery of facts and laws as a proper end in itself. A new fact, a new law is an accomplishment, worthy of publication…. But in design, in contrast with science, novelty in itself has no merit. If we recognize our artifacts as tools, we test them by their usefulness and their costs, not their novelty.
<br><br>
Third, we tend to forget our users and their real problems, climbing into our ivory towers to dissect tractable abstractions of those problems, abstractions that may have left behind the essence of the real problem. 
</blockquote>

<p>
I think Brooks’s summary is simple and elegant. If we judge the value of our work based on the utility of our tools, rather than the novelty of our ideas, we’ll spend our time on different problems and get excited about different advances. There’s room for both scientists and engineers in chemistry—but at the margin, I think our field would benefit from becoming less like science, and more like engineering.
</p>

<i>
Thanks to Michael Nielsen, Joe Gair, Ari Wagen, and Michael Tartre for editing drafts of this post. Michael Tartre sent me the Richard Hamming quote, and Phil Brooks sent me his grandfather’s article.
</i>

]]></description>
              <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>2022 Paper(s) of the Year</title>
              <link>public/blog/20230203_poy.html</link>
              <description><![CDATA[
<p>
Every year, our group participates in a “Paper of the Year” competition, where we each nominate five papers and then duke it out in a multi-hour debate. Looking through hundreds of papers in a few weeks is a great exercise: it helps highlight both creativity and its absence, and points towards where the field’s focus might turn next.
</p>

<p>
My picks are very personal—how could they not be?—and also biased towards towards experimental chemistry, owing to what our group focuses on. So, don’t take this as any attempt towards creating an objective list.
</p>

<p>
All the papers I really liked are listed below, with my top five listed in bold:
</p>

<ul>
<li><a href=https://www.science.org/doi/pdf/10.1126/science.abo0039>A lovely electrochemical cross-electrophile coupling</a> (Sevov). Great scope and forms challenging bonds.</li>

<li><a href=https://www.science.org/doi/pdf/10.1126/science.abo4282>Carbon deletion through photochemistry, converting quinolines to indoles</a> (Levin). 
My favorite of the skeletal editing papers, with apologies to the others (<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09616>1</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c10746>2</a>, 
<a href=https://www.science.org/doi/10.1126/science.add1383>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c10570>4</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202202703>5</a>, 
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202208014>6</a>,
<a href=https://pubs.acs.org/doi/10.1021/jacs.2c08464>7</a>). <b>Pick 1/5.</b></li>

<li><a href=https://www.science.org/doi/pdf/10.1126/science.abn1885>High-throughput additive screening reveals a new role for phthalimide in Ni-catalyzed cross-couplings</a> (MacMillan, Dreher).</li>

<li><a href=https://www.science.org/doi/10.1126/science.abo6443>Turning aldehydes into metal carbenoids</a> (Nagib); self-recommending.</li>

<li><a href=https://www.science.org/doi/10.1126/science.add6852>Epimerization at tertiary carbons for “stereochemical editing”</a> (Wendlandt).</li>

<li><a href=https://www.science.org/doi/10.1126/science.ade5320>Setting stereocenters with vinyl carbocations, somehow</a> (Nelson, Houk).</li>

<li><a href=https://www.nature.com/articles/s41586-022-04491-w.pdf>Expanding automated synthesis to Csp<sub>3</sub> centers with “TIDA boronates”</a> (Burke). 
An approach with a real chance of revolutionizing how we approach organic synthesis. <b>Pick 2/5.</b></li>

<li><a href=https://www.nature.com/articles/s41586-022-04524-4.pdf>Some beautiful asymmetric catalysis to form S-chiral sulfinate esters</a> (Tan).</li>

<li><a href=https://www.nature.com/articles/s41586-022-05211-0.pdf>Two</a>
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c05648>papers</a> disclosing the use of photoexcited nitroarenes to perform “ozonolysis without ozone” (Parasram, Leonori).

<li><a href=https://www.nature.com/articles/s41557-022-00895-3.pdf>Enantioselective conversion of carboxylic acids into amino acids</a> (Meggers, Chen).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c00527>Catalytic olefin hydroalkoxylation, through a tri-catalytic system</a> (Ohmiya). Reminds me a little of <a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c04735>Kanai’s tri-catalytic aldehyde allylation</a>; in both cases, it’s hard to believe so many catalysts play nicely together.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c04807>Deoxygenative trifluoromethylation</a> (MacMillan), a reaction which speaks for itself.</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c08778>Functional group-tolerant Kochi–Salamon [2+2] cycloaddition in water, catalyzed by copper sulfate</a> (Burns). Bizarre, unbelievable, outstanding. <b>Pick 3/5.</b> (With apologies to the other 2+2 cycloaddition papers:
<a href=https://www.nature.com/articles/s41586-022-05335-3.pdf>1</a>,
<a href=https://www.nature.com/articles/s41586-022-05342-4.pdf>2</a>,
<a href=https://www.nature.com/articles/s41586-022-04636-x.pdf>3</a>,
<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03606>4</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202200725>5</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202208800>6</a>,
<a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202211596>7</a>.)</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c08332>Pd-catalyzed C–H hydroxylation</a> (Yu). Addresses a lot of my misgivings about C–H activation: directed by carboxylic acids, silver-free, scalable, and uses aqueous H<sub>2</sub>O<sub>2</sub> (perhaps the best oxidant possible, after air).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c09745>Red-light-mediated C–N coupling, through Os photocatalysis</a> (Rovis). Suppresses a lot of the annoying mass balance issues that plague conventional photoredox.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.1c13151>A nice mechanistic study of an enantioselective C–N coupling reaction</a> (Peters, Fu).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01487>Mechanistic study and an improved phosphetane catalyst for C–N coupling</a> (Radosevich).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01375>Studying the factors that lead to hydroxyl versus chlorine radical transfer in non-heme Fe complexes</a> (Goldberg, de Visser).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c02283>Studying the mechanism of enantioselective enzymatic C–H amination</a> (Yang Yang, Peng Liu). Surprisingly, radical rebound is enantiodetermining! I remember being puzzled by the mechanism of this reaction when it was first published; this is a very satisfying resolution. Cool isotope studies &amp; <i>ab initio</i> molecular dynamics seal the deal. <b>Pick 4/5.</b></li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01356>The latest on photochemistry of Ni oxidative-addition complexes</a> (Hadt). Beautiful physical inorganic chemistry; a lot of the details are beyond me, but clearly important work.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c07099>Mechanistic study of PCET-mediated olefin hydroamination</a> (Knowles).</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c07643>A really beautiful metal-free hydroamination</a> (Wang/Wang).</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c09786>The latest on cobaltacene-based PCET mediators</a> (Peters). Big fan of this program.</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c00675>Using aryl nitriles as sensors for electric field strength</a> (Boxer). 
Also a really cool program: I liked <a href=https://chemrxiv.org/engage/chemrxiv/article-details/626b29f9ed4d8830581c6ed6>this preprint</a> too.
</li>

<li><a href=https://pubs.acs.org/doi/10.1021/jacs.2c00154>Studying cation–acetonitrile dynamics with 2D IR and isotope labelling</a> (Tokmakoff). I've <a href=https://corinwagen.github.io/public/blog/20220708_apotheosis_of_2d_ir.html>written about this</a> before</li>

<li><a href=https://pubs.acs.org/doi/10.1021/acscatal.1c05802>A very nice <sup>13</sup>C KIE study of the Suzuki reaction, emphasizing the importance of monoligated Pd even when using PPh<sub>3</sub></a> (Vetticatt, Hirschi). And another <a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c02941>cool <sup>13</sup>C KIE study</a> for Hirschi.</li>

<li><a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202114044>The first enantioselective beta-alkoxy elimination I’ve seen!</a> (Streuff)</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.orglett.2c03201>An interesting way to functionalize ketones, via phosphorus-mediated umpolung</a> (Ball).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.organomet.2c00504>Investigating a concentration-dependent KIE for protonolysis of (cod)PtMe<sub>2</sub></a> (Bowring).</li>

<li><a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202213056>Biocatalytic N-heterocycle methylation, with unbelievable regioselectivity</a> (Hammer). Hammer also published a <a href=https://onlinelibrary.wiley.com/doi/10.1002/anie.202215093>really exciting styrene hydration paper</a>, albeit technically in 2023.</li>

<li><a href=https://pubs.acs.org/doi/10.1021/acscatal.2c04316>Bifunctional redox-active esters add to styrenes to make a variety of heterocycles.</a> (Knowles/Doyle).</li>

<li><a href=https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc02274e>A nice illustration of the importance of conformer sampling workflows</a> (Neese, Bistoni). See also work from 
<a href=https://pubs.rsc.org/en/content/articlepdf/2022/sc/d2sc01714h>Laplaza/Corminboef</a>, 
<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01381>Peter Chen</a>, and <a href=https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202113905>Grimme</a>.</li>

<li><a href=https://arxiv.org/pdf/2204.05249.pdf>The latest equivariant neural network potential</a>, faster than <a href=https://www.nature.com/articles/s41467-022-29939-5>their previous work</a> (Kozinsky).</li>

<li><a href=https://arxiv.org/pdf/2010.01196.pdf>Espaloma, a differential neural network forcefield from the OpenForcefield folks.</a></li>

<li><a href=https://chemrxiv.org/engage/chemrxiv/article-details/62b1b0c97da6ce535c19d40c>Studying cycloaddition dynamics with neural network potentials</a> (Young, Duarte).</li>

<li><a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jpca.2c08301>Studying dynamic control of kinetic product ratios in cyclopropylidine opening using ML-learned potentials</a> (Carpenter).</li>

<li><a href=https://aip.scitation.org/doi/full/10.1063/5.0133026>wB97X-3c, the latest “composite method”</a> (Grimme). If the reported speed and accuracy are true “in the wild,” every computational chemist ought to use this method. <b>Pick 5/5.</b></li>
</ul>
]]></description>
              <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Importance of Integral Screening</title>
              <link>public/blog/20230123_integral_screening.html</link>
              <description><![CDATA[
<p>
For almost all Hartree–Fock-based computational methods, including density-functional theory, the rate-limiting step is calculating electron–electron repulsion. (This isn’t true for semiempirical methods, where matrix diagonalization is generally rate-limiting, or for <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c00876>calculations on very large systems</a>.)
</p>

<p>
When isolated single molecules are the subject of calculations (as opposed to solids or periodic systems), most programs describe electronic structure in terms of atom-centered basis sets, which reduces the electron–electron repulsion problem to one of calculating electron repulsion integrals (ERIs) over quartets of basis shells. Framed this way, it becomes obvious why ERIs are the bottleneck: the number of ERIs will scale as O(N<sup>4</sup>), meaning that millions of these integrals must be calculated even for relatively small molecules.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_dalle.png" style="width:350px;" />
  <figcaption> 
  I tried to get DALL-E to make a visual representation of integral screening; this was the best I got.
  </figcaption>
</figure>

<p>
One big advance in electronic structure calculations was the development of integral screening techniques, the most popular of which is the “Schwartz inequality” (derived from the <a href=https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality>Cauchy–Schwartz inequality</a>, but actually developed by <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540100111>Mario Häser and Reinhart Ahlrichs</a> [<b>EDIT: This is wrong, see correction at end!</b>]). If we denote an ERI over shells A, B, C, and D as (AB|CD), then the Schwartz inequality says:
</p>

<p>
(AB|CD) ≤ (AB|AB)<sup>0.5</sup> (CD|CD)<sup>0.5</sup>
</p>

<p>
This is pretty intuitive: each shell pair will interact with itself most, since it has perfect overlap with itself, and so the geometric mean of the interaction of each shell pair with itself is an upper bound for the interaction between the two shell pairs. (Why would (AB|AB) ever be a small value? Well, A and B might be super far away from each other, and so the “shell pair” has very little overlap is just negligible.)
</p>

<p>
This result is very useful. Since there are many fewer integrals of the form (AB|AB), we can start by calculating all of those, and then use the resulting values to “screen” each shell quartet. If the predicted value is less than some predefined cutoff, the integral is skipped. While these screening methods don’t help much with small molecules, where all of the shells are pretty close to each other, they become crucial for medium-sized molecules and above.
</p>

<p>
(What’s the cutoff value? Orca defaults to 10<sup>-8</sup>, <a href=https://gaussian.com/integral/>Gaussian</a> to 10<sup>-12</sup>, <a href=https://psicode.org/psi4manual/master/autodir_options_c/module__scf.html>Psi4</a> to 10<sup>-12</sup>, and <a href=https://manual.q-chem.com/5.2/Ch4.S3.SS2.html>QChem</a> to 10<sup>-8</sup>–10<sup>-10</sup> depending on the type of calculation.)
</p>

<p>
The Schwartz inequality neglects, however, another way in which (AB|CD) might be very small: if (AB| and |CD) aren’t independently negligible, but are just really far away from each other. One elegant way to address this (out of many) comes from recent-ish work by <a href=https://aip.scitation.org/doi/10.1063/1.4994190>Travis Thompson and Christian Ochsenfeld</a>. They define an intermediate quantity M for each pair of shells, derived from different high-symmetry integrals:
</p>

<p>
M<sub>AC</sub> := (AA|CC) /  ( (AA|AA)<sup>0.5</sup> (CC|CC)<sup>0.5</sup> )
</p>

<p>
M<sub>AC</sub> intuitively represents the distance between the two shells, and is guaranteed to be in the range [0,1]. Thompson and Ochsenfeld then use this quantity to propose an estimate of a shell quartet’s value:
</p>

<p>
(AB|CD) ≈ (AB|AB)<sup>0.5</sup> (CD|CD)<sup>0.5</sup> max(M<sub>AC</sub>M<sub>BD</sub>, M<sub>AD</sub>M<sub>BC</sub>)
</p>

<p>
This is no longer a rigorous upper bound like the Schwartz inequality, but it’s a pretty good estimate of the size of the integral.
</p>

How much of a difference does this make in practice? To test this, I ran HF/STO-3G calculations on dodecane in the fully linear configuration. As shown by <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540030314>Almlöf, Faegri, and Korsell</a>, linear molecules benefit the most from integral screening (since the shells are on average farther apart), so I hoped to see a sizable effect without having to study particularly large molecules.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_graph.png" style="width:450px;" />
  <figcaption> 
  Almlöf, Faegri, and Korsell, Figure 5. This paper is terrific.
  </figcaption>
</figure>

<p>
I compared both the Schwartz (“QQ”) bound and Ochsenfeld’s CSAM bound for integral thresholds ranging from 10<sup>-9</sup> to 10<sup>-13</sup>, and compared the result to a calculation without any integral screening. The total time for the calculation, as a percent of the unscreened time, is plotted below against the error in µHartree (for the organic chemists out there, 1 µH = 0.00063 kcal/mol):
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230125_graph2.png" style="width:400px;" />
  <figcaption> 
  Comparing CSAM and QQ.
  </figcaption>
</figure>

<p>
A few things are apparent for this data. First, even tight thresholds lead to dramatic speedups relative to the unscreened calculation—and with minimal errors. Secondly, the CSAM bound really does work better than the QQ bound (especially if you ignore the high-error 10<sup>-9</sup> threshold data point). For most threshold values, using CSAM leads to about a 20% increase in speed, at the cost of a 3-fold increase in an already small error. Viewed visually, we can see that the Pareto frontier for CSAM (blue) is just closer to the optimal bottom-left corner than the corresponding frontier for QQ (black).
</p>

<p>
I hope this post serves to explain some of the magic that goes on behind the scenes to make “routine” QM calculations possible. (If you thought these tricks were sneaky, wait until you hear how the integrals that aren’t screened out are calculated!)
</p>

<p>
<i>
<b>CORRECTION</b>: In this post, I credited Mario Häser and Reinhart Ahlrichs with developing the Cauchy–Schwartz method for integral screening. A (famous) theoretical chemist who shall remain nameless reached out to me to correct the record—in fact, Almlöf included an overlap-based screening method in <a href=https://onlinelibrary.wiley.com/doi/10.1002/jcc.540030314>his landmark 1982 paper</a>. To the untrained eye, this appears unrelated to ERI-based screening, but we are using Gaussian basis sets and so “one can therefore write the integrals in terms of overlaps,” meaning that what looked like a different expression is actually the same thing. (Section 9.12 of Helgaker/Jorgensen/Olsen's textbook </i>Molecular Electronic Structure Theory<i>, a book I sadly do not own, apparently discusses this more.) 
</i>
</p>

<p>
<i>
The professor traced this back to <a href=https://aip.scitation.org/doi/abs/10.1063/1.1681647?journalCode=jcp>Wilhite and Eumena</a> in 1974, and ultimately back to the work of Witten in the 1960s. It is a pleasure to get corrected by those you respect, and I welcome any readers who find errors in my writing to reach out; I will do my best to respond and take blame as appropriate.
</i>
</p>
]]></description>
              <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Who Will Optimize The Optimizers?</title>
              <link>public/blog/20230118_meta_optimization.html</link>
              <description><![CDATA[

<p>
While looking over papers from the past year, one theme in particular stood out to me: meta-optimization, or optimizing how we optimize things.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_algo.png" style="width:450px;" />
  <figcaption> 
  A generic optimization (image credit to <a href="https://mpalaourg.me/project/optimization-algorithms/">Georgios Balaouras</a>).
  </figcaption>
</figure>

<p>
Meta-optimization has long been a focus of research in computer science, where new optimization algorithms can have an incredibly high impact (e.g. <a href=https://arxiv.org/pdf/1412.6980.pdf>ADAM</a>, one of the most commonly used optimizers for neural network training). More recently, the advent of directed evolution has made optimization methods a central focus of biocatalysis, since (in many cases) the efficacy of the reaction one discovers is primarily dependent on the efficacy of the optimization method used.
</p>

<p>
In contrast, it seems that meta-optimization has historically attracted less attention from “classic” organic chemists, despite the central importance of reaction optimization to so much of what we do. This post aims to show some of the ways in which this paradigm is changing, and briefly summarize some of what I consider to be the most interesting and exciting recent advances in chemical meta-optimization.
<i>(This is a big and somewhat nebulous area, and I am certainly leaving things out or not paying due homage to everyone. Sorry in advance!)</i>
</p>

<h3>Design of Experiments, and Dealing with Discrete Variables</h3>
<p>
Perhaps the best-known optimization algorithm in chemistry is “design of experiments” (DoE), which uses statistical methods to estimate the shape of a multiparameter surface and find minima or maxima more efficiently than one-factor-at-a-time screening. (DoE is a pretty broad term that gets used to describe a lot of different techniques: for more reading, <a href=https://en.wikipedia.org/wiki/Response_surface_methodology>see</a> <a href=https://en.wikipedia.org/wiki/Central_composite_design>these</a> <a href=https://en.wikipedia.org/wiki/Box%E2%80%93Behnken_design>links</a>.)
</p>

<p>
DoE has been used for a long time, <a href=https://pubs.acs.org/doi/full/10.1021/op500169m>especially in process chemistry</a>, and is very effective at optimizing continuous variables (like temperature, concentration, and equivalents). However, it’s less obvious how DoE might be extended to discrete variables. (<a href=https://pubs.acs.org/doi/pdf/10.1021/ol1020898>This 2010 report</a>, from scientists at Eli Lilly, reports the use of DoE to optimize a palladium-catalyzed pyrazole arylation, but without many details about the statistical methods used.)
</p>

<p>
<a href=https://pubs.acs.org/doi/10.1021/op300275p>A nice paper</a> from Jonathan Moseley and co-workers illustrates why discrete variables are so tricky:
</p>

<blockquote>
How does one compare such different solvents as hexane and DMSO for example? Comparing their relative polarities, which is often related to solubility, might be one way, but this may not be the most important factor for the reaction in question. Additionally, this simple trend may not be relevant in any case, given for example that chlorinated solvents have good solubilising power despite their low-to-medium polarity (as judged by their dielectric constant). On the other hand, the high polarity and solubilising power of alcohols might be compromised in the desired reaction by their protic nature, whilst the “unrelated” hexane and DMSO are both aprotic.
<br><br>
<u>In summary, replacing any one of the discrete parameters with another does not yield a different value on the same axis of a graph, as it would for a continuous parameter; instead it requires a different graph with different axes which may have no meaningful relationship to the first one whatsoever.</u> This means that every single combination of catalyst/ligand/base/solvent is essentially a different reaction for the same two starting materials to produce the same product. <i>(emphasis added)</i>
</blockquote>

<p>
The solution the authors propose is to use principal component analysis (PCA) on molecular descriptors, such as “measurable physical factors (e.g., bp, density, bond length), or calculated and theoretical ones (e.g., electron density, Hansen solubility parameters, Kamlet–Taft solvent polarity parameters),” to convert discrete parameters into continuous ones. This general approach for handling discrete variables—generation of continuous molecular descriptors, followed by use of a dimensionality reduction algorithm—is widely used today for lots of tasks (see for instance <a href=https://pubs.rsc.org/en/content/articlelanding/2016/ob/c5ob01892g>this paper</a> on DoE for solvent screening, and <a href=https://corinwagen.github.io/public/blog/20220926_plotting_diversity.html>my previous post on UMAP</a>).
</p>

<h3>Choosing Intelligent Screening Sets</h3>

<p>
With continuous descriptors for formally discrete variables in hand, a natural next step is to use this data to choose catalyst/substrates that best cover chemical space. (This can be done with several algorithms; see <a href=https://pubs.acs.org/doi/10.1021/jm9700878?ref=PDF>this paper</a> for more discussion) In 2016, this technique was popularized by the Merck <a href=https://pubs.rsc.org/en/content/articlelanding/2016/sc/c5sc04751j>“informer library”</a> approach, which generated sets of aryl boronic esters and aryl halides that could be used to fairly evaluate new reactions against complex, drug-like substrates. (See also <a href=https://pubs.acs.org/doi/full/10.1021/acs.accounts.0c00760>this recent perspective</a> on the Merck informer libraries, and <a href=https://www.pnas.org/doi/full/10.1073/pnas.1409522111>similar work</a> from Matt Sigman a few years earlier.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_merck_informer.png" style="width:600px;" />
  <figcaption> 
  Illustration of the Merck informer library (yellow) in chemical space, compared to drugs (grey) and compounds from recent literature (blue).
  </figcaption>
</figure>

<p>
While the Merck informer libraries were intended to be shared and used by lots of research groups, recently it’s become more common for individual research groups to design their own project-specific screening sets. Abbie Doyle and co-workers <a href=https://pubs.acs.org/doi/10.1021/jacs.1c12203>kicked this off</a> in 2022 by using DFT-based descriptors, UMAP dimensionality reduction, and agglomerative hierarchical clustering to generate a maximally diverse set of commercial aryl bromides. Other groups soon followed suit: <a href=https://pubs.acs.org/doi/10.1021/acscatal.2c01813>Karl Gademann</a> used this approach to study bromotetrazine cross coupling, while Marion Emmert and co-workers at Merck <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c10557>employed similar methods</a> to investigate azole carboxylation. (I’ve also used this approach for substrate selection!)
</p>

<p>
This approach can also be used to design intelligent sets of catalysts/ligands at the outset of a screening project. Using <a href=https://pubs.acs.org/doi/10.1021/jacs.1c09718>their “kraken” dataset of phosphine properties</a>, Tobias Gensch and Matt Sigman <a href=https://pubs.acs.org/doi/10.1021/acscatal.2c01970>proposed a set of 32 commercially available ligands</a> which aims to cover as much of phosphine chemical space as possible in an initial screen. Jason Stevens and co-workers combined this idea with the substrate-selection methods from the previous paragraph to <a href=https://pubs.acs.org/doi/10.1021/acs.organomet.2c00089>perform a detailed study</a> of Ni-catalyzed borylation under many conditions, and tested a variety of ML models on the resulting dataset. (Scott Denmark and co-workers have also used a variant of this idea, called the Universal Training Set, to <a href=https://www.science.org/doi/10.1126/science.aau5631>initialize ML-driven reaction optimization</a>.)
</p>

<h3>New Optimization Algorithms</h3>

<p>
As in every area of life, ML-based approaches have been used a lot for optimization recently. This isn’t new; Klaus Jensen and Steve Buchwald <a href=https://pubs.rsc.org/en/content/articlelanding/2016/RE/C6RE00153J>used machine learning</a> to drive autonomous optimization in 2016, and <a href=https://pubs.acs.org/doi/10.1021/acscentsci.7b00492>Richard Zare</a> published a detailed methodological study in 2017. Nevertheless, as with computational substrate selection, these techniques have come into the mainstream in the past few years.
</p>

<p>
I mentioned the work of Scott Denmark on ML-driven optimization before, and his team published two more papers on this topic last year: <a href=https://pubs.acs.org/doi/10.1021/jacs.2c08820>one on atropselective biaryl iodination</a>, and <a href=https://pubs.acs.org/doi/pdf/10.1021/acs.oprd.1c00155>one on optimization of <i>Cinchona</i> alkaloid-based phase transfer catalysts</a>. In particular, the second paper (conducted in collaboration with scientists at Merck) illustrates how an ML model can be updated with new data as optimization progresses, allowing many sequential rounds of catalyst development to be conducted.
</p>

<p>
Abbie Doyle’s group has done a lot of work on using <a href=https://en.wikipedia.org/wiki/Bayesian_optimization>Bayesian optimization</a> (BO) to drive reaction optimization. <a href=https://www.nature.com/articles/s41586-021-03213-y>Their first paper</a> in this area illustrated the capacity of BO to avoid spurious local minima, and went on to validate this approach in a variety of complex problems. Even better, they compared the results of BO to chemist-guided optimization to see if computer-driven optimization could outcompete expert intuition. To quote the paper:
</p>

<blockquote>
In total, 50 expert chemists and engineers from academia and industry played the reaction optimization game (Fig. 4c). Accordingly, the Bayesian reaction optimizer also played the game 50 times (Fig. 4b), each time starting with a different random initialization. The first point of comparison between human participants and the machine learning optimizer was their raw maximum observed yield at each step during the optimization. Humans made significantly (<i>p</i> &lt; 0.05) better initial choices than random selection, on average discovering conditions that had 15% higher yield in their first batch of experiments. <u>However, even with random initialization, within three batches of five experiments the average performance of the optimizer surpassed that of the humans.</u> Notably, in contrast to human participants, Bayesian optimization achieved &gt;99% yield 100% of the time within the experimental budget. Moreover, Bayesian optimization tended to discover globally optimal conditions (CgMe-PPh, CsOPiv or CsOAc, DMAc, 0.153 M, 105 °C) within the first 50 experiments (Fig. 4b). <i>(emphasis added)</i>
</blockquote>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230118_bayesian_optimization.png" style="width:300px;" />
  <figcaption> 
  Average yield per batch from Bayesian optimization (black) vs. humans (blue), showing how Bayesian optimization outcompetes humans despite a worse start.
  </figcaption>
</figure>

<p>
<a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c08592>Their subsequent work</a> has made their optimization app available online, and illustrated the application of this strategy to other reactions.
</p>

<p>
Closely related is <a href=https://www.science.org/doi/10.1126/science.adc8743> this work</a> from Aspuru-Guzik, Burke, and co-workers, which uses a “matrix-down” approach to choosing representative substrates for the Suzuki reaction (similar to the substrate-selection algorithms discussed previously). The selected substrates are then subjected to automated high-throughput screening guided by an uncertainty-minimizing ML model (i.e., new reactions are chosen based on the regions of chemical space that the algorithm has the least knowledge about; this is similar to, but distinct from, Bayesian optimization). This is a pretty interesting approach, and I hope they study it further in the future. (Aspuru-Guzik has done <a href=https://arxiv.org/abs/2103.03716>lots of other work</a> in this area, including some <a href=https://pubs.acs.org/doi/10.1021/acscentsci.8b00307>Bayesian optimization</a>.)
</p>

<p>
Finally, two papers this year (that I’m aware of) put forward the idea of using multi-substrate loss functions for optimization: <a href=https://www.nature.com/articles/s41586-022-05263-2>our work on screening for generality</a> and <a href=https://chemrxiv.org/engage/chemrxiv/article-details/636a84ab80c9bf01cc8d95f9>a beautiful collaboration</a> from Song Lin, Scott Miller, and Matt Sigman. These papers used “low-tech” optimization methods that are familiar to practicing organic chemists (e.g. “screen different groups at this position”), but evaluated the output of this optimization not based on the yield/enantioselectivity of a single substrate but on aggregate metrics derived from many substrates. The results that our groups were able to uncover were good, but I’m sure adding robotics and advanced ML optimization will turbocharge this concept and find new and better catalysts with truly remarkable generality.
</p>

<h3>Conclusions</h3>

<p>
Reaction optimization is a common task in organic chemistry, but one that’s commonly done without much metacognition. Instead, many researchers will screen catalysts, substrates, and conditions based on habit or convenience, without necessarily dwelling on whether their screening procedure is optimal. While this may work well enough when you only need to optimize one or two reactions in your whole graduate school career (or when acquiring each data point takes days or weeks), <i>ad hoc</i> strategies will at some point simply fail to scale.
</p>

<p>
Organic chemistry, long the realm of “small data,” is slowly but inexorably catching up with the adjacent sciences. As progress in lab automation and instrumentation makes setting up, purifying, and analyzing large numbers of reactions easier, experimental chemists will have to figure out how to choose which reactions to run and how to handle all the data from these reactions, using tools like the ones discussed above. Like it or not, data science and cheminformatics may soon become core competencies of the modern experimental chemist!
</p>

]]></description>
              <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Misleading H/D KIE Experiments</title>
              <link>public/blog/20230103_hdkie_puzzle.html</link>
              <description><![CDATA[
<p>
<i>
Note: old versions of this post lacked a discussion of S<sub>N</sub>2. I've added an appendix which remedies this.
</i>
</p>

<p>
In <a href=https://corinwagen.github.io/public/blog/20220815_rate_determining_span.html>“The Rate-Limiting Span,”</a> I discussed how thinking in terms of the span from ground state to transition state, rather than in terms of elementary steps, can help prevent conceptual errors. Today, I want to illustrate why this is important in the context of a little H/D KIE puzzle. 
</p>

<p>
Consider the following reaction, which could conceivably proceed via an S<sub>N</sub>2 mechanism (red), an S<sub>N</sub>1 mechanism (blue), or really anywhere on the continuum:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_structures.png" style="width:550px;" />
  <figcaption> 
    This is a bit of a silly reaction, admittedly. The intermolecular version or the <i>endo</i> version would have been better.
  </figcaption>
</figure>

<p>
What experiment can be used to investigate the mechanism of this reaction? One possibility is an alpha H/D KIE experiment at the iminium position. Reactions with a sp<sup>3</sup> ground state and an sp<sup>2</sup> transition state display secondary normal alpha H/D KIEs, while reactions with an sp<sup>2</sup> ground state and an sp<sup>3</sup> transition state display secondary inverse KIEs.
Thus, one might think “if iminium formation is rate-limiting, the KIE will be normal, but if alkene addition is rate-limiting, the KIE will be inverse.”
</p>

<p>
Unfortunately this is not true. Instead, all mechanistic possibilities give secondary normal KIEs! I investigated this model system computationally at the wB97X-D/6-31G(d)/SMD(CH<sub>2</sub>Cl<sub>2</sub>) level of theory. Here’s a More O’Ferrall–Jencks plot of the H/D KIE at the iminium position, computed with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>
(breaking bond on Y axis, forming bond on X axis): 
</p>

<!--
<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_energy.png" style="width:500px;" />
  <figcaption> 
    Some of the grid points didn't finish, which explains the scattered gaps. I've also excluded anything with energy more than 30 kcal/mol above the ground state.
  </figcaption>
</figure>
-->

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_kie.png" style="width:550px;" />
  <figcaption> 
    The raw data from PyQuiver is a little fuzzy, so I applied a convolution to smooth the data. Details later on. 
  </figcaption>
</figure>

<p>
Rather than telling us which step is rate-limiting, all the KIE shows us is how much the transition state resembles an iminium ion (bottom right). Structures with long C–Cl bond distances and short C–C bond distances have substantial isotope effects (around 20%), while structures with forming or breaking bonds have smaller isotope effects. 
</p>

<p>
Why is this? Both ionization and addition proceed through iminium-like structures that are substantially sp<sup>2</sup>-hybridized at carbon, irrespective of whether sp<sup>2</sup> character is technically increasing or decreasing in the elementary step. Relative to the starting material, both transition states look like iminium ions and thus lead to large isotope effects. 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_structures2.png" style="width:300px;" />
  <figcaption>
  These both look pretty much like iminium ions.
  </figcaption>
</figure>

<p>
We can also conceptualize this in terms of elementary steps. Taken by itself, alkene addition does lead to an inverse kinetic isotope effect, as seen by the decreasing values as one moves left from the iminium—but an inverse isotope effect relative to the normal equilibrium isotope effect of the iminium. In this system, the equilibrium isotope effect of the iminium is larger than the kinetic isotope effect for alkene addition, and so the combination of these two effects leads to a (smaller) overall normal effect. 
</p>

<p>
(This is the opposite of primary H/D KIEs, where central transition states lead to the largest isotope effects and equilibrium effects are typically small. Here, the isotope effect is mainly a function of hybridization, and so the later the TS, the greater the difference in hybridization and the larger the isotope effect.)
</p>

<p>
In summary, although this experiment seems informative, it’s actually not very useful. It tells you something about the structure of the transition state, but not which step is rate-limiting! In this case, a better experiment would be to measure <sup>13</sup>C KIEs, or an H/D KIE on the nucleophile. 
</p>

<h3>Appendix I: What About S<sub>N</sub>2?</h3>

<p>
On Twitter, <a href="https://twitter.com/LevinChem/status/1610287881993728000">Mark Levin</a> asks about the KIE for the concerted path. I originally meant to include a discussion of this, but then totally forgot to! So let’s fix that.
</p>

<p>
As shown in the graph above, extremely concerted pathways (i.e. going right down the middle of the MOJ plot) will have pretty small isotope effects. These sorts of mechanisms are common where the carbocation would be extremely unstable (methyl iodide) but much less common for stabilized carbocations like we’re discussing here. When oxocarbeniums or iminiums are involved, even “concerted” mechanisms shift towards the bottom right corner: this is the <a href=https://pubs.acs.org/doi/pdf/10.1021/ja506092h>“loose”/“exploded” S<sub>N</sub>2</a> often seen in glycosylation. These pathways will have a modest to large H/D KIE, depending on the exact system and how “exploded” they are (see page 53 of <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>this SI</a> for examples)
</p>

<p>
Putting this together, then, what experimental results would be conclusive? A very small KIE would be diagnostic for a “classic” synchronous S<sub>N</sub>2 process, which I consider to be very unlikely here. But medium or large H/D KIEs are consistent with any possibility: S<sub>N</sub>1 with rate-limiting ionization, S<sub>N</sub>1 with rate-limiting nucleophilic attack, or a loose S<sub>N</sub>2. There’s an annulus of different mechanistic possibilities that all give about the same isotope effect, as shown below:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20230103_sn2_plot.png" style="width:470px;" />
  <figcaption> 
  Any TS in the red zone is consistent with a moderately large KIE (say, 15%).
  </figcaption>
</figure>

<p>
To make matters worse, H/D KIEs are pretty tough to simulate quantitatively, because of tunneling, so the annulus isn’t even that precise. That’s why I think this isn’t a very good experiment.
</p>

<h3>Appendix II: Smoothing the KIE Grid</h3>

<p>
The raw KIE values from PyQuiver were pretty noisy, probably because there are small or multiple imaginary frequencies for some of these non-stationary points, so I used convolution to smooth things out a bit. 
</p>

<pre class=code-block>
import numpy as np
from scipy.ndimage import convolve

#### this code block takes a 2d np.ndarray of KIE values 
#### and returns a smoothed np.ndarray with the same dimensions

corner_w = 0.05
edge_w = 0.2
kernel = np.array([
    [corner_w, edge_w, corner_w],
    [edge_w, 1 ,edge_w],
    [corner_w, edge_w, corner_w]
])
kernel = kernel / np.sum(kernel)

smooth_grid = convolve(kie_grid, kernel, mode="nearest")
</pre>

<p>
I haven’t seen this technique used before, but it doesn’t seem unreasonable to me.
</p>
]]></description>
              <pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Books from 2022</title>
              <link>public/blog/20221231_books.html</link>
              <description><![CDATA[
<p>
Last January, I aimed to read 50 books in 2022. I got through 32, which is at least more than I read in 2021.
</p>

<p>
There’s been a bit of <a href=https://twitter.com/nabeelqu/status/1608874209375313920>discourse</a> around whether setting numerical reading goals for oneself is worthwhile or counterproductive. I don’t have a strong opinion in the abstract, but I noticed that consciously tracking how many books I read served as a little nudge to read more than I would otherwise, without really compromising the quality of books I was reading.
</p>

<p>
In order, then:
</p>

<b>
#1. Neal Stephenson, <i>Snow Crash</i> (reread)
</b>

<p>
I read this in high school, and wanted to read it again in light of recent Metaverse-related discourse. It didn’t disappoint, although it’s a little more “action movie” than Stephenson’s later works.
</p>

<b>
#2. Aristotle, <i>Nicomachean Ethics</i>
</b>
<br>
<b>
#3. Tim Keller, <i>Prayer</i> (reread)
</b>
<br>
<b>
#4–17. Robert Jordan &amp; Brandon Sanderson, <i>Wheel of Time</i>
</b>

<p>
Beyond the surface-level plot (which is fun), <i>Wheel of Time</i> is a fascinating exploration of a number of societies structured around complementarianism at a deep level.
</p>

<b>
#18. Paul Tripp, <i>Parenting</i>
</b>

<br>
<b>
#19. Peter Thiel, <i>Zero To One</i>
</b>

<p>
I’ve discussed this book <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>previously</a>.
</p>

<b>
#20. Peter Scazzero, <i>Emotionally Healthy Spirituality</i>
</b>

<br>
<b>
#21. Eric Berger, <i>Liftoff</i>
</b>

<p>
This is a good account of the early days of SpaceX, and works well as a book-length answer to the question “What decisions or mindsets allowed Elon Musk to succeed in starting a rocket company when so many other billionaires failed?” or equivalently “What—besides money—explains SpaceX’s success?”
</p>

<p>
My summary, based on the book, would be (in no particular order): (1) a focus on recruiting top talent, (2) a “can-do” spirit / commitment to moving quickly and recklessly, (3) decisive decision-making at top levels of the organization, (4) a willingness to internalize lots of tasks to increase efficiency, and (5) luck.
</p>

<b>
#22. Roald Dahl, <i>Going Solo</i>
</b>
<br>
<b>
#23. Yiyun Li, <i>Must I Go</i>
</b>
<br>
<b>
#24. Tyler Cowen &amp; Daniel Gross, <i>Talent</i>
</b>

<p>
I’ve also discussed this book <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>previously</a>.
</p>

<b>
#25. Stanley Gundry (Ed.), <i>Five Views on Law and Gospel</i>
</b>

<p>
This book presents five different theological “takes” on the relationship between Old Testament law and the New Testament—there was much less consensus than I expected! It is interesting but scholarly, and not an easy read.
</p>

<p>
There are a whole bunch of books in this series; each author writes an essay explaining their position, and then writes brief responses to the other authors’ positions. This format should be more common!
</p>

<b>
#26. Albert Hirschman, <i>Exit, Voice, and Loyalty</i>
</b>

<p>
I discussed pieces of this book <a href="https://corinwagen.github.io/public/blog/20221018_omelas_hirschman_altom.html">here</a>; the rest is also good.
</p>

<b>
#27. Celeste Ng, <i>Little Fires Everywhere</i>
</b>
<br>
<b>
#28. Fuchsia Dunlop, <i>The Food of Sichuan</i>
</b>

<p>
As discussed on <a href=https://conversationswithtyler.com/episodes/fuchsia-dunlop/> Conversations with Tyler</a>.
</p>

<b>
#29. Geoffrey Moore, <i>Crossing The Chasm</i>
</b>

<p>
This book is about 100 pages longer than it needs to be.
</p>

<b>
#30. John Owen, <i>The Mortification of Sin</i> (abridged)
</b>

<p>
As recommended <a href=https://twitter.com/timkellernyc/status/1477975783641595905>by Tim Keller</a>!
</p>

<b>
#31. Margaret Atwood, <i>Oryx and Crake</i>
</b>
<br>
<b>
#32. Alison Weir, <i>The Wars of the Roses</i>
</b>

<p>
This book is a nice account of the Wars of the Roses in the style of a novel; I didn’t know anything beyond the broad strokes, so I found it quite gripping. My biggest complaint is that the book only goes through 1471, and so doesn’t cover any of the Bosworth Field-adjacent events.
</p>

<p>
My reading this year was about 50% fiction (18 books), with the remainder mostly divided between business (5 books) and Christianity (5 books). My main goal for next year is to read more history; I didn’t end up reading very much history this year, and I miss it.
</p>

]]></description>
              <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Evaluating Error in Boltzmann Weighting</title>
              <link>public/blog/20221228_boltzmann_error.html</link>
              <description><![CDATA[
<p>
A technique that I’ve seen employed more and more in computational papers over the past few years is to calculate Boltzmann-weighted averages of some property over a conformational ensemble. This is potentially very useful because most complex molecules exist in a plethora of conformations, and so just considering the lowest energy conformer might be totally irrelevant. 
To quote a <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.202205735>recent perspective</a> from Grimme and friends:
</p>

<blockquote>
For highly flexible structures, a molecular property, such as energy, nuclear magnetic resonance spectra, or optical rotation values may not be sufficiently described by a single structure. At finite temperatures, various conformers are populated and the overall property must be described as thermal average over the unique property values of each conformer.
</blockquote>

<p>
What's been bothering me about this, however, is that Boltzmann weights are calculated as <i>e</i> to the power of the relative energy:
</p>

<pre class=code-block>
def boltzmann_weight(conformer_properties, conformer_energies, temp=298):
    """ Some simple Python code to calculate Boltzmann weights. """

    energies = conformer_energies - np.min(conformer_energies)

    R = 3.1668105e-6 # eH/K
    weights = np.exp(-1*energies/(627.509*R*temp))
    weights = weights / np.sum(weights)

    return weights, np.average(conformer_properties, weights=weights)
</pre>

<p>
Since relative energies of conformers are usually calculated with only middling accuracy (±0.2 kcal/mol with <a href=https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j>common methods</a>), we’re taking the exponential of a potentially inaccurate value—which seems bad from the standpoint of error propagation!
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_xkcd.png style="width:375px;" />
  <figcaption>
  Excerpt from <a href="https://xkcd.com/2295/">xkcd</a>, on error propagation (line #3 is what's relevant here).
  </figcaption>
</figure>

<p>
Grimme and co-authors address this point explicitly in their review:
</p>

<blockquote>
At the same time, however, conformational energies need to be accurate to within about 0.1–0.2 kcal mol<sup>−1</sup> to predict Boltzmann populations at room temperature reasonably well. This is particularly important since properties can vary strongly and even qualitatively between populated conformers…
</blockquote>

<p>
Although the best answer is, of course, to just get more accurate energies, it's not always practical to do that in the real world.
If we take imperfect energies as our starting point, what's the best strategy to pursue?
</p>

<p>
One could imagine a scenario in which error causes relatively unimportant conformers to end up with large weights, making the answer even worse than the naïve approach would have been. If the lowest energy conformer accounts for 60-70% of the answer, might it be best to just stick with that, instead of trying to throw in some messy corrections?
</p>

<p>
To test this, I drew a random flexible-looking molecule with a few functional groups, conducted a conformational search using <i>crest</i>, and then optimized it and calculated <sup>19</sup>F NMR shieldings using wB97X-D/6-31G(d). (There are <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.8b00624>better NMR methods</a> out there, but the absolute accuracy of the shift isn’t really the point here.)
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_cylview.png style="width:350px;" />
  <figcaption>
  The lowest energy conformer of the molecule I drew (3-fluorohex-5-enal).
  </figcaption>
</figure>

<p>
I then computed more accurate energies using DLPNO-CCSD(T)/cc-pVTZ, and compared the results from Boltzmann weighting with DFT and coupled-cluster energies.
(<sup>19</sup>F values are just the isotropic shielding tensor, and energies are in kcal/mol.)
</p>

<table>
  <tr>
    <th>Conformer</th> 
    <th><sup>19</sup>F shift</th> 
    <th>DFT energy</th>
    <th>DFT weight</th>
    <th>CC energy</th>
    <th>CC weight</th>
  </tr>
  <tr><td>c00003</td><td>401.76</td><td>0.00</td><td>0.624</td><td>0.00</td><td>0.529</td></tr>
  <tr><td>c00001</td><td>403.08</td><td>1.02</td><td>0.112</td><td>0.68</td><td>0.167</td></tr>
  <tr><td>c00010</td><td>396.63</td><td>1.12</td><td>0.093</td><td>1.10</td><td>0.083</td></tr>
  <tr><td>c00007</td><td>391.45</td><td>1.56</td><td>0.045</td><td>1.54</td><td>0.039</td></tr>
  <tr><td>c00004</td><td>396.77</td><td>1.82</td><td>0.029</td><td>1.64</td><td>0.033</td></tr>
  <tr><td>c00006</td><td>400.16</td><td>2.31</td><td>0.013</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00029</td><td>400.37</td><td>2.36</td><td>0.012</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00032</td><td>393.96</td><td>2.05</td><td>0.020</td><td>1.76</td><td>0.027</td></tr>
  <tr><td>c00027</td><td>394.60</td><td>2.54</td><td>0.009</td><td>2.21</td><td>0.013</td></tr>
  <tr><td>c00017</td><td>394.69</td><td>3.12</td><td>0.003</td><td>2.27</td><td>0.011</td></tr>
  <tr><td>c00018</td><td>402.24</td><td>2.24</td><td>0.014</td><td>2.35</td><td>0.010</td></tr>
  <tr><td>c00011</td><td>381.31</td><td>2.59</td><td>0.008</td><td>2.49</td><td>0.008</td></tr>
  <tr><td>c00023</td><td>388.77</td><td>2.51</td><td>0.009</td><td>2.54</td><td>0.007</td></tr>
  <tr><td>c00013</td><td>390.32</td><td>3.02</td><td>0.004</td><td>2.61</td><td>0.006</td></tr>
  <tr><td>c00020</td><td>394.97</td><td>3.23</td><td>0.003</td><td>2.62</td><td>0.006</td></tr>
  <tr><td>c00015</td><td>398.24</td><td>3.02</td><td>0.004</td><td>2.97</td><td>0.004</td></tr>
  <tr><td>&nbsp;</td><td></td><td></td><td></td><td></td><td></td><tr>
  <tr><th>Final <sup>19</sup>F Shift</th><td></td><td></td><td>400.20</td><td></td><td>400.13</td></tr>
</table>

<p>
The match is really quite good, much better than just guessing the lowest energy conformer would have been! This is despite having a decent number of low-energy conformers, so I don’t think this is a particularly rigged case.
</p>

<p>
But, what if we just got lucky in this case? The relative energies are off by 0.28 kcal/mol on average. If we simulate adding 0.28 kcal/mol of error to each of the “true” energies a bunch of times, we can see how well Boltzmann weighting does on average, even with potentially unlucky combinations of errors.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_randomShifts.png style="width:400px;" />
  <figcaption>
  Shifts from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
The above image shows the predicted shift from 100,000 different randomly generated sets of “wrong” energies. We can see that the Boltzmann-weighted value is almost always closer to the true value than the shift of the major conformer is (99.01% of the time, to be precise). This is despite substantial changes in the weight of the major conformer:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221228_randomWeights.png style="width:400px;" />
  <figcaption>
  Major conformer weights from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
Thus, we can see that Boltzmann weighting is relatively resistant to random errors in this case. Although this is only one molecule, and no doubt scenarios can be devised where inaccurate energies lead to ludicrously incorrect predictions, this little exercise has helped temper my skepticism of Boltzmann weighting.
</p>

<i>
Thanks to Eugene Kwan and Joe Gair for discussions around these topics over the past few years. Data available upon request.
</i>

]]></description>
              <pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Low-Code Conformational Searching</title>
              <link>public/blog/20221219_low_code_csearch.html</link>
              <description><![CDATA[
<p>
Today I want to engage in some shameless self-promotion and highlight how <a href=https://github.com/ekwan/cctk><i>cctk</i></a>, an open-source Python package that I develop and maintain with Eugene Kwan, can make conformational searching easy.
</p>

<p>
Conformational searching is a really crucial task in computational chemistry, because pretty much everything else you do depends on having the correct structure in the computer. In simple cases you can just draw out every conformer manually, but as the system under study gains degrees of freedom it becomes increasingly impractical to think through every possibility.
</p>

<p>
Failure to identify the correct conformer can lead to completely incorrect results, as demonstrated by Neese and coworkeers in <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">this recent article</a>. They reexamine a reaction <a href="https://www.science.org/doi/10.1126/science.aaq0445">originally studied by Ben List</a> and demonstrate that the conformers examined in the initial publication are almost 5 kcal/mol above the true lowest-energy conformers.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221219_neese_csearch.gif style="width:450px;" />
  <figcaption>
  Figure 1 from the paper; the previously reported conformers are shown in green.
  </figcaption>
</figure>

<p>
Conformational searching approaches attempt to prevent this sort of error by automating the process of finding conformers. There are lots of different algorithms one can use, like <a href="https://pubs.acs.org/doi/10.1021/ja952478m">low-mode searching</a>, metadynamics, and replica exchange (to name just a few), and decades of literature on this topic.
</p>

<p>
Since conformational searching requires many individual calculations, it’s <a href=https://link.springer.com/protocol/10.1007/978-1-0716-0282-9_14>almost never practical</a> to do a conformational search at a high level of theory (e.g. using DFT or <i>ab initio</i> methods). Instead, <a href="https://pubs.acs.org/doi/10.1021/acs.joc.2c00066">forcefields</a> or <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.5b00671">semiempirical</a> methods are generally used, with the caveat that the conformers generated might have somewhat inaccurate geometries.
</p>

<p>
<i>cctk</i> uses <a href="https://crest-lab.github.io/crest-docs/">crest</a> (from Grimme and coworkers), which uses a <a href=https://crest-lab.github.io/crest-docs/page/overview/workflows.html#imtd-gc-algorithm>metadynamics-based algorithm</a> with the <i>GFN2-xtb</i> semiempirical method to generate and score conformers. Although <i>crest</i> isn’t perfect, it’s simple, easy to use, and often generates very reasonable results.
</p>

<p>
I personally find the <i>crest</i> syntax a little tough to remember, so I’ve created a Python script so that I don’t have to look it up every time. 
</p>

<h3>Installing Packages</h3>

<p>
To run this tutorial, you’ll need to have <i>cctk</i> and <i>crest</i> installed. It’s often easiest to manage dependencies using a <i>conda</i> environment; if you don’t already have one, you can create one for this project with this code:
</p>

<pre class=code-block>
conda create --name=chem python=3.8
pip install cctk
pip install pyyaml
conda install -c conda-forge crest
</pre>

<p>
And in the future, you can activate the environment like this:
</p>

<pre class=code-block>
conda activate chem
</pre>

<h3>Running the Tutorial</h3>

<p>
The files for this tutorial can be found <a href="https://github.com/corinwagen/utilities/tree/master/csearch">here</a>. <span class=code>ex.yaml</span>, which is the only file you should need to modify, contains all the information needed for the python script <span class=code>do_crest.py</span>:
</p>

<pre class=code-block>
# list of atoms to constrain
# atom1, atom2, distance (or "auto" to keep distance from initial geometry)
constraints:
    constraint1: 17 31 auto
    constraint2: 30 31 auto

# location of input geometry, either as Gaussian .gjf or .out file
input_geom: pictet_spengler.gjf

# directory in which crest will run (will be created)
directory: crest

# name of logfile
logfile: crest.log

# whether or not this is a noncovalent complex (true or false).
# this simply gets passed to crest; some settings are changed.
noncovalent: false
</pre>

<p>
To generate conformers, simply run:
</p>

<pre class=code-block>
python do_crest.py ex.yaml
</pre>

<p>
This takes about 30 seconds to run on my laptop, and will generate about a hundred output conformers, which can (if desired) be further refined using DFT.
</p>

<p>
Hopefully this is useful! Please feel free to contact me with questions or bug reports.
</p>
]]></description>
              <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Response to Comments on &#34;Against Carbon NMR&#34;</title>
              <link>public/blog/20221216_carbon_nmr_response.html</link>
              <description><![CDATA[
<p>
Since my previous <a href="https://twitter.com/CraftyCarbene/status/1603572282953289729">“based and red pilled”</a> post seems to have struck a nerve, I figured I should address some common objections people are raising. 
</p>

<p>
Although this is obvious, I wanted to preface all of this by saying: this is my opinion, I'm not some expert on systems of science,
and many of the criticisms come from people with much more scientific and institutional expertise than me. 
It's very possible that I'm just totally wrong here! 
But what I'm saying makes sense to me, and (it seems) to a lot of other people, so I think it's at least worth having this discussion.
</p>

<h2>Commenters Who Feel <sup>13</sup>C NMR Is Scientifically Crucial</h2>

<p>
A few people pointed out that there are lots of instances in which carbon NMR <i>is</i> very important
(<a href="https://twitter.com/MuhammadAdilSA/status/1603339486431416320">1</a>,
<a href="https://twitter.com/craigdc1983/status/1603300877208621056">2</a>,
<a href="https://twitter.com/Double_Anne_/status/1603358363580088320">3</a>,
<a href="https://twitter.com/BogdosMichael/status/1603413304885612545">4</a>,
<a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">5</a>).
I don't disagree with this at all; I've also used <sup>13</sup>C NMR to solve problems that <sup>1</sup>H NMR and mass spectrometry alone couldn't solve!
But just because it’s crucial sometimes doesn’t mean it’s crucial all the time.
Does anyone really think that you need carbon NMR to tell if Boc protection of a primary amine worked? 
</p>

<p>
Most of the reactions that people do—especially people for whom synthetic chemistry is a means and not an end—are <a href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409">pretty straightforward</a>, such that I think it’s fair to assume you could deduce the correct product with high confidence without <sup>13</sup>C NMR.
(Again, if carbon spectra were so crucial, it wouldn’t be the case that many people don’t get them until the very end of the project.)

<h2>Commenters Who Feel That It's Important To Have Non-Crucial Data To Test Your Hypotheses</h2>

<p>
This point was also made by a number of people
(<a href=https://twitter.com/OrthaberLab/status/1603120117444919301>1</a>,
<a href=https://twitter.com/dasingleton/status/1603453351592706067>2</a>,
<a href=https://twitter.com/andrechemist/status/1603465213436633088>3</a>), 
perhaps most succinctly by <a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">“Chris Farley”</a>:

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_chris_farley.png style="width:450px;" />
  <figcaption>
  Never meet your heroes.
  </figcaption>
</figure>

<p>
I think this is an important point—part of what we ought to do, as a scientific community, is challenge one another to test our hypotheses and scrutinize our assumptions. Nevertheless, I’m not convinced this is a particularly strong argument for carbon NMR specifically. What makes <sup>13</sup>C{<sup>1</sup>H} spectra uniquely powerful at challenging one’s assumptions, as opposed to other data?
</p>

<p>
<a href=https://twitter.com/kjfritzsc/status/1603224459753959424>Keith Fritzsching</a> points out that HSQC is much faster and gives pretty comparable information (as did other readers, privately), and simply replacing <sup>13</sup>C with HSQC in most cases seems like it would nicely balance hypothesis testing with efficiency.  
</p>

<p>
(Relatedly, <a href=https://twitter.com/XiaoX_chem/status/1603251065293373440>Xiao Xiao</a> recounts how reviewers will request <sup>13</sup>C spectra even when there’s plenty of other data, including HSQC and HMBC. This is a pretty nice illustration of how powerful status quo bias can be.)
</p>

<h2>Commenters Who Say Carbon Spectra Are Easy To Acquire</h2>

<p>
Both <a href="https://twitter.com/VT_Chemist/status/1603153621511806979">@VT_Chemist</a> and <a href="https://twitter.com/spfletcher/status/1603452172510924800">Steve Fletcher</a> made this point:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_steve_fletcher.png style="width:450px;" />
  <figcaption>
  (cue flashbacks to undergraduate me trying to dissolve enough of some tetracyclic monster in pyridine-d5 to see my last quat)
  </figcaption>
</figure>

<p>
I've heard this from plenty of people before, and it's true that sometimes it's not hard at all to get a nice carbon spectrum! But sometimes it <i>is</i> really hard, also.
Based on the other responses, it seems like lots of other people agree with this sentiment.
</p>

<p>
(Is it possible that some of this disagreement reflects whether one has access to a helium cryoprobe?)
</p>

<h2>Commenters Who Feel It's Important To Have Consistent Journal Standards</h2>

<p>
A few people pointed out that making carbon NMR necessary on a case-by-case basis would be burdensome for editors and reviewers, since they'd have to think through each case themselves
(<a href="https://twitter.com/Double_Anne_/status/1603358846734548992">1</a>, <a href="https://twitter.com/rapodaca/status/1603471344468840448">2</a>). 
This is a fair point, and one I don't have a good response to. 
</p>

<p>
However, it's worth noting that this is already what we do for pretty much every other claim, including many complex structural problems: give the data, draw a conclusion, and ask reviewers to evaluate the logic.
Arguments about where the burden of proof should lie are tedious and usually unproductive, but I think we should have a pretty high barrier to making specific methods <i>de jure</i> required for publication.
</p>

<h2>Commenters Who Dislike My Claim That Journals Could Permit More Errors</h2>

<p>
I'm going to highlight <a href="https://twitter.com/dasingleton/status/1603441344747388929">Dan Singleton</a> here, someone I respect a ton:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221216_dan_singleton.png style="width:450px;" />
  <figcaption>
  The thread goes on, obviously, and is worth reading.
  </figcaption>
</figure>

<p>
I’m not trying to suggest that journals ought not to care about accuracy at all; <i>ceteris paribus</i>, accuracy should be prioritized. But given that we’re all constrained by finite resources, it’s important to consider the tradeoffs we’re making with every policy decision. It’s possible that trying to increase accuracy by asking for more data could have deleterious consequences:
</p>

<blockquote>
There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. 
</blockquote>

<p>
I think it’s easiest to think about these issues in terms of two separate questions: (1) relative to where we are today, should we push for more accuracy in the literature or less, and (2) are we achieving our chosen degree of accuracy in the most efficient manner possible? 
</p>

<p>
The first question is clearly complex, and probably deserves a longer and fuller treatment that I can provide here—although I’ll note that <a href=https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review>others have espoused</a> <a href=https://www.liamkofibright.com/uploads/4/8/9/8/48985425/is_peer_review_a_good_idea_.pdf>more radical positions</a> than mine on peer review (h/t <a href=https://twitter.com/BogdosMichael/status/1603413314868060160>Michael Bogdos</a> for the second link). I hope to write more on this subject later.
</p>

<p>
But the second question seems more straightforward. Is requiring <sup>13</sup>C NMR for every compound a Pareto-optimal way to ensure accuracy, as opposed to HSQC or HMBC? I struggle to see how the answer can be yes.
</p>

]]></description>
              <pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Against Carbon NMR</title>
              <link>public/blog/20221214_against_carbon_nmr.html</link>
              <description><![CDATA[
<p>
<sup>13</sup>C NMR is, generally speaking, a huge waste of time.
</p>

<p>
This isn’t meant to be an attack on carbon NMR as a scientific tool; it’s an excellent technique, and gives structural information that no other methods can. Rather, I take issue with the requirement that the identity of every published compound be verified with a <sup>13</sup>C NMR spectrum.
</p>

<p>
Very few <sup>13</sup>C NMR experiments yield unanticipated results. While in some cases <sup>13</sup>C NMR is the only reliable way to characterize a molecule, in most cases the structural assignment is obvious from <sup>1</sup>H NMR, and any ambiguity can be resolved with high-resolution mass spectrometry. Most structure determination is boring. Elucidating the structure of bizarre secondary metabolites from sea sponges takes <sup>13</sup>C NMR; figuring out if your amide coupling or click reaction worked does not.
</p>

<p>
The irrelevance of <sup>13</sup>C NMR can be shown via the doctrine of revealed preference: most carbon spectra are acquired only for publication, indicating that researchers are confident in their structural assignments without <sup>13</sup>C NMR. It’s not uncommon for the entire project to be scientifically “done” before any <sup>13</sup>C NMR spectra are acquired. In most fields, people treat <sup>13</sup>C NMR like a nuisance, not a scientific tool—the areas where this isn’t true, like total synthesis or sugar chemistry, are the areas where <sup>13</sup>C NMR is actually useful.
</p>

<p>
Requiring <sup>13</sup>C NMR for publication isn’t costless. The low natural abundance of <sup>13</sup>C and poor gyromagnetic ratio means that <sup>13</sup>C NMR spectra are orders of magnitude more difficult to obtain than <sup>1</sup>H NMR spectra. As a result, a large fraction of instrument time in any chemistry department is usually dedicated to churning out <sup>13</sup>C NMR spectra for publication, while people with actual scientific problems are kept waiting. <sup>13</sup>C NMR-induced demand for NMR time means departments have to buy more instruments, hire more staff, and use more resources; the costs trickle down.
</p>

<p>
And it’s not like eliminating the requirement to provide <sup>13</sup>C NMR spectra would totally upend the way we do chemical research. Most of our field’s history, including some of our greatest achievements, were done in the age before carbon NMR—the first <sup>13</sup>C NMR study of organic molecules was done by <a href="https://aip.scitation.org/doi/10.1063/1.1743253">Lauterbur</a> in 1957, and it would take even longer for the techniques to advance to the point where non-specialists could use the technique routinely. Even in the early 2000s you can find <i>JACS</i> papers without <sup>13</sup>C NMR spectra in the SI, indicating that it's possible to do high-quality research without it. 
</p>

<p>
Why, then, do we require <sup>13</sup>C NMR today? I think it stems from a misguided belief that scientific journals should be the ultimate arbiters of truth—that what’s reported in a journal ought to be trustworthy and above reproach. We hope that by requiring enough data, we can force scientists to do their science properly, and ferret out bad actors along the way. (Perhaps the clearest example of this mindset is <i>JOC</i> &amp; <i>Org. Lett.</i>, who maintain an ever-growing <a href="https://publish.acs.org/publish/author_guidelines?coden=joceah#data_requirements">list of standards</a> for chemical data aimed at requiring all work to be high quality.) Our impulse to require more and more data flows from our desire to make science an institution, a vast repository of knowledge equipped to combat the legions of misinformation.
</p>

<p>
But this hasn’t always been the role of science. Geoff Anders, <a href="https://www.palladiummag.com/2022/10/10/the-transformations-of-science/">writing for <i>Palladium</i></a>, describes how modern science began as an explicitly anti-authoritative enterprise:
</p>

<blockquote>
Boyle maintained that it was possible to base all knowledge of nature on personal observation, thereby eliminating a reliance on the authority of others. He further proposed that if there were differences of opinion, they could be resolved by experiments which would yield observations confirming or denying those opinions. The idea that one would rely on one’s own observations rather than those of others was enshrined in the motto of the Royal Society—<i>nullius in verba</i>.
</blockquote>

<p>
<i>nullius in verba</i> translates to “take no one’s word for it,” not exactly a ringing endorsement of science as institutional authority. This same theme can be found in more recent history. Melinda Baldwin’s <a href="https://astralcodexten.substack.com/p/your-book-review-making-nature"><i>Making Nature</i></a> recounts how peer review—a now-core part of scientific publishing—became commonplace at <i>Nature</i> only in the 1970s. In the 1990s it was still common to publish organic chemistry papers without supporting information.
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221214_royal_society.jpeg style="width:450px;" />
  <figcaption>
  The Royal Society—a group of folks who definitely didn't take <sup>13</sup>C NMR spectra.
  </figcaption>
</figure>

<p>
The point I’m trying to make is not that peer review is bad, or that scientific authority is bad, but that the goal of enforcing accuracy in the scientific literature is a new one, and perhaps harder to achieve than we think. There are problems in the scientific literature everywhere, big and small. John Ioannidis memorably claimed that <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">“most published findings are false,”</a> and while chemistry may not suffer from the same issues as the social sciences, we have our own problems. 
<a href="https://pubs.acs.org/doi/10.1021/acscentsci.2c00325">Elemental analysis doesn’t work</a>, <a href="https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e">integration grids cause problems</a>, and even <a href="http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html">reactions from famous labs can’t be replicated</a>. Based on this, we might conclude that we’re very, very far from making science a robust repository of truth.
</p>

<p>
Nevertheless, progress marches on. A few misassigned compounds here and there don’t cause too many problems, any more than a faulty elemental analysis report or a sketchy DFT study. Scientific research itself has mechanisms for error correction: anyone who’s ever tried to reproduce a reaction has engaged in one such mechanism. Robust reactions get used, cited, and amplified, while reactions that never work slowly fade into obscurity. Indeed, despite all of the above failures, we’re living through a golden age for our field.
</p>

<p>
Given that we will never be able to eradicate bad data completely, the normative question then becomes “how hard should we try?” In an age of <a href="https://www.agrarheute.com/sites/agrarheute.com/files/2020-01/innovation_scientific_progress.pdf">declining research productivity</a>, we should be mindful not only of the dangers of low standards (proliferation of bad work) but also of the dangers of high standards (making projects take way longer). There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. The claim I hope to make here is that requiring <sup>13</sup>C NMR for every compound does more to slow down good work than it does to prevent bad work, and thus should be abandoned.
</p>

<i>
Update 12/16/2022: see <a href="https://corinwagen.github.io/public/blog/20221216_carbon_nmr_response.html">some followup remarks</a> based on feedback from Twitter.
</i>
]]></description>
              <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Definite Games, Indefinite Optimism</title>
              <link>public/blog/20221206_definite_games_indefinite_optimism.html</link>
              <description><![CDATA[
<p>
One of the more thought-provoking pieces I read last year was Alex Danco’s post <a href=”https://alexdanco.com/2021/01/11/why-the-canadian-tech-scene-doesnt-work/”>“Why the Canadian Tech Scene Doesn’t Work,”</a> which dissects the structural and institutional factors that make Silicon Valley so much more effective at spawning successful companies than Toronto.
I’ll briefly summarize the piece’s key arguments here, connect it to some ideas from <i>Zero to One</i>, and finish by drawing some conclusions for academia.
</p>

<p>
Danco’s key insight is applying James Carse’s distinction between <a href=”https://en.wikipedia.org/wiki/Finite_and_Infinite_Games”>finite and infinite games</a> to entrepreneurship. What makes a game “finite” or “infinite”?
</p>

<blockquote>
First, <i>finite</i> games are played <i>for the purpose of winning</i>. Whenever you’re engaging in an activity that’s definite, bounded, and where the game can be completed by mutual agreement of all the players, then that’s a finite game. Much of human activity is described in finite game metaphors: wars, politics, sports, whatever. When you’re playing finite games, each action you take is directed towards a pre-established goal, which is to win.
<br>
<br>
In contrast, <i>infinite</i> games are played <i>for the purpose of continuing to play</i>. You do not “win” infinite games; these are activities like learning, culture, community, or any exploration with no defined set of rules nor any pre-agreed-upon conditions for completion. The point of playing is to bring new players into the game, so they can play too. You never “win”, the play just gets more and more rewarding.
</blockquote>

<p>
In entrepreneurship, Danco argues that infinite games are good and finite games are bad. Good founders are playing infinite games: they want to build something important and keep on contributing to society and progress. In contrast, bad founders are playing to win a finite game—acquiring lots of funding, getting high valuation, or exiting with a big IPO.
</p>

<p>
Danco identifies numerous ways that the Canadian startup ecosystem incentivizes finite games at the expense of infinite games. One important factor favoring finitude is the scrutiny given to deals between founders and funders (e.g. in a seed round), which tends to favor conservative or incremental ventures over ambitious, idealistic ones:
</p>

<blockquote>
[High deal scrutiny] is bad, for two reasons. It’s bad because the very best startups, who have the longest time horizon and are most curious about the world, will look disproportionately uninspiring. They’ll have the fewest definite wins relative to their ambition, and the most things that can potentially go wrong.…
<br>
<br>
Conversely, [high deal scrutiny is] bad because startups will learn to optimize for how to get funded. So if seed deals take 3 months, then founders will learn to build companies that look good under that kind of microscope. And that means they’re going to optimize for playing determinate games, so that they can show definable wins that can’t be argued against; rather than what they should be focusing on, which is open-ended growth.
</blockquote>

<p>
Government support for startups also tends to prioritize finite games at the expense of infinite games, since the requisite bureaucracy tends to stifle fast-moving innovation:
</p>

<blockquote>
The problem with [research tax] credits, honestly through no fault of their own, is that you have to say what you’re doing with them. This seems like a pretty benign requirement; and honestly it’s pretty fair that a government program for giving out money should be allowed to ask what the money’s being used for. But in practice, once you take this money and you start filling out time sheets and documenting how your engineers are spending their day, and writing summaries of what kind of R&amp;D value you’re creating, you are well down the path to destroying your startup and killing what makes it work.
</blockquote>

<p>
Overall, Danco paints a picture of a place where an obsession with goals and benchmarks has almost completely crowded out sincere innovation:
</p>

<blockquote>
Quite in character with our love of milestones, Canada loves anything with structure: accelerators, incubators, mentorship programs; anything that looks like an “entrepreneurship certificate”, we can’t get enough of it. <b>We’re utterly addicted with trying to break down the problem of growing startups into bite-size chunks, thoughtfully defining what those chunks are, running a bunch of promising startups through them, and then coming out perplexed when it doesn’t seem to work.</b> <i>(emphasis added)</i>
</blockquote>

<p>
While Danco limits himself to comparing Canada and Silicon Valley, this failure mode is sadly not confined to Canada. Indeed, many of his observations are directly applicable to academic research. The large number of finite games in academia—publishing papers, writing a dissertation, submitting grants, getting tenure—tends to crowd out the more impactful infinite games that lead to real, meaningful progress, and promotes incremental projects with a high chance of success. (This is simply Charles Hummel’s <a href=”https://www.theartofsimple.net/fighting-the-tyranny-of-the-urgent-at-home/”>“tyranny of the urgent”</a> by a different name.)
</p>

<p>
My claim is that the distinction between finite and infinite games is best understood through <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>Peter Thiel’s</a> concept of definite and indefinite optimism. In Thiel’s dichotomy, indefinite optimists believe the world will get better but have no idea how, whereas definite optimists have a concrete proposal for how to make the world better.
</p>

<p>
How does this connect to finite and infinite games? Paradoxically, indefinite optimism leads to an obsession with finite games, since there’s no higher animating principle at work to drive progress. When you don’t have any positive vision for innovation, the natural solution is to write procedures and hope that progress will arise spontaneously if the right steps are followed. Companies, research groups, and other organizations can learn to mimic what actual innovation looks like from afar, but without the proper motivations their ultimate success is improbable.
</p>

<p>
In contrast, an organization playing an infinite game doesn’t need to be forced to jump through arbitrary hoops. Pharmaceutical companies don’t bother to acquire <sup>13</sup>C NMR and IR spectra for every intermediate; startups putting together a minimum viable product don’t worry about properly formatting all their code. Finite games are only a distraction for properly motivated organizations, and one which should be avoided whenever possible.<sup><a href="#fn1">1</a></sup>
</p>

<p>
What conclusions can we draw from this? On a personal level, seek to make your games as infinite as possible. Every startup has to raise money and every graduate student has to publish papers, but one shouldn’t spend most of one’s time worrying about how to publish papers as efficiently as possible.
If you can treat finite games as the distraction that they are, you can give them as little mental effort as possible and spend your time and talents on worthier pursuits.
</p>

<p>
On a broader level, I’m struck by the fact that finite games are a problem of our own making. Nobody becomes a scientist hoping to write papers or win grants;<sup><a href="#fn2">2</a></sup>
our aspirations start out infinite, and it’s only through exposure to the paradigms of the field that we learn to decrease our ambition. 
Indeed, calling a research proposal “ambitious” is reportedly one of the worst criticisms that an NIH study section can give.
</p>

<p>
We should therefore be skeptical about bureaucratic solutions to research stagnation. If our scientific institutions themselves are part of the problem, expanding their reach and importance is unlikely to fix the problem and may indeed do more harm than good. “If you find yourself in a hole, stop digging.”
</p>

<i>
Thanks to Jacob Thackston and Ari Wagen for reading drafts of this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  I’m intentionally eliding the normative question of “how do we assign funding if not through quantitative measures?”—it’s an excellent question, and one which deserves a larger treatment than I can offer here. The <a href="https://scienceplusplus.org/metascience/">Nielsen/Qiu metascience essay</a> has some ideas here.
  </li>
  <li id="fn2">
  This may not always be true, but I think it’s generally true. Most high schoolers or undergraduates are drawn to science because of a sense of wonder or curiosity, which gets transmuted to “publish JACS papers” through the alchemy of graduate school.
  Ari pointed me towards <a href="https://www.briantimar.com/notes/mimetic/mimetic/">this essay</a> by a physics graduate student, which seems relevant.
  </li>
</ol>
]]></description>
              <pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Introducing &lt;i&gt;quick_fuoss&lt;/i&gt;</title>
              <link>public/blog/20221205_quick_fuoss.html</link>
              <description><![CDATA[
<p>
Modeling ion-pair association/dissociation is an incredibly complex problem, and one that's often beyond the scope of conventional DFT-based techniques. 
(<a href="https://pubs.acs.org/doi/10.1021/acs.joc.1c01823">This study</a> is a nice demonstration of how hard modeling ion-pairing can be.)
Nevertheless, it's still often important to gain insight into what the relative energy of solvent-separated ion pairs and contact-ion pairs might be;
are solvent-separated configurations energetically accessible or not? 
</p>

<p>
I've run into this problem a few times myself: in measuring pKas in nonpolar solvents, and again recently when trying to understand 
<a href="https://pubs.acs.org/doi/10.1021/acs.orglett.2c03622">the solution structure of ethereal HCl.</a>
When digging through the pKa literature, I was surprised to learn that there's a simple and relatively accurate way to estimate the dissociation constant of contact-ion pairs, 
developed by Raymond Fuoss in the 1950s. 
</p>

<p>
Despite modeling ions as charged spheres and solvent as a featureless dielectric, the "Fuoss model" is surprisingly good at reproducing experimental data.
Here's one such example:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221205_exp_vs_fuoss.png style="width:450px;" />
  <figcaption>
  Comparison of experimental ∆G<sub>diss</sub> for tetraisoamylammonium nitrate in various dioxane–water blends vs. Fuoss-calculated ∆G values.
  (Data taken from <a href="https://pubs.acs.org/doi/10.1021/ja01330a023">Fuoss, <i>J. Am. Chem. Soc.</i>, <b>1933</b></a>, Table II.)
  </figcaption>
</figure>

<p>
It's not trivial to think about how to get similar results using more atomistic methods!
(In principle one could actually model the exact solvent mixture and compute the energy of ion-pair dissociation using biased sampling and MD, but this 
would be horrendously expensive and probably less accurate anyway.)
</p>

<p>
The Fuoss model is pretty simple to implement oneself—but to make things even easier, I've implemented it as an open-source Python package, 
which can be imported using <span class=code>pip</span>.
The package contains only a single function, <span class=code>compute_kd</span>, which accepts the name of the cation, the name of the anion, and the dielectric constant of the medium.
(Alternatively, <span class=code>.xyz</span> files, <span class=code>.gjf</span> files, or <span class=code>cctk.Molecule</span> objects can also be given.)
</p>

<p>
Under the hood, the program builds molecules using <i>cctk</i>, computes their volume, and then applies the Fuoss model.
The end result is a comically simple interface:
</p>

<pre class=code-block>
$ pip install quick_fuoss
$ python
&gt;&gt;&gt; import quick_fuoss
&gt;&gt;&gt; quick_fuoss.compute_kd("sodium", "chloride", 80)
1.0793241279015366
</pre>

<p>
On the associated <a href="https://github.com/corinwagen/quick-fuoss">Github repository</a>, there's also a little command-line script which makes this even simpler:
</p>

<pre class=code-block>
$ python quick_fuoss.py tetraisoamylammonium nitrate 8.5
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00004930 M
Ionization energy: 5.873 kcal/mol
$ python quick_fuoss.py tetraisoamylammonium nitrate 11.9
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00094706 M
Ionization energy: 4.122 kcal/mol
</pre>

<p>
My hope is that this program promotes wider adoption of the Fuoss model, and in general enables more critical thinking about ion-pair energetics in organic solvents.
Please feel free to send any bug reports, complaints, etc. my way!
</p>
]]></description>
              <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Business Card Lennard–Jones Simulation, Explained</title>
              <link>public/blog/20221128_business_card_explained.html</link>
              <description><![CDATA[
<p>
Last week, I posted a simple Lennard–Jones simulation, written in C++, that models the behavior of liquid Ar in only 1561 characters.
By popular request, I'm now posting a breakdown/explanation of this program, both to explain the underlying algorithm and illustrate how it can be compressed.
</p>

<p>
Here's the most current version of the program, now condensed even further to 1295 characters:
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;
#define H return
typedef double d;typedef int i;using namespace std;i N=860;d B=8.314e-7;d S=1544
.8;d q(d x){if(x&gt;17)x-=34;if(x&lt;-17)x+=34;H x;}d rd(){H 34*drand48()-17;}struct v
{d x,y,z;v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}d n(){H x*x+y*y+z*z;}v p(){H v(q(x
),q(y),q(z));}};v operator+(v a,v b){H v(a.x+b.x,a.y+b.y,a.z+b.z);}v operator*(d
 a,v b){H v(a*b.x,a*b.y,a*b.z);}struct p{v r,q,a,b;vector&lt;i&gt; W;p(v l):r(l){}void
 F(v f){b=0.025*f+b;}};vector&lt;p&gt; P;void w(){cout&lt;&lt;N&lt;&lt;"\n\n";for(p l:P)cout&lt;&lt;"Ar
"&lt;&lt;l.r.x&lt;&lt;" "&lt;&lt;l.r.y&lt;&lt;" "&lt;&lt;l.r.z&lt;&lt;"\n";cout&lt;&lt;"\n";}void E(){for(i j=N;j--;){for(
i k:P[j].W){v R=(P[j].r+-1*P[k].r).p();d r=R.n();if(r&lt;70){d O=r*r*r;R=2880*B*(2*
S*S/O/O-S/O)/r*R;P[j].F(R);P[k].F(-1*R);}}}}void e(){for(i j=0;j&lt;N;j++){P[j].W.c
lear();for(i k=j+1;k&lt;N;k++)if((P[j].r+-1*P[k].r).p().n()&lt;90)P[j].W.push_back(k);
}}i main(){i A=1e3;i Y=5e3;for(i j=N;j--;){for(i a=999;a--;){v r=v(rd(),rd(),rd(
));i c=0;for(p X:P){d D=(r+-1*X.r).p().n();if(D&lt;6.8)c=1;}if(!c){P.push_back(p(r)
);break;}}}for(i t=0;t&lt;=3e4;t++){for(p&amp; I:P)I.r=(I.r+I.q+0.5*I.a).p();if(t%20==0
)e();E();d K=0;for(p&amp; I:P){I.q=I.q+0.5*(I.b+I.a);I.a=I.b;I.b=v();K+=20*I.q.n();}
d T=2*K/(3*B*N);if(t&lt;2*Y){d C=75;if(t&lt;Y)C=75*t/Y+(A-75)*(Y-t)/Y;for(p&amp; I:P)I.q=s
qrt(C/T)*I.q;}if(t%100==0&amp;&amp;t&gt;2*Y)w();}}
</pre>

<p>
Let's add some whitespace and break this down, line by line:
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;

// some abbreviations
#define H return
typedef double d;
typedef int i;
using namespace std;

// constants
i N=860;        // number of particles
d B=8.314e-7;   // boltzmann's constant
d S=1544.8;     // the minimum of the Lennard-Jones potential, 3.4 Å, to the 6th power
</pre>

<p>
The program begins by importing the necessary packages, defining abbreviations, and declaring some constants. 
Redefining <span class=code>double</span> and <span class=code>int</span> saves a ton of space, as we'll see.
(This is standard practice in the code abbreviation world.)
</p>

<pre class=code-block>
// given a 1d coordinate, scale it to within [-17,17].
// (this only works for numbers within [-51,51] but that's fine for this application)
d q(d x){
    if(x&gt;17)
        x-=34;
    if(x&lt;-17)
        x+=34;
    H x;
}

// returns a uniform random number in [-17,17]
d rd(){
    H 34*drand48()-17;
}
</pre>

<p>
We now define a helper function to keep coordinates within the boundaries of our cubical box, and create another function to "randomly" initialize particles' positions.
(<span class=code>drand48</span> is not a particularly good random-number generator, but it has a short name and works well enough.)
</p>

<pre class=code-block>
// vector class
struct v{
    d x,y,z;
    v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}

    // return the squared length of the vector
    d n(){
        H x*x+y*y+z*z;
    }
   
    // return a vector with periodic boundary conditions applied
    v p(){
        H v(q(x),q(y),q(z));
    }
};

// vector addition
v operator+(v a,v b){
    H v(a.x+b.x,a.y+b.y,a.z+b.z);
}

// multiplication by a scalar
v operator*(d a,v b){
    H v(a*b.x,a*b.y,a*b.z);
}

</pre>

<p>
Here, we define a vector class to store positions, velocities, and accelerations, and define addition and multiplication by a scalar.
(It would be better to pass <span class=code>const</span> references to the operators, but it takes too many characters.)
</p>

<pre class=code-block>
// particle class
struct p{
    // r is position, q is velocity, a is acceleration, b is acceleration in the next timestep
    v r,q,a,b;
    // neighbor list, the list of particles close enough to consider computing interactions with
    vector&lt;i&gt; W;

    p(v l):r(l){}
   
    // apply a force to this particle. 
    // 0.025 = 1/40 = 1/(Ar mass)
    void F(v f){
        b=0.025*f+b;
    }
};

// global vector of all particles
vector&lt;p&gt; P;

// write current coordinates to stdout
void w(){
    cout&lt;&lt;N&lt;&lt;"\n\n";
    for(p l:P)
        cout&lt;&lt;"Ar "&lt;&lt;l.r.x&lt;&lt;" "&lt;&lt;l.r.y&lt;&lt;" "&lt;&lt;l.r.z&lt;&lt;"\n";
    cout&lt;&lt;"\n";
}

</pre>

<p>
Now, we define a class that represents a single argon atom. 
Each atom has an associated position, velocity, and acceleration, as well as <span class=code>b</span>, which accumulates acceleration for the next timestep.
Atoms also have a "neighbor list", or a list of all the particles close enough to be considered in force calculations. 
(To prevent double counting, each neighbor list only contains the particles with index larger than the current particle's index.)
</p>

<p>
We create a global variable to store all of the particles, and create a function to report the current state of this variable.
</p>

<pre class=code-block>
// compute forces between all particles
void E(){
    for(i j=N;j--;){
        for(i k:P[j].W){
            // compute distance between particles
            v R=(P[j].r+-1*P[k].r).p();
            d r=R.n();

            // if squared distance less than 70 (approximately 6 * 3.4Å**2), the interaction will be non-negligible
            if(r&lt;70){
                d O=r*r*r;
                // this is the expression for the lennard–jones force.
                // the second lennard–jones parameter, the depth of the potential well (120 kB), is factored in here.
                R=2880*B*(2*S*S/O/O-S/O)/r*R;

                // apply force to each particle
                P[j].F(R);
                P[k].F(-1*R);
            }
        }
    }
}
</pre>

<p>
Now, we create a function to calculate the forces between all pairs of particles. 
For each particle, we loop over the neighbor list and see if the distance is within six minima of the adjacent particle, 
using squared distance to avoid the expensive square-root calculation.
If so, we calculate the force and apply it to each particle.
</p>

<pre class=code-block>
// build neighbor lists
void e(){
    for(i j=0;j&lt;N;j++){
        // clear the old lists
        P[j].W.clear();
        for(i k=j+1;k&lt;N;k++)
            // if squared distance between particles less than 90 (e.g. close to above cutoff), add to neighbor list
            if((P[j].r+-1*P[k].r).p().n()&lt;90)
                P[j].W.push_back(k);
    }
}
</pre>

<p>
Finally, we create a function to build the neighbor lists, which is a straightforward double loop.
We add every particle which might conceivably be close enough to factor into the force calculations within 10–20 frames.
</p>

<pre class=code-block>
i main(){
    i A=1e3;    // initial temperature (1000 K)
    i Y=5e3;    // time to reach final temperature (5000 fs)

    // initialize the system. each particle will be randomly placed until it isn't too close to other particles.
    for(i j=N;j--;){
        for(i a=999;a--;){
            // generate random position
            v r=v(rd(),rd(),rd());
            i c=0;

            // check for clashes with each extant particle
            for(p X:P){
                d D=(r+-1*X.r).p().n();
                if(D&lt;6.8)
                    c=1;
            }

            // if no clashes, add particle to list
            if(!c){
                P.push_back(p(r));
                break;
            }
        }
    }
</pre>

<p>
To begin the program, we randomly initialize each particle and test if we're too close to any other particles. 
If not, we save the position and move on to the next particle. This is crude but works well enough for such a simple system.
</p>
   
<pre class=code-block>
    // run MD! this is basically just the velocity verlet algorithm.
    for(i t=0;t&lt;=3e4;t++){
        // update position
        for(p&amp; I:P)
            I.r=(I.r+I.q+0.5*I.a).p();
        
        // every 20 timesteps (20 fs), update neighbor lists
        if(t%20==0)
            e();

        // compute forces
        E();

        // finish velocity verlet, and sum up the kinetic energy.
        d K=0;  // kinetic energy
        for(p&amp; I:P){
            I.q=I.q+0.5*(I.b+I.a);
            I.a=I.b;
            I.b=v();
            K+=20*I.q.n();
        }
        
        d T=2*K/(3*B*N); // temperature

        // in the first 10 ps, apply berendsen thermostat to control temperature
        if(t&lt;2*Y){
            d C=75; // target temperature
            if(t&lt;Y)
                C=75*t/Y+(A-75)*(Y-t)/Y;
            for(p&amp; I:P)
                I.q=sqrt(C/T)*I.q;
        }
       
        // every 100 fs after the first 10 ps, write the configuration to stdout
        if(t%100==0&amp;&amp;t&gt;2*Y)
            w();
    }
}
</pre>

<p>
Finally, we simply have to run the simulation. We use the velocity Verlet algorithm with a 1 fs timestep, 
updating neighbor lists and writing to <span class=code>stdout</span> periodically.
The temperature is gradually lowered from 1000 K to 75 K over 5 ps, and temperature is controlled for the first 10 ps.
</p>

<p>
Hopefully this helps to shed some light on how simple a molecular dynamics simulation can be, and highlights the wonders of obfuscated C++!
</p>
]]></description>
              <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Business Card Lennard–Jones Simulation</title>
              <link>public/blog/20221121_business_card_lennard_jones.html</link>
              <description><![CDATA[
<p>
An (in)famous code challenge in computer graphics is to write a complete ray tracer small enough to fit onto a business card.
I've really enjoyed reading through some of the submissions over the years (e.g. 
<a href="https://fabiensanglard.net/rayTracing_back_of_business_card/">1</a>,
<a href="https://mzucker.github.io/2016/08/03/miniray.html">2</a>,
<a href="https://www.realtimerendering.com/blog/back-of-the-business-card-ray-tracers/">3</a>,
<a href="https://www.taylorpetrick.com/blog/post/business-rt">4</a>), and I've wondered what a chemistry-specific equivalent might be.
</p>

<p>
As a first step in this space—and as a learning exercise for myself as I try to learn C++—I decided to try and write a tiny Lennard–Jones simulation.
Unlike most molecular simulation methods, which rely on heavily parameterized forcefields or complex quantum chemistry algorithms, 
the Lennard–Jones potential has a simple functional form and accurately models noble gas systems.
Nice work from <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.136.A405">Rahman</a> in 1964 showed that Lennard–Jones simulations
of liquid argon could reproduce experimental X-ray scattering results for the radial distribution function:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221121_rahman_rdf.png style="width:350px;" />
  <figcaption>
  Figure 2 from Rahman, showing the radial distribution function of liquid argon with clear first, second, and third solvation shells.
  </figcaption>
</figure>

<p>
This thus seemed like a good target for a "tiny chemistry" program: easy enough to be doable, but tricky enough to be interesting.
After a bit of development, I was able to get my final program down to 1561 characters, easily small enough for a business card.
The final program implements periodic boundary conditions, random initialization, and temperature control with the Berendsen thermostat for the first 10 picoseconds.
Neighbor lists are used to reduce the computational cost, and an xyz file is written to stdout. 
</p>

<pre class=code-block>
#include &lt;iostream&gt;
#include &lt;random&gt;
#define H return
typedef double d;typedef int i;using namespace std;d L=17;i N=860;d B=8.314e-7;d
 s=3.4;d M=6*s*s;d S=s*s*s*s*s*s;d q(d x){if(x&gt;L)x-=2*L;if(x&lt;-L)x+=2*L;H x;}d rd
(){H 2*L*drand48()-L;}struct v{d x,y,z;v(d a=0,d b=0,d c=0):x(a),y(b),z(c){}d n(
){H x*x+y*y+z*z;}v p(){x=q(x);y=q(y);z=q(z);H*this;}};v operator+(v a,v b){H v(a
.x+b.x,a.y+b.y,a.z+b.z);}v operator*(d a,v b){H v(a*b.x,a*b.y,a*b.z);}struct p{d
 m;v r,q,a,b;vector&lt;i&gt; W;p(v l,d n=40):r(l),m(n),q(v()),a(v()),b(v()),W(vector&lt;i
&gt;()){}void F(v f){b=(1/m)*f+b;}};vector&lt;p&gt; P;void w(){cout&lt;&lt;N&lt;&lt;"\n\n";for(i j=N;
j--;){v l=P[j].r;cout&lt;&lt;"Ar "&lt;&lt;l.x&lt;&lt;" "&lt;&lt;l.y&lt;&lt;" "&lt;&lt;l.z&lt;&lt;"\n";}cout&lt;&lt;"\n";}void E(
){for(i j=0;j&lt;N;j++){for(i x=0;x&lt;P[j].W.size();x++){i k=P[j].W[x];v R=(P[j].r+-1
*P[k].r).p();d r2=R.n();if(r2&lt;M){d O=1/(r2*r2*r2);d f=2880*B*(2*S*S*O*O-S*O)/r2;
R=f*R;P[j].F(R);P[k].F(-1*R);}}}}void cW(){for(i j=0;j&lt;N;j++){P[j].W.clear();for
(i k=j+1;k&lt;N;k++)if((P[j].r+-1*P[k].r).p().n()&lt;1.3*M)P[j].W.push_back(k);}}i mai
n(){i A=1e3;i e=75;i Y=5e3;for(i j=N;j--;){for(i a=99;a--;){v r=v(rd(),rd(),rd()
);i c=0;for(i k=P.size();k--;){d D=(r+-1*P[k].r).p().n();if(D&lt;2*s)c=1;}if(!c){P.
push_back(p(r));break;}}}if(P.size()!=N)H 1;for(i t=0;t&lt;=3e4;t+=10){for(i j=N;j-
-;){P[j].r=P[j].r+P[j].q+0.5*P[j].a;P[j].r.p();}if(t%20==0)cW();E();d K=0;for(i
j=N;j--;){P[j].q=P[j].q+0.5*(P[j].b+P[j].a);P[j].a=P[j].b;P[j].b=v();K+=P[j].m*P
[j].q.n()/2;}d T=2*K/(3*B*N);if(t&lt;2*Y){d C=e;if(t&lt;Y)C=e*t/Y+(A-e)*(Y-t)/Y;d s=sq
rt(C/T);for(i j=N;j--;)P[j].q=s*P[j].q;}if(t%100==0&amp;&amp;t&gt;2*Y)w();}}
</pre>

<p>
The program can be compiled with <span class=code>g++ -std=gnu++17 -O3 -o mini-md mini-md.cc</span> and run:
</p>

<pre class=code-block>
$ time ./mini-md &gt; Ar.xyz

real	0m4.689s
user	0m4.666s
sys	0m0.014s
</pre>

<p>
Analysis of the resulting file in <i>cctk</i> results in the following radial distribution function, in good agreement with the literature:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221121_rdf.png style="width:450px;" />
  <figcaption>
  Radial distribution function generated by the above code.
  </figcaption>
</figure>

<p>
I'm pretty pleased with how this turned out, and I learned a good deal both about the details of C++ syntax and about molecular dynamics.
There are probably ways to make this shorter or faster; if you write a better version, let me know and I'll happily link to it here! 
</p>
]]></description>
              <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Structural Diversity in Science</title>
              <link>public/blog/20221026_structural_diversity.html</link>
              <description><![CDATA[
<p>
Michael Nielsen and Kanjun Qiu recently published a massive essay entitled <a href="https://scienceplusplus.org/metascience/index.html">“A Vision of Metascience: An Engine of Improvement for the Social Processes of Science.”</a> Metascience, the central topic of the essay, is science about science. As the authors put it, metascience “overlaps and draws upon many well-established fields, including the philosophy, history, and sociology of science, as well as newer fields such as the economics of science funding, the science of science, science policy, and others.”
</p>

<p>
The essay emphasizes how there are likely massive potential improvements to what they call the “social processes” of science, or the ways in which science is practiced by scientists. There are a lot of important ideas and conclusions in the essay, but here I want to focus on one specific theme that caught my attention: <u>the importance of structural diversity in driving scientific progress</u>. In this context, structural diversity means “the presence of many differently structured groups in science.” High structural diversity means many different types of scientific groups, while low structural diversity means only a few different types of scientific groups. To quote Nielsen and Qiu directly:
</p>

<blockquote>
…structural diversity is a core, precious resource for science, a resource enlarging the range of problems humanity can successfully attack. The reason is that different ambient environments enable different kinds of work. Problems easily soluble in one environment may be near insoluble in another; and vice versa. Indeed, often we don't <i>a priori</i> know what environment would best enable an attack on an important problem. Thus it's important to ensure many very different environments are available, and to enable scientists to liquidly move between them. In this view, structural diversity is a resource to be fostered and protected, not homogenized away for bureaucratic convenience, or the false god of efficiency. What we need is a diverse range of very different environments, expressing a wide range of powerful ideas about how to support discovery. In some sense, the range of available environments is a reflection of our collective metascientific intelligence. And monoculture is the enemy of creative work.
</blockquote>

<p>
Today, structural diversity is low: scientific research is overwhelmingly performed in universities by professor-led research groups. These groups typically contain from 5 to 30 people, have at most two additional administrators beyond the principal investigator (i.e. the professor), and are composed of undergrads, graduate students, and postdocs. (There are, of course, many exceptions to the template I’ve outlined above.)
</p>

<p>
This structure influences the sort of scientific work that gets done. To graduate, PhD students need to have papers, which means that they need to work on projects that have sufficiently short time horizons to conclude before they graduate. Additionally, they need to have first-author papers, which means that they can’t work in large teams; if three students work together, they can’t all be first authors. Taken together, these considerations imply that most projects should take 10 person-years or less to accomplish.<sup><a href="#fn1">1</a></sup>
</p>

<p>
This is a long time, but not that long for science: unfortunately, most truly groundbreaking projects are “too big” for a single academic lab. Conversely, students are incentivized to publish in high-impact journals, and so projects that are “too small” are penalized for not being ambitious enough. Skilled academics are able to thread the needle between projects that are “too big” and projects that are “too small” and provide exactly the right amount of effort to generate high-impact publications within a reasonable time horizon.
</p>

<p>
These same challenges are echoed (on grand scale) for assistant professors. New faculty typically have 5 to 7 years before they must submit their tenure package, which means they’re forced to choose projects likely to work in that time frame (with inexperienced students and relatively few resources, no less). This disincentives tool-building, which generally chews up too much time to be an efficient use of resources for young labs, and puts a ceiling on their ambition.
</p>

<p>
These aren’t the only consequences of academia’s structure. “Deep” skills requiring more than a year or two of training are tricky, because even PhD students are only there for 4–6 years, so the time it takes to acquire skills comes directly out of the time they can be doing productive research. Additionally, certain skill sets (e.g. software engineering) command such a premium that it’s <a href="https://www.lesswrong.com/posts/9GweYgHABZAjH6T6f/">difficult to attract such people to academia</a>. Specialized instrumentation is another challenge: a given lab might only have the budget for a few high-end instruments, implying that its projects must be chosen accordingly.
</p>

<p>
A defender of the status quo might reasonably respond that smaller labs do lead to smaller projects, but in a greater number of areas: “what is any ocean, but a multitude of drops?” The academic structure, with its incentives for demonstrable progress, certainly cuts back on the number of costly, high-profile failures: most failed research groups never get tenure, limiting the damage.
</p>

<p>
Nevertheless, it seems that at this point many fields have been picked clean of projects with low enough barriers to entry to make them accessible to academics. Many remaining insights, including those needed to spawn new fields of science, may be simply out of reach of a decentralized array of small, independent academic groups. As Nielsen and Qiu put it, “you can stack up as many canonical researchers as you like and they still won't do the non-canonical work; it's a bottleneck on our capacity for discovery.” To support this point, they cite examples where large organizations were able to produce big advances inaccessible to their smaller counterparts: LIGO, the Large Hadron Collider, Brian Nosek’s Center for Open Science, and the Human Genome Project.
</p>

<p>
If we accept the claim that structural diversity is important, we ought to look for opportunities to expand structural diversity wherever possible. At the margin, this might look like supporting non-traditional hires in academic groups, including people who don’t fit into the established undergrad–graduate student–postdoc pipeline, and allowing for greater flexibility in the structure of labs (i.e. multiple professors within the same lab). More radical solutions might look like scientific start-ups where profitability can realistically be achieved, or <a href="https://www.nature.com/articles/d41586-022-00018-5">“focused research organizations”</a> where it cannot.<sup><a href="#fn2">2</a></sup> What could a well-managed team of professional scientists, focused solely on work “not easily possible in existing environments,” accomplish when pitted against some of the toughest unsolved problems in the world? We won’t know until we try.
</p>

<i>Thanks to Ari Wagen for reading a draft of this post.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    It’s true that there are a lot of efforts to create larger supra-lab organizations to tackle big questions: in chemistry, the NSF has funded <a href="http://www.nsf-cchf.com/aboutCCI.html">“centers for chemical innovation”</a> to unite like-minded researchers. But trying to forge a functional organization from a myriad of independent sovereign teams seems much harder than simply starting a new organization <i>de novo</i>.
  </li> 
  <li id="fn2">
    What if the discoveries needed to advance science are too big for these proposed solutions? For instance, it’s tough to imagine funding a Manhattan Project-style endeavor this way. I don’t think that it’s the case that science requires government-scale resources to push past stagnation, but if that were really true, it might be an argument for using the full might of state capacity to drive research, like something out of Liu Cixin’s novels. Note, however, that this would make the task of “picking the right problems” that much more important.
  </li> 
</ol>


]]></description>
              <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Omelas, Hirschman, Altom: On Healing Academia</title>
              <link>public/blog/20221018_omelas_hirschman_altom.html</link>
              <description><![CDATA[
<p>
<i>Spoilers below for Ursula Le Guin’s short story “The Ones Who Walk Away From Omelas.” If you haven’t read it, it’s short—go and do so now!</i>
</p>

<p>
<i>TW: child abuse, suicide.</i>
</p>

<p>
In her short story “The Ones Who Walk Away From Omelas,” Ursula Le Guin describes an idyllic town (Omelas) built entirely on the misery of a single, innocent child. The inhabitants of Omelas lead a utopian life, but are burdened with the knowledge that this child suffers so that they can prosper. Although the story is brief—only five pages long—Le Guin pulls no punches in the emotional weight of her writing:
</p>

<blockquote>
The child, who has not always lived in the tool room, and can remember sunlight and its mother's voice, sometimes speaks. "I will be good, " it says. "Please let me out. I will be good!" They never answer. The child used to scream for help at night, and cry a good deal, but now it only makes a kind of whining, "eh-haa, eh-haa," and it speaks less and less often. It is so thin there are no calves to its legs; its belly protrudes; it lives on a half-bowl of corn meal and grease a day.
</blockquote>

<p>
The metaphor, and underlying social commentary, is perhaps obvious. Le Guin goes on to (implicitly) attack utilitarians:
</p>

<blockquote>
[The spectators] would like to do something for the child. But there is nothing they can do. If the child were brought up into the sunlight out of that vile place, if it were cleaned and fed and comforted, that would be a good thing, indeed; but if it were done, in that day and hour all the prosperity and beauty and delight of Omelas would wither and be destroyed. Those are the terms. To exchange all the goodness and grace of every life in Omelas for that single, small improvement: to throw away the happiness of thousands for the chance of happiness of one: that would be to let guilt within the walls indeed.
</blockquote>

<p>
And those who are good at rationalizing away injustice:
</p>

<blockquote>
…as time goes on [the people] begin to realize that even if the child could be released, it would not get much good of its freedom: a little vague pleasure of warmth and food, no real doubt, but little more. It is too degraded and imbecile to know any real joy. It has been afraid too long ever to be free of fear. Its habits are too uncouth for it to respond to humane treatment. Indeed, after so long it would probably be wretched without walls about it to protect it, and darkness for its eyes, and its own excrement to sit in. Their tears at the bitter injustice dry when they begin to perceive the terrible justice of reality, and to accept it.
</blockquote>

<p>
And those who think that suffering “gives life meaning”:
</p>

<blockquote>
[The people] know that they, like the child, are not free. They know compassion. It is the existence of the child, and their knowledge of its existence, that makes possible the nobility of their architecture, the poignancy of their music, the profundity of their science. It is because of the child that they are so gentle with children. They know that if the wretched one were not there sniveling in the dark, the other one, the flute-player, could make no joyful music as the young riders line up in their beauty for the race in the sunlight of the first morning of summer.
</blockquote>

<p>
The story concludes by describing the last, and rarest, response to Omelas:
</p>

<blockquote>
At times one of the adolescent girls or boys who go see the child does not go home to weep or rage, does not, in fact, go home at all. Sometimes also a man or a woman much older falls silent for a day or two, then leaves home.... They leave Omelas, they walk ahead into the darkness, and they do not come back. The place they go towards is a place even less imaginable to most of us than the city of happiness. I cannot describe it at all. It is possible that it does not exist. But they seem to know where they are going, the ones who walk away from Omelas.
</blockquote>

<p>
As I read the story, the conclusion is that few people have the moral courage to reject injustice entirely. Rejecting a broken system is scary, and risky; most people would rather lull themselves into complacency than try and build a better world. But implicit in this analysis is that the ones who walk away are making the right decision, and the ones who stay in Omelas are making the wrong one. In our own flawed world, when is this true?
</p>

<p>
The past decades have seen a great purge within American culture. The “Me Too” movement exposed the prevalence of sexual assualt within Hollywood, Boston Globe investigations revealed massive corruption within the Catholic Church, vast protests over the summer of 2020 decried systematic racism within American institutions, and in general institutions of all forms have come under attack for their failings. Every university has a racist legacy to reckon with; every business has an investor with unsavory political beliefs.
</p>

<p>
To some, this is the oft-derided “cancel culture”—finding fault with everyone, and viciously attacking people and organizations for even the slightest offenses. If “all have sinned and fall short,” then evil can never truly be eradicated, and all that this movement can do is destroy venerable institutions without constructing anything better. To others, however, making peace with evil is deplorable. Evil is the original pandemic; its capacity to spread is unparalleled, and its damage immeasurable. No compromise can be made with the enemy; no peace can be made with injustice.
</p>

<p>
Neither heuristic is sufficient; as usual, wisdom resides in the dialectic. For everyone who seeks to improve the world, then, the question presents itself anew: can one work within the system, flaws and all, or must change come from outside?
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Albert Hirschman analyzed these issues in his 1970 book <i>Exit, Voice, and Loyalty</i>, which analyzes how consumers respond to declining quality. Hirschman draws a basic distinction between “exit,” the movement of individuals away from their current allegiance and towards a competitor, and “voice,” when individuals remain loyal and protest the change from within the system. An insightful point that Hirschman makes is that different fields of study view these options in different ways. Economists, who often think in terms of “perfect competition,” tend to assume exit is the most meaningful option, whereas political scientists prefer voice (i.e. engagement with the political process), thinking of exit as craven or traitorous.
</p>

<p>
These two options can be described in other terms. Exit is the mindset of the young reformer, the naïve, the idealistic, who believes the old system is beyond saving and a new world must be birthed. In contrast, voice is the perspective of small-c conservatism and Chesterton’s Fence, the wisdom of the agéd seer who has seen countless evils and knows how fragile civilization can be. This dichotomy does not separate Red from Blue; a love of exit unites Robespierre, Thunberg, and Trump, while voice embraces Henry Clay and Deng Xiaopeng alike.
</p>

<p>
When is exit better, and when is voice better? In certain circumstances exit can preclude voice by allowing only the most motivated and skilled members to escape a failing system. This removes the very elements of the populace necessary for change through voice, resulting in what Hirschman calls “an oppression of the weak by the incompetent and an exploitation of the poor by the lazy.” In this scenario, then, voice is superior. Exit also only functions when there are legitimate alternatives to the current system: in a monopoly, there can be no exit.
</p>

<p>
(Different Christian sects present an interesting case study here. In Catholicism, the Church is one united organization, and so exit is not an option: only voice is permitted. In contrast, Protestants are divided and subdivided into innumerable organizations, and so any individual Protestant can easily “vote with their feet” and join a church they agree with. The myriad failures of both sects suggests that neither solution is perfect.)
</p>

<p>
In contrast, voice presumes that change is possible—that the system is able to be reformed, and that doing so is more effective than simply starting over. For governments or Catholics, this may be true; for smaller organizations, it seems less true. An idealized capitalist market proceeds through relentless “creative destruction”: old firms become stagnant and stop innovating and are replaced by young startups, who last a few decades before themselves becoming stagnant. Creating the next Google, in most cases, is easier than fixing Google.
</p>

<p>
In other cases, the failures of the current system are so all-encompassing, so total, that it’s almost impossible to conceptualize the right reforms from within. Scott Alexander (of <i>Astral Codex Ten</i>, née <i>Slate Star Codex</i>) describes this phenomenon in his piece <a href="https://slatestarcodex.com/2015/06/06/against-tulip-subsidies/">“Against Tulip Subsidies,”</a> which critiques the discourse around rising tuition costs. Reforming how we pay college tuition, Alexander argues, “would subsidize the continuation of a useless tradition that has turned into a speculation bubble” and thus entrench the very thing we ought to uproot. Voice is better suited for marginal or incremental change; if you want to think big, start from scratch.
</p>

<p>
The choice between voice and exit thus hinges on several factors: how hard would it be to fix the current system, and how hard would it be to build a new and better system from the ground up?
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
As a high school student learning about synthetic organic chemistry, I came across a New York Times article entitled <a href="https://web.archive.org/web/20220130071514/https://www.nytimes.com/1998/11/29/magazine/lethal-chemistry-at-harvard.html">“Lethal Chemistry At Harvard.”</a> The article told the story of Jason Altom, a brilliant Ph.D. candidate in organic chemistry who took his life after years of struggling on a challenging total synthesis problem. Altom’s story resonated with me so much that, as an undergraduate, I printed out his <a href="https://pubs.acs.org/doi/10.1021/ja9915201">aspidophytine synthesis</a> and pinned it above my desk as a sort of impromptu <i>memento mori</i>. (The synthesis, and Scheme 4 in particular, is transcendently beautiful.) He died just weeks after I was born; now I work in the same building in which he worked.
</p>

<p>
Altom’s story is by no means unique: <a href="https://www.chronicle.com/article/harvard-faces-the-aftermath-of-a-graduate-students-suicide/">according to the Chronicle of Higher Education</a>, he was one of three graduate students in his lab to commit suicide within an 11-year period, with more suicides occuring in other labs, departments, and universities. Although some of the issues highlighted by his suicide have been addressed (I have an advising committee now in addition to my professor), the core reality remains the same: the success or failure of a given graduate student depends almost completely on their relationship with their advisor, making students willing to sacrifice almost anything to please their PI. Even those who “succeed” often emerge damaged by the process, aspirations lowered and ambition quenched.
</p>

<p>
Nor are graduate-student <a href="https://www.vox.com/2020/4/15/21214734/deaths-of-despair-coronavirus-covid-19-angus-deaton-anne-case-americans-deaths">“deaths of despair”</a> the only problem within academia. Much has been written about the failures of the scientific establishment: <a href="https://newscience.substack.com/p/laws-of-science">problems</a> with <a href="https://www.science.org/content/blog-post/systematic-fraud">fraud</a> and <a href="https://guzey.com/how-life-sciences-actually-work/#large-parts-of-modern-scientific-literature-are-wrong"><i>p</i>-hacking</a>, <a href="https://guzey.com/how-life-sciences-actually-work/#universities-seem-to-maximize-their-profits-with-good-research-being-a-side-effect">parasitic university rent-seeking</a>, <a href="https://guzey.com/how-life-sciences-actually-work/#peer-review-is-a-disaster">problems with peer review</a>, <a href="https://www.notboring.co/p/gassing-the-miracle-machine">a general increase in bureaucracy</a> and concomitant decline in research productivity, <a href="https://newscience.org/how-software-in-the-life-sciences-actually-works-and-doesnt-work/">a failure to fund basic methods</a>, and many more. It’s obvious that the ivory tower is not without blemish. The human cost, typified by Altom, is but the most visible apex of a whole mountain of problems.
</p>

<p>
Nevertheless, although Le Guin might not approve, academia can be defended through a utilitarian argument: the considerable benefits of academic research and education outweigh the human costs and other drawbacks. This argument, while compelling, presumes that the functions of graduate school are irreplaceable—that no better alternative can be created, that exit is impossible. I would also argue that, although academic research is good, it’s likely it could be improved and made better. Indeed, the very importance of research makes it uniquely susceptible to decline; the “ceiling” of scientific progress is so high that even considerable atrophy still leaves behind an important and productive institution. Hirschman again:
</p>

<blockquote>
The wide latitude humans societies have for deterioration is the inevitable counterpart of man’s increasing productivity and control over his environment. Occasional decline as well as prolonged mediocrity—in relation to achievable performance levels—must be counted among the many penalties of progress.
</blockquote>

<p>
If we accept that improvements to the “prolonged mediocrity” of the academic research system are necessary, the question becomes familiar: can the system be fixed from within (voice), or would it be easier to start from scratch (exit)?
</p>

<p>
Reforming any huge system is non-trivial, but academia presents special difficulties; as Hirschman describes, academia protects itself by selecting for those who can stomach its evils. The conscientious and compassionate flee, for the most part, to undergraduate institutions or industrial positions, enriching R1 faculty positions in those less encumbered by moral concerns. (This statement notwithstanding, I don’t mean to condemn faculty <i>en masse</i>; I have the utmost love and admiration for many faculty members I know who push back against the excesses of the system.) This selection effect deprives voice of its strongest assets, making reform that much harder.
</p>

<p>
Exit, too, is not without its challenges. The problems we face cannot be solved simply by moving to a different university or a different country; the interconnectedness of academia makes it monolithic. To contemplate exit from the academic system means a total revolution, erasing centuries of norms and paradigms—the relationship between professors and graduate students that Altom blamed for his death, which echoes the ancient master–apprentice relationship, dates back to <a href="https://academic.oup.com/book/39837/chapter/339974060">Justus von Liebig</a> in 19th-century Germany. Exit means new journals, new funding paradigms, new ways to recruit students and new ways to train them. Proper regime change leaves no stone unturned.
</p>

<p>
Perhaps it's the optimism of youth, or a misguided belief in the possibility of progress. But day by day I find myself believing less in voice and more in exit. The past few years have seen a flourishing of non-academic models for science. New ventures like <a href="https://www.arcadia.science/">Arcadia</a>, the <a href="https://arcinstitute.org/">Arc Institute</a>, <a href="https://newscience.org/">New Science</a>, <a href="https://astera.org/">the Astera Institute</a>, and the more venerable <a href="https://www.santafe.edu/">Santa Fe Institute</a> are demonstrating that science can be done outside a university—and the <a href="https://www.science.org/content/article/data-check-us-government-share-basic-research-funding-falls-below-50">growing interest</a> from industry and philanthropists for funding basic research indicates that the demand is there. But at the same time, these little endeavors all combined represent less than a percent of the current governmental–academic complex. Time will tell, too, if the flaws of our current system are fated to be recapitulated anew in any replacement.
</p>

<p>
Will it be possible to build a robust and practical scientific ecosystem in parallel to the current system, gradually moving more and more pieces into position until at last <a href="https://en.wikipedia.org/wiki/Tl%C3%B6n,_Uqbar,_Orbis_Tertius">Uqbar</a> becomes reality? I don’t know—but I’m excited to find out.
</p>

<i>Thanks to Eugene Kwan, Ari Wagen, and Taylor Wagen for reading a draft of this piece.</i>

]]></description>
              <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Entropy in the Water Dimer</title>
              <link>public/blog/20221006_water_dimer.html</link>
              <description><![CDATA[
<p>
The failure of conventional calculations to handle entropy is well-documented. Entropy, which <a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula">fundamentally</a> depends on the number of microstates accesible to a system, is challenging to describe in terms of a single set of XYZ coordinates (i.e. a single microstate), and naïve approaches to computation simply disregard this important consideration.
</p>

<p>
Most programs get around this problem by partitioning entropy into various components—translational, rotational, vibrational, and configurational—and handling each of these separately. For many systems, conventional approximations perform well. Translational and rotational entropy depend in <a href="https://gaussian.com/thermo/">predictable ways</a> on the mass and moment of inertia of the molecule, and vibrational entropy can be estimated from normal-mode analysis at stationary points. Conformational entropy is less easily automated and as a result is often neglected in the literature (see the <a href="https://pubs.acs.org/doi/10.1021/ja5111392">discussion</a> in the SI), although <a href="https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc00621e">some</a> <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.0c01213">recent</a> <a href="https://pubs.rsc.org/en/content/articlelanding/2022/cp/d1cp05805c">publications</a> are changing that. 
</p>

<p>
In general, however, the approximations above only work for ground states. To quote the Gaussian vibrational analysis <a href="https://gaussian.com/vib/">white paper</a>:
</p>

<blockquote>
Vibrational analysis, as it’s descibed in most texts and implemented in <i>Gaussian</i>, is valid only when the first derivatives of the energy with respect to displacement of the atoms are zero. In other words, the geometry used for vibrational analysis must be optimized at the same level of theory and with the same basis set that the second derivatives were generated with. Analysis at transition states and higher order saddle points is also valid. Other geometries are not valid. 
</blockquote>

<p>
While this isn't a huge issue in most cases, since most processes are associated with a minima or first-order saddle point on the electronic energy surface, it can become a big deal for reactions where entropy significantly shifts the position of the transition state (e.g. Figure 4 in <a href="https://pubs.acs.org/doi/10.1021/ja208779k">this study</a> of cycloadditions). Even worse, however, are cases where entropy constitutes the entire driving force for the reaction: association/dissociation processes. In his elegant <a href="https://pubs.acs.org/doi/pdf/10.1021/acs.organomet.8b00456">review</a> of various failures of computational modelling, Mookie Baik illustrated this point by showing that no transition state could be found for dissociation of the water dimer in the gas phase:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221006_baik.png style="width:500px;" />
  <figcaption>
  Figure 11 from Baik's review.
  </figcaption>
</figure>

<p>
Panel (b) of this figure shows the electronic energy surface for dissociation, which monotonically increases out to infinity—there's never a maximum, and so there's no transition state. To estimate the position of the transition state, Baik proposes computing the entropy (using the above stationary-point approximations) at the first few points, where the two molecules are still tightly bound, and then extrapolating the curve into a sigmoid function. Combining the two surfaces then yields a nice-looking (if noisy) curve with a clear transition state at an O–H distance of around 3 Å.
</p>

<p>
This approach, while clever, seems theoretically a bit dubious—is it guaranteed that entropy must always follow a smooth sigmoidal interpolation between bound and unbound forms? I thought that a more direct solution to the entropy problem would take advantage of <i>ab initio</i> molecular dynamics. While too slow for most systems, AIMD intrinsically accounts for entropy and thus should be able to generate considerably more accurate energy surfaces for association/dissociation events.
</p>

<p>
Using <a href="https://github.com/corinwagen/presto"><i>presto</i></a>, I ran 36 constrained 100 ps <i>NVT</i> simulations of the water dimer with different O–O distances, and used the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540130812">weighted histogram analysis method</a> to stitch them together into a final potential energy surface. I then compared these results to those obtained from a direct <span class=code>opt=modredundant</span> calculation (with frequencies at every point) from Gaussian. (All calculations were performed in Gaussian 16 at the wB97XD/6-31G(d) level of theory, which overbinds the water dimer a bit owing to basis-set superposition error.)
</p>

<p>
The results are shown below (error bars from the AIMD simulation are derived from bootstrapping):
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20221006_PES.png style="width:550px;" />
  <figcaption>
  Comparison of different approaches for studying dissociation of the water dimer.
  </figcaption>
</figure>

<p>
As expected, no transition state can be seen on the black line corresponding to the electronic energy surface, or on the green line corresponding to enthalpy. All methods that depend on normal-mode analysis show sizeable variation at non-stationary points, which is perhaps unsurprising. What was more surprising was how much conventional DFT calculations (purple) overestimated entropy relative to AIMD! <a href="https://pubs.acs.org/doi/full/10.1021/jp205508z">Correcting for low-frequency vibrational modes</a> brought the DFT values more into line with AIMD, but a sizeable discrepancy persists.
</p>

<p>
Also surprising is how different the AIMD free energy surface looks from Baik's estimated free energy surface. Although the scales are clearly different (I used O–O distance for the X axis, whereas Baik used O–H distance), the absence of a sharp maximum in both the AIMD data and the corrected Gibbs free energy data from conventional DFT is striking. Is this another instance of <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4124006/">entropy–enthalpy compensation</a>?
</p>

<p>
In the absence of good gas-phase estimates of the free energy surface, it's tough to know how far the AIMD curve is from the true values; perhaps others more skilled in these issues can propose higher-level approaches or suggest additional sources of error. Still, on the meta-level, this case study demonstrates how molecular dynamics holds promise for modelling things that just can't be modelled other ways. Although this approach is still too expensive for medium to large systems, it's exciting to imagine what might be possible in 5 or 10 years!
</p>

<em>
Thanks to Richard Liu and Eugene Kwan for insightful discussions about these issues.
</em>
]]></description>
              <pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Talent</title>
              <link>public/blog/20220928_talent.html</link>
              <description><![CDATA[
<p>
<i>Talent</i>, by Tyler Cowen and Daniel Gross, is a book about talent selection—in other words, a book about hiring. Although I confess this sounded very boring to me initially, the authors address this concern right away:
</p>

<blockquote>
Talent search is one of the most important activities in virtually all human lives. Elon Musk personally interviewed the first <i>three thousand</i> employees at SpaceX because he wanted to make sure the company was hiring the right people. Don’t think of talent search as a problem faced by “the boss” or by human resource departments… Just about everyone is on a quest to find talent in others or to show off their own.
(emphasis original)
</blockquote>

<p>
Not only is finding talented people a necessary prerequisite for any great endeavor, but “excess credentialism and highly bureaucratic hiring procedures” means that existing protocols for finding excellence are at best inefficient and at worst counterproductive. This problem is everywhere, the authors assert: venture capital is full of money looking for people to fund, discrimination against women and minorities means that talented people don’t get the resources they deserve, and increasing globalization means that ever-larger pool of potential talent are entering the global market. The authors summarize by stating that “the world’s inability to find and mobilize enough talent [is] one of the most significant failures of our time,” and hope that this book can be the first of many seeking to address this problem.
</p>

<p>
How, then, do we do better at identifying talent?
</p>

<h2>Interviewing</h2>

<p>
Interviewing, although oft-derided, remains one of the best ways to learn about someone. The authors devote considerable time to the question of how best to interview someone, and especially how to gain useful, non-scripted information about personality:
</p>

<blockquote>
The best interviews are not formal interviews at all. We’re sure you can think of other creative ways to take the candidate out of interview mode and into their everyday self. This is important, because the everyday self is what you’ll get if you hire them.
</blockquote>

<p>
Many potential questions are discussed: some memorable examples focus on examining someone’s self-conception of their past and their current habits (e.g. “what are the open tabs on your browser right now?”). The best questions prompt authentic, off-the-cuff answers that give insight not into what the candidate wants to tell you but into who they really are. (There’s a fundamental pessimism about people’s self-deception that permeates this section.)
</p>

<p>
The authors close by emphasizing the importance of being a good conversationalist, a skill underrated by many technical people:
</p>

<blockquote>
Conversing well with potential hires or award winners is one of the most important things that you can do. Keep in mind that it not only brings you talent, but it helps you retain talent and mobilize those individuals to use their skills better. If you cannot relate to your talent at a conversational level, you will learn less, you will build less trust, and you will end up relying too heavily on direct monetary incentives to motivate people.
</blockquote>

<h2>Zoom Interviews</h2>

<p>
Cowen and Gross devote an entire chapter specifically to Zoom interviews, which they feel are underrated and can be just as useful as in-person interviews. Although many people find Zoom uncomfortable, this may actually be an advantage of the medium:
</p>

<blockquote>
Many women have remarked on Twitter that they feel on more equal footing on a Zoom call… A lot of people used to coming across as high-status and charismatic in person will feel a bit lost through the screen. Witty repartee also can be hard to pull off over an internet call, and that too may diminish the stature of those individuals who are used to using clever banter to command a room.
</blockquote>

<p>
You might be even one of these people:
</p>

<blockquote>
One of the hardest mental adjustments for people to make is to realize how much their positive affect relies on their in-person rejection of high social status. To give a simple example, you might not be as witty as you think! You will do better in the online call if you realize how much your in-person presence relies on a kind of phoniness.
</blockquote>

<p>
I also liked this observation about how Zoom interviews can be more equitable:
</p>

<blockquote>
The supposed information poverty of the online interview also may help some interviewers overcome potential biases against women and also some minority groups… The online interview, by making everyone less charismatic, may help counter your bias against these individuals.
</blockquote>

<h2>Intelligence</h2>

<p>
Cowen and Gross review a variety of data about the importance of intelligence in various careers. The picture they present is complicated; IQ is clearly important for many professions, but perhaps less so than many people think. In general, Cowen and Gross seem to conclude that intelligence is overrated in hiring:
</p>

<blockquote>
In what might seem like a paradox, it can be hard to spot intelligence, drive, and other positive qualities at the very, very top. Why? Well, the very, very top of the market usually is underexplored territory, virtually by definition. The most talented people usually are doing something extraordinary and fairly new, and often they are so unbelievably talented that most of us just don’t have the ability to appreciate their talents, at least not until their final achievements are on full display.
</blockquote>

<p>
Cowen and Gross also reference Marc Andressen’s essay <a href="https://fictivekin.github.io/pmarchive-jekyll/how_to_hire_the_best_people.html">“How To Hire the Best People You’ve Ever Worked With,”</a> which argues that drive, self-motivation, curiosity, and ethics are more important considerations than raw intelligence. Furthermore, they point out that intelligence is already priced into the market—everyone knows smart employees are good, and so “the obviously smart people are not always the obvious bargains.”
</p>

<p>
(It strikes me that humanities PhDs might be an underutilized pool of high-IQ workers, albeit with little technical training. Perhaps a business with an acute need for raw intelligence and few required technical skills might capitalize on this… this probably already exists.)
</p>

<h2>Personality</h2>

<p>
Much as they did for intelligence, Cowen and Gross analyze the five-factor personality model with an eye towards finding good hires at the margin. Their literature review finds that high conscientiousness is “the single best predictor of overall job performance” (other factors being poorly predictive), but they note that certain fields may benefit from a less responsible approach: 
</p>

<blockquote>
Sometimes leaders of organizations can have too much rather than too little conscientiousness…. leadership skills often involve a mix of creativity and daring and ability to reimagine the risky future, and those are not necessarily the traits found in the people who punch the time clock promptly every day. Elon Musk would have gotten in less trouble had he not smoked a joint on the live video stream of Joe Rogan’s podcast, but a more sedate Elon Musk probably would not have built SpaceX and Tesla with the same fervor.”
</blockquote>

<p>
(Additionally, the authors note that conscientiousness, like intelligence, is already priced into the market.)
</p>

<p>
The authors go on to contrast conscientiousness with stamina, which they call “one of the great underrated concepts for talent search, especially when you are looking for top performers and leaders and major achievers.” Stamina refers to perseverance of effort, or a person’s ability to keep working diligently for long periods of time: since returns to learning and improvement compound over time, high stamina ends up making a huge difference in the long run. The authors continue: 
</p>

<blockquote>
Don’t just think in terms of levels of current ability, because over time, rates of change very often prove to be more important. Think in terms of trajectories. When it comes to a job or fellowship candidate, think about the person’s developmental curve and whether the candidate is truly committed to consistent, perpetual self-improvement, as you might expect from a top athlete or musician…. If a person doesn’t seem to think much about self-improvement, they still might be a good hire, but then you had better be pretty content with their currently demonstrated level of expertise.
</blockquote>

<p>
Other “more exotic” traits, both good and bad, that the authors discuss are:
</p>

<ul>
<li>
<b>Morlockism</b>, the capacity to lock oneself “in a cave” and work very hard for several days. 
</li>
<li>
<b>Sturdiness</b>, “the quality of getting work done every day, with extreme regularity.” (Sturdiness and Morlockism appear to be substitutionary traits.) 
</li>
<li>
<b>Generativeness</b>, a capacity of people to “talk quickly, move quickly, and in general seem to be enthralled with life.” (Balaji Srinivasan is cited as an example.)
</li>
<li>
<b>Insecure overachievement</b>, the quality of “never quite feeling comfortable with your output despite knowing at a deep level that it <i>is</i> good” (emphasis original), which often results in high output but problematic psychology. Closely related, but more negative, is <b>pessimistic perfectionism</b>, exhibited by people who “believe that their work is never good enough” and “don’t have the ongoing drive and impetus of insecure overachievers.”
</li>
<li>
<b>Clutteredness</b>, a trait of otherwise smart people who are unable to express their thoughts in a coherent or intelligible manner. Perhaps obviously, such individuals ought not to be put in positions that emphasize clear communication.
</li>
<li>
The distinct trait of <b>vagueness</b>, where someone thinks in “mushy concepts and unspecific terms… not really drawing any conceptual distinctions at all.” This is problematic for strongly analytical roles.
</li>
<li>
<b>Precocity</b>, how young a person first displayed aptitude. Probably more important for fluid intelligence than for skills that require long accumulation of knowledge.
</li>
<li>
<b>Adhesiveness</b>, or the ability to be a good team player and fix problems (related to high social intelligence). This seems largely orthogonal to many other skills discussed, and may be crucial for some jobs and largely irrelevant for others.
</li>
<li>
<b>Ability to perceive and climb the right hierarchy</b>, which lacks a catchy moniker. Many promising and highly skilled people manage to waste time focusing on the wrong problems. Cowen references chess players who never consider the world outside chess, and Gross describes potential founders who focus more on prestige than actually building their company. (To quote Peter Thiel: “be long substance, short status.”) I’m reminded of incredibly brilliant quiz bowl players I knew who never managed to translate their skills into success outside academic competitions.
</li>
<li>
Another interesting trait is <b>how many conceptual frameworks one possesses</b>. A person with many frameworks can put themselves into the mind of an engineer, a salesperson, a regulator, or a user, whereas more limited people struggle to escape their own viewpoint.
</li>
</ul>

<p>
The discussion of these traits was one of my favorite parts of the book. I found it very useful to imagine various personalities and dissect what their strengths and flaws might be; except in rare cases, it seems that every strength has a corresponding flaw. As the authors write, “skill in spotting flaws in other people can lead to very positive matching outcomes, and that is another reason the dialectical perspective of seeing both the good and bad sides of talent is highly useful.”
</p>

<h2>Overcoming Bias</h2>

<p>
The authors first discuss disabilities, observing that often disability can augment talent through either “redirection of effort” or “compensation and adaptation.” The first case is typified by Richard Branson (founder of Virgin Galactic), who recounts how his dyslexia made it difficult for him to focus on details and pushed him towards important big-picture thinking. In contrast, the second case is typified by blind lawyers, who frequently know the law better than their sighted counterparts because they are unable to look it up as quickly. In either case, what appears to be purely a disadvantage in fact leads to subtle advantages which might be easily overlooked: “disability is a highly complex notion and by no means always negative on the whole.”
</p>

<p>
The subsequent section focuses on women and minorities. As alluded to in the section on Zoom interviewing, the authors observe that there are a “fairly limited range of behaviors allowed” for women in the workplace, and as a result that many women’s talents are not fairly assessed. In particular, aggression is viewed as a positive for high-status men but a negative for high-status women. The authors then summarize a variety of literature which supports “the notion of a <i>confidence gap</i> as one of the main differences between men and women in the workplace” (emphasis added). In light of this finding, Cowen and Gross make three points:
</p>

<ol>
<li>
Favor women for jobs where low confidence is advantageous; “for many jobs, including in politics, diplomacy, and prudential supervision, epistemic humility is more important than risk-taking.”
</li>
<li>
Look extra-hard for confident women, because their skills are likely undervalued by the market; you can “gain from the world’s statistical discrimination and in the process rectify an injustice.”
</li>
<li>
Be mindful that risk-taking and competitiveness are often viewed as key values in male-dominated organizations, even when they needn’t be, and that this faulty institutional self-image may lead to unnecessary barriers to the advancement of women.
</li>
</ol>

<p>
The authors conclude this section by citing work that suggests women are better at talent spotting, both “better at assessing the intelligence of both men and women” and “better than men at detecting deceit”; so good talent selection should involve women!
</p>

<p>
Cowen and Gross then discuss hiring minorities. As with women, the main challenge is perceiving the real talents of the people you talk to; cultural differences often lead to more awkward and formal conversations that struggle to escape “interview mode.” It’s hard to overcome this, but the authors propose the exercise of putting oneself into a situation where you feel culturally uncomfortable and observing how you struggle to present yourself and convey your ideas naturally. Emotionally internalizing this feeling can help you while interviewing those from different backgrounds. If nothing else, realizing that you struggle to perceive the abilities of minorities accurately can help you consciously compensate in the other direction. 
</p>

<h2>Conclusion</h2>

<p>
One of my key takeaways from <i>Talent</i> is this: every job requires aptitude along certain dimensions and is relatively insensitive to variation along other dimensions. The key to intelligent recruiting is to attune yourself to evaluating people only along important dimensions of talent while ignoring unimportant dimensions. Everyone has shortcomings; the most efficient hiring strategy is not to hire people without shortcomings, but to make sure their shortcomings are well-tolerated in the job you’re hiring for. (For instance, disagreeability is often viewed as positive for startup founders, but would certainly be deleterious for a salesperson. A disorganized chemist would do better in exploratory synthesis than in a job that required precise kinetic measurements.)
</p>

<p>
Another nice thought I got from <i>Talent</i> was a fundamental positivity about the ability of intelligent hiring to alleviate bias or prejudice. Cowen and Gross point out that if you believe a certain group of people is fundamentally overlooked or discriminated against by the market, the logical implication of that belief is that you should hire from that group: if you’re right, you’re not only helping yourself but also your hires. This little bit of free-market thinking turns the issue of bias from a negative one (“how is society mistreating people?”) to a positive one (“how can my talent search benefit by avoiding existing prejudice?”), which I found helpful.
</p>

<p>
But perhaps the most fundamental conclusion is simply that finding talent is an important, and underrated, skill for many areas of life. In the authors’ own words:
</p>

<blockquote>
The vision that talent search is ‘a thing,’ that it is an art that can be learned and improved on, and that it can be taught and communicated to others—that is the fundamental point of this presentation.
</blockquote>

<p>
I’d recommend this book both for people looking for talent (professors, founders, leaders) and for those hoping to display their own talent accurately to the world. 
</p>

]]></description>
              <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Plotting Chemical Space with UMAP and Molecular Fingerprints</title>
              <link>public/blog/20220926_plotting_diversity.html</link>
              <description><![CDATA[
<p>
In our recently published work on <a href="https://corinwagen.github.io/public/blog/20220901_screening_for_generality.html">screening for generality</a>, we selected our panel of model substrates in part using cheminformatic techniques. We're not the only people to do this, obviously: cheminformatics is a busy and important field, and even in organic chemistry there's lots of papers using similar techniques these days (I liked <a href="https://pubs.acs.org/doi/10.1021/jacs.1c12203">this work</a> from the Doyle lab). But since often the people who would benefit most from a new technique are the people who might be most intimidated by wading though documentation, I thought I'd post some simple example code here that others can copy-and-paste and modify to suit their own ends.
</p>

<p>
There are lots of ways to approach plotting chemical space, but fundamentally all approaches must address two big questions:
</p>

<ol>
  <li>
    How do you convert molecules into some numeric representation?
  </li>
  <li>
    Once you have numeric representations of all your molecules, how do you plot this?
  </li>
</ol>

<p>
I chose a relatively simple approach to the first question: molecular fingerprints (if you don't know what these are, I liked <a href=https://towardsdatascience.com/a-practical-introduction-to-the-use-of-molecular-fingerprints-in-drug-discovery-7f15021be2b1>this introduction</a> from <i>Towards Data Science</i>).
Based on <a href="https://greglandrum.github.io/rdkit-blog/similarity/reference/2021/05/26/similarity-threshold-observations1.html">Greg Landrum's findings</a>, I used the RDKit7 fingerprint. RDKit is the premier cheminformatics package, and well worth a download for anyone interested in these concepts.
</p>

<p>
For the second question (dimensionality reduction), I used the <a href="https://arxiv.org/abs/1802.03426">UMAP</a> algorithm. There are other approaches to this, like tSNE or PCA, but in my opinion there are <a href="https://blog.reverielabs.com/mapping-chemical-space-with-umap/">relatively convincing reasons</a> to favor UMAP (although <a href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x">this paper</a> points out some limitations).
</p>

<p>
Without further ado, then, here's some example code to take a list of IUPAC-type names and generate a 2D representation:
</p>

<pre class=code-block>
from rdkit import Chem
from urllib.request import urlopen
import re, tqdm, sys, umap
import numpy as np
import matplotlib.pyplot as plt

# make matplotlib look good
plt.rc('font', size=11, family="serif")
plt.rc('axes', titlesize=12, labelsize=12)
plt.rc(['xtick', 'ytick'], labelsize=11)
plt.rc('legend', fontsize=12)
plt.rc('figure', titlesize=14)
%matplotlib inline
%config InlineBackend.figure_format='retina'

# function for turning names into SMILES strings, because I find writing SMILES by hand impossible
def smiles_from_name(name):
    try:
        url_name = re.sub(" ", "%20", name)
        url = 'http://cactus.nci.nih.gov/chemical/structure/' + url_name + '/smiles'
        smiles = urlopen(url, timeout=5).read().decode('utf8')
        return smiles
    except Exception as e:
        print(name + " failed SMILES conversion")

class THbC():
    """ A tetrahydrobetacarboline. """
    def __init__(self, group, substituent, color="grey"):
        self.name = f"2-benzyl-1-({group})-{substituent}2,3,4,9-tetrahydro-1H-pyrido[3,4-b]indole"
        self.smiles = smiles_from_name(self.name)
        self.mol = Chem.MolFromSmiles(self.smiles)
        self.fingerprint = None
        self.color = color

    def get_fingerprint(self):
        if self.fingerprint is None:
            self.fingerprint = Chem.RDKFingerprint(self.mol, maxPath=7, branchedPaths=False)
        return self.fingerprint

# I just wrote out a lot of aromatic groups...
groups = [
    "phenyl", "4-methylphenyl", "4-methoxyphenyl", "4-fluorophenyl", "4-chlorophenyl", "4-bromophenyl",
    "4-(trifluoromethyl)phenyl", "4-nitrophenyl", "4-cyanophenyl", "piperonyl", "dihydrobenzofuryl",
    "3-methylphenyl", "3-methoxyphenyl", "3-fluorophenyl", "3-chlorophenyl", "3-bromophenyl",
    "3-(trifluoromethyl)phenyl", "3-nitrophenyl", "3-cyanophenyl", "2-methylphenyl", "2-methoxyphenyl",
    "2-fluorophenyl", "2-chlorophenyl", "2-bromophenyl", "2-(trifluoromethyl)phenyl",
    "2-nitrophenyl", "2-cyanophenyl", "2-pyridyl", "3-pyridyl", "4-pyridyl", "2-thiophenyl", "3-thiophenyl",
    "2-furyl", "3-furyl", "2-quinolinyl", "3-quinolinyl","6-quinolinyl", "5-quinolinyl", "8-quinolinyl",
    "5-indolyl", "3-indolyl", "7-azaindol-3-yl", "2-pyrrolyl", "3-pyrrolyl", "2-thiazolyl", "4-thiazolyl",
    "5-thiazolyl", "5-phenylisoxazol-3-yl", "imidazol-2-yl" "5-pyrimidyl", "5-indazolyl", "3-pyrazolyl",
    "4-pyrazolyl", "4-imidazolyl"
]

# substituents on the indole ring, and corresponding colors
subs = ["", "6-methoxy", "6-chloro"]
colors = ["grey", "red", "green"]

# build THbC objects (this might take a minute or two)
mols = list()
for group in tqdm.tqdm(groups):
    for sub, c in zip(subs, colors):
        mols.append(THbC(group=group, substituent=sub, color=c))

# generate UMAP embedding
crds = umap.UMAP(n_components=2, n_neighbors=20, min_dist=0.1, metric="jaccard").fit_transform([m.get_fingerprint() for m in mols])

# plot the result
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))
plt.scatter(crds[:,0], crds[:,1], c=[m.color for m in mols], s=20, alpha=0.8)
ax.set_xticks([])
ax.set_yticks([])
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.tight_layout()
plt.show()
</pre>

<p>
This code generates the following image:
</p>

<figure>
  <img class=centered-img src=https://corinwagen.github.io/public/img/20220926_umap.png style="width:400px;" />
  <figcaption>
    A 2D plot of the molecules shown above. Colors represent different substitution on the indole ring.
  </figcaption>
</figure>

<p>
  Although this program is a little clunky (slow calls to the CACTUS web service), it works well enough and is easy to modify as needed (to label the individual molecules, or apply a clustering algorithm to pick out model substrates). I hope you find this useful!
</p>
]]></description>
              <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Computational Report Cards</title>
              <link>public/blog/20220919_report_cards.html</link>
              <description><![CDATA[
<p>
In the course of preparing a literature meeting on post-Hartree–Fock computational methods last year, I found myself wishing that there was a quick and simple way to illustrate the relative error of different approximations on some familiar model reactions, like a "report card" for different levels of theory. I couldn't find any such graphic online, so I decided to make one (click image to view high-res PDF):
</p>

<figure>
  <a href="https://corinwagen.github.io/public/img/20220919_report_card.pdf">
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220919_report_card.png" style="width:700px;" />
  </a>
  <figcaption> Illustration of the accuracy of various computational methods for various model reactions</figcaption>
</figure>

<p>
All values are in kcal/mol, and the colors encode the error of the computed value: green values are within 10% or 1 kcal/mol of the truth, while yellow values are within 50% or 5 kcal/mol and red values are outside that range. (In each case the more restrictive cutoff was used.) Where possible, values have been benchmarked to experimental data; in the remaining cases, coupled-cluster calculations were employed.
</p>

<p>
While small relative to <a href="https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j">more professional benchmarks</a>, these data nicely illustrate a few important trends:
</p>

<ul>
<li>
    Dispersion corrections are badly needed for density-functional theory, but beyond that there isn't a clear "best" DFT method.
</li>
<li>
    Hartree–Fock theory consistently predicts electron-dense species to be less stable than they are, owing to the neglect of electron correlation: so anions are too basic/nucleophilic, bonds are too weak, and the aromatic Cope transition state is far too high in energy.
</li>
<li>
    Second-order Møller–Plesset perturbation theory (MP2) overcorrects the Hartree–Fock error, MP3 overcorrects the MP2 error, and MP4 overcorrects the MP3 error. The result is that HF and MP3 tend to "zig" where MP2 and MP4 tend to "zag." (The crude solution of just averaging MP2 and MP3 is <a href="https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/cphc.201200850">surprisingly effective</a>.) 
</li>
<li>
    <a href="https://pubs.acs.org/doi/10.1021/acs.jpca.9b05734">DLPNO-CCSD(T)</a>, from Neese and co-workers, really is a magical combination of speed and accuracy!
</li>
</ul>

<p>
  Hopefully this overview, while simple, helps to build intuition about how good or bad computational predictions at a given level of theory are.
</p>
]]></description>
              <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Book Review: Zero to One</title>
              <link>public/blog/20220914_zero_to_one.html</link>
              <description><![CDATA[
<p>
Who is Peter Thiel? <a href="https://conversationswithtyler.com/episodes/peter-thiel/">Tyler Cowen</a> calls him one of the most important public intellectuals of our era. <a href="https://www.bloomberg.com/news/features/2021-09-15/peter-thiel-gamed-silicon-valley-tech-trump-taxes-and-politics">Bloomberg</a> called him responsible for the ideology of Silicon Valley “more than any other living Silicon Valley investor or entrepreneur.” Depending on who you ask, he’s either a shadowy plutocratic genius or a visionary forward-thinking genius: but everyone seems to at least agree that he’s a genius.
</p>
<p>
<i>Zero to One</i> is his book of business advice. Given that Thiel started two very successful businesses (PayPal and Palantir) and has been a key early investor in many others (Facebook, SpaceX, and Airbnb, to name a few), I reasoned that it had to be pretty good advice. What I didn’t anticipate is how surprising the advice would be. Given Thiel's vast influence on the tech world, I assumed that any wisdom would have filtered into mainstream awareness and become part of the established dogma. Instead, I had the opposite experience: I was surprised by much of the advice, and thought some parts were even in active conflict with other advice I’d read.
</p>

<p>
(Vox’s <a href="https://www.vox.com/2014/11/30/7300019/how-peter-thiel-repackaged-conventional-wisdom-as-bold-contrarianism">Timothy Lee</a> takes the other position, accusing Thiel of repackaging “conventional wisdom as bold contrarianism” and “contrasting his own views against caricatured positions that hardly anyone actually agrees with.” I’ll present some examples below that illustrate why I think this is wrong.)
</p>

<p>
<i>Zero to One</i> is ostensibly a regular book organized into chapters, but reading it felt more like reading a collection of essays loosely connected by a few <i>leitmotifs</i>. Accordingly, I’ll summarize what I consider to be the big ideas of the book below, without any attempt to mirror the actual order that they’re presented in.
</p>

<h2>Big Ideas:</h2>

<h3>1. Competition is bad.</h3> 
<p>
This seems counterintuitive: isn’t the whole point of the free market that competition is good? But Thiel argues that, from the perspective of businesses, competition is the ultimate evil. A competitive environment is one in which resources must be expended on staying ahead of other businesses, not in investing in the future. At the limit, perfect competition leads to perfect stagnation. In one of <i>Zero to One</i>’s many iconic quotes, Thiel quips:
</p>

<blockquote>
If you can recognize competition as a destructive force instead of a sign of value, you're already more sane than most.
</blockquote>

<p>
Viewing competition as bad has non-obvious implications. One implication is that you should only start a business when you have a clear path towards a competition-free market. This might be a new market, or a technological advance that obliterates the existing market. For examples of the latter, Thiel cites <i>inter alia</i> Google (for search) and Apple (for iPads). As a rough rule of thumb, a business needs a order-of-magnitude advantage over the competition to be free from competitive insecurity.
</p>

<p>
Thiel then goes on to criticize the push to maximize the total addressable market (TAM) of a startup. This is surprising, because every startup pitch I’ve seen tries to emphasize how big their TAM is. But if competition is bad, then the perfect place to start a company is a small, competition-free pond adjacent to a much larger ocean. 1% of a $10B market and 100% of a $100M market are the same number, but it’s much easier to grow a company in the latter.
(This whole point reminds me of <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy"><i>Blue Ocean Strategy</i></a>.)
</p>

<p>
(You might think that this principle is less true for businesses dependent on network effects, like Ebay or Facebook, but Thiel paradoxically asserts the exact opposite. Since network effects rely on almost complete saturation, these startups in particular need to start in a small, easily dominated area—like Harvard for Facebook.)
</p>

<p>
Perhaps a less obvious implication of viewing competition as bad is that thinking in terms of “disruption” is also bad. I was surprised to read this, because the Silicon Valley-adjacent people I’ve interacted with seem obsessed with disruption: everyone wants to disrupt healthcare, or education, or government. But Thiel is skeptical of this urge:
</p>

<blockquote>
If you think of yourself as an insurgent battling dark forces, it’s easy to become unduly fixated on the obstacles in your path. But if you truly want to make something new, the act of creation is far more important than the old industries that might not like what you create.
</blockquote>

<p>
Disruption is a way to frame innovation in negative terms, highlighting how new ideas can destroy existing systems. Thiel instead wants founders to be motivated by a desire to create new wonders. A critical reader might call this a glass-half-full/glass-half-empty rephrasing, but how we frame our own motivations often has a variety of subtle, downstream effects that are hard to appreciate in the moment.
</p>

<h3>2. Founders Need Definite Optimism.</h3> 
<p>
It’s pretty obvious that founders need to be optimistic, because convicted pessimists lack the desire to create anything new: a precondition for succeeding is believing that success is possible. But Thiel further bisets optimism into definite and indefinite halves. Indefinite optimists believe that things are going to get better, but lack a clear vision of how or why. In contrast, definite optimists have a positive vision for the future.
</p>

<p>
Thiel argues that indefinite optimism epitomizes the modern world (from c. 1970 to the present). Much of our economy is devoted to indefinite optimism:
</p>

<blockquote>
Finance epitomizes indefinite thinking because it’s the only way to make money when you have no idea how to create wealth… the fundamental tenet is that the market is random.
</blockquote>

<p>
Thiel also criticizes philosophy for succumbing to indefinite optimism. Previously, philosophers offered substantive visions of the good life; in the late 20th century, philosophers like Robert Nozick and John Rawls, although ideological adversaries, both focused on procedural theories of philosophy that emphasized the fairness of processes, not the nature of their outcomes.
</p>

<p>
Thiel proceeds to criticize government for indefinite optimism, focusing on entitlements and procedural fairness rather than centralized planning for the future, and biotech:
</p>

<blockquote>
Today it’s possible to wonder whether the genuine difficulty of biology has become an excuse for biotech startups’ indefinite approach to business in general. Most of the people involved expect some things to work eventually, but few want to commit to a specific company with the level of intensity necessary for success. It starts with the professors who often become part-time consultants instead of full-time employees—even for the biotech startups that begin from their own research. Then everyone else imitates the professors’ indefinite attitude. <u>It’s easy for libertarians to claim that heavy regulation holds biotech back—and it does—but indefinite optimism may pose an even greater challenge for the future of biotech.</u>
<i>(emphasis added)</i>
</blockquote>

<p>
The conclusion of all this is what you might expect: as a founder, you should have definite optimism. Just thinking the world is likely to change is not enough to make you start a good company, no matter how exciting the field: “No sector will ever be important enough that merely participating in it will be enough to build a great company.” Thiel concludes:
</p>

<blockquote>
Darwinism may be a fine theory in other contexts, but in startups, intelligent design works best…. A startup is the largest endeavor over which you can have definite mastery. You can have agency not just over your own life, but over a small and important part of the world.
</blockquote>

<p>
This seems like good advice. Nevertheless, I find it tough to square Thiel’s view that a company should have a clearly defined mission and purpose with the conventional wisdom that startups ought to be flexible in their early days. Paul Graham describes this in his <a href="http://www.paulgraham.com/startupmistakes.html">piece</a> on common startup mistakes:
</p>

<blockquote>
Don't get too attached to your original plan, because it's probably wrong. Most successful startups end up doing something different than they originally intended — often so different that it doesn't even seem like the same company. You have to be prepared to see the better idea when it arrives. And the hardest part of that is often discarding your old idea.
</blockquote>

<p>
And in another <a href="http://www.paulgraham.com/notnot.html">essay</a>, he describes how common it is for startups to change their goal:
</p>

<blockquote>
In the average Y Combinator startup, I'd guess 70% of the idea is new at the end of the first three months. Sometimes it's 100%.
</blockquote>

<p>
Do Graham and Thiel disagree with one another on this point, or are these essays arguing about something slightly different? One attempt to synthesize these two views might be the opinion that a startup should work towards a definite goal but be somewhat path-agnostic, especially early on. I’m not sure if this synthesis would satisfy either party.
</p>

<h3>3. A company’s culture proceeds from its mission, not vice versa.</h3>
<p>
Thiel is quick to criticize the increasingly lavish corporate perks used to lure top talent, claiming that putting culture ahead of mission ultimately dooms companies:
</p>

<blockquote>
No company <i>has</i> a culture; every company <i>is</i> a culture. A startup is a team of people on a mission, and a good culture is just what that looks like on the inside. <i>(emphasis original)</i>
</blockquote>

<p>
So, how do you build a team of people on a mission? The answer, Thiel claims, is to organize your company around a shared secret: a belief that the world can be changed for the better in a specific way. This is similar to the previous point about the importance of definite optimism. All founders should believe that they’ve discovered a way to change the world that other people haven’t realized—that they’ve discovered a secret insight. A company, then, is just the natural way to actualize and communicate that insight:
</p>

<blockquote>
The best entrepreneurs know this: every great company is built around a secret that’s hidden from the outside. A great company is a conspiracy to change the world; when you share your secret, the recipient becomes a fellow conspirator.
</blockquote>

<p>
Thiel uses this insight to drive various points about corporate structure. For instance, every member of the conspiracy needs to be fully invested in the mission, to prevent problems arising from imperfect alignment of individual goals: “You need good people who get along, but you also need a structure to help keep everyone aligned for the long term.” This is true for management, who should be mostly paid in stock, to encourage long-termism, and for regular employees:
</p>

<blockquote>
Everyone you involve with your company should be involved full-time… anyone who doesn’t own stock options or draw a regular salary from your company is fundamentally misaligned…. Even working remotely should be avoided.
</blockquote>

<p>
Having a good mission is also key to recruiting, in Thiel’s mind. The uncomfortable truth about startups is that any potential hire could get a better offer, with more benefits, from an established company. Why would anyone want to work for you? The answer is that they believe in your mission—that they agree with your vision of the world and want to change it accordingly. Without a vision you’re just bidding for mercenaries.
</p>

<h2>Smaller Points Which I Found Interesting:</h2>

<h3>4. Aspects of Governance </h3>
<p>
Thiel breaks corporate governance down into three parts, which I found a useful conceptual distinction:
</p>

<ul>
<li>Ownership (equity, who controls the stock)</li>
<li>Possession (who makes day-to-day decisions, typically the CEO)</li>
<li>Control (who formally governs, e.g. a board of directors)</li>
</ul>

<h3>5. The Importance of Sales</h3>
<p>
Perhaps one of the most explicitly esoteric ideas in the book is the claim that sales is omnipresent and rules every area of life, but is cloaked by the systematic attempt every salesman makes to disguise the nature of their art. In Thiel’s words:
</p>

<blockquote>
Whatever the career, sales ability distinguishes superstars from also-rans… Even university professors, who claim authority from scholarly achievement, are envious of the self-promoters who define their field. Academic ideas about history or English don’t just sell themselves on their intellectual merits. Even the agenda of fundamental physics and the future path of cancer research are results of persuasion. The most fundamental reason that even businesspeople underestimate the importance of sales is the systematic effort to hide it at every level of every field in a world secretly driven by it…. If you’ve invented something new but you haven’t invented an effective way to sell it, you have a bad business—no matter how good the product.
</blockquote>

<p>
I don’t think anyone currently in academia would dispute this characterization.
</p>

<h3>6. Technological Innovation</h3>
<p>
Thiel thinks that computers are better treated as assistants than replacements for humans:
</p>

<blockquote>
Computers are complements for humans, not substitutes. The most valuable businesses of coming decades will be built by entrepreneurs who seek to empower people rather than try to make them obsolete… We have let ourselves become enchanted by big data only because we exoticize technology.
</blockquote>

<p>
This makes sense, but also might be a bit tautological. Any sufficiently advanced tool will invariably become a complement for some human’s workflow: AlphaFold and DALL-E have automated some human tasks, and now everyone’s expectations have adjusted and we use these tools as complements for our own capabilities. What’s the difference, <i>a priori</i>, between seeking to empower people and trying to make them obsolete? Does AlphaFold empower medicinal chemists or make structural biologists obsolete? (The critical reader will respond “Neither, at the moment,” which is fair.)
</p>

<h2>Additional Memorable Quotes:</h2>
<ul>
<li>“The most contrarian thing of all is not to oppose the crowd but to think for yourself.”</li>
<li>“All Rhodes Scholars had a great future in their past.”</li>
<li>“If everything worth doing has already been done, you may as well feign an allergy to achievement and become a barista.”</li>
<li> “Recruiting is a core competency for any company. It should never be outsourced.” </li>
<li> “If you don’t see any salespeople, you’re the salespeople.” </li>
<li> “You can’t dominate a submarket if it’s fictional.” </li>
<li> “The best projects are likely to be overlooked, not trumpeted by a crowd; the best problems to work on are often the ones nobody else even tries to solve.” </li>
</ul>

<h2>Conclusions:</h2>
<p>
How can such a famous book remain countercultural almost a decade after it was published? One possibility is that Thiel presents only one side of a dialectic, while other authors present the natural opposing views. Since community consensus follows the synthesis of the two views, both extremes appear to be arguing against the norm even at equilibrium. This might be true; I’m not well-versed enough in the startup literature to know.
</p>

<p>
Another possibility is that Thiel’s advice is simply difficult to understand, or difficult to follow. It’s easy to read <a href="https://en.wikipedia.org/wiki/Wisdom_literature">wisdom literature</a> (like the Book of Proverbs) but hard to apply it to your life; simply reading something wise doesn’t automatically make you wise. Maybe <i>Zero to One</i> is like a modern-day Proverbs for founders—multitudes read it, but only a rare breed of person is able to successfully understand and actualize its insights.
</p>

<p>
Perhaps the biggest drawback of <i>Zero to One</i> is apparent just from the title. Thiel is primarily interested in businesses that aim to change the world, to build something completely new and revolutionary, to go not from 1 to N but from 0 to 1. And so much of the advice in the book is only applicable to businesses trying to go from 0 to 1: how does a restaurant gain an order-of-magnitude advantage over other restaurants and avoid competition? Is the French Laundry 10x better than Alinea? Is Chipotle 10x better than Qdoba?
</p>

<p>
Thiel’s answer to this question is probably “don’t start a restaurant”; in an <a href="https://conversationswithtyler.com/episodes/peter-thiel/">interview with Tyler Cowen</a> he said that the Straussian reading of <i>Zero to One</i> was “don’t start a business.” And indeed every piece of positive advice in the book can be inverted to the corresponding cautionary wisdom: don’t start a business unless (i) you think you’ve found a secret about the world and (ii) you have a concrete plan for implementing it without (iii) competing with existing businesses.
</p>

<p>
But it’s good that restaurants exist, and so at the very least we can conclude that <i>Zero to One</i> shouldn’t be read as a categorical imperative for all businesses, but advice only for a narrower subset thereof. Not every founder needs to start a “Zero to One” business. Not every business needs to be PayPal or Facebook.
</p>

<p>
What wisdom does <i>Zero to One</i> have for academia? The principle of avoiding competition is an obvious one: nobody in their right mind should start a new program in photoredox catalysis right now. In contrast, some research areas seem almost totally neglected at the moment (<a href="https://sites.google.com/view/the-harman-lab-uva/publications">W. Dean Harman</a> and his work on η<sub>2</sub>–arene complexes comes to mind).
</p>

<p>
Thiel’s point about definite optimism also translates readily to the research sphere. If our goal is to advance scientific progress, definite optimism starts with a vision of the future of our field and generates a roadmap of how to get there. This vision could be a lot of things—designing arbitrary enzymes <i>in silico</i>, perfect prediction of absolute binding constants in relevant environments, CO<sub>2</sub> reduction on megaton scale, fully automated synthesis of arbitrary natural products—but the correct research direction then becomes “whatever gets us closer to that goal.” In contrast, indefinite optimism is a belief that science will advance more or less through Brownian motion, by following random grants and research directions that lead to publications. Needless to say, the former is better than the latter.
</p>

<p>
(The point about misalignment of incentives is also excellent when applied to academia, but deserves a longer treatment than I can give here.)
</p>

<p>
Overall, I think <i>Zero to One</i> is an excellent book, and worth reading for anyone considering anything startup-related (including starting a research group).
</p>
]]></description>
              <pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Singleton Saturday: Nitration of Toluene</title>
              <link>public/blog/20220909_nitration_of_toluene.html</link>
              <description><![CDATA[
<p>
<i>
This is the second in what will hopefully become a series of blog posts (<a href="https://corinwagen.github.io/public/blog/20220805_singleton_saturday_decarboxylation.html">previously</a>) focusing on the fascinating work of Dan Singleton (professor at Texas A&amp;M). My goal is to provide concise and accessible summaries of his work and highlight conclusions relevant to the mechanistic or computational chemist. 
</i>
</p>

<p>
Today I want to discuss one of my favorite papers, a detailed study of the nitration of toluene by Singleton lab at Texas A&amp;M. 
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_exp_regiochem.png" style="width:450px;" />
<figcaption>
Regiochemistry of the nitration of toluene with nitronium tetrafluoroborate in dichloromethane.
</figcaption>
</figure>

<p>
This reaction, a textbook example of electrophilic aromatic substitution, has long puzzled organic chemists. The central paradox is that intermolecular selectivity is typically quite poor between different arenes, but positional selectivity is high—implying that NO<sub>2</sub><sup>+</sup> is unable to recognize more electron-rich arenes but somehow still able to recognize more electron-rich sites within a given arene. 
</p>

<p>
George Olah devoted considerable effort to studying this paradox:
</p>

<blockquote>
Kuhn and I have subsequently developed a new efficient nitration method by using stable nitronium salts (like tetrafluoroborate) as nitrating agents. Nitronium salt nitrations are also too fast to measure their absolute rates, but the use of the competition method showed in our work low substrate selectivity, e.g., k<sub>t</sub>/k<sub>b</sub> of 1-2. <i>[In other words, competition experiments show at most a 2-fold preference for toluene.]</i> <u>On the basis of the Brown selectivity rules, if these fast reactions followed σ-complex routes they would also have a predictably low positional selectivity (with high meta isomer content).</u> However, the observed low substrate selectivities were all accompanied by high discrimination between available positions (typical isomer distributions of nitrotoluenes were (%) ortho:meta:para = 66:3:31). Consequently, a meta position would seem to be sevenfold deactivated compared to a benzene position, giving a partial rate factor of m<sub>f</sub> = 0.14. These observations are inconsistent with any mechanism in which the individual nuclear positions compete for the reagent (in the σ-complex step).
<br>
<br>
In explanation, we suggested the formation of a π complex in the first step of the reactions followed by conversion into σ complexes (which are of course separate for the individual ortho, para, and meta positions), allowing discrimination in orientation of products. 
(<a href=https://pubs.acs.org/doi/pdf/10.1021/ar50043a002>ref</a>, emphasis added)
</blockquote>

<p>
His conclusion, summarized in the last sentence of the above quote, was that two different sets of complexes were involved: π complexes which controlled arene–arene selectivity, and σ complexes which controlled positional selectivity. Thus, the paradox could be resolved simply by invoking different ∆∆G<sup>‡</sup> values for the transition states leading to π- and σ-complex formation. The somewhat epicyclic nature of this proposal led to pushback from the community, and (as Singleton summarizes) no cogent explanation for this reactivity had yet been advanced at the time of writing. 
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_olah_mech.png" style="width:425px;" />
<figcaption>A summary of Olah’s proposed resolution, featuring π- and σ-complexes.</figcaption>
</figure>

<p>
The authors of this paper initiated their studies of this reaction by performing an extensive series of “traditional” DFT calculations in implicit solvent. M06-2X/6-311G(d) was chosen by benchmarking against coupled-cluster calculations, and the regiochemistry was examined with a variety of computational methods. 
</p>

<p>
In the absence of BF<sub>4</sub><sup>-</sup>, naïve calculations predict entirely the wrong result: the <i>para</i> product is predicted to be more favorable than the <i>ortho</i> product, and no <i>meta</i> product is predicted to form at all. However, closer examination of the transition states reveals post-transition-state bifurcation in each case: for instance, the “<i>para</i>” transition state actually leads to <i>para</i>/<i>meta</i> in a 89:11 ratio. When all possible bifurcations for all transition states are taken into account in a Boltzmann-weighted way, the results remain wrong: <i>para</i> is still incorrectly favored over <i>ortho</i>, and <i>meta</i> is now predicted to form in a much higher proportion than observed.
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_implicit.png" style="width:550px;" />
<figcaption>Illustration of transition states and post-TS bifurcations for traditional DFT calculations.</figcaption>
</figure>

<p>
The authors examine various potential resolutions of this problem, including inclusion of BF<sub>4</sub><sup>-</sup>, use of explicit solvent within an ONIOM scheme, and other nitration systems which might lead to more dissociated counterions. These methods lead to different, but equally wrong, conclusions. 
</p>

<p>
They then perform free energy calculations to determine the energetics of nitronium approach to toluene (in dichloromethane). Surprisingly, no barrier exists to NO<sub>2</sub><sup>+</sup> attack: once nitronium comes within 4.5 Å of the arene, it is “destined to form some isomer” of product (in the authors’ words). Singleton and Nieves-Quinones dryly note:
</p>

<blockquote>
…The apparent absence of transition states (more on this later) after formation of the encounter complex has never previously been suggested. This absence is in fact counter to basic ideas in all previous explanations of the selectivity.
</blockquote>

<p>
This observation explains one horn of the dilemma—why selectivity between different arenes is low—but leaves unanswered why positional selectivity is so high. To examine this question, the authors then directly run reactions <i>in silico</i> by using unconstrained <i>ab initio</i> molecular dynamics (AIMD) and observe the product ratio. The product ratio (45:2:53 <i>o</i>/<i>m</i>/<i>p</i>) they observe matches the experimental values (41:2:57 <i>o</i>/<i>m</i>/<i>p</i>) almost perfectly!
</p>

<p>
With this support for the validity of the computational model in hand, the authors then examine the productive trajectories in great detail. Surprisingly, they find that although no barrier exists to nitronium attack, the reaction is relatively slow to proceed, taking on average of 3.1 ps to form the product. Trajectories lacking either explicit solvent or tetrafluoroborate lack both this recalcitrance and the observed selectivity: instead, nitronium opportunistically reacts with the first carbon it approaches. This suggests that selectivity is only possible when the nitronium–toluene complex is sufficiently persistent.
</p>

<figure>
<img class=centered-img src="https://corinwagen.github.io/public/img/20220910_explicit.png" style="width:550px;" />
<figcaption>
  Nitronium roams within the π complex, sampling different carbons before ultimately forming product.
</figcaption>
</figure>

<p>
The authors attribute the long life of the NO<sub>2</sub><sup>+</sup>—toluene complex to the fact that the explicit solvent cage must reorganize to stabilize formation of the Wheland intermediate. This requires both reorientation of the dichloromethane molecules and repositioning of the tetrafluoroborate anion, which both occur on the timescale of the trajectories (<a href=https://corinwagen.github.io/public/blog/20220719_timescales.html>ref</a>). Accordingly, the reaction is put on hold while cage reorganization occurs, giving NO<sub>2</sub><sup>+</sup> time to preferentially attack the <i>ortho</i> and <i>para</i> carbons. (I would be tempted to call the π complex a dynamic/entropic intermediate, in the language of either <a href="https://corinwagen.github.io/public/blog/20220805_singleton_saturday_decarboxylation.html">Singleton</a> or <a href="https://www.sciencedirect.com/science/article/abs/pii/S2589597419300103">Houk</a>.)
</p>

<p>
This computational picture thus accurately reproduces the experimental observations and explains the initial paradox we posed: selectivity does not arise through competing transition state energies, but through partitioning of a product-committed π complex which is prevented from reacting further by non-instantaneous solvent relaxation. Since similar π complexes can be formed under almost any nitration conditions, this proposal explains the often similar selectivities observed with other reagents or solvents.
</p>

<p>
More philosophically, this proposal explains experimental results without invoking any transition states whatsoever. In their introduction, the authors quote Gould as stating:
</p>

<blockquote>
If the configuration and energy of each of the intermediates and transition states through which a reacting system passes are known, it is not too much to say that the mechanism of the reaction is understood.
</blockquote>

<p>
While this may be true to a first approximation in most cases, Singleton and co-workers have demonstrated here that this is not true in every case. This is an important conceptual point. As our ability to study low-barrier processes and reactive intermediates grows, I expect that we will more clearly appreciate the limitations of transition-state theory, and have to develop new techniques to interpret experimental observations.  
</p>

<p>
But perhaps the reason I find this paper most exciting is simply the beautiful match between theory and experiment for such a complex and seemingly intractable system. This work not only predicts the observed reaction outcomes under realistic conditions, but also allows (through AIMD) the complete analysis of the entire reaction landscape in exquisite detail, from approach to post-bond-forming steps. In other words, this is a mechanistic chemist’s dream: a perfect movie of the entire reaction’s progress, from beginning to end. Go and do likewise!
</p>

<i>Thanks to Daniel Broere and Joe Gair for noticing that "electrophilic aromatic substitution" was erroneously written as "nucleophilic aromatic substitution." This embarassing oversight has been corrected.</i>

]]></description>
              <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Elegy for the MIT of Yesteryear</title>
              <link>public/blog/20220907_mit_elegy.html</link>
              <description><![CDATA[
<figure>
<img class=centered-img src=https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Old_drawing_of_MIT.JPG/800px-Old_drawing_of_MIT.JPG style="width:550px;" />
<figcaption>Old image of MIT, c/o <a href="https://commons.wikimedia.org/wiki/File:Old_drawing_of_MIT.JPG">Wikimedia Commons</a></figcaption>
</figure>

<p>
Over the past few weeks, I’ve been transfixed, and saddened, by Eric Gilliam’s three-part series about the history of MIT (my <i>alma mater</i>). I’ll post a few quotations and responses below, but if you’re interested you should just go read the original essays
(<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">1</a>, 
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">2</a>,
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-045">3</a>).
</p>

<h3>Why MIT Was Created</h3>

<blockquote>
Professors who are not steeped in hands-on industrial practice could not produce the kinds of workers that were immediately useful to industry. These schools were outputting the kind of men that [Thomas] Edison, and many others mentioned above, did not believe were meeting the needs of industry. And the technical know-how taught in trade schools was great, but an ideal institute of technology should also impart some higher engineering and scientific knowledge to students to enable them to be more innovative, intelligent problem-solvers.
<br>
<br>
So, MIT was founded to solve this problem. This school was not designed to be a place for purely lecturing and rote learning. A smattering of intelligent men from industry and university men with an applied bent to them made up the original faculty. Content was lectured as needed, but what differentiated MIT was its innovative use of the laboratory method. Instructors taught “through actual handling of the apparatus and by working on problems, shoulder to shoulder with the boys.” And the schedule, from 9-5 (with a lunch break) 5 days a week and additional class on Saturday was meant to simulate a normal work schedule and, thus, ease the eventual transition to life in the working world.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">(part 1)</a>
</blockquote>

<p>
This quote highlights how MIT was intended to be a counter-cultural university, founded on a distinctly different model than other institutions (like Harvard).
MIT was not meant to be a center of learning and theoretical research, but a school focusing on training the next generation of industrial leaders.
</p>

<h3>How MIT Supported Itself</h3>

<blockquote>
But [MIT President] Maclaurin had an idea: self-support. MIT would capitalize on its own assets and earn money by formally offering its services to industry on a larger scale. High numbers of industrial partners had been eager to engage in ad-hoc courses of research with MIT’s applied professors, often paid for by the company, anyway. Why not turn this into a much larger, more formal program that was facilitated by the Institute? The idea would grow into what was known as the Technology Plan.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<p>
MIT operated on a different funding model than other universities, relying on support from industry.
This is, in essence, what I proposed several weeks ago in my <a href=https://corinwagen.github.io/public/blog/20220728_consulting_as_grad_school.html>reflection</a> on the similarities between graduate school and consulting.
This was seen as important and honorable by its leaders at the time:
</p>

<blockquote>
“There could be no more legitimate way for a great scientific school to seek support than by being paid for the service it can render in supplying special knowledge where it is needed... Manufacturers may come to us with problems of every kind, be they scientific, simple, technical or foolish. We shall handle each seriously, giving the best the institute has at its disposal” - William Walker, head of the Division for Industrial Cooperation and Research
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<h3>Why MIT Changed Paths</h3>

<p>
The answer to this question is the subject of Gilliam's 
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-045">third post</a>.
It's a bit too complex to fully summarize here, but there were a few key factors:
</p>

<ul>
  <li>
    The massive increase in post-WWII funding for science allowed MIT to shift away from applied research and towards basic research.
  </li>
  <li>
    The growing importance of industrial research labs at <i>inter alia</i> Bell Labs, GE, and DuPont meant that the need for academic applied research was decreasing, and also that academically trained researchers were valuable assets to industry. (This was the case only in the post-war period, however, and by 1980 the era of large industrial research labs had mostly concluded.)
  </li>
  <li>
    A growing distaste for pursing "boundary work" that could also be done in industry, as opposed to pursuing a uniquely academic niche.
  </li>
</ul>

<p>
  Crucially, the first two factors are less true today than they were when MIT made this decision, implying that the niche filled by "Old MIT" could be occupied again today.
</p>

<h3>Why A School Like Old MIT Should Still Exist</h3>

<blockquote>
It seems clear, given MIT’s transition to a more university style of education, that we are left with a hole. We do not have an elite hybrid technical school/applied research institute like this that can draw top talent away from places like Harvard and Stanford to its more hands-on style of education. But, as a country where the manufacturing sector is shrinking (and median wages aren’t doing so well either), we may need a new MIT now more than ever.
<br>
<br>
There are plenty of individuals at top schools who COULD be swayed to attend a place like this. Speaking for Stanford, where I went to undergrad, there was a large population of people who majored in mechanical engineering and were disenchanted because they did almost exclusively problem set work and very little building of anything real. And I knew even more people majoring in other subjects who abandoned mechanical engineering and majors like it for this reason! “We’re training you to be mechanical engineering managers, not traditional mechanical engineers,” was a common line used in the department. And, while that is a fine goal for a program, it is not what many of the students seem to want. What if I just want to be a top-flight regular engineer who can build awesome stuff?
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early">(part 1)</a>
</blockquote>

<blockquote>
There can and should be individuals who spend almost all of their time on pure research. But it is probably bad for future progress to allow too many of these individuals to work in an environment in which few of their peers are spending a substantial amount of time working on industrial applications and problems. No matter what, some basic research will always find a way of trickling its way down into practical industrial importance. But allowing pure researchers to be siloed from the acquaintance of those who work on industrial applications — and not just the need to work on those problems themselves — feels like it is setting the system up for inefficiency. When we look back on the era of explosive productivity in areas of basic research like physics and math in the early 1900s, even the purest of pure researchers at the time tended to have regular interactions either with industry or with researchers who did industry-related research — due to industry contracts themselves, close friends who did industry work regularly, or conscription to work on military.
<a href="https://freaktakes.substack.com/p/a-progress-studies-history-of-early-001">(part 2)</a>
</blockquote>

<p>
  Gilliam's conclusions seem broadly correct to me. While MIT is still a great school, it's no longer pursuing a distinct model for education. The main factors distinguishing MIT from peer institutions are cultural, and even those are being <a href="https://www.theatlantic.com/education/archive/2017/06/the-fall-of-mits-counter-culture-dorm/532074/">actively</a> 
  <a href="https://thetech.com/2022/08/25/fortress-mit">suppressed</a> by the current administration.
  In total it took less than a century for the entrepreneurial mindset of MIT, a "startup university", to be replaced by the exact institutional conservatism it was founded to oppose. "You either die a hero or live long enough to see yourself become the villain."
</p>

<p>
More broadly, there's a broad sense today that innovation, especially in the physical world, is slowing (<a href="http://danwang.co/why-is-peter-thiel-pessimistic-about-technological-innovation/">Peter Thiel</a> may be the most notable proponent of this claim).
A century ago, Americans could build whole networks of subways with comparatively primitive technology; now, something as simple as <a href="https://www.vox.com/policy-and-politics/2017/1/1/14112776/new-york-second-avenue-subway-phase-2">building a single subway station</a> has become a Herculean task.
I don't mean to draw too direct of a causal connection between the end of Old MIT and the decline in real-world innovation, but perhaps a new school focused on unglamorous, nuts-and-bolts innovation rather than holistic education is exactly what the US needs now.
</p>
]]></description>
              <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Potential Energy Surfaces in Python</title>
              <link>public/blog/20220905_pypes.html</link>
              <description><![CDATA[
<p>
    Organic chemists often think in terms of <a href="https://corinwagen.github.io/public/blog/20220815_rate_determining_span.html">potential energy surfaces</a>, especially when plotting the results of a computational study.
    Unfortunately it is non-trivial to generate high-quality potential energy surfaces. It's not too difficult to sketch something crude in ChemDraw or Powerpoint, but getting the actual
    barrier heights correct and proportional has always seemed rather tedious to me.
</p>

<p>
    I've admired the smooth potential energy surfaces from the <a href="https://baik-laboratory.com/publications">Baik group</a> for years, and so several months ago I decided to try and write my own
    program to generate these diagrams. I initially envisioned this as a python package (with the dubiously clever name of <i>pypes</i>), but it turned out to be simpler than expected, such that I 
    haven't actually ever turned it into a library. It's easier to just copy and paste the code into various Jupyter notebooks as needed.
</p>

<p>
    Here's the code:
</p>

<pre class=code-block>
# get packages
import numpy as np
import scipy.interpolate as interp
import matplotlib.pyplot as plt

# make matplotlib look good
plt.rc('font', size=11, family="serif")
plt.rc('axes', titlesize=12, labelsize=12)
plt.rc(['xtick', 'ytick'], labelsize=11)
plt.rc('legend', fontsize=12)
plt.rc('figure', titlesize=14)

%matplotlib inline
%config InlineBackend.figure_format='retina'

# x and y positions. y in kcal/mol, if you want, and x in the range [0,1].
Y = [2.49, 3.5, 0, 20.2, 19, 21.5, 20, 20.3, -5]
X = [0, 0.15, 0.3, 0.48, 0.55, 0.63, 0.70, 0.78, 1]

# labels for points. False if you don't want a label
label = ["label1", False, "label2", "label3", "label4", "label5", "label6", "label7", "label8"]

#### shouldn't need to modify code below this point too much...

# autodetect which labels correspond to transition states
TS = []
for idx in range(len(Y)):
    if idx == 0 or idx == len(Y)-1:
        TS.append(False)
    else:
        TS.append((Y[idx] > Y[idx+1]) and (Y[idx] > Y[idx-1]))

# sanity checks
assert len(X) == len(Y), "need X and Y to match length"
assert len(X) == len(label), "need right number of labels"

# now we start building the figure, axes first
f = plt.figure(figsize=(8,8))
ax = f.gca()
xgrid = np.linspace(0, 1, 1000)
ax.spines[['right', 'bottom', 'top']].set_visible(False)

YMAX = 1.1*max(Y)-0.1*min(Y)
YMIN = 1.1*min(Y)-0.1*max(Y)

plt.xlim(-0.1, 1.1)
plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)
plt.ylim(bottom=YMIN, top=YMAX)
ax.plot(-0.1, YMAX,"^k", clip_on=False)

# label axes
plt.ylabel("Gibbs Free Energy (kcal/mol)")
plt.xlabel("Reaction Coordinate")

# plot the points
plt.plot(X, Y, "o", markersize=7, c="black")

# add labels
for i in range(len(X)):
    if label[i]:
        delta_y = 0.6 if TS[i] else -1.2
        plt.annotate(
            label[i],
            (X[i], Y[i]+delta_y),
            fontsize=12,
            fontweight="normal",
            ha="center",
        )

# add connecting lines
for i in range(len(X)-1):
    idxs = np.where(np.logical_and(xgrid&gt;=X[i], xgrid&lt;=X[i+1]))
    smoother = interp.BPoly.from_derivatives([X[i], X[i+1]], [[y, 0] for y in [Y[i], Y[i+1]]])
    plt.plot(xgrid[idxs], smoother(xgrid[idxs]), ls="-", c="black", lw=2)

# finish up!
plt.tight_layout()
plt.show()
</pre>

<p>
The output looks like this:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220905_pes.png" style="width:550px;"/>
  <figcaption>The potential energy surface generated by the above code.</figcaption>
</figure>

<p>
If you like how this looks, feel free to use this code; if not, modify it and make it better! I'm sure this isn't the last word in potential-energy-surface creation, but it's good enough for me.
</p>
]]></description>
              <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Screening for Generality: Reflections</title>
              <link>public/blog/20220901_screening_for_generality.html</link>
              <description><![CDATA[
<p>
Now that our work on screening for generality has finally been published in <a href="https://www.nature.com/articles/s41586-022-05263-2"><i>Nature</i></a>, I wanted to first share a few personal reflections and then highlight the big conclusions that I gleaned from this project. 
</p>

<p>
This project originated from conversations I had with Eugene Kwan back in February 2019, when I was still an undergraduate at MIT. Although at the time our skills were almost completely non-overlapping, we shared both an interest in “big data” and high-throughput experimentation and a conviction that organic chemistry could benefit from more careful thinking about optimization methods. 
</p>

<p>
After a few months of work, Eugene and I had settled on the idea of a “catalytic reaction atlas” (in analogy to the <a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga">cancer genome atlas</a>) where we would exhaustively investigate catalysts, conditions, substrates, etc. for a single asymmetric reaction and then (virtually) compare different optimization methods to see which algorithms led to the best hits. Even with fairly conservative assumptions, we estimated that this would take on the order of 10<sup>5</sup> reactions, or about a year of continuous HPLC time, meaning that some sort of analytical advance was needed.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220901_slide.png" style="width:550px;"/>
  <figcaption>A slide comparing different optimization strategies, from April 2019.<br>Multi-substrate screening is proposed as one of many different algorithms.</figcaption>
</figure>

<p>
When I proposed this project to Eric, he was interested but suggested we focus more narrowly on the question of generality, or how to discover reactions with broad substrate scope. In an excited phone call, Eugene and I had the insight that we could screen lots of substrates at once by using mass spectrometry, thus bypassing our analytical bottleneck and enabling us to access the “big data” regime without needing vast resources to do so.<sup><a href="#fn1">1</a></sup>
</p>

<p>
Getting the analytical technology to work took about two years of troubleshooting. We were lucky to be joined by Spencer, an incredible analytical chemist and SFC guru, and eventually were able to get reproducible and accurate data by a combination of experimental insights (running samples at high dilution) and computational tweaks (better peak models and fitting algorithms). To make sure that the method was working properly, we ran validation experiments both on a bunch of scalemic samples and on a varied set of complex pharmaceutical racemates.
</p>

<p>
Choosing the proper reaction took a bit of thought, but once we settled on a set of substrates and catalysts the actual experiments were a breeze. Almost all the screening for this project was done in November–December 2021: in only a few hours, I could easily run and analyze hundreds of reactions per week. 
</p>

<p>
I want to conclude by sharing three high-level conclusions that I’ve taken away from working on this project; for the precise scientific conclusions of this study, you can read the paper itself.
</p>

<h3> 1. Chemical space is big, so how you search matters</h3>

<p>
There are a ton of potential catalysts waiting to be discovered, and it seems likely that almost any hit can be optimized to 90% ee by sufficient graduate-student hours. Indeed, one of the reasons we selected the Pictet–Spengler reaction was the diversity of different catalyst structures capable of giving high enantioselectivity. But just because you can get 90% ee from a given catalyst family doesn’t mean you should: it might be terrible for other substrates, or a different class of catalysts might be much easier to optimize or much more reactive. 
</p>

<p>
Understanding how many catalysts are out there to be discovered should make us think more carefully about which hits we pursue, since our time is too valuable to waste performing needless catalyst optimizations. In this study, we showed that screening only one substrate can be misleading when the goal is substrate generality, but one might prefer to screen for other factors: low catalyst loading, tolerance of air or water, or recyclability all come to mind. In all cases, including these considerations in initial screens means that the hits generated are more likely to be relevant to the final goal. Just looking for 90% ee is almost certainly not the best way to find a good reaction.
</p>

<h3>2. Don’t ignore analytical chemistry</h3>

<p>
Although assay development is a normal part of many scientific fields, many organic chemists seem to barely consider analytical chemistry in their research. Any ingenuity is applied to developing new catalysts, while the analytical method remains essentially a constant factor in the background. This is true even in cases where the analytical workflow represents a large fraction of the project (e.g. having to remove toluene before NMR for every screen).
</p>

<p>
This shouldn’t be the case! Spending time towards the beginning of a project to develop a nice assay is an investment that can yield big returns: this can be as simple as making a GC calibration curve to determine yield from crude reaction mixtures, or as complex as what we undertook here. Time is too valuable to waste running endless columns.
</p>

<p>
More broadly, it seems like analytical advances (e.g. NMR and HPLC) have had a much bigger impact on the field than any individual chemical discoveries. Following this trend forward in time would imply that we should be making bigger investments in new analytical technologies now, to increase scientist productivity in the future.
</p>

<h3>3. A little computer science can go a long way</h3>

<p>
A key part of this project (mentioned only briefly in the paper) was developing our own peak-fitting software that allowed us to reliably fit overlapped peaks. This was computationally quite simple and relied almost entirely on existing libraries (e.g. <i>scipy</i> and <i>lmfit</i>), but took a certain amount of comfort with signal processing / data science.<sup><a href="#fn2">2</a></sup> We later ended up moving our software pipeline out of unwieldy Jupyter notebooks and into a little Streamlit web app that Eugene wrote, which allowed us to quickly and easily get ee values from larger screens. 
</p>

<p>
Neither of these two advances required significant coding skill; rather, just being able to apply some computer science techniques to our chemistry problem unlocked new scientific opportunities and massive time savings (a la <a href="https://en.wikipedia.org/wiki/Pareto_principle#In_computing">Pareto principle</a>). Moving forward, I expect that programming will become a more and more central tool in scientific research, much like Excel is today. Being fluent in both chemistry and CS is currently a rare and valuable combination, and will only grow in importance in the coming decades.
</p>

<i>Thanks to Eugene Kwan for reading a draft of this post.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    I'd like to propose the following principle: any sufficiently clever analytical technique inevitably depends on mass spectrometry. If you don't believe me, just look at the field of proteomics...
  </li>
  <li id="fn2">
    I heavily recommend <a href="https://terpconnect.umd.edu/~toh/spectrum/"><i>A Pragmatic Introduction to Signal Processing</i></a> by Tom O'Haver.
  </li>
</ol>
]]></description>
              <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Rate-Determining Span</title>
              <link>public/blog/20220815_rate_determining_span.html</link>
              <description><![CDATA[
<p>
One common misconception in mechanistic organic chemistry is that reactions are accelerated by speeding up the rate-determining step. 
This mistaken belief can lead to an almost monomaniacal focus on determining the nature of the rate-determining step.
In fact, it's more correct to think of reactions in terms of the <u>rate-determining span</u>: the difference between the resting state and the highest-energy transition state.
(I thank Eugene Kwan's <a href=https://ekwan.github.io/pdfs/chem106/28%20-%20First-Order%20Kinetics.pdf>notes</a> for introducing me to this idea.)
</p>

<p>
In this post, I hope to demonstrate the veracity of this concept by showing that, under certain idealized assumptions, the existence of a low-energy intermediate has no effect on rate. Consider the following system:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_scheme.png" style="width:375px;" />
  <figcaption> Concerted and stepwise mechanisms. </figcaption>
</figure>

<p>
We can imagine plotting these two mechanisms on a potential energy surface:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_rds.png" style="width:450px;" />
  <figcaption> Concerted and stepwise mechanisms, on a PES. </figcaption>
</figure>

<p>
In this example, <b>X</b> = <b>Y</b> + <b>Z</b>; the energy of the transition state and ground state are the same in both cases, and only the presence (or absence) of an intermediate differentiates the two potential energy surfaces. We will now compute the rate of product formation in both cases.
Using the <a href="https://en.wikipedia.org/wiki/Eyring_equation">Eyring–Polyani equation</a>, it's straightforward to arrive at an overall rate for the concerted reaction as a function of the barrier:
</p>

<br>
<p>
k = k<sub>B</sub>T/h * exp(-<b>X</b>/RT)
</p>
<p>
rate<sub>concerted</sub> = k * [SM]
</p>
<p>
rate<sub>concerted</sub> = k<sub>B</sub>T/h * exp(-<b>X</b>/RT) * [SM]
</p>
<br>

<p>
The stepwise case is only slightly more complicated. Assuming that the barrier to formation of the intermediate is much lower than the barrier to formation of the product, and that the intermediate is substantially lower in energy than the rate-limiting transition state, we can apply the <a href="https://www-jmg.ch.cam.ac.uk/tools/magnus/kineticnotes.html">pre-equilibrium approximation</a>:
</p>

<br>
<p>
rate<sub>stepwise</sub> = k<sub>2</sub> * [INT] 
</p>
<p>
k<sub>2</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT)
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT) * [INT]
</p>
<br>

<p>
Solving for [INT] is straightforward, and we can plug the result in to get our final answer:
</p>

<br>
<p>
<b>Y</b> = -RT * ln([INT]/[SM])
</p>
<p>
[INT] = exp(-<b>Y</b>/RT)*[SM]
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>Z</b>/RT) * exp(-<b>Y</b>/RT) * [SM] 
</p>
<p>
rate<sub>stepwise</sub> = k<sub>B</sub>T/h * exp(-<b>X</b>/RT) * [SM] = rate<sub>concerted</sub>
</p>
<br>

<p>
As promised, the rates are the same—where the preequilibrium approximation holds, the existence of an intermediate has no impact on rate.
All that matters is the relative energy of the transition state and the ground state.
</p>

<p>
This method of thinking is particularly useful for rationalizing tricky Hammett trends. For instance, it's known that electron-rich indoles <a href="https://pubs.acs.org/doi/abs/10.1021/jacs.7b06811">react much faster</a> in Brønsted-acid-catalyzed Pictet–Spengler reactions, even though these reactions proceed through rate-determining elimination from a carbocation. Since electron-poor carbocations are more acidic, simple analysis of the rate-determining step predicts the opposite trend.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220815_ps.png" style="width:650px;" />
  <figcaption> Brønsted-acid-catalyzed Pictet–Spengler reactions. </figcaption>
</figure>

<p>
However, if we ignore the intermediate, it's clear that the transition state contains much more carbocationic character than the ground state, and so electron-donating groups will stabilize the transition state relative to the ground state and thereby accelerate the reaction. Thinking about intermediates is a great way to get confused; to understand trends in reactivity, all you need to consider is the transition state and the ground state.
</p>

]]></description>
              <pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Combating Computational Nihilism</title>
              <link>public/blog/20220810_viewpoints_on_simulation.html</link>
              <description><![CDATA[
<p>
The growing accessibility of computational chemistry has, unfortunately, led to a preponderance of papers with bad computations. Organic chemists are all too familiar with the “DFT section” of an otherwise high-quality publication which typically contains a transition-state structure or two, some sort of enigmatic cartoon purporting to explain the observed selectivity, and perhaps an uninterpretable NCIPLOT cited as evidence for the preceding claims.<sup><a href="#fn1">1</a></sup>
</p>

<p>
Faced with this sort of landscape, experimental chemists typically adopt one of two faulty heuristics: excessive credulity or universal skepticism. Being too trusting is dangerous, as evidenced by <a href="https://pubs.acs.org/doi/abs/10.1021/acs.organomet.8b00456">work</a> showcasing the manifold ways that simulations can deceive the unwary scientist. Almost anyone who’s made a catalyst predicted to be better by computations knows this well (even when the computations are your own).
</p>

<p>
However, equally dangerous—and, in my view, less appreciated—is the creeping ennui that diminishes the entire field. This is exemplified by statements like “I don’t believe computations can ever be predictive,” “You can make DFT say anything you want to,” or, more delicately, “Computations are more for generating hypotheses, not being physically correct.” Although most people may be too polite to admit this to their computational collaborators, this nihilism is pervasive—just listen to the conversations as students walk back from a departmental seminar.
</p>

<p>
This viewpoint is wrong. The existence of bad computational models does not mean that all models are bad, nor does it imply that the task of creating models is inherently futile. Examples from other scientific fields, like orbital mechanics and fluid dynamics, indicate that computations can achieve impressive degrees of accuracy and become pivotal and trustworthy components of the scientific process. Closer to home, even the most skeptical chemists would admit that for e.g. calculating IR frequencies in the ground state, DFT shows impressive predictive accuracy (modulo the usual systematic error). There’s no intrinsic reason why accurately modeling chemical systems, even prospectively, ought to be impossible; chemistry is not a social science.
</p>

<p>
Why, then, is this variety of skepticism so common? Part of the problem comes from the bewildering milieu of options available to practitioners in the field. While a seasoned expert can quickly assess the relative merits of BYLP/MIDI! and DSD-PBEP86/def2-TZVP, to the layperson it’s tough to guess which might be superior. Without transparent heuristics by which to judge the quality of computational results, it’s no surprise that zeroth-order approximations (“DFT good” or “DFT bad”) have become so common among non-experts.<sup><a href="#fn2">2</a></sup>
</p>

<p>
Another issue is the generally optimistic demeanor of computational chemists towards their field. While the temptation to emphasize the potential upside of one’s research area is understandable, overestimating the capabilities of state-of-the-art technology inevitably leads to a reckoning when the truth becomes obvious. Except in certain circumscribed cases, we are still far from any predictive models of reactivity or selectivity for typical solution-phase reactions, various purported “breakthroughs” notwithstanding. Based on questions I’ve heard in talks, this uncomfortable truth is not universally understood by experimental audiences.
</p>

<p>
What, then, are the practical conclusions for computational chemists? Firstly, we should not be afraid to be our field’s own harshest critics. Allowing low-quality work into the literature erodes trust in our field; although raising our standards may be difficult and unpopular in the short term (kinetics), in the long run it will benefit the field (thermodynamics). You never get a second chance at a first impression; every bad paper published causes good papers to get that much less attention.
</p>

<p>
Secondly, we should work to develop consistent standards and workflows by which one can obtain reliable computational results. Just like there are accepted means by which new compounds are characterized (<sup>1</sup>H NMR, <sup>13</sup>C NMR, HRMS, IR), there ought to be transparent methods by which transition states can reliably be found and studied. The manifold diversity of parameters employed today is a sign of the field’s immaturity—in truly mature fields, there’s an accepted right way to do things.<sup><a href="#fn3">3</a></sup> The growing popularity of tools like <a href=https://xtb-docs.readthedocs.io/en/latest/crest.html><i>crest</i></a> is an important step in this direction, as is the ability to to use high-level post-Hartree–Fock wavefunction methods like DLPNO-CCSD(T) to refine single-point energies.
</p>

<p>
Finally, we must be honest about the limitations of our techniques and our results. So much about the chemical world remains mysterious and far beyond our understanding, let alone our ability to reproduce <i>in silico</i>. Far from being a failure for the field, however, this is something to be acknowledged and celebrated; science is only possible when there remain secrets to be found.
</p>

<p>
Between the Scylla of gullible credulity and the Charybdis of defensive nihilism, we must chart a middle way.
</p>

<i>
Thanks to Hayden Sharma for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    <a href="https://pubs.acs.org/doi/10.1021/ct100641a">NCIPLOT</a> is a program which allows one to visualize non-covalent interactions; although the output can be useful, for large molecules it's also very overwhelming.
  </li>
  <li id="fn2">
    A related idea is "<a href="https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/">epistemic learned helplessness</a>", where people unable to evaluate the quality of a certain kind of argument resolve not to be persuaded by this argument one way or the other.
  </li>
  <li id="fn3">
    I want to thank Frank Neese and coworkers for <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">publishing</a> their entire transition-state-finding workflow in detail, which I've found very useful and is certainly a step in the right direction.
  </li>
</ol>
]]></description>
              <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Singleton Saturday: Dynamics at the Border Between General- and Specific-Acid Catalysis</title>
              <link>public/blog/20220805_singleton_saturday_decarboxylation.html</link>
              <description><![CDATA[
<p>
<i>
This is the first in what will hopefully become a series of blog posts focusing on the fascinating work of Dan Singleton (professor at Texas A&amp;M). My goal is to provide concise and accessible summaries of his work and highlight conclusions relevant to the mechanistic or computational chemist. 
</i>
</p>

<p>
A central theme in mechanistic chemistry is the question of concertedness: if two steps occur simultaneously (“concerted”) or one occurs before the other (“stepwise”). One common way to visualize these possibilities is to plot the reaction coordinate of each step on two axes to form a 2D More O’Ferrall–Jencks (<a href="https://en.wikipedia.org/wiki/More_O%27Ferrall%E2%80%93Jencks_plot">MOJ</a>) plot. On an MOJ plot, a perfectly concerted reaction looks like a straight line, since the two steps occur together, while a stepwise reaction follows the border of the plot, with an intermediate located at one of the corners:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_moj.png" style="width:450px;" alt="generic MOJ plot" />
  <figcaption> Generic More O'Ferrall–Jencks plot for general- and specific-acid catalysis (Figure 1 in the paper) </figcaption>
</figure>

<p>
In the context of acid catalysis, where a Brønsted acid activates a substrate towards further transformations, the concerted mechanism is known as “general-acid catalysis” and the stepwise mechanism is known as “specific-acid catalysis.” This case is particularly interesting because the timescales of heavy-atom motion and proton motion are somewhat different, as can be seen by comparing typical O–H and C–O IR stretching frequencies:
</p>
<p>
1/(3500 cm<sup>-1</sup> * 3e10 cm/s) = 9.5 fs for O–H bond vibration
</p>
<p>
1/(1200 cm<sup>-1</sup> * 3e10 cm/s) = 28 fs for C–O bond vibration
</p>

<p>
Since these timescales are so different, it’s impossible for the two steps to proceed perfectly synchronously, since the proton transfer will be done before heavy-atom motion is even half complete; in other words, the slope of the reaction’s path on the MOJ diagram can’t be 1. <i>Ceteris paribus</i>, then, one might expect stepwise specific-acid mechanisms to be favored. In some cases, however, the putative intermediate would be so unstable that its lifetime ought to be roughly zero (an <a href="https://pubs.acs.org/doi/10.1021/ar50150a001">enforced concerted mechanism</a>, to paraphrase Jencks). 
</p>

<p>
In <a href="https://pubs.acs.org/doi/full/10.1021/jacs.7b02148">this week's paper</a>, Aziz and Singleton investigate the mechanism of one such example, the decarboxylation of benzoylacetic acid, which in the stepwise limit proceeds through a bizarre-looking zwitterion:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_mech.png" style="width:450px;" alt="mechanistic possibilities" />
  <figcaption> Concerted vs. stepwise mechanisms for decarboxylation of benzoylacetic acid (taken from the paper)</figcaption>
</figure>

<p>
Distinguishing concerted and stepwise mechanisms is, in general, a very tough question. In rare cases an intermediate can actually be observed spectroscopically, but inability to observe the intermediate proves nothing: the intermediate could be 10 kcal/mol above the ground state (leading to a vanishingly low concentration) or could persist only briefly before undergoing subsequent reactions. Accordingly, other techniques must be used to study the mechanisms of these reactions. 
</p>

<p>
In this case, the authors measured the <sup>12</sup>C/<sup>13</sup>C kinetic isotope effects using their group’s 
<a href="https://pubs.acs.org/doi/10.1021/ja00141a030">natural abundance method</a>. Heavy-atom kinetic isotope effects are one of the best ways to study these sorts of mechanistic questions because isotopic perturbation is at once <a href="https://pubs.acs.org/doi/full/10.1021/jacs.1c07351">extremely informative</a> and very gentle, causing minimal perturbation to the potential energy surface (unlike e.g. a Hammett study). The KIEs they found are shown below:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_kies.png" style="width:450px;"/>
  <figcaption> Kinetic isotope effects for decarboxylation of benzoylacetic acid (Figure 2 from the paper)</figcaption>
</figure>

<p>
These KIEs match the computed structure shown below nicely, which shows that proton transfer precedes C–C bond breaking: 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_ts.png" style="width:400px;"/>
  <figcaption> Transition state for decarboxylation of benzoylacetic acid (M06-2X/6-311+G**) (taken from the paper)</figcaption>
</figure>

<p>
To probe the stepwise/concerted nature of this reaction, the authors conducted quasiclassical <i>ab initio</i> molecular dynamics, propagating trajectories forwards and backwards from the transition state. Surprisingly, the dynamics show that proton transfer is complete before C–C bond scission occurs, forming an intermediate (<b>6</b>) which persists for, on average, 3.4 O–H bond vibrations despite not being a minimum on the PES. This reaction therefore inhabits the border between general- and specific-acid catalysis—proton transfer does occur before decarboxylation, but the intermediate species (in the nomenclature of <a href="https://www.pnas.org/doi/full/10.1073/pnas.1209316109">Houk and Doubleday</a>, a “dynamic intermediate”) is incredibly ephemeral.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220805_trajs.png" style="width:450px;"/>
  <figcaption> Results of quasiclassical trajectories showing the existence of a dynamic intermediate (<b>6</b>) before C–C bond breakage occurs (Figure 4a from the paper)</figcaption>
</figure>

<p>
This surprising scenario occurs because of the different timescales of the two elementary mechanistic steps, as discussed above. In the words of the authors:
</p>

<blockquote>
It is well understood in chemistry that concerted multibond reactions often involve highly asynchronous bonding changes. However, the normal understanding of asynchronous concerted reactions is that the bonding changes overlap. If otherwise, why should the reaction be concerted at all? This view fails to take into account the differing physics of the heavy-atom versus proton motions. <b>Because of the uneven contribution of the motions, their separation is arguably intrinsic and unavoidable whenever the reaction is highly asynchronous.</b> <i>(emphasis added)</i>
</blockquote>

<p>
Aziz and Singleton also observe a curious phenomenon in the quasiclassical trajectories, wherein some trajectories initiated backwards from the (late) transition state fully form the C–C bond before reverting to enol + CO<sub>2</sub>. This phenomenon, termed “deep recrossing,” occurs because the oxygen of the carboxylate is unable to receive the proton, stalling the reaction in the region of the unstable zwitterion; unable to progress forward, the species simply extrudes CO<sub>2</sub> and reverts back to the enol. Thus, even though the O–H bond is formed after the C–C bond (in the reverse direction) and little O–H movement occurs in the TS, inability to form the O–H bond prevents productive reaction, just like one might expect for a concerted TS.
</p>

<p>
The picture that emerges, then, is a reaction which “wants” to be concerted, owing to the absence of a stable intermediate along the reaction coordinate, but ends up proceeding through a stepwise mechanism because of the speed of proton transfer. Importantly, the dynamic intermediate “undergoes a series of relevant bond vibrations, as would any intermediate, and it can proceed from this structure in either forward or backward directions”: it is, in meaningful ways, an intermediate.
</p>

<p>
Given the ubiquity of proton transfer in organic chemistry, it is likely that many more reactions proceed through this sort of rapidly stepwise mechanism than is commonly appreciated. One case which I find particularly intriguing is “E<sub>2</sub>” reactions, which typically feature proton transfer to a strong base (e.g. potassium <i>tert</i>-butoxide) at the same time as C–Br or C–I bond dissociation. How do these reactions actually proceed on the femtosecond timescale? Is it possible that, <a href="https://pubs.acs.org/doi/pdf/10.1021/ar50059a003">as Bordwell proposed</a>, many E<sub>2</sub> reactions are actually stepwise? So much remains to be learned.
</p>

]]></description>
              <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Consulting as Private-Sector Graduate School</title>
              <link>public/blog/20220728_consulting_as_grad_school.html</link>
              <description><![CDATA[<p>
  As an undergraduate student in the sciences at MIT, contempt for management consulting was commonplace. 
  Consulting was the path for people who had ambition devoid of any real interests, 
  the “sellout road” where you made endless Powerpoints instead of providing any tangible improvement to the world. 
  In contrast, going to graduate school was a choice that showed commitment and integrity. 
  If you were willing to sacrifice your 20s in service to a scientific discipline, that showed true passion and an honorable commitment to the field. 
</p>

<p>
  I’m now midway through my PhD, and I’ve come to the conclusion that my previous impressions were mistaken and that consulting and graduate school are in fact more alike than they seem. This change has been spurred by making new friends from the world of management consulting and realizing not only that they enjoyed and benefited greatly from their experience, but also that their experience seemed broadly similar to mine. 
</p>

<p>
  This essay is an attempt to outline some similarities and differences between consulting and graduate school, speculate about why these differences exist, and finally determine if graduate schools can learn anything from the consulting model.
  My tentative conclusion is that, at the margin, research groups would benefit from acting more like consultants and directly solving industry-relevant problems for pay.
</p>

<p>
  <i>
    Epistemic status: moderate to low. 
    These thoughts are based mainly on my own experience as a chemistry PhD student, and likely do not translate to the humanities or social sciences. 
    I also have only a secondhand knowledge of management consulting and so probably suffer from myriad misconceptions.
  </i>
</p>

<h2>
  Similarities and Differences
</h2>

<p>
  At a superficial level, management consulting and graduate school both fill the same role: 
  a safe and prestigious opportunity for a new graduate to diversify his or her skills and accrue <a href="https://80000hours.org/articles/career-capital/">“career capital.”</a> 
  In consulting, much like in graduate school, learning is key: in the words of
  <a href="https://web.archive.org/web/20220720040016/https://www.theatlantic.com/politics/archive/2019/12/pete-buttigieg-mckinsey/603421/">Pete Buttigieg</a>, McKinsey was 
  “a place where I could learn as much as I could by working on interesting problems and challenges.” 
  Both occupations can segue smoothly into a variety of opportunities afterwards, in part due to the shared emphasis on connections, networking, and presentation skills,
  and as a result both professions attract a steady stream of bright, highly motivated people. 
  Easy access to human capital seems to be a shared prerequisite: 
  without a supply of new graduates willing to work long hours in a high-stress environment, neither BCG nor Harvard could survive.
</p>

<p>
  Given the plethora of interesting, well-paid opportunities available for high-achieving graduates, the popularity of these more grueling professions might be surprising. 
  In her essay <a href="https://palladiummag.com/2020/07/27/harvard-creates-managers-instead-of-elites/">“Harvard Creates Managers Instead of Elites,”</a> 
  Saffron Huang describes the thought process behind why so many of her Harvard classmates took “the decreasing returns of another NGO internship or McKinsey job” 
  over more inventive careers, concluding that “school-validated options” appeal to students who are “naïve and uncertain about [their] own futures.” 
  In other words, the safety of taking a job well-known to be prestigious is what makes consulting and similar options so appealing. 
</p>

<p>
  Although Huang doesn’t mention graduate school specifically, I would argue that staying within academia is the most “school-validated” of all choices. 
  Getting a PhD not only gives one a defensible claim to domain expertise and a chance at a higher-status job but also allows students to stay within the familiar academic system for longer. 
  Faced with all the manifold diversity of the private sector, the chance for a graduate to stay within the familiar confines of the university for a few more years 
  is a safe and socially acceptable way to delay one’s arrival into corporate America. 
  And the high status that professors have in the eyes of undergraduates only strengthens the appeal of graduate school: 
  if all one’s academic role models went down this path, surely it can’t be a bad choice.
</p>

<p>
  From the perspective of the student, one obvious difference is the pay: a typical starting consulting salary is 
  <a href="https://managementconsulted.com/consultant-salary/">$100k</a>, 
  while my Harvard graduate student stipend is currently <a href="https://chemistry.harvard.edu/financial-support">$43k</a>. 
  Given that a first-year consultant and a first-year graduate student have essentially the same skills (i.e. what you’d expect from an undergraduate education and not much more), 
  this difference is surprising. Based on anecdotal reports from consulting, my intuition is that this difference is not limited to salaries: 
  the consulting world is flush with cash, while the academic world often runs on the verge of bankrupcy.<sup><a href="#fn1">1</a></sup>
</p>

<p>
  (I’m intentionally avoiding questions around the ethics of consulting because I think it’s not particularly relevant to this piece, 
  and because I don’t think I have any unique insights on this topic.<sup><a href="#fn2">2</a></sup>)
</p>

<h2>
  Academia’s Unique Niche
</h2>

<p>
  Why does consulting have so much more money than academia? 
  One simple model of academia is as follows: discoveries that provide “present value” can easily be funded by companies, because there’s a quick return-on-investment. 
  On the other hand, discoveries that provide “future value” are hard to fund through the private sector, because there’s no guarantee that the real-world value will be captured by the funder. 
  Accordingly, the government sponsors research into interesting problems with uncertain timeframes to do what the free market cannot.<sup><a href="#fn3">3</a></sup>
  This comports with what Vannevar Bush wrote in his landmark 1945 work <a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm#ch1.1"><i>Science, The Endless Frontier</i></a>:
</p>

<blockquote>
New impetus must be given to research in our country. Such impetus can come promptly only from the Government…. Further, we cannot expect industry adequately to fill the gap. Industry will fully rise to the challenge of applying new knowledge to new products. The commercial incentive can be relied upon for that. But basic research is essentially noncommercial in nature. It will not receive the attention it requires if left to industry.
</blockquote>

<p>
  Viewed within this model, we might hypothesize that consulting is lucrative because it’s easier to finance providing present value than providing future value 
  (or because the free market is more efficient than the NIH/NSF). 
  But this picture is oversimplified. Much current chemistry research at least ostensibly addresses present problems in the chemical industry, 
  and research groups frequently collaborate with (and receive money from) chemical companies. Why, then, is consulting better at capturing returns on present value than academia?
</p>

<p>
  Structural factors disincentivize academic labs from acting as consultants.<sup><a href="#fn4">4</a></sup> 
  Harvard’s stated <a href="https://osp.finance.harvard.edu/consulting-or-related-service-agreements">policy</a> on academic–industrial collaborations involving specific deliverables is that they are discouraged, 
  allowed “only if the activity in question advances a core academic mission of the faculty member’s school and either provides a significant institutional benefit or a public benefit that is consistent with the University’s mission and charitable status.” 
  This matches my experience collaborating with Merck, a pharmaceutical company; it was clear that we were not accepting money for rendering Merck a service, but instead simply working together because our intellectual interests aligned. Although we did receive some money, it was a fraction of what our total costs in salary, materials, etc were for the project.
</p>

<p>
Policies like this prevent companies from hiring research labs on a purely transactional basis, forcing academics to decouple their incentives from those of industry. Even if an academic lab is running out of money, it must find some way to justify its collaborations beyond pure economic necessity: research groups cannot simply remake their interests to suit whichever employer they want to attract. Viewed within the above model, this is good! Academia is supposed to focus on problems that can’t be solved by industry, not act as a contractor in service of corporate profits. 
</p>

<p>
Yet the preponderance of academic–industrial collaborations suggests that academia’s ostensible focus on long-term projects is not as strong as it could be. In a world where funding for basic research seems to be <a href="https://www.science.org/content/article/data-check-us-government-share-basic-research-funding-falls-below-50">declining</a> on a per-lab basis, it is perhaps unsurprising that professors turn to alternate sources of funding to keep their labs afloat; moving forward, we can expect this trend only to intensify. 
</p>

<p>
Perhaps the biggest omission from the above discussion is another key role of academia: training students. Graduate school, after all, seeks not only to advance the frontiers of human knowledge but also to train students in this pursuit. But from the perspective of the typical graduate student, it strikes me as unlikely that the specific nature of the problems under study (i.e. purely academic versus industrially relevant) has a massive impact on the student’s learning. Indeed, many students might be better prepared for their careers by having more encounters with industrial problems and techniques. The existence of current <a href="https://www.science.org/content/article/industrial-postdocs-road-less-traveled">industrial postdoctoral positions</a> suggests that gaining scientific experience through industry-relevant problems can be a successful strategy.
</p>

<h2>
  Conclusions
</h2>

<p>
Although the idealized model of the university—a place dedicated to advancing long-term human flourishing through the pursuit of knowledge 
<a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm#ch3.3">“without thought of practical ends”</a>—is indeed utopian, the present problems with academic funding suggest that a more pragmatic outlook may be needed in the short term. In particular, finding new ways to efficiently fund scientific research and education is a pressing challenge for the field (absent major changes to the funding ecosystem) which remains, from my point of view, unsolved.
</p>

<p>
Accordingly, the consulting model presents an interesting alternative to the current system. Consulting firms sustain themselves solely by providing solutions to current problems in industry, training their “students” without any need for external subsidies. Is it possible for research groups to support part-time basic research by consulting the rest of the time? At the margin, should graduate schools be more like consulting firms? This approach would require reducing the stigma around research groups acting as contractors, and in so doing perhaps run the risk of lessening the prestige of the university. On the other hand, directly applying university knowledge to solving practical problems might raise public appreciation for science. 
</p>

<p>
We may see the results of this experiment sooner rather than later. As acquiring scientific funding continues to grow more difficult, I expect that smaller, more poorly funded departments will begin to pursue money from industry more aggressively to keep themselves afloat, moving more and more towards the consulting model out of necessity. Time will tell whether this proves to be an alternate, or even superior, model for funding research, or a negative development that undermines what makes universities distinctive.  
</p>

<p>
If forced to guess, my tentative prediction would be that these changes will be good. The present funding model seems wasteful and unsustainable, a relic of 
<a href="https://newscience.org/nih/#the-boom-decade">massive growth </a> in federal science funding over the past 100 years. A correction is coming, and it will be brutal when it does. Finding new ways to fund research beyond just federal grants, then, is important for the future of research in the US; it’s been done before, and it can be done again. In fact, some of the greatest scientific discoveries have originated not from universities but from <a href="https://en.wikipedia.org/wiki/Bell_Labs">the corporate sphere!</a> Disrupting our institutions of science will be painful, but I think the potential upside is high—that is, if academic researchers can accept corporate money while still preserving some ability to pursue basic science.
</p>

<p>
Another conclusion from this area of thinking is that federally funded scientists ought, as much as possible, to focus on their area of comparative advantage—long-term research with uncertain payoffs, “essentially noncommercial” in nature. At least in organic chemistry, most funding applications that I’ve seen are very careful to point out how their discoveries could lead to immediate deliverables with practical impact.<sup><a href="fn5">5</a></sup> If these claims are really true, then these discoveries should be funded by the private sector, not by federal money. These assertions may be part of what a competitive grant application today requires, but their existence seem to point to a fundamental disconnect between what academic research is and what it should be. 
</p>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    It's tough to find sources on this, but anecdotally even at top universities most research labs seem strapped for cash. For structural discussions, see 
    <a href="https://www.npr.org/2014/09/10/347305805/built-in-better-times-university-labs-now-lack-research-funding">this NPR article</a> and 
    <a href="https://newscience.org/nih/#the-boom-decade">this New Science report</a>.
  </li>
  <li id="fn2">
    This issue has been discussed a lot: one particularly influential piece in this area is Daniel Markovits's 
    <a href="https://web.archive.org/web/20220604022838/https://www.theatlantic.com/ideas/archive/2020/02/how-mckinsey-destroyed-middle-class/605878/">“How McKinsey Destroyed The Middle Class”</a>.
  </li>
  <li id="fn3">
    This is equivalent to saying that scientific progress is a public good. There are more ways that things could be public goods than just long timeframes, but without loss of generality we’ll elide these considerations here.
  </li>
  <li id="fn4">
    Many professors do serve as consultants for industry, but they generally do this apart from the university, without involving their students, and the money goes to them personally, not to their research groups.
  </li>
  <li id="fn5">
    The <a href="https://newscience.org/nih/#aversion-to-high-risk-research">New Science NIH report</a> discusses this phenomenon at length: over the past 20 years, the NIH has changed its standards, such that now new grants are expected to have “clear research goals with obvious practical applications.”
  </li>
</ol>
]]></description>
              <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Timescales</title>
              <link>public/blog/20220719_timescales.html</link>
              <description><![CDATA[<p>
  For many organic chemists, it’s hard to grasp the vast difference between various “fast” units of time. 
  For instance, if a reactive intermediate has a lifetime of microseconds, does that mean it can escape the solvent shell and react with a substrate? 
  What about a lifetime of nanoseconds, picoseconds, or femtoseconds?
</p>

<p>
  To help answer these questions for myself, I made the following graphic about a year ago, which compares the timescale of various molecular processes on a logarithmic axis.
  Although someone skilled in Adobe Illustrator could doubtless make a prettier version, 
  I've still found this to be a useful reference, and frequently use it as a slide in talks or group meetings:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220719_timescales.png" style="width:650px;" alt="illustration of chemical timescales" />
  <figcaption> Illustration of timescales of chemical processes—open in new tab for higher resolution</figcaption>
</figure>

<p>
  Based on this graphic, it becomes easier to think about the interplay between competing fast processes. 
  Species that persist for less than ~5 ps, for instance, are effectively “frozen” in the solvent configuration they’re formed in, 
  whereas molecules that persist for longer can sample different solvent configurations. 
  If a species can persist for 10-100 ps, it can begin to sample the nearby conformational landscape through low-barrier processes (e.g. single-bond rotation), 
  although larger conformational changes might still be inaccessible.
</p>

<p>
  As lifetimes stretch into the nanosecond regime, diffusion and solvent-cage escape become more realistic possibilities: 
  based on <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.200503273>Mayr’s value</a> for the “diffusion limit” (2–4e9 M<sup>-1</sup>s<sup>-1</sup>), 
  we can estimate that bimolecular association with a 1.0 M reactant will take 200-400 ps, while association with a 100 mM reactant will take 2-4 ns. 
  On the far right of the graph, being able to distinguish different species by NMR (e.g. amide methyl groups in DMF) means that these species have a very long lifetime indeed.
</p>

<p>
  Framed in these terms, then, it becomes obvious why the 10-100 ps timescales currently accessible by <i>ab initio</i> molecular dynamics (AIMD) are unable to model many important 
  molecular processes.  Indeed, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00833>work from Grossman and coworkers</a> 
  has shown that the results of AIMD simulations can be very dependent on the pre-AIMD equilibration method used, 
  since the actual solvent environment is unable to fully relax over the timescale of the simulation. 
  For AIMD to become a truly useful alternative to forcefield molecular dynamics, much faster <i>ab initio</i> methods will be needed!
<p>

<h3>
  Sources:
</h3>
<ul>
  <li>
    intramolecular vibrational redistribution: <a href=http://www.cchem.berkeley.edu/millergrp/pdf/237.pdf>ref</a>
  </li>
  <li>
    hydrogen bonding: <a href=https://web.stanford.edu/group/fayer/articles/351.pdf>ref</a>
  </li>
  <li>
    NMR: <a href= http://www.satyensaha.com/pdf%20files/literature/SS-NMR-timescale.pdf>ref</a>
  </li>
  <li>
    bond rotation: <a href=https://pubs.rsc.org/en/content/articlelanding/2015/an/c5an00558b>ref</a>, 
    <a href=https://web.stanford.edu/group/fayer/articles/350.pdf>ref</a>
  </li>
  <li>
    carbocation lifetimes: <a href=https://catalogimages.wiley.com/images/db/pdf/0471233242.01.pdf>ref</a>, 
    <a href=https://www.sciencedirect.com/science/article/abs/pii/S0301010408000050?via%3Dihub>ref</a>
  </li>
  <li>
    excited state lifetimes: <a href=https://pubs.acs.org/doi/10.1021/acs.chemrev.6b00057>ref</a>, 
    <a href=https://pubs.rsc.org/en/content/articlelanding/2019/cc/c9cc01047e>ref</a>, 
    <a href=https://pubs.acs.org/doi/10.1021/cr300503r>ref</a>, 
    <a href=https://pubs.rsc.org/en/content/articlelanding/2016/cp/c6cp01635a>ref</a>
  </li>
</ul>

<i>Thanks to Richard Liu and Eugene Kwan for feedback on this figure.</i>

<h3>Corrections (updated 7/25/2022):</h3>
  <ul>
    <li>
      The lifetime given for benzophenone is for the singlet excited state, not the triplet excited state 
      (which has a much longer lifetime, explaining why benzophenone can be used as a triplet sensitizer).
      Thanks to <a href="https://twitter.com/DanielEFalvey/status/1549868458673901568">Daniel Falvey</a> for pointing this out.
    </li>
    <li>
      The illustration of "NMR timescale" refers only to the timescale in which different peaks can be distinguished in a 1D NMR experiment.
      More sophisticated NMR experiments can probe much faster processes, such as molecular tumbling, so the use of the phrase "NMR timescale" is misleading.
      Thanks to <a href="https://twitter.com/DominikJKubicki/status/1549809467247689728">Dominik Kubicki</a> for pointing this out.
    </li>
  </ul>
]]></description>
              <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>The Apotheosis of 2D Infrared Spectroscopy</title>
              <link>public/blog/20220708_apotheosis_of_2d_ir.html</link>
              <description><![CDATA[
<p>
  While IR spectroscopy is still taught in introductory organic chemistry classes, it has been almost completely replaced by NMR spectroscopy and mass spectrometry for 
  routine structural assignments. Still, IR spectroscopy offers unique advantages to the mechanistic chemist: the short timescale of IR allows for the 
  observation of transient molecular interactions even when slower techniques like NMR only yield time-averaged data, 
  and IR absorbances can easily be perturbed by isotopic substitution while leaving the underlying potential-energy surface unchanged. 
</p>

<p>
  These advantages are nicely illustrated in the IR spectrum of a mixture of phenol, benzene, and CCl<sub>4</sub>, 
  which shows two peaks corresponding to free phenol (2665 cm<sup>-1</sup>) and phenol complexed to benzene (2631 cm<sup>-1</sup>). 
  (The phenolic proton was replaced by deuterium, moving the O–D stretch away from C–H stretches and into a vacant region of the spectrum.) 
  From the standpoint of assessing purity, it might be upsetting that a pure compound shows two peaks; 
  from the standpoint of a mechanistic chemist, the ability to distinguish two different solvation environments experimentally is incredible.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_1d_ir.png" style="width:400px;" alt="IR spectrum of phenol in benzene/CCl4" />
  <figcaption> IR spectrum of phenol in benzene/CCl<sub>4</sub> (Fayer Figure 2). </figcaption>
</figure>

<p>
  Unfortunately, measuring this spectrum tells us about the thermodynamics of the equilibrium, but not the kinetics; 
  there’s not a good way to determine how fast these two species are exchanging from these data.<sup><a href="#fn1">1</a></sup> 
  In 2005, <a href=https://www.science.org/doi/10.1126/science.1116213>Fayer and coworkers</a> developed a pump–probe infrared spectroscopy method called “2D IR” to tackle this problem. 
  In 2D IR, the system is excited, allowed to evolve for a variable length of time <i>T<sub>w</sub></i>, 
  and then triggered to emit a vibrational “echo” (in analogy to spin-echo NMR experiments) 
  which still contains phase information from the original excitation. 
  (There are a lot of <a href=https://pubs.acs.org/doi/pdf/10.1021/ar068010d>non-trivial spectroscopic details</a> here which I don’t really understand.) 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_2d_ir.png" style="width:320px;" alt="2D IR spectrum of phenol in benzene/CCl4" />
  <figcaption> 2D IR spectrum of phenol in benzene/CCl<sub>4</sub> (Fayer Figure 3). </figcaption>
</figure>

<p>
  The net result of this is a two-dimensional plot showing initial and final frequencies, in which cross-peaks represent molecules which have moved between one state and another during 
  <i>T<sub>w</sub></i>. 
  By surveying a range of <i>T<sub>w</sub></i> values, the kinetics of exchange can be quantitatively determined: in this case, the time constant τ was found to be 8 ± 2 ps. 
  This result might not seem thrilling (“fast exchange is fast”), but this experiment can be used to measure rates of phenol dissociation from electronically-varied aromatic rings, 
  or <a href=https://web.stanford.edu/group/fayer/articles/351.pdf>compared to results from molecular dynamics simulations</a> for benchmarking purposes. 
</p>

<p>
  While many groups are now using 2D IR, <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.2c00154>this recent paper</a> 
  from Tokmakoff and coworkers studying superconcentrated electrolytes stood out to me as particularly exceptional. 
  In superconcentrated solutions like those found in batteries (e.g. 15 M LiTFSI in acetonitrile), the extreme salt concentration leads to high viscosity and substantial aggregation, 
  leading to questions about how charge transport in batteries occurs. 
  Some simulations seem to suggest that, rather than “vehicular diffusion” wherein a cation diffuses along with its solvent shell, 
  charge transport occurs through “structural diffusion” involving breaking/reforming of cation–solvent interactions. 
  (This is analogous to the Grotthuss mechanism of proton transport in water.)
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_transport_mechs.png" style="width:450px;" alt="different transport mechanisms" />
  <figcaption> Illustration of different transport mechanisms (Tokmakoff Figure 8). </figcaption>
</figure>

<p>
  Since distinct C≡N stretches are visible for cation-bound and free acetonitrile, it might seem straightforward to simply measure time evolution of the cross-peaks 
  and thereby determine the rate of solvent exchange. 
  Unfortunately studying exchange in the bulk solvent is complicated by the fact that direct vibrational energy transfer can occur through collisions, 
  meaning that cross-peaks are observed even in the absence of exchange. 
  The authors solve this problem by using a mixture of D<sub>3</sub>CCN and D<sub>3</sub>C<sup>13</sup>CN: 
  while cross-peaks between the heavy and light isotopologues can only occur through energy transfer, 
  cross-peaks between the same isotopologue can also occur through chemical exchange.<sup><a href="#fn2">2</a></sup> 
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220708_2d_ir_tokmakoff.png" style="width:600px;" alt="2D IR of electrolyte" />
  <figcaption> 2D IR measurement of 1.9 M LiTFSI in acetonitrile (Tokmakoff Figure 5). </figcaption>
</figure>

<p>
  They find that the time evolution of all cross-peaks is identical under all conditions, 
  indicating that solvent exchange must be slower than energy transfer (~20 ps) for any cation or concentration studied. 
  This suggests that, contrary to a variety of theoretical studies, structural-diffusion mechanisms for cation transport are quite slow and unlikely to be relevant for these electrolytes. 
</p>

<p>
  This study is a beautiful example of designing a cutting-edge spectroscopic experiment to solve a key scientific problem, 
  and reminds me how much we still don’t know about “simple” systems like ionic solutions. 
  I would love to see techniques like this applied to study reactive intermediates in the ground state, e.g. Olah-style superacid solutions! 
  More broadly, it’s exciting to see how 2D IR can advance in less than two decades from being limited to simple model systems to 
  now being used to tackle the biggest open questions in chemistry. What new techniques being developed today will rise to prominence in the coming decades?
</p>

<i>Thanks to Joe Gair for reading a draft of this.</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    For a different way to extract rates from vibrational spectra, see <a href=https://pubs.acs.org/doi/abs/10.1021/ja00882a048>this work</a> on line-broadening analysis.
  </li>
  <li id="fn2">
    Deuterated acetonitrile is used owing to <a href=https://pubs.acs.org/doi/10.1021/acs.jpcb.1c09572>complications with Fermi resonances</a> in protio-acetonitrile.
  </li>
</ol>
]]></description>
              <pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Four Handy Bash One-Liners</title>
              <link>public/blog/20220706_bash_oneliners.html</link>
              <description><![CDATA[
<p>
  In my experience, most computational chemists only know a handful of basic Bash commands, which is a shame because Bash is incredibly powerful.
  Although I'm far from an expert, here are a few commands I frequently find myself using:
</p>

<h3> 1. <i>sed</i> For Find-and-Replace.</h3>
<p class=code-block>
  $ sed -i “s/b3lyp/m062x/” *.gjf
</p>
<p>
  If you want to resubmit a bunch of transition states at a different level of theory, don't use a complex package like <a href=https://github.com/ekwan/cctk>cctk</a>! 
  You can easily find and replace text using <a href=https://www.gnu.org/software/sed/manual/sed.html>sed</a>, which runs almost instantly even for hundreds of files. 
  (Note that the syntax for modifying in-place is <a href=https://stackoverflow.com/questions/4247068/sed-command-with-i-option-failing-on-mac-but-works-on-linux>slightly different</a> on macOS.)
</p>

<h3> 2. Renaming Lots of Files </h3>
<p class=code-block>
  $ for f in *.gjf; do mv $f ${f/.gjf/_resubmit.gjf}; done
</p>
<p>
  Unfortunately, you can't rename lots of files with a single command in Bash, but using a <span class=code>for; do; done</span> loop is almost as easy.
  Here, we simply use <a href=https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html>parameter expansion</a> to replace the end of the filename, 
  but the possibilities are endless.
</p>

<h3> 3. Counting Occurrences in a File</h3>
<p class=code-block>
  $ for f in */*.out; do echo $f; grep "SCF Done" $f | wc -l; done
</p>
<p>
  Here we again use a <span class=code>for</span> loop, but in this case we use <i>grep</i> to search for the string "SCF Done". 
  We then pipe the output of this search to the <i>wc -l</i> command, which counts the number of lines. 
  Since <i>grep</i> returns each result on a new line, this prints the number of optimization steps completed.
</p>

<h3> 4. Cancelling Jobs By Matching Name </h3>
<p class=code-block>
  $ squeue -u cwagen | grep "g16_ts_scan" | awk '{print $1}' | xargs -n 1 scancel
</p>
<p>
  Although the <i>slurm</i> workload manager allows one to cancel jobs by partition or by user, to my knowledge there isn't a way to cancel jobs that match a certain name.
  This is a problem if, for instance, you're working on two projects at once and want to resubmit only one set of jobs.
  Here, we use <i>squeue</i> to get a list of job names, search for the names that match, extract the job number using <a href=https://www.gnu.org/software/gawk/manual/gawk.html>awk</a>,
  and finally cancel each job by building the <i>scancel</i> commands with <a href=https://www.man7.org/linux/man-pages/man1/xargs.1.html>xargs</a>.
  (This should be easily modifiable for other workload managers.)
</p>
]]></description>
              <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Plenty of Room at the Bottom</title>
              <link>public/blog/20220628_plenty_of_room_at_the_bottom.html</link>
              <description><![CDATA[
<p>
  <a href=https://edoc.ub.uni-muenchen.de/15211/1/Sailer_Christian.pdf>This thesis</a>, from Christian Sailer at Ludwig Maximilian University in Munich, 
  is one of the most exciting studies I’ve read this year. 
</p>

<p>
  Sailer and coworkers are able to generate benzhydryl carbocations from photolysis of the corresponding phosphonium salts, and can monitor their 
  formation and lifetime via femtosecond transient absorption spectroscopy. 
  (There are some technical challenges which I’ll omit here.) 
  They then use this platform to study the addition of alcohols to these cations and obtain nice kinetic data on some shockingly fast reactions:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220628_electrophile.png" style="width:400px;" alt="rates of reactions with electrophiles" />
  <figcaption> Rates of reaction between methanol and various cations (page 77). </figcaption>
</figure>

<p>
  Although these species are extremely reactive, substituent effects are still paramount: 
  adding two methyl groups to stabilize the benzhydryl carbocation extends its lifetime by 6.3x, whereas adding two electron-withdrawing fluorines shortens its lifetime by 5.6x. 
  The most electrophilic species studied, (dfp)<sub>2</sub>CH<sup>+</sup>, reacts with methanol in only 2.6 ps! 
  These findings demonstrate how even an extremely reactive carbocation like Ph<sub>2</sub>CH<sup>+</sup> doesn’t react at the rate of diffusion with nucleophiles; 
  although this reaction is almost instant on an absolute scale, there are still tens or hundreds of unproductive alcohol–carbocation interactions before product is finally formed.
</p>

<p>
  In contrast to Sailer’s results on the electrophile, which align nicely with results from more conventional kinetic measurements, different alcohols behave very differently:
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220628_nucleophile.png" style="width:350px;" alt="rates of reactions with nucleophiles" />
  <figcaption> Rates of reaction between mfp(Ph)CH<sup>+</sup> and various alcohols (page 75). </figcaption>
</figure>

<p>
  This rate difference is surprising, since Mayr’s measurements demonstrate that there’s no conventional difference in nucleophilicity between these species—and intuitively, 
  one wouldn’t expect adding additional carbons to substantially hinder approach to the oxygen. 
  However, as Sailer notes, larger molecules rotate and reorient much more slowly than smaller molecules. 
  This is evident from a number of different physical properties: larger alcohols form more viscous liquids and have slower dielectric relaxation times 
  (how long it takes them to reorient in response to a new charge).
</p>

<table>
  <tr>
    <th> alcohol </th> 
    <th> viscosity (<a href=http://murov.info/orgsolvents.htm>ref</a>) </th>
    <th> dielectric relaxation time (<a href=https://pubs.acs.org/doi/pdf/10.1021/j100048a004>ref</a>) </th>
    <th> reaction time (above) </th>
  </tr>
  <tr>
    <td> methanol </td>
    <td> 0.54 cP </td>
    <td> 5.0 ps </td>
    <td> 22 ps </td>
  </tr>
  <tr>
    <td> ethanol </td>
    <td> 1.08 cP </td>
    <td> 16 ps </td>
    <td> 41 ps </td>
  </tr>
  <tr>
    <td> <i>n</i>-propanol </td>
    <td> 1.95 cP </td>
    <td> 26 ps </td>
    <td> 62 ps </td>
  </tr>
  <tr>
    <td> <i>iso</i>-propanol </td>
    <td> 207 cP </td>
    <td> </td>
    <td> 106 ps</td>
  </tr>
  </tr>
</table>

<p>
  Sailer thus concludes that, as the timescale of reaction approaches the timescale of molecular motion, nucleophile reorientation becomes rate-limiting. 
  This is a really cool conclusion, and highlights how the environment that reacting molecules “see” differs from our macroscopic chemical intuition: 
  although we think of reorientation as barrierless, for very fast reactions reorientation is actually slower than bond formation.
  (Philosophically related, in my mind, is Dan Singleton’s <a href=https://pubs.acs.org/doi/10.1021/jacs.0c06295>work on non-instantaneous solvent relaxation</a>, 
  and how this influences the stability of reactive intermediates.)
</p>

<p>
  Thus, this work simultaneously highlights both similarities and differences between ultrafast reactions and their benchtop congeners. 
  My hope is that future studies can find ways to move beyond just benzhydryl cations and study elementary questions of selectivity with different nucleophiles (e.g. addition/elimination). 
  The ability to observe what is typically the “invisible” step in S<sub>N</sub>1 is incredibly powerful, and I think we’ve barely scratched the surface with what we can learn from measurements like these.
</p>

]]></description>
              <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
          </item>
          <item>
              <title>Site-Selective Glycosylation: Reflections</title>
              <link>public/blog/20220620_glycosylation.html</link>
              <description><![CDATA[
<p>
  Now that our lab’s site-selective glycosylation has been <a href="https://www.nature.com/articles/s41586-022-04958-w">published</a>,
  I wanted to share some reflections from the computational portion of the work.
</p>

<p>
  As one might expect, finding transition states for these large and flexible catalysts was a substantial challenge.
  We ended up using an ONIOM-type scheme to model most of the catalyst using PM7, 
which meant only the thioureas and the “northern” amide (which acts as a general base) needed to be computed using DFT. 
  Even with this approximation, it took three months to find the first glycosylation transition state. 
  Although we ended reoptimizing everything with all-atom DFT for publication, 
  using PM7 for non-essential atoms was crucial in accelerating the initial transition-state search.
</p>

<p>
  Working on this project also gave me a new appreciation for all of the work done to develop linear-scaling DFT methods. 
  The size of the full system meant that even routine Hessian calculations with a double-zeta basis set took most of a week; 
  without all the work that’s been done to speed up calculations on large systems (e.g. the <a href="https://www.science.org/doi/10.1126/science.271.5245.51">fast multipole method</a>), 
  these computations would not even have been possible. 
  Hopefully, calculations on systems of this size will become routine in the coming decade.
</p>

<p>
  After finding the (1,2) and (1,3) transition states, we were surprised (and disappointed) to find that the predicted selectivity was completely backwards from that observed experimentally. 
  Closer observation of the transition states showed that the hydrogen-bonding network in the unprotected acceptor was quite different between the two structures, 
  leading us to suspect that this energetic difference might be an artifact of implicit solvation. Since the reaction is run in diisopropyl ether, a hydrogen-bond acceptor, 
  we reasoned that the unprotected hydroxyls would be able to form hydrogen bonds with solvent ether molecules and not with the donor.
</p>

<figure>
  <img class="centered-img" src="https://corinwagen.github.io/public/img/20220620_presto.png" style="width:450px;" alt="picture of a solvated complex" />
  <figcaption> CYLView visualization of the solvated complex </figcaption>
</figure>

<p>
  To test this idea, we decided to attempt explicit solvent calculations. Although a full <i>ab initio</i> molecular dynamics study of this system was clearly out of the question, 
  we were able to run molecular dynamics using Grimme’s GFN2-xtb method for the catalyst, donor, and acceptor and the GFN-FF polarizable forcefield for the solvent. 
  Examination of the pre-(1,3) complex shows that the C4 hydroxyl is indeed solvated by a diisopropyl ether, meaning that the donor–acceptor hydrogen bond predicted by DFT is just wrong. 
  (As a bonus, we found that no (1,2) preference exists in the ground state, in agreement with the experimental observation that <i>K<sub>M</sub></i> is not lower for more selective catalysts.)
</p>

<p>
  Although the coolest solution would have been to do free energy perturbation in explicit solvent to get an accurate ∆∆G between the (1,2) and (1,3) transition states, 
  technical barriers meant we had to settle for modeling the C4-protected acceptor, which indeed favored the correct product. 
  Still, I think this approach demonstrates the computational insight that explicit solvent calculations can give even when a full, high-level treatment of the system is unreasonable. 
  We’ve been developing <a href="https://github.com/corinwagen/presto">software</a> to make this sort of molecular dynamics more routine—if you’re interested in using this in your research, 
  please contact me!
</p>
]]></description>
              <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
          </item>
      </channel>
  </rss>