<!DOCTYPE html>
<html>
  <head>
    <title>
      Teach Yourself Quantum Chemistry (For Fun And Profit)
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--twitter-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@github" />
    <meta name="twitter:title" content="Teach Yourself Quantum Chemistry (For Fun And Profit)" />
    <meta name="twitter:description" content="What I found to be helpful in learning quantum chemistry." />
    <meta name="twitter:image" content="https://corinwagen.github.io/public/img/20250604_damon.png" />

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="../main/index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="../main/index.html#about" class="menu-link">About</a></li>
      <!--<li class="menu-item"><a href="../main/index.html#projects" class="menu-link">Projects</a></li>-->
      <!--<li class="menu-item"><a href="../main/index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="../main/blog_p1.html" class="menu-link">Blog</a>
        <a href='../main/archive.html' class="menu-link">(Archive)</a>
      </li>   
    </ul>
    <h1> Teach Yourself Quantum Chemistry (For Fun And Profit) </h1>
    <i>June 4, 2025</i>
    
<figure>
  <img class=centered-img src="../img/20250604_damon.png" style="width:500px;" />
</figure>

<p>“You dropped a hundred and fifty grand on a f***** education you coulda' got for a dollar fifty in late charges at the public library.” —<em>Good Will Hunting</em></p>

<p>I was a user of computational chemistry for years, but one with relatively little understanding of how things actually worked below the input-file level. As I became more interested in computation in graduate school, I realized that I needed to understand everything much more deeply if I hoped to do interesting or original work in this space. From about August 2021 to December 2023, I tried to learn as much about how computational chemistry worked as I could, with the ultimate goal of being able to recreate my entire computational stack from scratch.</p>
<p>I found this task pretty challenging. Non-scientists don’t appreciate just how esoteric and inaccessible scientific knowledge can be: Wikipedia is laughably bad for most areas of chemistry, and most scientific knowledge isn’t listed on Google at all but trapped in bits and pieces behind journal paywalls. My goal in this post is to describe my journey and index some of the most relevant information I’ve found, in the hopes that anyone else hoping to learn about these topics has an easier time than I did.</p>
<p>This post is a directory, not a tutorial. I’m not the right person to explain quantum chemistry from scratch. Many wiser folks than I have already written good explanations, and my hope is simply to make it easier for people to find the “key references” in a given area. This post also reflects my own biases; it’s focused on molecular density-functional theory and neglects post-Hartree–Fock methods, most semiempirical methods, and periodic systems. If this upsets you, consider creating a companion post that fills the lacunæ—I would love to read it!</p>
<h2>0. Background</h2>
<p>I started my journey to understand computational chemistry with roughly three years of practice using computational chemistry. This meant that I already knew how to use various programs (Gaussian, ORCA, xTB, etc), I was actively using these tools in research projects, and I had a vague sense of how everything worked from reading a few textbooks and papers. If you don’t have any exposure to computational chemistry at all—if the acronyms “B3LYP”, “def2-TZVPP”, or “CCSD(T)” mean nothing to you—then this post probably won’t make very much sense.</p>
<p>Fortunately, it’s not too hard to acquire the requisite context. For an introduction to quantum chemistry, a good resource is Chris Cramer's <a href="https://www.youtube.com/watch?v=pu4uL7deCNw&amp;list=PLkNVwyLvX_TFBLHCvApmvafqqQUHb6JwF">video series</a> from his class at UMN. This is pretty basic but covers the main stuff; it's common for one of the classes in the physical chemistry sequence to also cover some of these topics. If you prefer books, <a href="https://www.amazon.com/Essentials-Computational-Chemistry-Theories-Models/dp/0470091827">Cramer</a> and <a href="https://www.amazon.com/Introduction-Computational-Chemistry-Frank-Jensen/dp/0470011874">Jensen</a> have textbooks that go slightly more in depth. </p>
<p>For further reading:</p>
<ul>
  <li>Steve Bachrach has a book on <a href="https://comporgchem.com/">computational organic chemistry</a> which discusses applying these techniques to a variety of problems in organic chemistry.</li>
  <li><a href="https://chemistlibrary.wordpress.com/wp-content/uploads/2015/02/modern-quantum-chemistry.pdf">Szabo/Ostlund</a> and <a href="https://www.amazon.com/Molecular-Electronic-Structure-Theory-Trygve-Helgaker/dp/1118531477">Helgaker/Jorgensen/Olsen</a> go much more into detail on the mathematics and implementation. (I confess I've only read parts of these books.)</li>
</ul>
<p>In general, computational chemistry is a fast-moving field relative to most branches of chemistry, so textbooks will be much less valuable than e.g. organic chemistry. There are lots of good review articles like <a href="https://onlinelibrary.wiley.com/doi/10.1002/anie.202205735">this one</a> which you can use to keep up-to-date with what's actually happening in the field recently, and reading the literature <a href="https://corinwagen.github.io/public/blog/20230329_literature.html">never goes out of style</a>.</p>
<p>All of the above discuss how to learn the theory behind calculations—to actually get experience running some of these, I'd suggest trying to reproduce a reaction that you've seen modeled in the literature. <a href="https://ekwan.github.io/notes.html#computational-chemistry">Eugene Kwan's notes are solid</a> (if Harvard-specific), and there are lots of free open-source programs that you can run like Psi4, PySCF, and xTB. (If you want to use Rowan, <a href="https://docs.rowansci.com/tutorials">we've got a variety of tutorials too.</a>) It's usually easiest to learn to do something by trying to solve a specific problem that you're interested in—I started out trying to model the effect of different ligands on the barrier to Pd-catalyzed C–F reductive elimination, which was tough but rewarding.</p>

<figure>
  <img class=centered-img src="../img/20250604_cf.png" style="width:650px;" />
</figure>

<p>Molecular dynamics is important but won’t be covered further here. The best scientific introduction to MD I've come across is <a href="https://sites.engineering.ucsb.edu/~shell/che210d/">these notes from M. Scott Shell</a>. If you want to go deeper, I really liked <a href="https://www.amazon.com/Understanding-Molecular-Simulation-Applications-Computational/dp/0122673514">Frenkel/Smit</a>; any knowledge of statistical mechanics will be very useful too, although I don’t have any recommendations for stat-mech textbooks. To get started actually running MD, I'd suggest looking into the OpenFF/OpenMM ecosystem, which is free, open-source, and moderately well-documented by scientific standards. (<a href="https://corinwagen.github.io/public/blog/20240613_simple_md.html">I've posted some intro scripts here</a>.)</p>
<p>Cheminformatics is a somewhat vaguely defined field, basically just "data science for chemistry," and it's both important and not super well documented. If you're interested in these topics, I recommend just reading <a href="https://practicalcheminformatics.blogspot.com/">Pat Walters' blog</a> or <a href="https://greglandrum.github.io/rdkit-blog/">Greg Landrum's blog</a>: these are probably the two best cheminformatics resources.</p>
<p>As with all things computational, it's also worth taking time to build up a knowledge of computer science and programming—knowing how to code is an investment which will almost always pay itself back, no matter the field. That doesn't really fit into this guide, but being good at Python/Numpy/data science/scripting is worth pursuing in parallel, if you don't already have these skills.</p>

<h2>1. How do Things Work, Basically?</h2>
<p>I started my computational journey by trying to write my own quantum chemistry code from scratch. My disposition is closer to “tinkerer” than “theorist,” so I found it helpful to start tinkering with algorithms as quickly as possible to give myself <a href="https://marginalrevolution.com/marginalrevolution/2022/02/context-is-that-which-is-scarce-2.html">context</a> for the papers I was trying to read. There are several toy implementations of Hartree–Fock theory out there that are easy enough to read and reimplement yourself:</p>
<ul>
  <li><a href="https://nznano.blogspot.com/2018/03/simple-quantum-chemistry-hartree-fock.html">NZNano</a> has a compact Jupyter notebook that runs an HF/STO-3G calculation on helium hydride.</li>
  <li><a href="https://adambaskerville.github.io/posts/HartreeFockGuide/">Adam Baskerville</a> has a somewhat cleaner implementation of HF/STO-3G on HeH+, albeit one which doesn’t compute the one- and two-electron integrals.</li>
  <li><a href="https://github.com/jjgoings/McMurchie-Davidson">Joshua Goings</a> has a great general implementation of HF for any molecule or basis set, and <a href="https://joshuagoings.com/2017/04/28/integrals/">an associated blog post</a> explaining how to compute the integrals.</li>
</ul>
<p>Based on these programs, I wrote my own Numpy-based implementation of Hartree–Fock theory, and got it to match values from the NIST CCCBDB database.</p>

<pre class=code-block>
  iteration 05:	∆P 0.000001	E_elec -4.201085		∆E -0.000000

SCF done (5 iterations):
-2.83122 Hartree

orbital energies: [-2.30637605 -0.03929435]
orbital matrix: 
[[-0.71874445  0.86025802]
 [-0.44259188 -1.02992712]]
density matrix: 
[[1.03318718 0.63622091]
 [0.63622091 0.39177514]]
Mulliken charges:
[[1.32070648 0.81327091]
 [1.10313469 0.67929352]]
</pre>

<p>I highly recommend this exercise: it makes quantum chemistry much less mysterious, but also illustrates just how slow things can be when done naïvely. For instance, my first program took 49 minutes just to compute the HF/6-31G(d) energy of water, while most quantum chemistry codes can perform that calculation in under a second. Trying to understand this discrepancy was a powerful motivation to keep reading papers and learning new concepts.</p>

<h2>2. Overview</h2>
<p>Armed with a basic understanding of what goes into running a calculation, I next tried to understand how “real” computational chemists structure their software. The best single paper on this topic, in my opinion, is the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540030314">“direct SCF” Almlof/Faegri/Korsell paper from 1982</a>. It outlines the basic method by which almost all calculations are run today, and it’s pretty easy to read. (See also <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540100111">this 1989 followup by Haser and Ahlrichs</a>, and <a href="https://link.springer.com/article/10.1007/s002149900072">this 2000 retrospective by Head-Gordon.</a>)</p>

<figure>
  <img class=centered-img src="../img/20250604_direct_scf.png" style="width:550px;" />
</figure>

<p>I found several other papers very useful for building high-level intuition. <a href="https://rsc.anu.edu.au/~pgill/papers/066ECC.pdf">This overview of self-consistent field theory by Peter Gill</a> is very nice, and <a href="https://schlegelgroup.wayne.edu/Pub_folder/32.pdf">Pople’s 1979 paper</a> on analytical first and second derivatives within Hartree–Fock theory is a must-read. <a href="https://www.worldscientific.com/doi/abs/10.1142/9789812832115_0008?srsltid=AfmBOopXU4ZCfegA2U-8HABNK_sBs_gU29cWUgfwv0diFtmKgs_RC2CF">This 1995 Pulay review</a> is also very good.</p>
<p>Using the intuition in these papers, I was able to outline and build a simple object-oriented Hartree–Fock program in Python. My code (which I called <code>hfpy</code>, in allusion to <a href="https://www.sigmaaldrich.com/US/en/product/aldrich/184225">the reagent</a>) was horribly inefficient, but it was clean and had the structure of a real quantum chemistry program (unlike the toy Jupyter Notebooks above). Here’s some of the code that builds the Fock matrix, for instance:</p>

<pre class=code-block>
    for classname in quartet_classes.keys():
        num_in_class = len(quartet_classes[classname])
        current_idx = 0
        while current_idx &lt; num_in_class:
            batch = ShellQuartetBatch(quartet_classes[classname][current_idx:current_idx+Nbatch])

            # compute ERI
            # returns a matrix of shape A.Nbasis x B.Nbasis x C.Nbasis x D.Nbasis x NBatch
            ERIabcd = eri_MMD(batch)
            shell_quartets_computed += batch.size

            for i, ABCD in enumerate(batch.quartets):
                G = apply_ERI(G, *ABCD.indices, bf_idxs, ERIabcd[:,:,:,:,i], ABCD.degeneracy(), dP)

            current_idx += Nbatch

    # symmetrize final matrix
    # otherwise you mess up, because we've added (10|00) and not (01|00) (e.g.)
    G = (G + G.T) / 2
    if incremental:
        molecule.F += G
    else:
        molecule.F = molecule.Hcore + G

    print(f"{shell_quartets_computed}/{shell_quartets_possible} shell quartets computed")
</code></pre>
<p>Any quantum-chemistry developer will cringe at the thought of passing 5-dimensional ERI arrays around in Python—but from a pedagogical perspective, it’s a good strategy.</p>

<h2>3. Details</h2>
<p>Armed with a decent roadmap of what I would need to build a decent quantum chemistry program, I next tried to understand each component in depth. These topics are organized roughly in the order I studied them, but they’re loosely coupled and can probably be addressed in any order.</p>
<p>As I read about various algorithms, I implemented them in my code to see what the performance impact would be. (By this time, I had rewritten everything in C++, which made things significantly faster.) Here’s a snapshot of what my life looked like back then:</p>
<pre class=code-block>benzene / 6-31G(d)
    12.05.22 - 574.1 s - SHARK algorithm implemented

    12.20.22 - 507.7 s - misc optimization
    12.21.22 - 376.5 s - downward recursion for [0](m)
    12.22.22 - 335.9 s - Taylor series approx. for Boys function
    12.23.22 - 300.7 s - move matrix construction out of inner loops
    12.25.22 - 267.4 s - refactor electron transfer relation a bit
    12.26.22 - 200.7 s - create electron transfer engine, add some references
    12.27.22 - 107.9 s - cache electron transfer engine creation, shared_ptr for shell pairs
    12.28.22 -  71.8 s - better memory management, refactor REngine
    12.29.22 -  67.6 s - more memory tweaks

    01.03.23 -  65.1 s - misc minor changes
    01.05.23 -  56.6 s - create PrimitivePair object
    01.07.23 -  54.7 s - Clementi-style s-shell pruning
    01.08.23 -  51.7 s - reduce unnecessary basis set work upon startup
    01.10.23 -  49.9 s - change convergence criteria
    01.11.23 -  61.1 s - stricter criteria, dynamic integral cutoffs, periodic Fock rebuilds
    01.12.23 -  42.5 s - improved DIIS amidst refactoring

    01.20.23 -  44.7 s - implement SAD guess, poorly
    01.23.23 -  42.6 s - Ochsenfeld's CSAM integral screening
    01.30.23 -  41.7 s - improve adjoined basis set generally

    02.08.23 -  38.2 s - introduce spherical coordinates. energy a little changed, to -230.689906932 Eh.

    02.09.23 -  12.3 s - save ERI values in memory. refactor maxDensityMatrix.
    02.15.23 -   9.0 s - rational function approx. for Boys function
    02.18.23 -   8.7 s - minor memory changes, don't recreate working matrices in SharkEngine each time
</pre>

<p>
  Here's a rough list of the topics I studied, with the references that I found to be most helpful in each area.
</p>

<h3>3.1 ERI Computation</h3>
<p>Writing your own QM code illustrates (in grisly fashion) just how expensive computing electron–electron repulsion integrals (ERIs) can be. This problem dominated the field’s attention for a long time. In the memorable words of Jun Zhang:</p>
<blockquote>
  Two electron repulsion integral. It is quantum chemists’ nightmare and has haunted in quantum chemistry since its birth.
</blockquote>
<p>To get started, Edward Valeev has written <a href="https://arxiv.org/ftp/arxiv/papers/2007/2007.12057.pdf">a good overview of the theory behind ERI computation</a>. For a broad overview of lots of different ERI-related ideas, my favorite papers are these two accounts by <a href="https://rsc.anu.edu.au/~pgill/papers/045Review.pdf">Peter Gill</a> and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/jcc.26942">Frank Neese</a>. (I’ve probably read parts of Gill’s review almost a hundred times.)</p>
<p>Here are some other good ERI-related papers, in rough chronological order:</p>
<ul>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261473800600">This 1973 paper by Raffinetti</a> discusses a non-direct SCF strategy for storing integrals on disk, which is still commonly used for small systems.</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/89/9/5777/220762/A-method-for-two-electron-Gaussian-integral-and?redirectedFrom=fulltext">This 1989 paper from Head-Gordon and Pople</a> outlines the basic algorithm by which most programs tackle ERI computation today. (The core ideas were developed by <a href="https://pubs.aip.org/aip/jcp/article-abstract/84/7/3963/91928/Efficient-recursive-computation-of-molecular?redirectedFrom=fulltext">Obara and Saika</a>, but I find that paper pretty unintelligible.) <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.560360831">Peter Gill’s extension of this work</a> is also quite nice (see also his review above).</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/104/7/2620/479230/A-J-matrix-engine-for-density-functional-theory?redirectedFrom=fulltext">This 1996 White/Head-Gordon paper</a> proposes the “<em>J</em>-matrix engine” and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0009261400005248">this 2000 paper</a> suggests some enhancements.</li>
  <li>Todd Martínez has a great series of papers discussing ERI evaluation on GPUs: <a href="https://pubs.acs.org/doi/10.1021/ct700268q">1</a>, <a href="https://pubs.acs.org/doi/abs/10.1021/ct800526s">2</a>, <a href="https://pubs.acs.org/doi/10.1021/ct9003004">3</a></li>
  <li>Ochsenfeld and Head-Gordon’s <a href="https://pubs.aip.org/aip/jcp/article-abstract/109/5/1663/529170/Linear-and-sublinear-scaling-formation-of-Hartree?redirectedFrom=fulltext">LinK strategy</a> for accelerated exchange computation is very nice. (And resolution-of-the-identity approaches can help here too: see <a href="https://pubs.aip.org/aip/jcp/article/143/2/024113/825080/Fast-accurate-evaluation-of-exact-exchange-The-occ">Manzer’s “occ-RI-K” strategy</a>.)</li>
  <li><a href="https://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00788">Jun Zhang has a paper</a> on metaprogramming-based approaches to ERI computation.</li>
  <li>Benjamin Pritchard has <a href="https://onlinelibrary.wiley.com/doi/10.1002/jcc.24483">a paper on vectorization in ERI computation</a>.</li>
  <li>C. David Sherrill discusses permutational symmetry in <a href="http://vergil.chemistry.gatech.edu/notes/permsymm/permsymm.html">these notes</a>.</li>
  <li>Integral screening matters a lot; I discuss it in <a href="https://corinwagen.github.io/public/blog/20230123_integral_screening.html">this blog post</a> from January 2023, which is roughly when I was learning all of this.</li>
</ul>

<h3>3.2 SCF Convergence</h3>
<p>Writing your own QM code also illustrates how important SCF convergence can be. With the most naïve approaches, even 15–20 atom molecules often struggle to converge—and managing SCF convergence is still relatively unsolved in lots of areas of chemistry today, like radicals or transition metals.</p>

<figure>
  <img class=centered-img src="../img/20250604_scf.png" style="width:500px;" />
  <figcaption>
    Different SCF-convergence strategies and how they fare on tricky systems (from Scuseria, <i>JCP</i>, <b>2012</b>; <i>vide infra</i>)
  </figcaption>
</figure>

<p>This is obviously a big topic, but here are a few useful references:</p>
<ul>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/126.pdf">This 1991 review</a> by Schlegel and Douall is a good overview of SCF convergence questions.</li>
  <li>The most fundamental approach is Pulay’s “direct inversion of the iterative subspace” (DIIS): the original paper is <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261480803964?via%3Dihub">here</a>, but this is famous enough that there are plenty of good and more modern summaries out there (<a href="https://joshipulkit.github.io/notes/diis/">1</a>, <a href="http://vergil.chemistry.gatech.edu/notes/diis/node2.html">2</a>, <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/03_Hartree-Fock/3b_rhf-diis.ipynb">3</a>).</li>
  <li><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2830258/">This 2010 paper</a> describes the “ADIIS” strategy, and <a href="https://pubs.aip.org/aip/jcp/article-abstract/137/5/054110/671630/Comparison-of-self-consistent-field-convergence?redirectedFrom=fulltext">this 2012 Scuseria paper</a> shows that ADIIS and “energy DIIS” (EDIIS) are equivalent.</li>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.20393">This 2006 paper</a> is a good guide to the “superposition of atomic density” guess strategy.</li>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0065327608603391">Lowdin’s 1970 paper</a> on orthonormalization and removing linear dependencies in basis sets is solid.</li>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.560070407">This 1973 Saunders paper</a> describes “level shifting,” which is ubiquitous today.</li>
</ul>

<h3>3.3 Density Fitting/Resolution of the Identity</h3>
<p>Density-fitting/resolution-of-the-identity (“RI”) approaches are becoming <em>de rigueur</em> for most applications. <a href="https://www.sciencedirect.com/science/article/abs/pii/000926149500621A">This 1995 Ahlrichs paper</a> explains “RI-J”, or density fitting used to construct the J matrix (<a href="https://www.sciencedirect.com/science/article/abs/pii/0009261493891517">this 1993 paper</a> is also highly relevant.) <a href="https://pubs.rsc.org/en/content/articlelanding/2002/cp/b204199p">This 2002 Weigend paper</a> extends RI-based methods to K-matrix construction. <a href="http://vergil.chemistry.gatech.edu/notes/df.pdf">Sherrill’s notes</a> are useful here, as is <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/03_Hartree-Fock/density-fitting.ipynb">the Psi4Numpy tutorial</a>.</p>
<p>RI methods require the use of auxiliary basis sets, which can be cumbersome to deal with. <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.6b01041">This 2016 paper</a> describes automatic auxiliary basis set generation, and <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.3c00670">this 2023 paper</a> proposes some improvements. <a href="https://molssi-bse.github.io/basis_set_exchange/developer_api.html#basis_set_exchange.manip.autoaux_basis">There’s now a way</a> to automatically create auxiliary basis sets through the Basis Set Exchange API.</p>

<h3>3.4 Mechanics of Density-Functional Theory</h3>
<p>For Hartree–Fock theory, there are a lot of nice example programs (linked above); for density-functional theory, there are fewer examples. Here are a few resources which I found to be useful:</p>
<ul>
  <li><a href="https://pubs.aip.org/jcp/article/88/4/2547/91134/A-multicenter-numerical-integration-scheme-for">This 1988 paper</a>, from Becke, basically describes how all DFT quadrature is done today. I find <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261496006008">this 1996 Stratmann paper</a> to be much more readable, though (plus it’s a linear-scaling method).</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/98/7/5612/843250/The-performance-of-a-family-of-density-functional?redirectedFrom=fulltext">This 1993 paper</a> by Gill and Head-Gordon discusses how to get analytical DFT gradients (among other things). Here's the key equation:

  <figure>
    <img class=centered-img src="../img/20250604_dft_grad.png" style="width:400px;" />
  </figure>

    This is the clearest exposition of the math behind DFT that I’ve read, although it doesn’t cover meta-GGA functionals or range-separated hybrids. (There’s a <a href="https://github.com/psi4/psi4numpy/blob/master/Tutorials/04_Density_Functional_Theory/4c_GGA_and_Meta_GGA.ipynb">Psi4Numpy explanation</a> of the math behind meta-GGA functionals, if you can parse the insane Psi4 syntax.)</li>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261494001995">This 1994 paper</a> discusses how to make DFT calculations with numerical quadrature rotationally invariant, which <a href="https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e">many programs still haven’t figured out</a>…</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/101/10/8894/472028/The-effect-of-grid-quality-and-weight-derivatives?redirectedFrom=fulltext">This other 1994 paper</a> argues that it’s not necessary to add grid weight derivatives with large grids. People still argue about this one.</li>
  <li>The first paper describing range-separated hybrids, as far as I can tell, is <a href=https://pubs.aip.org/aip/jcp/article-abstract/118/18/8207/460359/Hybrid-functionals-based-on-a-screened-Coulomb?redirectedFrom=fulltext>this 2003 work</a> from Scuseria and co-workers.</li>
</ul>

<h3>3.5 Linear Scaling</h3>
<p>For large systems, it’s possible to find pretty large speedups and reach a linear-scaling regime for DFT (excepting operations like matrix diagonalization, which are usually pretty fast anyway). <a href="https://www.sciencedirect.com/science/article/abs/pii/0009261494011281">This 1994 paper</a> discusses extending the fast multipole method to Gaussian distributions, and how this can lead to linear-scaling <em>J</em>-matrix construction, and <a href="https://www.science.org/doi/10.1126/science.271.5245.51">this 1996 paper</a> discusses a similar approach. <a href="https://pubs.aip.org/aip/jcp/article-abstract/105/19/8969/479837/A-linear-scaling-method-for-Hartree-Fock-exchange?redirectedFrom=fulltext">This other 1996 paper</a> describes a related near-linear-scaling approach for <em>K</em> matrices. There are a bunch more papers on various approaches to linear scaling (Barnes–Hut, CFMM, GvfMM, Turbomole’s RI/CFMM, etc), but I think there are diminishing marginal returns in reading all of them.</p>

<h3>3.6 Geometry Optimization</h3>
<p>Geometry optimization for molecular systems is pretty complicated. Here’s a sampling of different papers, with the caveat that this doesn’t come close to covering everything:</p>
<ul>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/50.pdf">This 1981 Schlegel paper</a> explores the general considerations in optimizing molecules with quantum chemical methods.</li>
  <li><a href="https://link.springer.com/article/10.1007/BF00554788">This 1984 Schlegel paper</a> discusses the importance of initial Hessians in geometry optimization.</li>
  <li><a href="https://schlegelgroup.wayne.edu/Pub_folder/180.pdf">This 1996 Schlegel/Frisch paper</a> discusses use of internal coordinates for geometry optimizations, which is standard today, and <a href="https://schlegelgroup.wayne.edu/Pub_folder/390.pdf">this 2016 paper</a> explores some enhancements.</li>
  <li><a href="https://pubs.acs.org/doi/10.1021/ct050275a">This 2006 paper</a> extends the DIIS scheme to geometry optimization; this still seems underrated today.</li>
  <li><a href="https://pubs.aip.org/aip/jcp/article-abstract/140/16/164115/841317/A-finite-difference-Davidson-procedure-to-sidestep?redirectedFrom=PDF">This 2014 paper</a> discusses a way to obtain the lowest eigenvalues of the Hessian without computing the full matrix, thus accelerating TS searches.</li>
  <li><a href="https://chemrxiv.org/engage/chemrxiv/article-details/64e37e3700bbebf0e68dd9c4">This 2023 paper</a> benchmarks existing optimization algorithms, albeit for somewhat boring molecules.</li>
</ul>

<h3>3.7 Frequency + Thermochemistry</h3>
<p>I think the best guides here are <a href="https://gaussian.com/vib/">the Gaussian vibrational analysis white paper</a> and <a href="https://gaussian.com/vib/">the Gaussian thermochemistry white paper</a>—they basically walk through everything that’s needed to understand and implement these topics. It’s now pretty well-known that small vibrational frequencies can lead to thermochemistry errors; <a href="https://rowansci.com/blog/dft-errors">Rowan has a blog post</a> that discusses this and similar errors.</p>

<h3>3.8 Solvent</h3>
<p>Implicit solvent models, while flawed, are still essential for a lot of applications. <a href="https://pubs.rsc.org/en/content/articlelanding/1993/p2/p29930000799">This 1993 paper</a> describes the COSMO solvation method, upon which most modern implicit solvent methods are built. <a href="https://pubs.acs.org/doi/10.1021/jp992097l">Karplus and York found a better way to formulate these methods in 1999</a>, which makes the potential-energy surface much less jagged: </p>

<figure>
  <img class=centered-img src="../img/20250604_solvent.png" style="width:500px;" />
</figure>

<p>While there are many more papers that one could read in this field, these are the two that I found to be most insightful.</p>

<h3>3.9 Basis Sets Miscellania</h3>
<p>I haven’t talked much about basis sets specifically, since most basis-set considerations are interwoven with the ERI discussion above. But a few topics warrant special mention:</p>
<ul>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/0009261496009177">This 1996 paper by Davidson</a> discusses how to convert general contraction into segmented contraction, which is important since the latter is much easier to handle.</li>
  <li>The issue of converting between spherical and Cartesian basis sets is tackled by <a href="https://onlinelibrary.wiley.com/doi/10.1002/qua.560540202">this 1995 work from Schelgel and Frisch</a>.</li>
</ul>




<h2>4. Conclusions</h2>
<p>There are hundreds of other papers which could be cited on these topics, to say nothing of the myriad topics I haven’t even mentioned here, but I think there’s diminishing marginal utility in additional links. The knowledge contained in the above papers, plus ancillary other resources, were enough to let me write my own quantum-chemistry code. Over the course of learning all this content, I wrote four different QM programs, the last of which, “Kestrel,” ended up powering Rowan for the first few months after we launched.</p>
<p>These programs are no longer directly relevant to anything we do at Rowan. We retired Kestrel last summer, and now exclusively use external quantum-chemistry software to power our users’ calculations (although I’ve retained some strong opinions; we explicitly override Psi4’s defaults to use the Stratmann–Scuseria–Frisch quadrature scheme, for instance).</p>
<p>But the years I spent working on this project were, I think, an important component of my scientific journey. Rene Girard says that <a href="https://corinwagen.github.io/public/blog/20230228_gilliam_and_girard.html">true innovation requires a “mastery” of the past’s achievements</a>. While I’m far from mastering quantum chemistry, I think that I’d be ill-suited to understand the impact of recent advances in neural network potentials without having immersed myself in DFT and conventional physics-based simulation for a few years.</p>
<p>And, on a more personal note, learning all this material was deeply intellectually satisfying and a fantastic use of a few years. I hope this post conveys some of that joy and can be helpful for anyone who wants to embark on a similar adventure. If this guide helped you and you have thoughts on how I could make this better, please feel free to reach out!</p>
<p><em>Thanks to Peter Gill, Todd Martínez, Jonathon Vandezande, Troy Van Voorhis, Joonho Lee, and Ari Wagen for helpful discussions.</em></p>


    <br>
    <br>
    <i>If you want email updates when I write new posts, you can subscribe <a href=https://cwagen.substack.com/>on Substack.</a></i>
  </body>
  <br>
  <br>
  <footer>
    <a href="mailto:corin.wagen+blog@gmail.com">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">twitter</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
