<!DOCTYPE html>
<html>
  <head>
    <title>
      Blog
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>
      <!--<li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    <h1 class='blogroll-header'>Blog</h1><div class='previous-link'><a href='blog_p2.html'>previous page</a></div><div class='next-link'><a href='blog_p4.html'>next page</a></div><br><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230516_hamming.html'>Book Review: The Art of Doing Science and Engineering</a></h2><i>May 16, 2023</i>
<br>
<br>
<p class=epigraph>
“They performed his signs among them, and miracles in the land of Ham.”
</p>
<p class=epigraph-byline>
—Psalm 105:27
</p>

<p>
Who was Richard Hamming, and why should you read his book?
</p>

<p>
If you’ve taken computer science courses or messed around enough with <i>scipy</i>, you might recognize his name in a few different places—<a href=https://en.wikipedia.org/wiki/Hamming_code>Hamming error-correction codes</a>, the <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hamming.html>Hamming window function</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_distance>Hamming distance</a>, the <a href=https://en.wikipedia.org/wiki/Hamming_bound>Hamming bound</a>, etc. I had heard of some of these concepts, but didn’t know anything concrete about him before I started reading this book.
</p>

<p>
His brief biography is as follows: Richard Hamming (1915–1998) studied mathematics at the University of Chicago, earned a PhD in three years from Illinois, and then started as a professor at the University of Louisville… in 1944. He was almost immediately recruited for the Manhattan Project, where he worked on large-scale simulations of imploding spherical shells of explosives, and generally acted as a “computational janitor” for various projects.
</p>

<p>
In 1946, he moved to Bell Telephone Laboratories, <a href=https://www.theverge.com/2012/3/21/2887206/jon-gertner-idea-factory-bell-labs-great-american-age-innovation-book-review>“arguably the most innovative research institution in the country,”</a> and worked there until 1976. During his time at Bell Labs, he was involved in “nearly all of the laboratories' most prominent achievements” (<a href=https://en.wikipedia.org/wiki/Richard_Hamming>Wikipedia</a>), and was rewarded with such accolades as the third-ever Turing Award (essentially the Nobel Prize for computer science) and the IEEE Richard W. Hamming medal. (You know you’re successful when someone else names an award after you!)
</p>

<p>
After he retired from Bell Labs, Hamming went on to go teach classes at the Naval Postgraduate School. This book—published in 1996, just before his death—is based on the capstone engineering class he taught there, which attempted to prepare students for their technical future by conveying the “style of thinking” necessary to be a great scientist and engineer. Stripe Press calls it “a provocative challenge to anyone who wants to build something great” and “a manual of style for how to get there,” while the foreword calls it a “tour of scientific greatness” which prepares “the next generation for even greater greatness” while challenging them to serve the public good.
</p>

<p>
I was excited to read this book because:
</p>

<ol>
<li>
Hamming was present at arguably the two most successful scientific institutions of the 20th century—the Manhattan Project and Bell Labs—and so was witness to more innovative scientific discoveries than almost anyone today. So if anyone has a shot at teaching how to be an effective scientist, it’s Hamming.
</li>

<li>
The interface between academic science and real-world advances seems to be one of the most broken elements of the modern research ecosystem, and I was interested to see what Hamming, as a self-proclaimed scientist and engineer, had to say on the subject.
</li>

<li>
It’s a first-hand account of the creation of the most important field of the past century—computer science—and thus a way to witness, albeit second-hand, the important process of <a href=https://freaktakes.substack.com/p/when-do-ideas-get-easier-to-find>scientific branch-creation</a>.
</li>

<li>
The book is a chance to see computing, and computer science, as it was before the implications and importance of the field became obvious, and thus a way to understand what a promising area of research looks like without the benefit of hindsight.
</li>

<li>
This is a chance to read frank and honest reflections from someone who both was a great scientist and was adjacent to a huge number of other great scientists, and thus learn about the culture of successful science (especially successful science in the mid-20th century, which might be different than the science of today).
</li>
</ol>

<h2>
1. What’s In The Book?
</h2>

<p>
<i>The Art of Doing Science and Engineering</i> is not a conventional textbook, but neither is it a self-help book, a memoir, or a guide to personal strategy. The two books that remind me of it most are <i>Godel, Escher, Bach</i>, by Douglas Hofstadter, and <i>Zero To One</i>, by Peter Thiel. All three books are composed of various pseudo-independent chapters which, in isolation, could likely function as essays, but which echo certain themes over and over again in a way that makes the whole greater than the sum of its parts in a hard-to-summarize way.
</p>

<p>
Hamming’s goal in structuring the book this way is clear, and explicitly stated: he doesn’t want to teach object-level facts, because the facts needed to succeed in any discipline will inevitably change over the course of time. Instead he seeks to teach the patterns of thought which will enable anyone to succeed in an evolving technical landscape. To do so, he uses a mix of first-person stories, historical reflections, mathematical proofs, and graphs, all with the goal of teaching something that’s essentially incommunicable: style. I quote:
</p>

<blockquote>
I will show you my style as best I can, but, again, you must take those elements of it which seem to fit you, and you must finally create your own style. Either you will be a leader or a follower, and my goal for you is to be a leader. You cannot adopt every trait I discuss in what I have observed in myself and others; you must select and adapt, and make them your own if the course is to be effective.
</blockquote>

<p>
Despite Hamming’s insistence that there is “really no technical content” in the book and that any mathematics is only “window dressing,” I found that my lack of background knowledge made several chapters—particularly those on digital filters—quite difficult to understand.
</p>

<h2>
2. Representative Anecdotes
</h2>

<p>
In keeping with Hamming’s desire to convey the “style of thinking” rather than actual object-level scientific ideas, I’ll share a few anecdotes, insights, and quotes in an attempt to reproduce the style of his writing.
</p>

<h3>
2.1 The Second Mouse Gets The Cheese
</h3>

<p>
Hamming recounts the invention of interpreters and compilers and then reflects on how hard it was for early computer pioneers to think of computers as “symbol manipulators and not just number crunchers,” observing that often the first people in a field do not understand it as well as those who come after them. Why is this?
</p>

<blockquote>
The reason this happens is that the creators have to fight through so many dark difficulties, and wade through so much misunderstanding and confusion, they cannot see the light as others can, now the door is open and the path made easy…. in time the same will probably be true of you.
</blockquote>

<p>
This is a nice observation, and perhaps explains the value of startups: incumbents in a market can be inefficient not only for bureaucratic reasons, but also because they’re intellectually less suited to see new opportunities—the young can see the status quo more clearly than those who’ve had to create it. This also explains why “really new ideas seldom arise from the experts in the field”—experts always bring their expertise when looking at something new, which makes them more likely to be correct, but also disincentivizes new ways of thinking and thus creates a sort of status quo bias.
</p>

<h3>
2.2 Order-of-Magnitude Changes
</h3>

<p>
Many people, faced with early computers, dismissed them as simply a faster way to do rote calculations—which seems silly in hindsight, but was a real barrier for early computer pioneers to overcome. Hamming argues “a single order of magnitude change (a factor of ten) produces fundamentally new effects” in any piece of technology, and immediately thereafter reflects on how it’s difficult for people to accept something new:
</p>

<blockquote>
People always want to think that something new is just like the past—they like to be comfortable in their minds as well as their bodies—and hence they prevent themselves from making any significant contribution to the new field being created under their noses. Not everything which is said to be new really is new, and it is hard to decide in some cases when something is new, yet the all too common reaction of “it’s nothing new” is stupid.
</blockquote>

<p>
I had previously attributed the idea that ten-fold improvement creates a fundamentally new product to Peter Thiel (<i>Zero To One</i>, pp. 48–49), but it seems Hamming (as usual) got there first.
</p>

<h3>
2.3 Intuition In High Dimensions
</h3>

<p>
Since most complex problems occur in high-dimensional spaces, Hamming illustrates a few ways that our 2D or 3D intuition can fail us.
</p>

<p>
One particularly nice thought experiment is this: take a square with side length 4 and divide it into four squares, each containing a unit circle. Now draw a circle in the middle of the square, such that the circle just touches each of the four unit circles. Realizing that the distance from the center of this circle to the center of each unit circle must be √2, and that the radius of the unit circle is, of course, 1, we can see that the radius of the inner circle must be √2 - 1 ≈ 0.414.
</p>

<p>
Now, we can perform the same mental exercise for a cube with side length four, and find that the analogous inner sphere has side length √3 - 1 ≈ 0.732. More generally, as we extend this exercise to higher dimensions, we find that the radius of the inner <i>n</i>-dimensional hypersphere is √<i>n</i> - 1, which is bizarre! For instance, in ten dimensions, the inner sphere has radius √10 - 1 ≈ 2.162, meaning that it reaches outside of the surrounding cube:
</p>

<blockquote>
Yes, the sphere is convex, yes it touches each of the 1,024 packed spheres on the inside, yet it reaches outside the cube! So much for your raw intuition about n-dimensional space, but remember the n-dimensional space is where the design of complex objects generally takes place.
</blockquote>

<p>
This sort of intuition is difficult to obtain, but Hamming gives a few examples. One that stuck with me was the claim that, in high-dimensional shapes, almost all of the volume is on the surface—so “almost surely the optimal design will be on the surface and will not be inside, as you might think from taking the calculus and doing optimizations in that course.”
</p>

<h3>
2.4 The Fast Fourier Transform
</h3>

<p>
Here’s a memorable story from Hamming’s life:
</p>

<blockquote>
You have all heard about the fast Fourier transform [FFT] and the Tukey-Cooley paper. It is sometimes called the <i>Tukey-Cooley</i> transform or algorithm. Tukey had suggested to me, sort of, the basic ideas of the FFT. I had at the time an IBM Card Programmed Calculator (CPC) and the “butterfly” operation meant it was completely impractical to do with the equipment I had. Some years later I had an internally programmed IBM 650 and he remarked on it again. All I remembered was that it was one of Tukey’s few bad ideas; I completely forgot why it was bad—namely because of the equipment I had at the time. So I did not do the FFT, though a book I had already published (1961) shows I knew all the facts necessary, and could have done it easily!
<br><br>
Moral: when you know something cannot be done, also remember the essential reason why, so later, when the circumstances have changed, you will not say “It can’t be done.” Think of my error! How much more stupid can anyone be?
</blockquote>

<p>
Later in the book, Hamming puts forward the following “old statement” about experts:
</p>

<blockquote>
If an expert says something can be done he is probably correct, but if he says it is impossible then consider getting another opinion.
</blockquote>

<h3>
2.5 The Mixed Blessings of Jargon
</h3>

<p>
Hamming emphasizes the importance, as an interdisciplinary scientist, of mastering the language of the field in which you’re working, but warns against embracing jargon too much. Why? Jargon serves “to facilitate communication over a restricted area of things or events… [but] also blocks thinking outside the original area it was designed to cover.” So jargon makes intra-domain communication easier, but makes inter-domain communication harder.
</p>

<p>
More philosophically, jargon is a consequence of how “we have been mainly selected by evolution to resent outsiders,” and thus the “instinctive use of jargon” is a base instinct that must be consciously resisted.
</p>

<h3>
2.6 Optimal Components, Suboptimal Systems
</h3>

<p>
Hamming discusses the field of systems engineering, which he defines as “the attempt to keep at all times the larger goals in mind and to translate local actions into global results” (emphasis original), and coins the first rule of systems engineering:
</p>

<blockquote>
If you optimize the components, you will probably ruin the system performance.
</blockquote>

<p>
This point is illustrated with a few examples. One of these examples is the progressive optimization of calculus and linear algebra classes in college, where “we have stripped out anything not immediately relevant to each course,” leading to “large parts of any mathematical education being omitted in the urge to optimize the individual courses.” Only when the proper goal of a mathematical education is taken into account—producing well-trained students with a firm grasp of math and the ability to apply it to important problems—can the constituent courses sanely be optimized.
</p>

<p>
I found this idea pretty insightful, and have thought about it a lot since reading this book. For instance, one can see many researchers as over-optimizing for “publishing papers” or “winning grants” rather than working towards maximizing total scientific progress. (Alex Danco has a great piece discussing the same ideas in the context of the Canadian tech ecosystem, which I wrote about <a href=https://corinwagen.github.io/public/blog/20221206_definite_games_indefinite_optimism.html>here</a>.)
</p>

<h3>
2.7 Learning Should Be Difficult
</h3>

<p>
I like this story so much I’ll just reproduce it in its entirety:
</p>

<blockquote>
When I first came to the Naval Postgraduate School in 1976 there was a nice dean of the extension division concerned with education. In some hot discussions on education we differed. One day I came into his office and said I was teaching a weightlifting class (which he knew I was not). I went on to say that graduation was lifting 250 pounds, and I had found many students got discouraged and dropped out, some repeated the course, and a very few graduated. I went on to say thinking this over last night I decided the problem could be easily cured by simply cutting the weights in half—the student in order to graduate, would lift 125 pounds, set them down, and then lift the other 125 pounds, thus lifting the 250 pounds.
<br><br>
I waited a moment while he smiled (as you probably have) and I then observed that when I found a simpler proof for a theorem in mathematics and used it in class, was I or was I not cutting the weights in half? What is your answer? Is there not an element of truth in the observation that the easier we make the learning for the student, the more we are cutting the weights in half?
</blockquote>

<p>
This story reflects a key belief of Hamming’s: that creativity and talent in technical disciplines are not innate traits given only to rare geniuses, but trainable skills which anyone can hope to acquire and improve at, given the appropriate training regimen. In Hamming’s worldview, staring at a confusing math problem is not a sign that you’re a failure, but the process by which you become successful.
</p>

<h3>
2.8 The Importance of Presentation
</h3>

<p>
Hamming emphasizes that being able to “sell” one’s ideas is a key part of being a scientist:
</p>

<blockquote>
All [methods of conveying ideas] are essential—you must learn to sell your ideas, not by propaganda, but by force of clear presentation. I am sorry to have to point this out; many scientists and others think good ideas will win out automatically and need not be carefully presented. They are wrong; many a good idea has had to be rediscovered because it was not well presented the first time, years before! New ideas are automatically resisted by the establishment, and to some extent justly. The organization cannot be in a continual state of ferment and change, but it should respond to significant changes.
</blockquote>

<p>
In this view, a certain degree of institutional conservatism is necessary to avoid being swept up by any new fad (in machine learning terms, we might say that organizations need to limit their learning rate), and so you alone must convince your peers that your insights are the real deal and deserve to be taken seriously.
</p>

<p>
Hamming then extends this idea to needing to sell your abilities more broadly:
</p>

<blockquote>
You do not hire a plumber to learn plumbing while trying to fix your trouble; you expect he is already an expert. Similarly, only when you have developed your abilities will you generally get the freedom to practice your expertise, whatever you choose to make it, including the expertise of “universality,” as I did.
</blockquote>

<p>
My experience within science is that most people are a bit allergic to the idea of “selling” themselves or their research—with the exception of a few people who become almost addicted to it. Hamming (who never shies away from quoting a Greek philosopher) would probably think that there’s an Aristotelian mean between these two extremes: the ideal scientist/engineer recognizes that self-promotion is a necessary means to an end, but never engages in self-promotion absent a higher goal.
</p>

<h2>
3. Overall Themes
</h2>

<p>
I would summarize Hamming’s key themes—those leitmotifs which pop up time and time again in the book—as the following:
</p>

<h3>
3.1 Fundamentals Are Key
</h3>

<p>
Wherever possible, try to understand the intellectual underpinnings of a field as well as possible, rather than the surface-level results. If you do so, you will not only understand the field better than most of its practitioners, but also be better at transferring knowledge between fields. Perceiving “the essential unity of all knowledge rather than the fragments which appear as the individual topics are taught” allows one to quickly access relevant information to the problem at hand, no matter the field of origin.
</p>

<p>
Hamming frequently points out the failings of domain experts to perceive the fundamental underpinnings of their knowledge:
</p>

<blockquote>
Lo and behold, the famous <i>transfer function</i> is exactly the <i>eigenvalues</i> of the corresponding eigenfunctions! Upon asking various electrical engineers what the transfer function was, no one has ever told me that! Yes, when pointed out to them that it is the same idea they have to agree, but the fact it is the same idea never seemed to have crossed their minds! The same, simple idea, in two or more different disguises in their minds, and they knew of no connection between them! Get down to the basics every time! <i>(emphasis original)</i>
</blockquote>

<p>
Not only is a good grasp of fundamentals important for understanding your own domain, it also helps with creativity. Hamming argues that creative insights come from the subconscious, and that “flexible access to pieces of knowledge” is the most important way to give the subconscious the tools it needs to solve a problem. This flexible access arises from “looking at knowledge while you are acquiring it from many different angles,” making sure to capture its key features rather than simply memorizing the aspect relevant to the task at hand.
</p>

<p>
(The idea that creativity comes from the subconscious is pretty common—see, for instance, Nisbett and Wallace’s article <a href=https://home.csulb.edu/~cwallis/382/readings/482/nisbett%20saying%20more.pdf>“Telling More Than We Can Know,”</a> which argues that basically all higher order cognitive processes are subconscious.)
</p>

<h3>
3.2 Gain Insight Where Possible
</h3>

<p>
No matter the task at hand, Hamming argues that the correct immediate step is always to gain insight about the situation, and then go from there. He uses the example of Planck and the “ultraviolet catastrophe” to illustrate how crucial insight can be:
</p>

<blockquote>
Max Planck (1858–1947) fit the black-body radiation experimental data with an empirical curve, and it fit so well he “knew” it was “the right formula.” He set out to derive it, but had trouble. Finally he used a standard method of breaking up the energy into finite sizes and then going to the limit… Fortunately for Planck the formula fit only so long as he avoided the limit, and no matter how he took the limit the formula disappeared. He finally, being a very good, honest physicist, decided he had to stop short of the limit, and that is what defines Planck’s constant!
<br><br>
<i>[another historical paragraph omitted]</i>
<br><br>
Before going on, let me discuss how this piece of history has affected my behavior in science. Clearly Planck was led to create the theory because the approximating curve fit so well, and had the proper form. I reasoned, therefore, if I were to help anyone do a similar thing, I had better represent things in terms of functions they believed would be proper for their field rather than in the standard polynomials. I therefore abandoned the standard polynomial approach to approximation, which numerical analysts and statisticians among others use most of the time, for the harder approach of finding which class of functions I should use.
</blockquote>

<p>
This episode demonstrates how insight can arise from a simulation, and enable future work (like, in this case, all of quantum mechanics), and also illustrates how the manner in which one performs simulations can make it either easier or harder to obtain underlying insights about the problem. Hamming emphasizes this point with a few stories from his time at Bell Labs, and argues that there are times where more computational power is actually counterproductive:
</p>

<blockquote>
I have often wondered what would have happened [in the Nike guided missile project] if I had had a modern, high-speed computer. Would I ever have acquired the feeling for the missile, upon which so much depended in the final design? I often doubt hundreds more trajectories would have taught me as much—I simply do not know. But that is why <i>I am suspicious, to this day, of getting too many solutions and not doing enough very careful thinking about what you have seen.</i> Volume output seems to me to be a poor substitute for acquiring an intimate feeling for the situation being simulated… doing simple simulations at the early stages lets you get insights into the whole system which would be disguised in any full-scale simulation. <i>(emphasis original)</i>
</blockquote>

<p>
This point—that simulation is not the same as understanding—is not unique to Hamming (see <i>inter alia</i> Roald Hoffmann on the subject: <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201902527>1</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910283>2</a>, <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.201910285>3</a>), but hearing this from the father of scientific simulation certainly drives the message home!
</p>

<h3>
3.3 Vision Matters
</h3>

<p>
Hamming goes to great lengths to emphasize the importance of having a vision for your life:
</p>

<blockquote>
It is well known the drunken sailor who staggers to the left or right with n independent random steps will, on the average, end up about √<i>n</i> steps from the origin. But if there is a pretty girl in one direction, then his steps will tend to go in that direction and he will go a distance proportional to <i>n</i>. In a lifetime of many, many independent choices, small and large, a career with vision will get you a distance proportional to <i>n</i>, while no vision will get you only the distance √<i>n</i>. In a sense, the main difference between those who go far and those who do not is some people have a vision and the others do not and therefore can only react to the current events as they happen… having a vision is what tends to separate the leaders from the followers.
</blockquote>

<p>
This theme permeates his discussion of systems engineering: a successful systems engineer must, at all times, keep the overall vision and purpose of the system in mind, so as to optimize in the right direction. It also motivates what problems you must choose:
</p>

<blockquote>
If you do not work on important problems, how can you expect to do important work? Yet direct observation and direct questioning of people show most scientists spend most of their time working on things they believe are not important and are not likely to lead to important things.
</blockquote>

<p>
In Hamming’s view, it seems the precise vision one follows is less important than the simple act of having a vision at all. Forcing oneself to decide on goals and then strive to fulfill them will naturally lead you to excellence, even if the goals you choose aren’t the same as the one Hamming would have chosen:
</p>

<blockquote>
The chief gain is in the effort to change yourself, in the struggle with yourself, and it is less in the winning than you might expect. Yes, it is nice to end up where you wanted to be, but the person you are when you get there is far more important. I believe a life in which you do not try to extend yourself regularly is not worth living—but it is up to you to pick the goals you believe are worth striving for.
</blockquote>

<h2>
4. Should You Read This Book?
</h2>

<p>
I began this book review by claiming that <i>The Art of Doing Science and Engineering</i> isn’t a textbook, or a self-help book, or a memoir, but failed to offer a positive vision of what it was. I now reveal my true opinion: <i>The Art of Doing Science and Engineering</i> is best viewed as a modern example of <a href=https://en.wikipedia.org/wiki/Wisdom_literature>“wisdom literature,”</a> in the style of ancient scriptures.
</p>

<p>
Why is this? Wisdom literature frequently has the curious property that it’s accessible only to the wise. For instance, the book of Proverbs is ostensibly written to convey wisdom to those who seek it, but this hardly seems compatible with the following passage (Proverbs 26:7–9):
</p>

<blockquote>
Like a lame man's legs, which hang useless, is a proverb in the mouth of fools.<br>
Like one who binds the stone in the sling is one who gives honor to a fool.<br>
Like a thorn that goes up into the hand of a drunkard is a proverb in the mouth of fools.<br>
</blockquote>

<p>
If proverbs are useless—or worse than useless, a thorn in the hand of a drunkard—to those without wisdom, then what is the point of proverbs? If only the wise can understand your book of wisdom, why bother writing it at all?
</p>

<p>
There are several ways to resolve this tension (one being “the book of Proverbs is stupid”), but I think the right answer goes something like this: wisdom is accessible to those who seek it, but simply reading through a book of wisdom isn’t sufficient to make one wise. Rather, the search for wisdom requires discipline and vigilance—one must meditate on wise sayings, appreciate the underlying principles, and learn to discern what’s right even in complicated circumstances. So, wisdom literature can help us on the journey to wisdom, but ultimately we will have to take the intellectual burden upon ourselves if we hope to get anywhere. (This is roughly how <a href=https://www.desiringgod.org/articles/the-best-discoveries-begin-as-problems>John Piper interprets Proverbs</a>.)
</p>

<p>
Viewing Hamming’s book as essentially modern wisdom literature makes sense of his focus on the “style” of thinking, his insistence that the reader must rediscover much of what he’s saying for themselves, and his admonitions not to accept what he’s saying blindly but to think it over at length:
</p>

<blockquote>
You the reader should take your own opinions and try first to express them clearly, and then examine them with counterarguments, back and forth, until <i>you are fairly clear as to what you believe and why you believe it</i>. It is none of the author’s business in this matter what you believe, but it is the author’s business to get you to think and articulate your position clearly. <i>(emphasis original)</i>
</blockquote>

<p>
If we view this book—Hamming’s guide to future scientists and engineers, his <i>magnum opus</i> as a teacher and mentor—as wisdom literature, it implies that wisdom, not any specific technical skill, is rate-limiting for technical progress. This is very encouraging, because wisdom, unlike innate intelligence, is an acquired trait, and one which we can all cultivate in ourselves. We can’t all be Ramanujan or von Neumann, but (at least as Hamming tells it) we can all be Hamming.
</p>

<p>
Thus far, I’ve mostly given reasons why you should read this book. Why shouldn’t you read this book? One reason is that this book is aimed at scientists and engineers, and furthermore it seems primarily aimed at people with an interest in the “hard” sciences—much of the advice assumes some contact with simulation, math, or physics. So a reader without at least a glancing interest in these topics might struggle to find some of the content relevant. (But maybe the act of extending his advice to other domains would prompt deeper consideration of the fundamental principles at play, and thus serve to cultivate wisdom!)
</p>

<p>
Another reason you shouldn’t read this book is that it’s very much framed as a personal guide—it addresses the needs of an individual scientist, not ideas for how science writ large could be improved. So aspiring metascientists might be disappointed by Hamming’s perspective; he dedicates a lot of time to thinking about how one can navigate imperfect organizations, and much less time to thinking about what a perfect organization would look like.
</p>

<p>
The strongest reason for reading this book, though, is that the world Hamming hopes to write for is almost exactly our world today. Hamming anticipates “the inevitable and extensive computerization of your organization and society generally,” a world in which scientists are frequently overwhelmed by the “rate of innovation and change of the dominant paradigm,” and a world where there is “not time to allow us the leisure which comes from separating the two fields” of science and engineering. From my perspective, this almost perfectly captures the feeling of working in science or tech today.
</p>

<p>
And so Hamming’s message—the vision of scientists “trying to achieve excellence” through making “significant contributions to humanity” on important problems—seems more relevant today than ever. If you yourself work in a scientific field, and want to know how to have the greatest positive impact on your own character and on society, then Hamming’s wisdom is for you: but not without some struggle
</p>

<i>
A transcript of Hamming’s talk “You and Your Research” (a shorter exposition of some of the ideas discussed above) is available <a href=https://www.cs.virginia.edu/~robins/YouAndYourResearch.html>here</a>, and </i>The Art of Doing Science and Engineering<i> can be purchased from Stripe Press <a href=https://press.stripe.com/the-art-of-doing-science-and-engineering>here</a>.
</i>
<br>
<br>
<i>Thanks to Michael Tartre for giving me this book originally, and to Jacob Thackston and Ari Wagen for extensive edits.</i>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230512_irms.html'>What Happened to IRMS?</a></h2><i>May 12, 2023</i>
<p>
A few days ago, <a href=https://corinwagen.github.io/public/blog/20230508_aldehyde_eie.html>I wrote</a> about kinetic isotope effects (KIEs), probably my favorite way to study the mechanism of organic reactions. To summarize at a high level: if the bonding around a given atom changes over the course of a reaction, then different isotopes of that atom will react at different rates. The exact magnitude of the effect depends on the vibrational modes involved, but is often quite different for different mechanisms, meaning that you can computationally predict isotope effects for a lot of mechanisms and then use KIE measurements to figure out which one is actually happening.
</p>

<p>
The trouble is that the magnitude of the effect depends on the difference in the mass of the two isotopologues. <sup>1</sup>H/<sup>2</sup>H isotope effects are quite large: H reacts up to 7x faster than D (more for mechanisms that involve quantum tunneling), meaning that it’s not too hard to measure the value accurately. But as the atom gets heavier, the effects get smaller. For the next most common pair of isotopomers,<sup>12</sup>C/<sup>13</sup>C, the effect is usually 5% or less.
</p>

<p>
Small KIEs are usually measured by one-pot competition experiments: a mixture of the two isomers is reacted to partial conversion, and then the isotopic composition of either the starting material or the product is determined. The product will be enriched in the isotope that reacts more quickly, and the starting material will be enriched in the isotope that reacts more slowly. If you know the starting ratio of isotopes, the conversion, and the ratio of isotopes at partial conversion, then you can use the Bigeleisen−Mayer equation to figure out the KIE.
(<a href=https://pubs.acs.org/doi/pdf/10.1021/jacs.1c07351>This</a> is a really good review on isotope effects in general, if you want more than this cursory summary.)
</p>

<p>
The accuracy of the KIE measurement is thus limited by (1) how accurately you can determine conversion and (2) how accurately you can measure the isotopic composition of a sample. Although conversion can be annoying, the second is the more serious limitation—<i>a priori</i> it’s not obvious how to figure out what the relative abundance of various isotopes is.
</p>

<p>
Today, most people use approaches based on NMR spectroscopy: since <sup>1</sup>H and <sup>13</sup>C are both NMR-active nuclei, you can just integrate the peak of interest against another peak to figure out how much there is. (Quantitative <sup>13</sup>C NMR is super slow, so <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>various</a> <a href=https://www.nature.com/articles/s41557-018-0079-7>tricks</a> can be employed to speed things up.)
</p>

<p>
But there was an age before the advent of accurate NMR spectroscopy where people measured isotope effects differently. I was awestruck by <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00834a064>this 1975 paper</a> from Cromartie and Swain reporting the measurement of a <sup>35</sup>Cl/<sup>37</sup>Cl isotope effect in the cyclization of 4-chlorobutanol: they report an isotope effect of 1.000757 ± 0.00015 using hydroxide as base, which they differentiate from an isotope effect of 1.000796 ± 0.00013 using water as base by Student’s <i>t</i> test. These numbers are way, way smaller and more precise than any isotope effect I’ve seen measured in the last few decades.
</p>

<p>
Digging a little deeper reveals a <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00418a038>whole</a> <a href=https://pubs.acs.org/doi/pdf/10.1021/ja00426a047>wealth</a> of papers using <sup>35</sup>Cl/<sup>37</sup>Cl isotope effects to study various mechanistic phenomena. The instrument Swain and others use (described <a href=https://pubs.acs.org/doi/pdf/10.1021/jo00972a016>here</a>) is an isotope-ratio mass spectrometer, which as the name implies is a special sort of mass spectrometer designed specifically to measure isotopic composition. These instruments, although a little obscure from my point of view, are <a href=https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry/isotope-ratio-mass-spectrometry-irms.html>commercial</a>!
</p>

<p>
So, why isn’t IRMS used more frequently in organic chemistry today? I think it’s for a few reasons. IRMS, at least historically, only works on gases, meaning that you have to either use gaseous reactants or convert your analytes to gases, both of which are pretty annoying. In the Swain work, they (i) incinerate their samples with nitric acid, (ii) precipitate out silver chloride by adding silver nitrate, and then (iii) convert silver chloride to gaseous methyl chloride by <a href=https://pubs.acs.org/doi/abs/10.1021/ja00873a025>heating with methyl iodide</a> in a sealed tube. This is certainly a lot of hassle to put up with for a single measurement—and you generally want to get a good number of replicates.
</p>

<p>
(There are some <a href=https://www.thermofisher.com/order/catalog/product/11206175>all-in-one solutions</a> available for sale, which automatically combust samples à la elemental analysis, but they don’t seem to work on non-standard isotopes like chlorine.)
</p>

<p>
Another reason why IRMS might have fallen out of favor is that it requires a dedicated instrument, whereas NMR-based methods can be done using the NMR spectrometers that any university already has. Most labs only have budgets for a handful of instruments—is an IRMS really worth the investment? (Owing to the typical aura of secrecy around instrument prices, I’m not sure how much one costs, but I’m guessing it’s a few hundred thousand dollars or so.)
</p>

<p>
These downsides notwithstanding, I think there is a lot of good science that could be done if a mechanistic group decided to make IRMS a core part of their program. In particular, <sup>35</sup>Cl/<sup>37</sup>Cl KIEs seem really powerful: there are a growing number of organometallic reactions which involve chlorine atoms in the key step(s), and for which Cl KIEs might be complementary or superior to more conventional KIEs. I’m envisioning studying <a href=https://www.science.org/doi/10.1126/science.aad6981>transmetallation from Pd(II) chlorides</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.1c13333>chlorine radical-mediated C–H activation</a>, or <a href=https://pubs.acs.org/doi/10.1021/jacs.2c01356>photolysis of Ni(II) chlorides</a>.
</p>

<p>
(And why stop at Cl? According to ThermoFisher, thermal ionization mass spectrometry lets you analyze the isotopic composition of metals with really high accuracy [five decimal places, per their <a href=https://assets.thermofisher.com/TFS-Assets/CMD/brochures/br-30537-triton-xt-tims-br30537-en.pdf>brochure</a>]. Would a <sup>58</sup>Ni/<sup>60</sup>Ni isotope effect be possible to measure? This might provide a handle on some mechanistically ambiguous Ni(III) scenarios, like those reported <a href=https://pubs.acs.org/doi/10.1021/jacs.5b13211>here</a>: is radical trapping or reductive elimination rate- and enantioselectivity-determining?)
</p>

<p>
It doesn’t seem like it’s that easy to start a purely mechanistic research group these days, so maybe this is an unfundable idea. But it seems sad that a technique as powerful for physical (in)organic chemists as IRMS could just fade into obscurity, and I hope somebody finds the time and resources to apply it to modern mechanistic problems.
</p>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230508_aldehyde_eie.html'>Isotope Effects in Aldehyde Protonation</a></h2><i>May 8, 2023</i>
<p>
I’m writing my dissertation right now, and as a result I’m going back through a lot of old slides and references to fill in details that I left out for publication.
</p>

<p>
One interesting question that I’m revisiting is the following: when protonating benzaldehyde, what is the H/D equilibrium isotope effect at the aldehyde proton? This question was relevant for the H/D KIE experiments we conducted in <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c06688>our study of the asymmetric Prins cyclization</a>. (The paper hasn’t gotten much attention, but it’s probably the most “classic” organic chemistry paper I’ve worked on, with a minimum of weird computational details or bizarre analytical techniques.)
</p>

<figure>
  <img class="centered-img" src="../img/20230508_aldehyde_nores.png" style="width:375px;" />
</figure>

<p>
Since the H/D bond isn’t involved in the reaction, we won’t see a primary effect; so we know we have to be thinking in terms of secondary effects. The most common reason to observe a secondary isotope effect is changes in hybridization: sp<sup>3</sup> to sp<sup>2</sup> gives a normal effect, whereas sp<sup>2</sup> to sp<sup>3</sup> gives an inverse effect. From this perspective, it looks like the effect should be unity, since the carbon in question is sp<sup>2</sup> in both structures.
</p>

<p>
Reality, however, disagrees. <a href=https://www.sciencedirect.com/science/article/abs/pii/0040403976800573>Hall and Milosevich</a> report a EIE of 0.94 for benzaldehyde in aq. sulfuric acid, and <a href=https://pubs.acs.org/doi/pdf/10.1021/ja982504r>Gajewski and co-authors</a> compute an EIE of 0.83 for acetaldehyde at the MP2/6-31G(d,p) level of theory. I performed my own calculations at the M06-2X/jun-cc-pVTZ level of theory and obtained an EIE of 0.851 with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>, qualitatively consistent with the above results.
</p>

<p>
Where does this EIE come from? It’s helpful to think of benzaldehyde as possessing multiple resonance forms:
</p>

<figure>
  <img class="centered-img" src="../img/20230508_aldehyde_res.png" style="width:400px;" />
</figure>

<p>
We typically think of the neutral resonance form on the top left, but you can also imagine putting a positive charge on carbon and a negative charge on oxygen to create a zwitterion with a C–O single bond (bottom left). In neutral benzaldehyde, this resonance form is substantially disfavored, but in protonated benzaldehyde it doesn’t look any worse than the “normal” top resonance form!
</p>

<p>
If this is true, we’d expect the C–O bond order to decrease from 2 in neutral benzaldehyde to ~1.5 in protonated benzaldehyde. Indeed, in my calculations the bond length increases from 1.20 Å to 1.28 Å upon protonation—so it seems the double bond character is decreasing! It’s not quite the same as going from sp<sup>2</sup> to sp<sup>3</sup>, but the inverse KIE begins to make sense.
</p>

<p>
(This is purely guesswork, but my guess would be that the differences between the two structures are attenuated in a polar solvent like water. The zwitterionic resonance form of the neutral structure will be stabilized and thus the neutral aldehyde will be more polar, making the change to the oxocarbenium less drastic. This might explain why the measured EIE in water is smaller—although this might also be due to counterion effects, or something completely unrelated.)
</p>

<p>
Let’s go a level deeper. According to <a href=https://pubs.acs.org/doi/abs/10.1021/ja01542a075>Streitwieser</a>, secondary KIEs associated with hyperconjugation originate from the creation or destruction of the c. 800 cm<sup>-1</sup> out-of-plane bending vibrations of Csp<sup>2</sup>–H hydrogens, which are markedly lower in frequency than the c. 1350 cm<sup>-1</sup> bending vibrations associated with Csp<sup>3</sup>–H hydrogens.
</p>

<p>
Raising the frequency of a mode increases the energy required to inhabit the ground vibrational state (the “zero-point energy”)—but deuterium is heavier and vibrates more slowly, meaning that it possesses less ZPE and is less affected by these changes. So when an 800 cm<sup>-1</sup> sp<sup>2</sup> mode transforms to a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode, the ZPE increases, but less for D than for H, so D is favored. Conversely, when a 1350 cm<sup>-1</sup> sp<sup>3</sup> mode transforms to a 800 cm<sup>-1</sup> sp<sup>2</sup> mode, the ZPE decreases, but less for D than for H, so H is favored. (For a more complete explanation, see <a href=https://macmillan.princeton.edu/wp-content/uploads/RRK-KIE.pdf>this presentation by Rob Knowles</a>.)
</p>

<p>
This effect is complicated for benzaldehyde by the fact that the out-of-plane bend of the aldehyde couples to the out-of-plane bend of the phenyl ring, so there are several modes involving out-of-plane vibration of the aldehyde proton. When I compared the out-of-plane bend of the aldehyde H in both structures, I saw only minimal differences: 771, 963, 1040, and 1051 cm<sup>-1</sup> for the neutral species, as compared to 790, 1003, and 1061 cm<sup>-1</sup> for the protonated species. These small differences can’t be responsible for the observed effect.
</p>

<p>
In contrast, the in-plane C–H bend shows a big change—1430 cm<sup>-1</sup> for benzaldehyde, but 1644 cm<sup>-1</sup> for the oxocarbenium (it seems to couple to the C–O stretch; the reduced mass increases from 1.26 amu to 3.52 amu). Applying Streitweiser’s formula for estimating the isotope effect for a specific mode gives a pretty good match:
</p>

<p>
kH/kD ≈ exp(0.187/T * ∆ν) = exp(0.187/298 * (-214)) = 0.87
</p>

<p>
I don’t understand this area well enough to comment on why there’s a change in the in-plane vibrational frequency and not the out-of-plane vibrational frequency, nor do I understand how to deconvolute the effects of mode-to-mode coupling. Nevertheless, this provides a tentative physical rationale for the observation.
</p>

<p>
On a more abstract level, this case study illustrates why isotope effects are such a good tool. Any transformation that perturbs the vibrational frequencies of a given molecule can, in principle, be monitored by isotope effects without affecting the electronic energy surface at all. So, although the precise nature and magnitude of the effect might be hard to predict <i>a priori</i>, it’s not surprising that a transformation as dramatic as protonating a functional group produces a sizable isotope effect.
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230502_startups.html'>Tech As Control Group For Science</a></h2><i>May 2, 2023</i>
<p>
I frequently wonder what the error bars on my life choices are. What are the chances I ended up a chemist? A scientist of any type? Having two children in graduate school? 
</p>

<p>
If I had the ability, I would want to restart the World Simulator from the time I started high school, run a bunch of replicates, and see what happened to me in different simulations. And this wouldn’t just be useful for me personally—there are lots of things in the world that are just as contingent and path-dependent as one’s life choices. What would have happened if Charles the Bold hadn’t died in 1477 and Burgundy had preserved its independence? If the 1787 convention were rerun several times, how might the US Constitution differ? 
</p>

<p>
Sadly, we’ll never know the answer to these questions. But what we can do instead is find cases in which analogous institutions evolved in parallel, and try to learn from the similarities and differences between them. It’s an imperfect substitute for rerunning the World Simulator, but it’s still pretty cool. (This is far from an original idea: see for instance <a href=https://www.amazon.com/Legal-Systems-Very-Different-Ours/dp/1793386722/><i>Legal Systems Very Different From Ours</i></a>.)
</p>

<p>
Lately, I’ve come to think about the tech/startup world as somewhat parallel to academic science in this manner. Why? For one, both tech and academia deal with hard problems that demand obscure/arcane domain-specific knowledge inaccessible to non-experts. (It’s true that the problems are typically scientific in academia and engineering-related in tech, but I’ve argued <a href=https://corinwagen.github.io/public/blog/20230215_science_engineering.html>previously</a> that this distinction is flimsier than it seems.) And in both fields, a few high performers <a href=https://nadia.xyz/top-talent>vastly outperform</a> the rest of the field, be it a “10x engineer” or a Nobel laureate.
</p>

<p>
Startups, like academic labs, are small and agile institutions which face the task of raising money, building a team, selecting a hard yet solvable problem, and finding a solution all within a few years. In both cases, too, there are nonlinear returns to success: moderate success is not much better than failure, pushing founders/assistant professors to be as ambitious as possible.
</p>

<p>
If we accept these two fields as vaguely analogous, what interesting differences can we observe? 
</p>

<h3>
Startups Have Multiple Founders
</h3>

<p>
I’ll quote from <a href=http://www.paulgraham.com/startupmistakes.html>an essay by Paul Graham</a>, founder of Y Combinator and noted startup sage:
</p>

<blockquote>
Have you ever noticed how few successful startups were founded by just one person? Even companies you think of as having one founder, like Oracle, usually turn out to have more. It seems unlikely this is a coincidence.
<br><br>
What's wrong with having one founder? To start with, it's a vote of no confidence. It probably means the founder couldn't talk any of his friends into starting the company with him. That's pretty alarming, because his friends are the ones who know him best.
<br><br>
But even if the founder's friends were all wrong and the company is a good bet, he's still at a disadvantage. Starting a startup is too hard for one person. Even if you could do all the work yourself, you need colleagues to brainstorm with, to talk you out of stupid decisions, and to cheer you up when things go wrong.
</blockquote>

<p>
Ever since I read this, I’ve wondered why no labs ever have multiple PIs. I guess this would mess with the semi-feudal organization of university bureaucracy, but it doesn’t seem intrinsically bad—after all, lots of startups seem to do just fine.
</p>

<h3>
Startup Winners Can’t Be Picked <i>Ex Ante</i>
</h3>

<p>
The VC strategy, as I understand it, is basically “fund a bunch of companies, and one or two of them will make it all worth our while.” This is a little bit different than how universities approach hiring assistant professors: each university will typically hire a small number of professors each year, after much deliberation, and they have a pretty high likelihood of giving them tenure, at least relative to the likelihood of any given startup succeeding. (Basically, <a href=https://www2.nau.edu/lrm22/lessons/r_and_k_selection/r_and_k.html>startups are r-selected, whereas academic labs are K-selected.</a>)
</p>

<p>
There are a lot of reasons why this might be. For one, faculty members aren’t just trying to pick a winner but also their future colleague, so personal considerations probably matter more. Failure in science seems more cruel, too: while a failed startup founder can often negotiate the “sale” of their company and parlay that into new jobs and the constant churn of tech means that there are always new openings for talented ex-startup employees, a lab that doesn’t get tenure takes a toll on professor and students alike. 
</p>

<p>
A hypothesis for why the success rate for new labs is so much higher than the success rate for new businesses is that many labs only succeed a little bit. They don’t actually achieve what they dreamed about in their initial proposals, but they pivot and accrue enough publications and cachet to earn tenure nevertheless. In business, it seems harder to succeed a little bit—the market is a harsher critic than one’s peers.
</p>

<h3>
Founders Should Be Focused
</h3>

<p>
Paul Graham again, this time talking about <a href=http://www.paulgraham.com/ramenprofitable.html>the dangers of fundraising</a>:
</p>

<blockquote>
Raising money is terribly distracting. You're lucky if your productivity is a third of what it was before. And it can last for months.
<br><br>
I didn't understand (or rather, remember) precisely why raising money was so distracting till earlier this year. I'd noticed that startups we funded would usually grind to a halt when they switched to raising money, but I didn't remember exactly why till YC raised money itself. We had a comparatively easy time of it; the first people I asked said yes; but it took months to work out the details, and during that time I got hardly any real work done. Why? Because I thought about it all the time.
</blockquote>

<p>
The broader conclusion, from this and other essays, is that any distractions from the core mission of the startup are very dangerous, and should be avoided at all costs. This is very different from the lifestyle of new PIs, who are typically juggling departmental responsibilities, writing a curriculum, lecturing for the first time, and writing grants all while trying to get their lab up and running.
</p>

<h3>
Talent Acquisition Is Crucial
</h3>

<p>
In tech, people obsess about recruiting the best people possible—I reviewed <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>a whole book about this</a> last year. Hiring bad programmers is #6 on PG’s <a href=http://www.paulgraham.com/startupmistakes.html>list of mistakes that kill startups</a>, and there seems to be a general consensus that a great company takes great engineers, no matter what. 
</p>

<p>
In contrast, professors don’t have full control over whom they hire (for graduate students), making recruiting much harder. Graduate students are selected through a complex two-stage system involving admission to a school and then a subsequent group-joining process (and new assistant professors sometimes aren’t even around for the first of these stages). You can obviously try to coax talented students to work for you, but the pool of accepted students interested in your subfield might be tiny, and they might all prefer to work for an established group… 
</p>

<p>
(Plus, there’s not a good way to reward top performers in academia. All graduate students are equal, at least on paper—you can’t give someone a year-end bonus, or a promotion.)
</p>

<p>
A nice concrete example of this is how professors <a href=https://www.jefftk.com/p/hiring-programmers-in-academia>struggle to hire competent programmers</a>, even as research scientists—they aren’t allowed to pay enough to match market rates, even when the expense would be well worth the money. To quote <a href=https://acoup.blog/2023/04/28/collections-academic-ranks-explained-or-what-on-earth-is-an-adjunct/?s=03#easy-footnote-bottom-8-18391>Bret Devereaux</a>: “academic hiring, to be frank, is not conducted seriously” (he’s discussing the humanities, but the point stands). 
</p>

<h3>
Successful Startups Grow
</h3>

<p>
As a startup succeeds, it grows: while a seed-stage startup typically has &lt;15 people, startups at Series A often have 20–40, and startups at Series B–C might have as many as 300 employees (<a href=https://www.quora.com/How-many-employees-do-early-stage-startups-have>one ref</a>; rough numbers broadly consistent with other sources). Good companies grow, while bad ones die. 
</p>

<p>
In contrast, it’s rare for even the most successful US academic labs to grow past 30 people (although <a href=https://go.gale.com/ps/i.do?id=GALE%7CA195680544&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=00280836&p=HRCA&sw=w&userGroupName=mlin_oweb&isGeoAuthType=true>it occasionally happens</a>), limiting the reach of top-performing professors. While a huge proportion of tech employees work for the best companies (Google, Meta, Amazon, etc), only a very small number of students work for the best professors. 
</p>

<h3>
Concluding Thoughts
</h3>

<p>
The imperfect nature of the analogy means that some of these points might not be useful in a normative sense: universities are not really optimized to produce research as efficiently as possible, and maybe that’s fine. Likewise, startups aren’t optimized to produce unprofitable research or train future scientists, even if these activities may in the long run be beneficial. (This is <a href=https://corinwagen.github.io/public/blog/20220728_consulting_as_grad_school.html#:~:text=Accordingly%2C%20the%20government%20sponsors%20research%20into%20interesting%20problems%20with%20uncertain%20timeframes%20to%20do%20what%20the%20free%20market%20cannot>why basic science is considered a public good</a>, and why the government funds it at all!)
</p>

<p>
Nevertheless, I think there’s a lot that scientists can learn from startups. There is a whole army of people working to solve challenging technical problems in the most efficient way, and it’d be prudent to study the wisdom that emerges. 
</p>

<i>
Thanks to Ari Wagen and Jacob Thackston for reading drafts of this piece. 
</i>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230427_journals.html'>Do We Still Need Journals?</a></h2><i>April 27, 2023</i>
<figure>
  <img class="centered-img" src="../img/20230427_compass.png" style="width:550px;" />
  <figcaption> 
    This image inspired by the rightly famous <a href=https://images.are.na/eyJidWNrZXQiOiJhcmVuYV9pbWFnZXMiLCJrZXkiOiIxMDI4NDUyNy9vcmlnaW5hbF80ZmNlOTY3ODYxZTFiYWEwZjkwODMxYzlkZTNhNjhhZS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjE4MDAsImhlaWdodCI6MTgwMCwiZml0IjoiaW5zaWRlIiwid2l0aG91dEVubGFyZ2VtZW50Ijp0cnVlfSwid2VicCI6eyJxdWFsaXR5Ijo5MH0sImpwZWciOnsicXVhbGl0eSI6OTB9LCJyb3RhdGUiOm51bGx9fQ==?bc=0>original</a>.
  </figcaption>
</figure>

<p>
One of the most distinctive parts of science, relative to other fields, is the practice of communicating findings through peer-reviewed journal publications. Why do scientists communicate in this way? As I see it, scientific journals provide three important services to the community: 
</p>

<ol>
  <li>
    Journals help scientists communicate; they disseminate scientific results to a broad audience, both within one’s community and to a broader scientific audience.
  </li>
  <li>
    Through the peer review process, journals ensure scientific correctness and keep standards high. You never know which of your scientific adversaries might be scrutinizing your work for flaws, so you’re incentivized to do the best job possible.
  </li>
  <li>
    Journals, and peer review, help scientists to read high-quality, impactful work by filtering out low-impact papers (even those which are scientifically correct). This makes journals a somewhat “fair” way to score the importance of publications without being a subject-matter expert; I might not know what’s happening these days with topological quantum materials, but if I see three <i>Science</i>/<i>Nature</i> papers on a CV, I’ll certainly pay attention! 
  </li>
</ol>

<p>
(There are certainly other services that journals provide, like DOIs and typesetting, but these seem much less important to me.)
</p>

<p>
In this post, I want to (1) discuss the problems with scientific journals today, (2) briefly summarize the history of journals and explain how they came to be the way they are today, and (3) imagine how journals might evolve in the coming decades to adapt to the changing landscape of science. My central claim is that <u>the scientific journal, as defined by the above criteria, has only existed since about the 1970s, and will probably not exist for very much longer—and that’s ok.</u> (I’ll also try and explain the esoteric meme at the top.)
</p>

<h2>
1. Journals Present
</h2>

<p>
Many people are upset about scientific journals today, and for many different reasons. 
</p>

<h3>
1.1 Profits and Paywalls
</h3>

<p>
The business model of scientific journals is, to put it lightly, unusual. <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Writing for <i>The Guardian</i></a>, Stephen Buranyi describes how “scientific publishers manage to duck most of the actual costs” that normal magazines incur by outsourcing their editorial duties to scientists: the very people who both write and read the articles:
</p>

<blockquote>
It is as if the New Yorker or the Economist demanded that journalists write and edit each other’s work for free, and asked the government to foot the bill. Outside observers tend to fall into a sort of stunned disbelief when describing this setup. A 2004 parliamentary science and technology committee report on the industry drily observed that “in a traditional market suppliers are paid for the goods they provide”. A 2005 Deutsche Bank report referred to it as a “bizarre” “triple-pay” system, in which “the state funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product”.
</blockquote>

<p>
And this cost-dodging is very successful: scientific journals are a huge moneymaker, with Elsevier (one of the largest publishers) having a margin <a href=https://tidsskriftet.no/en/2020/08/kronikk/money-behind-academic-publishing>in excess of 30%</a>, and ACS’s “information services” (publication) division close behind, <a href=https://www.acs.org/content/dam/acsorg/about/aboutacs/financial/2022-audited-financial-statements.pdf>with a profit margin of 27%</a>.
</p>

<p>
The exorbitant fees charged by journals, and the concomitantly huge profits they earn, have led to increasing pushback against the paywall-based status quo. The Biden administration <a href=https://www.nature.com/articles/d41586-022-02351-1>has called</a> for all government-funded research to be free-to-read without any embargo by 2025, and other countries have been pursuing similar policies <a href=https://www.chemistryworld.com/news/eu-and-uk-bitten-by-the-open-access-bug/5236.article>for some time</a>. Similarly, <a href=https://news.mit.edu/2020/guided-by-open-access-principles-mit-ends-elsevier-negotiations-0611>MIT</a> and <a href=https://www.universityofcalifornia.edu/news/why-uc-split-publishing-giant-elsevier>the UC system</a> recently terminated their subscriptions to Elsevier over open-access issues. (And the rise of SciHub means that, even without a subscription, most scientists can still read almost any article they want—threatening to completely destroy the closed-access model.) 
</p>

<p>
In response to this pressure, journals have begun offering open-access alternatives, where the journal’s fees are paid by the submitting author rather than the reader. While in theory this is a solution to this problem, in practice the fees for authors are so high that it’s not a very good solution. The board of editors of NeuroImage <a href=https://www.nature.com/articles/d41586-023-01391-5>recently resigned</a> over their journal’s high open-access fees—and they’re <a href=https://www.nature.com/articles/d41586-019-00135-8>not the first</a> board of editors to do this. As <a href=https://www.vox.com/the-highlight/2019/6/3/18271538/open-access-elsevier-california-sci-hub-academic-paywalls>a 2019 <i>Vox</i> summary</a> put it: “Publishers are still going to get paid. Open access just means the paychecks come at the front end.” 
</p>

<figure>
  <img class="centered-img" src="../img/20230427_oa_tweet.png" style="width:475px;" />
</figure>

<h3>
1.2 Efficacy of Peer Review
</h3>

<p>
In parallel, the <a href=https://en.wikipedia.org/wiki/Replication_crisis>“replication crisis”</a> has led to growing skepticism about the value of peer review. In his article <a href=https://www.experimental-history.com/p/the-rise-and-fall-of-peer-review>“The Rise and Fall of Peer Review,”</a> Adam Mastroianni describes how experiments to measure its value have yielded dismal outcomes:
</p>

<blockquote>
Scientists have run studies where they deliberately add errors to papers, send them out to reviewers, and simply count how many errors the reviewers catch. Reviewers are pretty awful at this. In <a href=https://www.sciencedirect.com/science/article/pii/S019606449870006X?casa_token=D5MklJnYP0MAAAAA:CzyYl8VLg-M-bvKIHxl7vWCIh8AVrkTUizQ9LZPZWh_eVT5jLf3WJlGaJQzYCsHMraF5Fh8mqw>this study</a> reviewers caught 30% of the major flaws, in <a href=https://jamanetwork.com/journals/jama/article-abstract/187748>this study</a> they caught 25%, and in <a href=https://journals.sagepub.com/doi/full/10.1258/jrsm.2008.080062>this study</a> they caught 29%. These were critical issues, like “the paper claims to be a randomized controlled trial but it isn’t” and “when you look at the graphs, it’s pretty clear there’s no effect” and “the authors draw conclusions that are totally unsupported by the data.” Reviewers mostly didn’t notice.
</blockquote>

<p>
While the worst of the replication crisis seems to be contained to the social sciences, my own field—chemistry—is by no means exempt. As I wrote <a href=https://corinwagen.github.io/public/blog/20221214_against_carbon_nmr.html>previously</a>, “<a href=https://pubs.acs.org/doi/10.1021/acscentsci.2c00325>elemental analysis doesn’t work</a>, <a href=https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e>integration grids cause problems</a>, and even <a href=http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html>reactions from famous labs can’t be replicated</a>.” There are a lot of bad results in the scientific literature, even in top journals—I don’t think many people in the field actually believe that a generic peer-reviewed publication is guaranteed to be correct.
</p>

<figure>
  <img class="centered-img" src="../img/20230427_pr_tweet.png" style="width:475px;" />
</figure>

<p>
And the process of soliciting peer reviews is by no means trivial: prominent professors are commonly asked to peer review tons of articles as an unpaid service to the community, which isn’t really rewarded in any way. As the number of journals and publications grows faster than the number of qualified peer reviewers, burnout can result:
</p>

<figure>
  <img class="centered-img" src="../img/20230427_bo_tweet.png" style="width:475px;" />
</figure>

<h3>
1.3 Preprint Servers
</h3>

<p>
The rise of preprint servers like arXiv, BioRxiv, and ChemRxiv also means journals aren’t necessary for communication of scientific results. More and more, preprints dominate discussions of cutting-edge science, while actual peer-reviewed publications lag months to years behind. 
</p>

<p>
While in theory preprints aren’t supposed to be viewed as scientifically authoritative—since they haven’t been reviewed—in practice most preprints are qualitatively identical to the peer-reviewed papers that they give rise to. <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001285>A retrospective analysis</a> of early COVID preprints found that the vast majority of preprints survived peer review without any substantive changes to their conclusions (although this might be biased by the fact that the worst pre-prints will never be accepted at all.)
</p>

<p>
If this is the case, why bother with journals at all? To a growing degree this seems to be the norm in CS and CS-adjacent fields: the landmark Google transformer paper from 2017, <a href=https://arxiv.org/pdf/1706.03762.pdf>“Attention Is All You Need,”</a> is still just a PDF on arXiv six years later, despite being potentially the most impactful discovery of the 2010s. Similarly, UMAP, <a href=https://corinwagen.github.io/public/blog/20230417_dimensionality_reduction.html>which I discussed last week,</a> is also just <a href=https://arxiv.org/abs/1802.03426>hanging out on arXiv</a>, no peer-reviewed publication in sight. Still, in chemistry and other sciences we’re expected to publish in “real journals” if we want to graduate or get jobs. 
</p>

<h3>
1.4 Impact-Neutral Reviewing
</h3>

<p>
An implicit assumption of the scientific journal is that high-impact publications can be distinguished from low-impact publications without the benefit of hindsight. Yet many of the most impactful scientific discoveries—like the Krebs cycle, the weak interaction, lasers, continental drift, and CRISPR—<a href=https://nintil.com/discoveries-ignored>were rejected</a> when first submitted to journals. How is this possible? 
</p>

<p>
I’d argue that peer review creates a bias towards incrementalism. It’s easy to see how an improvement over something already known is significant; it’s perhaps harder to appreciate the impact of a field-defining discovery, or to believe that such a result could even be possible. To quote Antonio Garcia Martinez on startups: “If your idea is any good, it won’t get stolen, you’ll have to jam it down people’s throats instead.” True zero-to-one thinking can provoke a strong reaction from the establishment, and rarely a positive one. 
</p>

<p>
(It’s worth noting that some of the highest profile organic chemistry papers from 2022 were new takes on old, established, reactions: <a href=https://pubs.acs.org/doi/full/10.1021/jacs.2c05648>Parasram</a> and <a href=https://www.nature.com/articles/s41586-022-05211-0>Leonori’s</a> “ozonolysis without ozone” and <a href=https://www.science.org/doi/10.1126/science.abo6443>Nagib’s</a> “carbenes without diazoalkanes.” I love both papers—but I also think it’s easier for audiences to appreciate why “ozonolysis without ozone” is a big deal than to process an entirely new idea.)
</p>

<p>
Even for more quotidian scientific results, the value of impact-based peer review is limited. Matt Clancy at <a href=https://www.newthingsunderthesun.com/pub/nc5341ua/release/3><i>New Things Under the Sun</i></a> writes that, for preprints, paper acceptance is indeed correlated with number of eventual citations, but that the correlation is weak: reviewers seem to be doing better than random chance, but worse than we might hope. (Similar results emerge when studying the efficacy of peer review for grants.) On the aggregate, it does seem true that the average <i>JACS</i> paper is better than the average <i>JOC</i> paper, but the trend is far from monotonic.
</p>

<p>
These concerns aren’t just mine; indeed, a growing number of scientists seek to reject impact-based refereeing altogether. The <a href=http://proteinsandwavefunctions.blogspot.com/2016/01/writing-impact-neutral-review.html?m=1&s=03>“impact-neutral reviewing”</a> movement thinks that papers should be evaluated only on the basis of their scientific correctness, not their perceived potential impact. Although I wouldn’t say this is a mainstream idea, journals like <a href=https://journals.plos.org/plosone/s/journal-information><i>PLOS One</i></a>, <a href=https://blog.frontiersin.org/2015/12/21/4782/><i>Frontiers</i></a>, and <a href=https://elifesciences.org/inside-elife/54d63486/elife-s-new-model-changing-the-way-you-share-your-research><i>eLife</i></a> have adopted versions of it, and perhaps more journals will follow in the years to come.
</p>

<p>
Taken together, these anecdotes demonstrate that all three pillars of the modern scientific journal—communication, peer review, and impact-based sorting—are threatened today. 
</p>

<h2>
2. Journals Past
</h2>

<p>
How did we get here? 
</p>

<p>
The importance of journals as a filter for low-quality work is a modern phenomenon. Of course, editors have always had discretion over what to publish, but until fairly recently <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>the total volume of papers was much lower</a>, meaning that it wasn’t so vital to separate the wheat from the chaff. In fact, <a href=https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science>Stephen Buranyi</a> attributes the modern obsession with impact factor and prestige to the founding of <i>Cell</i> in 1974:
</p>

<blockquote>
[<i>Cell</i>] was edited by a young biologist named Ben Lewin, who approached his work with an intense, almost literary bent. Lewin prized long, rigorous papers that answered big questions – often representing years of research that would have yielded multiple papers in other venues – and, breaking with the idea that journals were passive instruments to communicate science, he rejected far more papers than he published….
<br><br>
Suddenly, where you published became immensely important. Other editors took a similarly activist approach in the hopes of replicating <i>Cell</i>’s success. Publishers also adopted a metric called “impact factor,” invented in the 1960s by Eugene Garfield, a librarian and linguist, as a rough calculation of how often papers in a given journal are cited in other papers. For publishers, it became a way to rank and advertise the scientific reach of their products. The new-look journals, with their emphasis on big results, shot to the top of these new rankings, and scientists who published in “high-impact” journals were rewarded with jobs and funding. Almost overnight, a new currency of prestige had been created in the scientific world.
</blockquote>

<p>
As Buranyi reports, the changes induced by <i>Cell</i> rippled across the journal ecosystem. The acceptance rate at <i>Nature</i> <a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>dropped from 35% to 13%</a> over the following decade-and-a-half (coincidentally also the years when peer review was introduced), making journal editors the “kingmakers of science” (Buranyi). 
</p>

<p>
Peer review is also a modern addition. In <a href=https://physicstoday.scitation.org/doi/10.1063/PT.3.3463><i>Physics Today</i></a>, Melissa Baldwin recounts how peer review only became ubiquitous following a series of contentious House subcommittee hearings in 1974 that questioned the value of NSF-funded science:
</p>

<blockquote>
Spending on both basic and applied research had increased dramatically in the 1950s and 1960s—but when doubts began to creep in about the public value of the work that money had funded, scientists were faced with the prospect of losing both public trust and access to research funding. Legislators wanted publicly funded science to be accountable; scientists wanted decisions about science to be left in expert hands. Trusting peer review to ensure that only the best and most essential science received funding seemed a way to split the difference.
</blockquote>

<p>
Our expectation that journals ought to validate the correctness of the work they publish, too, is quite modern. Baldwin again:
</p>

<blockquote>
It also seems significant that refereeing procedures were not initially developed to detect fraud or to ensure the accuracy of scientific claims…. Authors, not referees, were responsible for the contents of their papers. It was not until the 20th century that anyone thought a referee should be responsible for the quality of the scientific literature, and not until the Cold War that something had to be peer-reviewed to be seen as scientifically legitimate.
</blockquote>

<p>
If journals didn’t do peer review and they didn’t do (as much) impact-based filtering before the 1970s, what <i>did</i> they do? The answer is simple: communication. Scientists started communicating in journals because writing books was too slow, and it was important that they be able to share results and get feedback on their ideas quickly. This was a founding aim of <i>Nature</i>:
</p>

<blockquote>
…to aid Scientific men themselves, by giving early information of all advances made in any branch of Natural knowledge throughout the world, and by affording them an opportunity of discussing the various Scientific questions which arise from time to time.
</blockquote>

<p>
Although perhaps underwhelming to a modern audience, this makes sense. Scientists back in the day didn’t have preprints, Twitter, or Zoom—so they published journal articles because it was “one of the fastest ways to bring a scientific issue or idea to their fellow researchers’ attention” (<a href=https://astralcodexten.substack.com/p/your-book-review-making-nature>ref</a>), not because it would look good on their CV. Journals became “the place to discuss big science questions” among researchers, and frequently featured acrimonious and public disputes between researchers—far from celebrated storehouses of truth, journals were simply the social media of pre-communication age scientists. 
</p>

<h2>
3. Journals’ Future?
</h2>

<p>
So, is the solution “reject modernity, embrace tradition”? Should we go back to the way things used to be and stop worrying about whether published articles are correct or impactful?
</p>

<p>
Anyone who’s close to the scientific publishing process knows that this would be ridiculous and suicidal. We’ve come a long way from the intimate scientific community of 18th-century England, where scientists had reputations to uphold and weren’t incentivized to crank out a bunch of <i>Tet. Lett.</i> papers. Like it or not, today’s scientists have been trained to think of their own productivity in terms of publications, and the editorial standards we have today are just barely keeping a sea of low-quality crap at bay (cf. <a href=https://academic.oup.com/gigascience/article/8/6/giz053/5506490>Goodhart’s Law</a>). Sometimes it feels like peer reviewers are the only people who are willing to give me honest criticism about my work—if we get rid of them, what then? 
</p>

<p>
We can understand the changes in journals by borrowing some thinking from economics: as the scale of communities increases, the norms and institutions of the community must progress from informal to formal. This process <a href=https://www.cambridge.org/core/journals/journal-of-economic-history/article/abs/development-of-property-rights-on-frontiers-endowments-norms-and-politics/23F4D5DB23AE6EC415ADF049F6CD0B54>has been documented nicely</a> for the development of property rights on frontiers: at first, land is abundant, and no property rights are necessary. Later on, inhabitants develop a <i>de facto</i> system of informal property rights to mediate disputes—and still later, these <i>de facto</i> property rights are transformed into <i>de jure</i> property rights, raising them to the status of law. Communities with 10,000 people need more formal institutions than communities with 100 people. 
</p>

<p>
If we revisit the history of scientific journals, we can see an analogous process taking place. Centuries ago there were relatively few scientists, and so journals could simply serve as a bulletin board for whatever these scientists were up to. As the scale and scope of science expanded in the late 20th century, peer review became a way to deal with the rising number of scientific publications, sorting the good from the bad and providing feedback. Today, as the scale of science continues to increase and the communication revolution renders many of the historical functions of journals moot, it seems that journals will have to change again, to adapt to the new needs of the community.
</p>

<p>
To the extent that this post has a key prediction, it’s this: <u>scientific journals are going to change a lot in the decade or two to come.</u> If you’re a scientist today—even a relatively venerable one—you’ve lived your whole career in the post-peer review era, and so I think people have gotten used to the status quo. Submitting papers to journals, getting referee reviews, etc are part of what we’re taught “being a scientist” means. But this hasn’t always been true, and it may not be true within your lifetime! 
</p>

<p>
Sadly, I don’t really have a specific high-confidence prediction for how journals will change, or how they should change. Instead, I want to sketch out nine little vignettes of what could happen to journals, good or bad. These options are neither mutually exclusive nor collectively exhaustive; it’s meant simply as an exercise in creativity, and to provide a little basis set with which to imagine the future. 
</p>

<p>
I’ll repost the initial image of the post here, for ambiance, and then walk through the possibilities. 
</p>

<figure>
  <img class="centered-img" src="../img/20230427_compass.png" style="width:550px;" />
</figure>

<h3>
3.1 The Journal as Bastion of Verified Truth
</h3>

<p>
One scenario is that journals, no longer being needed to distribute results widely, will double down on their role as defenders of scientific correctness. To a much greater degree, journals will focus on only publishing truly correct work, and thus make peer review their key “value add.” This is already being done post-replication crisis in some fields; Michael Nielsen and Kanjun Qiu describe the rise of “Registered Reports” in their <a href=https://scienceplusplus.org/metascience/>essay on metascience</a>:
</p>

<blockquote>
The idea [behind Registered Reports] is for scientists to design their study in advance: exactly what data is to be taken, exactly what analyses are to be run, what questions asked. That study design is then pre-registered publicly, and before data is taken the design is refereed at the journal. The referees can't know whether the results are "interesting", since no data has yet been taken. There are (as yet) no results! Rather, they're looking to see if the design is sound, and if the questions being asked are interesting – which is quite different to whether the answers are interesting! If the paper passes this round of peer review, only then are the experiments done, and the paper completed.
</blockquote>

<p>
This makes more sense for medicine or psychology than it does for more exploratory sciences—if you’re blundering around synthesizing novel low-valent Bi complexes, it’s tough to know what you’ll find or what experiments you’ll want to run! But there are other ways we could make science more rigorous, if we wanted to.
</p>

<p>
A start would be requiring original datafiles (e.g. for NMR spectra) instead of just providing a PDF with images, and having reviewers examine these data. ACS has made some moves in this direction (<a href=https://publish.acs.org/publish/data_policy>e.g.</a>), although to my knowledge no ACS journal yet requires original data. One could also imagine requiring all figures to be linked to the underlying data, with code supplied by the submitting group (like a Jupyter notebook). A more drastic step would be to require all results to be independently reproduced by another research group, <a href=http://www.orgsyn.org/about.aspx>like <i>Organic Syntheses</i> does</a>. 
</p>

<p>
These efforts would certainly make the scientific literature more accurate, but at what cost? Preparing publications already consumes an excessive amount of time and energy, and making peer review stricter might just exacerbate this problem. Marc Edwards and Siddhartha Roy discuss this in a nice <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5206685/>perspective</a> on perverse incentives in modern science:
</p>

<blockquote>
Assuming that the goal of the scientific enterprise is to maximize true scientific progress, a process that overemphasizes quality might require triple or quadruple blinded studies, mandatory replication of results by independent parties, and peer-review of all data and statistics before publication—such a system would minimize mistakes, but would produce very few results due to overcaution.
</blockquote>

<p>
It seems good that there are some “overcautious” journals, like <i>Org. Syn.</i>, but it also seems unlikely that all of science will adopt this model. In fact, a move in this direction might create a two-tiered system: some journals would adopt stringent policies, but there’s a huge incentive for some journals to defect and avoid these policies, since authors are lazy and would prefer not to do extra work. It seems unlikely that all of science could realistically be moved to a “bastion of truth” model in the near future, although perhaps we could push the needle in that direction.
</p>

<h3>
3.2 The Journal as Guild-Approved Periodical
</h3>

<p>
If peer review is so vital, why not make it a real career? Imagine a world in which journals like <i>Nature</i> and <i>Science</i> have their own in-house experts, recruited to serve as professional overseers and custodians of science. Instead of your manuscript getting sent to some random editor, and thence to whomever deigns to respond to that editor’s request for reviewers, your manuscript would be scrutinized by a team of hand-picked domain specialists. This would certainly cost money, but journals seem to have a bit of extra cash to spare.
</p>

<p>
I call this scenario the “guild-approved periodical” because the professionals who determined which papers got published would essentially be managers, or leaders, of science—they would have a good amount of power over other scientists, to a degree that seems uncommon today. Thus, this model would amount to a centralization of science: if <i>Nature</i> says you have to do genomics a certain way, you have to do it that way or <i>Nature</i> journals won’t publish your work! I’m not sure whether this would be good or bad. 
</p>

<p>
(It is a little funny that the editors of high-tier journals—arguably the most powerful people in their field—are chosen without the knowledge or consent of the field, through processes that are completely opaque to rank-and-file scientists. To the extent that this proposal allows scientists to choose their own governance, it might be good.)	
</p>

<h3>
3.3 The Journal as Living Knowledge Aggregator
</h3>

<p>
This scenario envisions a world in which “publications” are freed from the tyranny of needing to be complete at a certain point. While that was true in the days when you actually got a published physical issue in the mail, it’s not necessary in the Internet age! Instead, one can imagine a dynamic process of publishing, where a journal article is continually updated in response to new data. 
</p>

<p>
A 2020 article in <a href=https://www.facetsjournal.com/doi/10.1139/facets-2019-0012><i>FACETS</i></a> proposes exactly this model:
</p>

<blockquote>
The paper of the future may be a digital-only document that is not only discussed openly after the peer-review process but also regularly updated with time-stamped versions and complementary research by different authors… Living systematic reviews are another valuable way to keep research up to date in rapidly evolving fields. The papers of the future that take the form of living reviews can help our understanding of a topic keep pace with the research but will also add complexities. <i>(citations removed from passage)</i>
</blockquote>

<p>
The idea of the living systematic review is being tried out by the <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031><i>Living Journal of Computational Molecular Science</i></a>, which (among other things) has published a 60-page <a href=https://livecomsjournal.org/index.php/livecoms/article/view/v1i1e2031>review of enhanced sampling methods in MD</a>, which will continue being updated as the field evolves.
</p>

<p>
These ideas are cool, but I wonder what would happen if more research became “living.” Disputes and acrimony are part of the collective process of scientific truth-seeking. What will happen if bitter rivals start working on the same “living” publications—who will adjudicate their disputes? 
</p>

<p>
Wikipedia manages to solve this problem through a plethora of editors, who can even lock down particularly controversial pages, and perhaps editors of living journals will assume analogous roles. But the ability of our collective scientific intelligence to simultaneously believe contradictory ideas seems like a virtue, not a vice, and I worry that living journals will squash this. 
</p>

<p>
An even thornier question is who adjudicates questions of impact. The enhanced sampling review linked above has over 400 references, making it a formidable tome for a non-expert like myself. There’s a lot of merit in a non-comprehensive and opinionated introduction to the field, which takes some subjective editorial liberties, but it’s not clear to me how that would work in a collaborative living journal. What’s to stop me from linking to my own papers everywhere? 
</p>

<p>
(I’m sure that there are clever organizational and administrative solutions to these problems; I just don’t know what they are.)
</p>

<h3>
3.4 The Journal as Curated Scientific Vision
</h3>

<p>
If “objective impact” is so hard to determine fairly, why not just accept that we’re basically just subjectively scoring publications based on how much we like them, and abandon the pretense of objectivity? One can imagine the rise of a new kind of figure: the editor with authorial license, who has a specific vision for what they think science should look like and publishes work in keeping with that vision. The role is as much aesthetic as it is analytic. 
</p>

<p>
There’s some historical precedent for this idea—Eric Gilliam’s written about how Warren Weaver, a grant director for the Rockefeller Foundation, essentially <a href=https://freaktakes.substack.com/p/a-report-on-scientific-branch-creation>created the field of molecular biology</a> <i>ex nihilo</i> by following an opinionated thesis about what work ought to be funded. Likewise, one can envision starting a journal as an act of community-building, essentially creating a Schelling point for like-minded scientists to collaborate, share results, and develop a common approach to science.
</p>

<p>
We can see hints of this today: newsletters like <a href=https://pubs.acs.org/doi/10.1021/acs.oprd.3c00060>“Some Items of Interest to Process Chemists”</a> or Elliot Hershberg’s <a href=https://centuryofbio.substack.com/about><i>Century of Bio</i> Substack</a> highlight a particular vision of science, although they haven’t quite advanced to the stage of formally publishing papers themselves. But perhaps it will happen soon; new movements, like molecular editing or digital chemistry, might benefit from forming more tightly-knit communities. 
</p>

<h3>
3.5 The Journal as Post Hoc Impact Archive
</h3>

<p>
If preprints take over every field of science as thoroughly as they have computer science, journals may find themselves almost completely divorced from the day-to-day practice of science, for better or for worse. Papers might still be submitted to journals, and the status of the journal might still mean something, but it wouldn’t be a guess anymore—journals could simply accept the advances already proven to be impactful and basically just publish a nicely formatted “version of record,” like a scientific Library of Congress. 
</p>

<p>
This is essentially equivalent to the “publish first, curate second” proposal of <a href=https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000116>Stern and O’Shea</a>—preprints eliminate the need for journals to move quickly, so we can just see what work the community finds to be best and collect that into journals. The value of journals for specialists, who already need to be reading a large fraction of the papers in their area, would be much lower—journals would mainly be summarizing a field’s achievements for those out-of-field. In this scenario, “many specialized journals that currently curate a large fraction of the literature will become obsolete.”
</p>

<p>
(This already happens sometimes; I remember chuckling at the 2020 Numpy <i>Nature</i> <a href=https://www.nature.com/articles/s41586-020-2649-2>paper</a>. Numpy isn’t successful because it was published in <i>Nature</i>; Numpy got into <i>Nature</i> because it was already successful.)
</p>

<h3>
3.6 The Journal as Antediluvian Status Symbol
</h3>

<p>
Pessimistically, one can imagine a world in which journal publications still carry weight with the “old guard” and certain sentimental types, but the scientific community has almost completely moved to preprints for day-to-day communication. In this scenario, one might still have to publish journal articles to get a job, but it’s just a formality, like a dissertation: the active work of science is done through preprints. Like Blockbuster, journals might limp along for some time, but their fate is pretty much sealed.
</p>

<h3>
3.7 The Journal as Philanthropic Pravda
</h3>

<p>
Another reason why journals might persist in a world driven by preprints is the desire of philanthropic agencies to appear beneficent. If a certain organization, public or private, is giving tens of millions of dollars to support scientific progress, the only real reward it can reap in the short term is the prestige of having its name associated with a given discovery. Why not go one step further and control the means of publication?
</p>

<p>
In this <i>Infinite Jest</i>-like vision, funding a certain project buys you the right to publish its results in your own journal. We can imagine <i>J. Pfizer-Funded Research</i> and <i>J. Navy Research</i> competing to fund and publish the most exciting work in a given area, since no one wants to sponsor a loser. (Why stop there? Why not name things after corporate sponsors? We could have the Red Bull–Higgs Boson, or the Wittig–Genentech olefination.)
</p>

<p>
As discussed at the beginning of this article, the government “funds most research, pays the salaries of most of those checking the quality of research, and then buys most of the published product.” There’s a certain simplicity in a funding agency just taking over the whole process, but  I doubt this would be good for scientists. Unifying the roles of funder, publisher, and editor would probably lower the agency of actual researchers to an untenably low level. 
</p>

<h3>
3.8 The Journal as Rent-Seeking Data Troll
</h3>

<p>
Another depressing scenario is one in which journals cease contributing to the positive progress of science, and start essentially just trying to monetize their existing intellectual property. As ML and AI become more important, legal ownership of data rights will presumably increase in economic value, and one can easily imagine the Elseviers of the world vacuuming up any smaller journals they can and then charging exorbitant fees for access to their data. (Goodbye, Reaxys…)
</p>

<p>
I hope this doesn’t happen. 
</p>

<h3>
3.9 No Journals; Just an Anarchic Preprint Lake
</h3>

<p>
The obvious alternative to these increasingly far-fetched scenarios is also the simplest; we get rid of journals all together, and—just like in the 1700s—rely solely on communication-style preprints on arXiv, bioRxiv, ChemRxiv, etc. This has been termed a “preprint lake,” in analogy to <a href=https://en.wikipedia.org/wiki/Data_lake>“data lakes.”</a> 
</p>

<p>
To help scientists make sense of the lake, one can envision some sort of preprint aggregator: a Reddit or Hacker News for science, which sorts papers by audience feedback and permits <a href=https://en.wikipedia.org/wiki/PubPeer>PubPeer</a>-type public comments on the merits and shortcomings of each publication. The home page of Reddit-for-papers could serve as the equivalent to <i>Science</i>; the chemistry-specific subpage, the equivalent to <i>JACS</i>. Peer review could happen in a decentralized fashion, and reviews would be public for all to see.
</p>

<p>
There’s an anarchic appeal to this proposal, but it has potential drawbacks too:
</p>

<ol type="i">
  <li>
  For those not immersed in a given field, it’s very difficult to know what’s good research and what isn’t. This is doubly true for non-scientists—what will become of high-school students trying to write papers?
  </li>

  <li>
    Existing status symbols will become more important absent journal status. To quote <a href=https://luispedro.substack.com/p/against-publication-lakes-glam-journals?s=03>Luis Pedro Coelho</a>:

    <blockquote>
    I mostly read preprints by people whose names I already recognize. When thousands of papers are thrown into the “level playing field” of biorxiv, pre-existing markers of prestige end up taking an even greater role.
    </blockquote>

    This presumably will disadvantage up-and-coming scientists, or scientists without access to existing networks of prestige. That being said, one might make the same arguments for the Internet, and the real effect seems to have been exactly the opposite! So I’m not quite sure how to think about this.
  </li>

  <li>
    What “goes viral” may not always be what’s the best science. Rarely do thoughtful or contemplative ideas rise to popularity out of the unstructured morass of the Internet, and I find it naïve to expect that scientists would be any different here. That being said, the wisdom of crowds might be the lesser of two evils, given that our current system is basically “ask three random rivals in the field.”
  </li>

  <li>
    There are also just so many papers out there today that it might just become overwhelming, even to specialists. I miss papers in “my areas” constantly, and I try pretty hard to keep up with the literature! (Some have proposed that <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/leap.1514>AI might help us sift through things</a>, but AI might also help people write more papers faster—tough to say who will win.)
  </li>

  <li>
Without the implicit threat of peer review, standards might ebb across the board. I think this is a real concern, but it’s possible that the same community norms enforced through today’s peer review might also be enforced through whatever decentralized review process replaces it. <a href=https://link.springer.com/article/10.1007/s10657-013-9420-1>There’s some evidence</a> that, in knowledge industries with high information asymmetry (like science), communities tend to spontaneously develop strong systems of self-regulation.
  </li>
</ol>

<h2>
4. Conclusion: Archipelagic Multiverse
</h2>

<p>
The most likely scenario, to me, is that all of this sorta happens simultaneously. Most cutting-edge scientific discussion will move to the anarchic world of preprints, but there will still be plenty of room for more traditional journals: some journals will have very high standards and represent the <a href=https://www.palladiummag.com/2022/10/10/the-transformations-of-science/>magisterium of scientific authority</a>, while other journals will act as living repositories of knowledge and still others will become subjectively curated editorial statements. 
</p>

<p>
We can see journals moving in different directions even today: some journals are indicating that they’ll start requiring original data and implement more aggressive fraud detection, while others are moving away from impact-based reviewing. And I can’t help but notice that it seems to be increasingly acceptable to cite preprints in publications, suggesting that the needle might be moving towards the “anarchic preprint lake” scenario ever so slightly. 
</p>

<p>
For my part, I plan to continue writing and submitting papers as necessary, reviewing papers when asked, and so forth—but I’m excited for the future, and to see how the new world order compares to the old. 
</p>

<i>
Thanks to Melanie Blackburn, Jonathan Wong, Joe Gair, and my wife for helpful discussions, and Ari Wagen, Taylor Wagen, and Eugene Kwan for reading drafts of this piece.
</i>

</div><div class='previous-link'><a href='blog_p2.html'>previous page</a></div><div class='next-link'><a href='blog_p4.html'>next page</a></div><br>
  </body>
  <br>
  <footer>
    <a href="mailto:corin.wagen+blog@gmail.com">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">twitter</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
