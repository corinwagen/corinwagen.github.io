<!DOCTYPE html>
<html>
  <head>
    <title>
      Blog
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>
      <li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    <h1 class='blogroll-header'>Blog</h1><div class='next-link'><a href='blog_p2.html'>next page</a></div><br><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221219_low_code_csearch.html'>Low-Code Conformational Searching</a></h2><i>December 19, 2022</i>
<p>
Today I want to engage in some shameless self-promotion and highlight how <a href=https://github.com/ekwan/cctk><i>cctk</i></a>, an open-source Python package that I develop and maintain with Eugene Kwan, can make conformational searching easy.
</p>

<p>
Conformational searching is a really crucial task in computational chemistry, because pretty much everything else you do depends on having the correct structure in the computer. In simple cases you can just draw out every conformer manually, but as the system under study gains degrees of freedom it becomes increasingly impractical to think through every possibility.
</p>

<p>
Failure to identify the correct conformer can lead to completely incorrect results, as demonstrated by Neese and coworkeers in <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">this recent article</a>. They reexamine a reaction <a href="https://www.science.org/doi/10.1126/science.aaq0445">originally studied by Ben List</a> and demonstrate that the conformers examined in the initial publication are almost 5 kcal/mol above the true lowest-energy conformers.
</p>

<figure>
  <img class=centered-img src=../img/20221219_neese_csearch.gif style="width:450px;" />
  <figcaption>
  Figure 1 from the paper; the previously reported conformers are shown in green.
  </figcaption>
</figure>

<p>
Conformational searching approaches attempt to prevent this sort of error by automating the process of finding conformers. There are lots of different algorithms one can use, like <a href="https://pubs.acs.org/doi/10.1021/ja952478m">low-mode searching</a>, metadynamics, and replica exchange (to name just a few), and decades of literature on this topic.
</p>

<p>
Since conformational searching requires many individual calculations, it’s <a href=https://link.springer.com/protocol/10.1007/978-1-0716-0282-9_14>almost never practical</a> to do a conformational search at a high level of theory (e.g. using DFT or <i>ab initio</i> methods). Instead, <a href="https://pubs.acs.org/doi/10.1021/acs.joc.2c00066">forcefields</a> or <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.5b00671">semiempirical</a> methods are generally used, with the caveat that the conformers generated might have somewhat inaccurate geometries.
</p>

<p>
<i>cctk</i> uses <a href="https://crest-lab.github.io/crest-docs/">crest</a> (from Grimme and coworkers), which uses a <a href=https://crest-lab.github.io/crest-docs/page/overview/workflows.html#imtd-gc-algorithm>metadynamics-based algorithm</a> with the <i>GFN2-xtb</i> semiempirical method to generate and score conformers. Although <i>crest</i> isn’t perfect, it’s simple, easy to use, and often generates very reasonable results.
</p>

<p>
I personally find the <i>crest</i> syntax a little tough to remember, so I’ve created a Python script so that I don’t have to look it up every time. 
</p>

<h3>Installing Packages</h3>

<p>
To run this tutorial, you’ll need to have <i>cctk</i> and <i>crest</i> installed. It’s often easiest to manage dependencies using a <i>conda</i> environment; if you don’t already have one, you can create one for this project with this code:
</p>

<pre class=code-block>
conda create --name=chem python=3.8
pip install cctk
conda install -c conda-forge crest
</pre>

<p>
And in the future, you can activate the environment like this:
</p>

<pre class=code-block>
conda activate chem
</pre>

<h3>Running the Tutorial</h3>

<p>
The files for this tutorial can be found <a href="https://github.com/corinwagen/utilities/tree/master/csearch">here</a>. <span class=code>ex.yaml</span>, which is the only file you should need to modify, contains all the information needed for the python script <span class=code>do_crest.py</span>:
</p>

<pre class=code-block>
# list of atoms to constrain
# atom1, atom2, distance (or "auto" to keep distance from initial geometry)
constraints:
    constraint1: 17 31 auto
    constraint2: 30 31 auto

# location of input geometry, either as Gaussian .gjf or .out file
input_geom: pictet_spengler.gjf

# directory in which crest will run (will be created)
directory: crest

# name of logfile
logfile: crest.log

# whether or not this is a noncovalent complex (true or false).
# this simply gets passed to crest; some settings are changed.
noncovalent: false
</pre>

<p>
To generate conformers, simply run:
</p>

<pre class=code-block>
python do_crest.py ex.yaml
</pre>

<p>
This takes about 30 seconds to run on my laptop, and will generate about a hundred output conformers, which can (if desired) be further refined using DFT.
</p>

<p>
Hopefully this is useful! Please feel free to contact me with questions or bug reports.
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221216_carbon_nmr_response.html'>Response to Comments on "Against Carbon NMR"</a></h2><i>December 16, 2022</i>
<p>
Since my previous <a href="https://twitter.com/CraftyCarbene/status/1603572282953289729">“based and red pilled”</a> post seems to have struck a nerve, I figured I should address some common objections people are raising. 
</p>

<p>
Although this is obvious, I wanted to preface all of this by saying: this is my opinion, I'm not some expert on systems of science,
and many of the criticisms come from people with much more scientific and institutional expertise than me. 
It's very possible that I'm just totally wrong here! 
But what I'm saying makes sense to me, and (it seems) to a lot of other people, so I think it's at least worth having this discussion.
</p>

<h2>Commenters Who Feel <sup>13</sup>C NMR Is Scientifically Crucial</h2>

<p>
A few people pointed out that there are lots of instances in which carbon NMR <i>is</i> very important
(<a href="https://twitter.com/MuhammadAdilSA/status/1603339486431416320">1</a>,
<a href="https://twitter.com/craigdc1983/status/1603300877208621056">2</a>,
<a href="https://twitter.com/Double_Anne_/status/1603358363580088320">3</a>,
<a href="https://twitter.com/BogdosMichael/status/1603413304885612545">4</a>,
<a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">5</a>).
I don't disagree with this at all; I've also used <sup>13</sup>C NMR to solve problems that <sup>1</sup>H NMR and mass spectrometry alone couldn't solve!
But just because it’s crucial sometimes doesn’t mean it’s crucial all the time.
Does anyone really think that you need carbon NMR to tell if Boc protection of a primary amine worked? 
</p>

<p>
Most of the reactions that people do—especially people for whom synthetic chemistry is a means and not an end—are <a href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409">pretty straightforward</a>, such that I think it’s fair to assume you could deduce the correct product with high confidence without <sup>13</sup>C NMR.
(Again, if carbon spectra were so crucial, it wouldn’t be the case that many people don’t get them until the very end of the project.)

<h2>Commenters Who Feel That It's Important To Have Non-Crucial Data To Test Your Hypotheses</h2>

<p>
This point was also made by a number of people
(<a href=https://twitter.com/OrthaberLab/status/1603120117444919301>1</a>,
<a href=https://twitter.com/dasingleton/status/1603453351592706067>2</a>,
<a href=https://twitter.com/andrechemist/status/1603465213436633088>3</a>), 
perhaps most succinctly by <a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">“Chris Farley”</a>:

<figure>
  <img class=centered-img src=../img/20221216_chris_farley.png style="width:450px;" />
  <figcaption>
  Never meet your heroes.
  </figcaption>
</figure>

<p>
I think this is an important point—part of what we ought to do, as a scientific community, is challenge one another to test our hypotheses and scrutinize our assumptions. Nevertheless, I’m not convinced this is a particularly strong argument for carbon NMR specifically. What makes <sup>13</sup>C{<sup>1</sup>H} spectra uniquely powerful at challenging one’s assumptions, as opposed to other data?
</p>

<p>
<a href=https://twitter.com/kjfritzsc/status/1603224459753959424>Keith Fritzsching</a> points out that HSQC is much faster and gives pretty comparable information (as did other readers, privately), and simply replacing <sup>13</sup>C with HSQC in most cases seems like it would nicely balance hypothesis testing with efficiency.  
</p>

<p>
(Relatedly, <a href=https://twitter.com/XiaoX_chem/status/1603251065293373440>Xiao Xiao</a> recounts how reviewers will request <sup>13</sup>C spectra even when there’s plenty of other data, including HSQC and HMBC. This is a pretty nice illustration of how powerful status quo bias can be.)
</p>

<h2>Commenters Who Say Carbon Spectra Are Easy To Acquire</h2>

<p>
Both <a href="https://twitter.com/VT_Chemist/status/1603153621511806979">@VT_Chemist</a> and <a href="https://twitter.com/spfletcher/status/1603452172510924800">Steve Fletcher</a> made this point:
</p>

<figure>
  <img class=centered-img src=../img/20221216_steve_fletcher.png style="width:450px;" />
  <figcaption>
  (cue flashbacks to undergraduate me trying to dissolve enough of some tetracyclic monster in pyridine-d5 to see my last quat)
  </figcaption>
</figure>

<p>
I've heard this from plenty of people before, and it's true that sometimes it's not hard at all to get a nice carbon spectrum! But sometimes it <i>is</i> really hard, also.
Based on the other responses, it seems like lots of other people agree with this sentiment.
</p>

<p>
(Is it possible that some of this disagreement reflects whether one has access to a helium cryoprobe?)
</p>

<h2>Commenters Who Feel It's Important To Have Consistent Journal Standards</h2>

<p>
A few people pointed out that making carbon NMR necessary on a case-by-case basis would be burdensome for editors and reviewers, since they'd have to think through each case themselves
(<a href="https://twitter.com/Double_Anne_/status/1603358846734548992">1</a>, <a href="https://twitter.com/rapodaca/status/1603471344468840448">2</a>). 
This is a fair point, and one I don't have a good response to. 
</p>

<p>
However, it's worth noting that this is already what we do for pretty much every other claim, including many complex structural problems: give the data, draw a conclusion, and ask reviewers to evaluate the logic.
Arguments about where the burden of proof should lie are tedious and usually unproductive, but I think we should have a pretty high barrier to making specific methods <i>de jure</i> required for publication.
</p>

<h2>Commenters Who Dislike My Claim That Journals Could Permit More Errors</h2>

<p>
I'm going to highlight <a href="https://twitter.com/dasingleton/status/1603441344747388929">Dan Singleton</a> here, someone I respect a ton:
</p>

<figure>
  <img class=centered-img src=../img/20221216_dan_singleton.png style="width:450px;" />
  <figcaption>
  The thread goes on, obviously, and is worth reading.
  </figcaption>
</figure>

<p>
I’m not trying to suggest that journals ought not to care about accuracy at all; <i>ceteris paribus</i>, accuracy should be prioritized. But given that we’re all constrained by finite resources, it’s important to consider the tradeoffs we’re making with every policy decision. It’s possible that trying to increase accuracy by asking for more data could have deleterious consequences:
</p>

<blockquote>
There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. 
</blockquote>

<p>
I think it’s easiest to think about these issues in terms of two separate questions: (1) relative to where we are today, should we push for more accuracy in the literature or less, and (2) are we achieving our chosen degree of accuracy in the most efficient manner possible? 
</p>

<p>
The first question is clearly complex, and probably deserves a longer and fuller treatment that I can provide here—although I’ll note that <a href=https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review>others have espoused</a> <a href=https://www.liamkofibright.com/uploads/4/8/9/8/48985425/is_peer_review_a_good_idea_.pdf>more radical positions</a> than mine on peer review (h/t <a href=”https://twitter.com/BogdosMichael/status/1603413314868060160”>Michael Bogdos</a> for the second link). I hope to write more on this subject later.
</p>

<p>
But the second question seems more straightforward. Is requiring <sup>13</sup>C NMR for every compound a Pareto-optimal way to ensure accuracy, as opposed to HSQC or HMBC? I struggle to see how the answer can be yes.
</p>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221214_against_carbon_nmr.html'>Against Carbon NMR</a></h2><i>December 14, 2022</i>
<p>
<sup>13</sup>C NMR is, generally speaking, a huge waste of time.
</p>

<p>
This isn’t meant to be an attack on carbon NMR as a scientific tool; it’s an excellent technique, and gives structural information that no other methods can. Rather, I take issue with the requirement that the identity of every published compound be verified with a <sup>13</sup>C NMR spectrum.
</p>

<p>
Very few <sup>13</sup>C NMR experiments yield unanticipated results. While in some cases <sup>13</sup>C NMR is the only reliable way to characterize a molecule, in most cases the structural assignment is obvious from <sup>1</sup>H NMR, and any ambiguity can be resolved with high-resolution mass spectrometry. Most structure determination is boring. Elucidating the structure of bizarre secondary metabolites from sea sponges takes <sup>13</sup>C NMR; figuring out if your amide coupling or click reaction worked does not.
</p>

<p>
The irrelevance of <sup>13</sup>C NMR can be shown via the doctrine of revealed preference: most carbon spectra are acquired only for publication, indicating that researchers are confident in their structural assignments without <sup>13</sup>C NMR. It’s not uncommon for the entire project to be scientifically “done” before any <sup>13</sup>C NMR spectra are acquired. In most fields, people treat <sup>13</sup>C NMR like a nuisance, not a scientific tool—the areas where this isn’t true, like total synthesis or sugar chemistry, are the areas where <sup>13</sup>C NMR is actually useful.
</p>

<p>
Requiring <sup>13</sup>C NMR for publication isn’t costless. The low natural abundance of <sup>13</sup>C and poor gyromagnetic ratio means that <sup>13</sup>C NMR spectra are orders of magnitude more difficult to obtain than <sup>1</sup>H NMR spectra. As a result, a large fraction of instrument time in any chemistry department is usually dedicated to churning out <sup>13</sup>C NMR spectra for publication, while people with actual scientific problems are kept waiting. <sup>13</sup>C NMR-induced demand for NMR time means departments have to buy more instruments, hire more staff, and use more resources; the costs trickle down.
</p>

<p>
And it’s not like eliminating the requirement to provide <sup>13</sup>C NMR spectra would totally upend the way we do chemical research. Most of our field’s history, including some of our greatest achievements, were done in the age before carbon NMR—the first <sup>13</sup>C NMR study of organic molecules was done by <a href="https://aip.scitation.org/doi/10.1063/1.1743253">Lauterbur</a> in 1957, and it would take even longer for the techniques to advance to the point where non-specialists could use the technique routinely. Even in the early 2000s you can find <i>JACS</i> papers without <sup>13</sup>C NMR spectra in the SI, indicating that it's possible to do high-quality research without it. 
</p>

<p>
Why, then, do we require <sup>13</sup>C NMR today? I think it stems from a misguided belief that scientific journals should be the ultimate arbiters of truth—that what’s reported in a journal ought to be trustworthy and above reproach. We hope that by requiring enough data, we can force scientists to do their science properly, and ferret out bad actors along the way. (Perhaps the clearest example of this mindset is <i>JOC</i> &amp; <i>Org. Lett.</i>, who maintain an ever-growing <a href="https://publish.acs.org/publish/author_guidelines?coden=joceah#data_requirements">list of standards</a> for chemical data aimed at requiring all work to be high quality.) Our impulse to require more and more data flows from our desire to make science an institution, a vast repository of knowledge equipped to combat the legions of misinformation.
</p>

<p>
But this hasn’t always been the role of science. Geoff Anders, <a href="https://www.palladiummag.com/2022/10/10/the-transformations-of-science/">writing for <i>Palladium</i></a>, describes how modern science began as an explicitly anti-authoritative enterprise:
</p>

<blockquote>
Boyle maintained that it was possible to base all knowledge of nature on personal observation, thereby eliminating a reliance on the authority of others. He further proposed that if there were differences of opinion, they could be resolved by experiments which would yield observations confirming or denying those opinions. The idea that one would rely on one’s own observations rather than those of others was enshrined in the motto of the Royal Society—<i>nullius in verba</i>.
</blockquote>

<p>
<i>nullius in verba</i> translates to “take no one’s word for it,” not exactly a ringing endorsement of science as institutional authority. This same theme can be found in more recent history. Melinda Baldwin’s <a href="https://astralcodexten.substack.com/p/your-book-review-making-nature"><i>Making Nature</i></a> recounts how peer review—a now-core part of scientific publishing—became commonplace at <i>Nature</i> only in the 1970s. In the 1990s it was still common to publish organic chemistry papers without supporting information.
</p>

<figure>
  <img class=centered-img src=../img/20221214_royal_society.jpeg style="width:450px;" />
  <figcaption>
  The Royal Society—a group of folks who definitely didn't take <sup>13</sup>C NMR spectra.
  </figcaption>
</figure>

<p>
The point I’m trying to make is not that peer review is bad, or that scientific authority is bad, but that the goal of enforcing accuracy in the scientific literature is a new one, and perhaps harder to achieve than we think. There are problems in the scientific literature everywhere, big and small. John Ioannidis memorably claimed that <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">“most published findings are false,”</a> and while chemistry may not suffer from the same issues as the social sciences, we have our own problems. 
<a href="https://pubs.acs.org/doi/10.1021/acscentsci.2c00325">Elemental analysis doesn’t work</a>, <a href="https://chemrxiv.org/engage/chemrxiv/article-details/60c74474ee301c02d6c7916e">integration grids cause problems</a>, and even <a href="http://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html">reactions from famous labs can’t be replicated</a>. Based on this, we might conclude that we’re very, very far from making science a robust repository of truth.
</p>

<p>
Nevertheless, progress marches on. A few misassigned compounds here and there don’t cause too many problems, any more than a faulty elemental analysis report or a sketchy DFT study. Scientific research itself has mechanisms for error correction: anyone who’s ever tried to reproduce a reaction has engaged in one such mechanism. Robust reactions get used, cited, and amplified, while reactions that never work slowly fade into obscurity. Indeed, despite all of the above failures, we’re living through a golden age for our field.
</p>

<p>
Given that we will never be able to eradicate bad data completely, the normative question then becomes “how hard should we try?” In an age of <a href="https://www.agrarheute.com/sites/agrarheute.com/files/2020-01/innovation_scientific_progress.pdf">declining research productivity</a>, we should be mindful not only of the dangers of low standards (proliferation of bad work) but also of the dangers of high standards (making projects take way longer). There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. The claim I hope to make here is that requiring <sup>13</sup>C NMR for every compound does more to slow down good work than it does to prevent bad work, and thus should be abandoned.
</p>

<i>
Update 12/16/2022: see <a href="https://corinwagen.github.io/public/blog/20221216_carbon_nmr_response.html">some followup remarks</a> based on feedback from Twitter.
</i>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221206_definite_games_indefinite_optimism.html'>Definite Games, Indefinite Optimism</a></h2><i>December 6, 2022</i>
<p>
One of the more thought-provoking pieces I read last year was Alex Danco’s post <a href=”https://alexdanco.com/2021/01/11/why-the-canadian-tech-scene-doesnt-work/”>“Why the Canadian Tech Scene Doesn’t Work,”</a> which dissects the structural and institutional factors that make Silicon Valley so much more effective at spawning successful companies than Toronto.
I’ll briefly summarize the piece’s key arguments here, connect it to some ideas from <i>Zero to One</i>, and finish by drawing some conclusions for academia.
</p>

<p>
Danco’s key insight is applying James Carse’s distinction between <a href=”https://en.wikipedia.org/wiki/Finite_and_Infinite_Games”>finite and infinite games</a> to entrepreneurship. What makes a game “finite” or “infinite”?
</p>

<blockquote>
First, <i>finite</i> games are played <i>for the purpose of winning</i>. Whenever you’re engaging in an activity that’s definite, bounded, and where the game can be completed by mutual agreement of all the players, then that’s a finite game. Much of human activity is described in finite game metaphors: wars, politics, sports, whatever. When you’re playing finite games, each action you take is directed towards a pre-established goal, which is to win.
<br>
<br>
In contrast, <i>infinite</i> games are played <i>for the purpose of continuing to play</i>. You do not “win” infinite games; these are activities like learning, culture, community, or any exploration with no defined set of rules nor any pre-agreed-upon conditions for completion. The point of playing is to bring new players into the game, so they can play too. You never “win”, the play just gets more and more rewarding.
</blockquote>

<p>
In entrepreneurship, Danco argues that infinite games are good and finite games are bad. Good founders are playing infinite games: they want to build something important and keep on contributing to society and progress. In contrast, bad founders are playing to win a finite game—acquiring lots of funding, getting high valuation, or exiting with a big IPO.
</p>

<p>
Danco identifies numerous ways that the Canadian startup ecosystem incentivizes finite games at the expense of infinite games. One important factor favoring finitude is the scrutiny given to deals between founders and funders (e.g. in a seed round), which tends to favor conservative or incremental ventures over ambitious, idealistic ones:
</p>

<blockquote>
[High deal scrutiny] is bad, for two reasons. It’s bad because the very best startups, who have the longest time horizon and are most curious about the world, will look disproportionately uninspiring. They’ll have the fewest definite wins relative to their ambition, and the most things that can potentially go wrong.…
<br>
<br>
Conversely, [high deal scrutiny is] bad because startups will learn to optimize for how to get funded. So if seed deals take 3 months, then founders will learn to build companies that look good under that kind of microscope. And that means they’re going to optimize for playing determinate games, so that they can show definable wins that can’t be argued against; rather than what they should be focusing on, which is open-ended growth.
</blockquote>

<p>
Government support for startups also tends to prioritize finite games at the expense of infinite games, since the requisite bureaucracy tends to stifle fast-moving innovation:
</p>

<blockquote>
The problem with [research tax] credits, honestly through no fault of their own, is that you have to say what you’re doing with them. This seems like a pretty benign requirement; and honestly it’s pretty fair that a government program for giving out money should be allowed to ask what the money’s being used for. But in practice, once you take this money and you start filling out time sheets and documenting how your engineers are spending their day, and writing summaries of what kind of R&amp;D value you’re creating, you are well down the path to destroying your startup and killing what makes it work.
</blockquote>

<p>
Overall, Danco paints a picture of a place where an obsession with goals and benchmarks has almost completely crowded out sincere innovation:
</p>

<blockquote>
Quite in character with our love of milestones, Canada loves anything with structure: accelerators, incubators, mentorship programs; anything that looks like an “entrepreneurship certificate”, we can’t get enough of it. <b>We’re utterly addicted with trying to break down the problem of growing startups into bite-size chunks, thoughtfully defining what those chunks are, running a bunch of promising startups through them, and then coming out perplexed when it doesn’t seem to work.</b> <i>(emphasis added)</i>
</blockquote>

<p>
While Danco limits himself to comparing Canada and Silicon Valley, this failure mode is sadly not confined to Canada. Indeed, many of his observations are directly applicable to academic research. The large number of finite games in academia—publishing papers, writing a dissertation, submitting grants, getting tenure—tends to crowd out the more impactful infinite games that lead to real, meaningful progress, and promotes incremental projects with a high chance of success. (This is simply Charles Hummel’s <a href=”https://www.theartofsimple.net/fighting-the-tyranny-of-the-urgent-at-home/”>“tyranny of the urgent”</a> by a different name.)
</p>

<p>
My claim is that the distinction between finite and infinite games is best understood through <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>Peter Thiel’s</a> concept of definite and indefinite optimism. In Thiel’s dichotomy, indefinite optimists believe the world will get better but have no idea how, whereas definite optimists have a concrete proposal for how to make the world better.
</p>

<p>
How does this connect to finite and infinite games? Paradoxically, indefinite optimism leads to an obsession with finite games, since there’s no higher animating principle at work to drive progress. When you don’t have any positive vision for innovation, the natural solution is to write procedures and hope that progress will arise spontaneously if the right steps are followed. Companies, research groups, and other organizations can learn to mimic what actual innovation looks like from afar, but without the proper motivations their ultimate success is improbable.
</p>

<p>
In contrast, an organization playing an infinite game doesn’t need to be forced to jump through arbitrary hoops. Pharmaceutical companies don’t bother to acquire <sup>13</sup>C NMR and IR spectra for every intermediate; startups putting together a minimum viable product don’t worry about properly formatting all their code. Finite games are only a distraction for properly motivated organizations, and one which should be avoided whenever possible.<sup><a href="#fn1">1</a></sup>
</p>

<p>
What conclusions can we draw from this? On a personal level, seek to make your games as infinite as possible. Every startup has to raise money and every graduate student has to publish papers, but one shouldn’t spend most of one’s time worrying about how to publish papers as efficiently as possible.
If you can treat finite games as the distraction that they are, you can give them as little mental effort as possible and spend your time and talents on worthier pursuits.
</p>

<p>
On a broader level, I’m struck by the fact that finite games are a problem of our own making. Nobody becomes a scientist hoping to write papers or win grants;<sup><a href="#fn2">2</a></sup>
our aspirations start out infinite, and it’s only through exposure to the paradigms of the field that we learn to decrease our ambition. 
Indeed, calling a research proposal “ambitious” is reportedly one of the worst criticisms that an NIH study section can give.
</p>

<p>
We should therefore be skeptical about bureaucratic solutions to research stagnation. If our scientific institutions themselves are part of the problem, expanding their reach and importance is unlikely to fix the problem and may indeed do more harm than good. “If you find yourself in a hole, stop digging.”
</p>

<i>
Thanks to Jacob Thackston and Ari Wagen for reading drafts of this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  I’m intentionally eliding the normative question of “how do we assign funding if not through quantitative measures?”—it’s an excellent question, and one which deserves a larger treatment than I can offer here. The <a href="https://scienceplusplus.org/metascience/">Nielsen/Qiu metascience essay</a> has some ideas here.
  </li>
  <li id="fn2">
  This may not always be true, but I think it’s generally true. Most high schoolers or undergraduates are drawn to science because of a sense of wonder or curiosity, which gets transmuted to “publish JACS papers” through the alchemy of graduate school.
  Ari pointed me towards <a href="https://www.briantimar.com/notes/mimetic/mimetic/">this essay</a> by a physics graduate student, which seems relevant.
  </li>
</ol>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221205_quick_fuoss.html'>Introducing <i>quick_fuoss</i></a></h2><i>December 5, 2022</i>
<p>
Modeling ion-pair association/dissociation is an incredibly complex problem, and one that's often beyond the scope of conventional DFT-based techniques. 
(<a href="https://pubs.acs.org/doi/10.1021/acs.joc.1c01823">This study</a> is a nice demonstration of how hard modeling ion-pairing can be.)
Nevertheless, it's still often important to gain insight into what the relative energy of solvent-separated ion pairs and contact-ion pairs might be;
are solvent-separated configurations energetically accessible or not? 
</p>

<p>
I've run into this problem a few times myself: in measuring pKas in nonpolar solvents, and again recently when trying to understand 
<a href="https://pubs.acs.org/doi/10.1021/acs.orglett.2c03622">the solution structure of ethereal HCl.</a>
When digging through the pKa literature, I was surprised to learn that there's a simple and relatively accurate way to estimate the dissociation constant of contact-ion pairs, 
developed by Raymond Fuoss in the 1950s. 
</p>

<p>
Despite modeling ions as charged spheres and solvent as a featureless dielectric, the "Fuoss model" is surprisingly good at reproducing experimental data.
Here's one such example:
</p>

<figure>
  <img class=centered-img src=../img/20221205_exp_vs_fuoss.png style="width:450px;" />
  <figcaption>
  Comparison of experimental ∆G<sub>diss</sub> for tetraisoamylammonium nitrate in various dioxane–water blends vs. Fuoss-calculated ∆G values.
  (Data taken from <a href="https://pubs.acs.org/doi/10.1021/ja01330a023">Fuoss, <i>J. Am. Chem. Soc.</i>, <b>1933</b></a>, Table II.)
  </figcaption>
</figure>

<p>
It's not trivial to think about how to get similar results using more atomistic methods!
(In principle one could actually model the exact solvent mixture and compute the energy of ion-pair dissociation using biased sampling and MD, but this 
would be horrendously expensive and probably less accurate anyway.)
</p>

<p>
The Fuoss model is pretty simple to implement oneself—but to make things even easier, I've implemented it as an open-source Python package, 
which can be imported using <span class=code>pip</span>.
The package contains only a single function, <span class=code>compute_kd</span>, which accepts the name of the cation, the name of the anion, and the dielectric constant of the medium.
(Alternatively, <span class=code>.xyz</span> files, <span class=code>.gjf</span> files, or <span class=code>cctk.Molecule</span> objects can also be given.)
</p>

<p>
Under the hood, the program builds molecules using <i>cctk</i>, computes their volume, and then applies the Fuoss model.
The end result is a comically simple interface:
</p>

<pre class=code-block>
$ pip install quick_fuoss
$ python
&gt;&gt;&gt; import quick_fuoss
&gt;&gt;&gt; quick_fuoss.compute_kd("sodium", "chloride", 80)
1.0793241279015366
</pre>

<p>
On the associated <a href="https://github.com/corinwagen/quick-fuoss">Github repository</a>, there's also a little command-line script which makes this even simpler:
</p>

<pre class=code-block>
$ python quick_fuoss.py tetraisoamylammonium nitrate 8.5
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00004930 M
Ionization energy: 5.873 kcal/mol
$ python quick_fuoss.py tetraisoamylammonium nitrate 11.9
Reading ion #1 from rdkit...
Reading ion #2 from rdkit...
Dissociation constant:	0.00094706 M
Ionization energy: 4.122 kcal/mol
</pre>

<p>
My hope is that this program promotes wider adoption of the Fuoss model, and in general enables more critical thinking about ion-pair energetics in organic solvents.
Please feel free to send any bug reports, complaints, etc. my way!
</p>
</div><div class='next-link'><a href='blog_p2.html'>next page</a></div><br>
  </body>
  <br>
  <footer>
    <a href="mailto:cwagen@g.harvard.edu">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">twitter</a>
    <!--<div style="float:right;">
      <a href='archive.html'>blog archive</a>
    </div> -->
  </footer>
</html>
