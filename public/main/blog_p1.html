<!DOCTYPE html>
<html>
  <head>
    <title>
      Blog
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>
      <!--<li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    <h1 class='blogroll-header'>Blog</h1><div class='next-link'><a href='blog_p2.html'>next page</a></div><br><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20250101_books.html'>Books from 2024</a></h2><i>December 31, 2024</i>
<p>
(Previously: <a href=https://corinwagen.github.io/public/blog/20221231_books.html>2022</a>, <a href=https://corinwagen.github.io/public/blog/20231321_books.html>2023</a>.)
</p>

<b>#1. Baldassar Castiglione, <i>The Book of the Courier</i></b> 

<p>
This book gets cited from time to time as a sort of historical guide to "being cool," since the characters spend some time discussing the idea of <i>sprezzatura</i>, basically grace or effortlessness. More interesting to me was the differences between Renaissance conceptions of virtue, character, & masculinity / femininity and how our culture's used to thinking about these concepts—"the past is a foreign country."
</p>

<b>#2. Grant Cardone, <i>Sell Or Be Sold</i></b> 
<br>
<b>#3. Andrew Chen, <i>The Cold Start Problem</i></b> 
<br>
<b>#4–7. Stephanie Meyer, <i>The Twilight Saga</i></b> 

<p>
Having never read or watched any <i>Twilight</i> before this year, I found them much weirder than I was expecting.
</p>

<b>#8. Fuschia Dunlop, <i>Invitation to a Banquet</i></b> 

<p>
As featured on <i><a href=https://conversationswithtyler.com/episodes/fuchsia-dunlop-3/>CWT</a></i>!
</p>

<b>#9. Iris Murdoch, <i>The Black Prince</i></b> 
<br>
<b>#10. David Kushner, <i>Masters of Doom</i></b> 

<p>
A history of id Software, the company behind <i>Wolfenstein 3D</i>, <i>Doom</i>, <i>Quake</i>, and the <a href=https://en.wikipedia.org/wiki/Fast_inverse_square_root>fast inverse square root</a> algorithm. John Carmack is a <a href=https://waivek.github.io/website/tooltip.html>legendary figure in the software world</a>, and after reading a fictionalized history inspired by id last year (<i>Tomorrow and Tomorrow and Tomorrow</i>) it was good to read the real thing.
</p>

<b>#11. Michael Gerber, <i>The E-Myth Revisited</i></b> 
<br>
<b>#12. William Gibson, <i>Neuromancer</i></b> 

<p>
A lot of old science fiction is hard to appreciate properly—the best ideas have been sucked out and copied a hundredfold, leaving only the author's weirder musings behind to be appreciated.
<i>Neuromancer</i>'s been copied as much as any novel, but I was impressed by the pace and general bleakness of this novel; it holds up well.
</p>

<b>#13–26. Lois McMaster Bujold, <i>The Vorkosigan Saga</i></b> 

<p>
I adored this series, which I read pretty steadily over the course of the year. Bujold writes satisfying, well-constructed plots that keep the focus on characters, not setting. The books fit together nicely, too: each story stands alone, but together paint a decades-long picture of her characters aging, gaining wisdom through their mistakes, and learning to handle the responsibilities placed on them. I think <i>Captain Vorpatril's Alliance</i> is my favorite one.
</p>

<b>#27. R. F. Kuang, <i>Babel</i></b> 
<br>
<b>#28. Clay Christiansen, <i>The Innovator’s Dilemma</i></b> 

<p>
As recommended by <a href=https://www.acquired.fm/episodes/jensen-huang>Jensen Huang</a>; unlike most business books, this one is worth reading all the way through.
</p>

<b>#29. Rob Fitzpatrick, <i>The Mom Test</i></b> 

<p>A canonical book for startup founders, which I probably should have read 1–2 years ago.</p>

<b>#30. Elena Ferrante, <i>My Brilliant Friend</i></b> 

<p>
At its core, this is a very similar story to <i>Wicked</i>: a coming-of-age story focusing on the envious and unstable friendship between two women.
I liked this book, but haven't yet picked up the rest of the Neopolitan Novels; somehow keeping track of the names must intimidate me on a subconscious level.
</p>

<b>#31. Andy Grove, <i>Only The Paranoid Survive</i></b> 
<br>
<b>#32. Vernor Vinge, <i>A Fire Upon The Deep</i></b> 

<p> I liked this book a lot. I would have adored it if I'd read it as a kid, I think; there's something viscerally compelling about Vinge's "Zones of Thought."</p>

<b>#33. C. S. Lewis, <i>The Discarded Image</i></b> 

<p>
This book examines what medieval Europeans thought of the world: how did they see their universe and their place in it? This is a surprisingly subtle question: obviously they were Christian, but their cosmology was considerably different than what even the most "traditional" modern people believe. Last year, I wrote this about <i>The Canterbury Tales</i>:
</p>

<blockquote>
Reading Chaucer fills me with questions about the medieval mind. The stories are steeped in Christianity, as one might expect. Any argument goes back to the Bible, even those among animals, and Chaucer assumes a level of familiarity with e.g. the Psalms far exceeding that of most modern Christians. Yet at the same time the Greco-Roman world looms large: Roman gods appear as plot characters in three tales (the Knight’s Tale, the Merchant’s Tale, and the Manciple’s Tale), and Seneca is viewed as a moral authority on par with Scripture. I’m curious how all these beliefs and ideas fit together and welcome any recommendations on this subject.
</blockquote>

<p>
<i>The Discarded Image</i> exactly answers these questions. If you're at all interested in medieval thought, I highly recommend it.
</p>

<b>#34. Jim Collins, <i>Good To Great</i></b> 
<br>
<b>#35. R. T. France, <i>The Gospel of Mark</i></b> 
<br>
<b>#36. Nathan Azrin, <i>Toilet Training In Less Than One Day</i></b> 

<p>We didn't quite live up to the book's promise, but it took less than a week, so I'm happy.</p>

<b>#37. Tim Keller, <i>Every Good Endeavor</i></b> 
<br>
<b>#38. Brad Feld, <i>Venture Deals</i></b> 

<p>Another canonical book for startup founders, which I also probably should have read before now.</p>

<b>#39. Abigail Shrier, <i>Bad Therapy</i></b> 

<p>Shrier invites controversy here as with her other writing. Sweeping conclusions about American youth aside, I found this surprisingly compelling when viewed as a self-help book about how to be less fearful.</p>

<b>#40. Sheldon Vaunaken, <i>A Severe Mercy</i></b> 

<p>Caused me to weep uncontrollably while stuck in a middle seat on a five-hour flight: you've been warned.</p>

<b>#41. Thich Nhat Hanh, <i>You Are Here</i></b> 
<br>
<b>#42. Gunther Hagen, <i>This Is Germany: An Art Book</i></b> 
<br>
<b>#43. Thomas Malory, <i>Le Mort D’ Arthur</i></b> 
<br>
<b>#44. Georgette Heyer, <i>A Civil Contract</i></b> 
<br>
<b>#45. Alex Hormozi, <i>$100M Offers</i></b> 
<br>
<b>#46. R. F. Kuang, <i>Yellowface</i></b> 
<br>
<b>#47. Barry Werth, <i>The Billion-Dollar Molecule</i></b> 

<p>This book is crazy, and I can't believe I hadn't read it before, particularly since I'm not too distant from a lot of the action, professionally or physically. It's framed as a science story, but I think it works even better at conveying the sheer desperation of early-stage startup life.</p>

<b>#48. Diarmid McCullough, <i>The Reformation</i></b> 

<p>
The Reformation is much weirder than most people, Protestant or Catholic, realize: I was surprised by the diversity of pre-Reformation religious practice in Europe, which was mostly stamped out in the doctrinal standardization of the 1500s. For both Protestants and Catholics, it became very important to separate "us" from "them," which led to the rise of catechisms, inquisitions, and so on.
</p>

<p>
This book also soured me on the "Albion's Seed" idea, as popularized by <a href=https://slatestarcodex.com/2016/04/27/book-review-albions-seed/>the SSC book review</a>. Viewed in isolation, the Puritans seem like a bunch of religious fanatics, but really McCullough argues that the same impulse predominated all over Europe in a "Reformation of Manners," from Charles Borromeo's Milan to Plymouth Colony. Perhaps it's less about the Puritans and more about the 1620s. 
</p>

<b>#49. Amy Chua, <i>Battle Hymn Of The Tiger Mother</i></b>

<p>
This book made it back into <a href=https://x.com/melissa/status/1871793185955172429>the discourse</a>, so I decided I'd actually read it—it's much better than I was expecting, and I don't think most of Chua's critics really understand the book. Conclusions for my own parenting have yet to be determined.
</p>

<br>
<br>

<p> I also read good chunks of a number of textbooks this year, including: </p>

<ul>
  <li>Barto and Sutton's <i>Reinforcement Learning</i></li>
  <li>Kleppmann's <i>Designing Data-Intensive Applications</i></li>
  <li>Di and Kern's <i>Drug-like Properties: Concepts, Structure Design and Methods</i></li>
</ul>

<p>
Overall, this was a good year for books. As the stress of Rowan has ramped up more, I've found it more difficult to write creatively in my free time, and easier to just read other people's words—this manifests in a much-diminished rate of blogging, and a lot more energy diverted to reading fiction.
</p>

<p>
Next year, I hope to read:
</p>

<ul>
  <li>More history. I've picked up a number of books on Napoleon and the Napoleonic wars, since reading <i>A Civil Contract</i> reminded me that my understanding of the whole time period is pretty crude. I'm now a few hundred pages into Andrew Roberts's biography of Napoleon, and I'm realizing that I don't really understand pre-unification Italian history very well either. History just keeps going...</li>
  <li>Relatedly, I want to re-read LeRoy Ladurie's microhistory of Montaillou, with an eye towards understanding family structures, privacy, and the relationship of the family to the broader community. David Brooks argues that <a href=https://www.theatlantic.com/magazine/archive/2020/03/the-nuclear-family-was-a-mistake/605536/>"the nuclear family was a mistake"</a>; I've had a lot of conversations about this topic over the past few years, and I'm not convinced that I or anyone else has a great sense of what these sorts of family dynamics were like five centuries ago. Montaillou seems like the right place to start acquiring some actual data, but any other recommendations are welcome!</li>
  <li>And it might finally be the year I tackle <i>Institutes</i> or <i>City of God</i>...</li>
</ul>

<p>
  Happy new year, and feel free to leave book recommendations in the Substack comments!
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20241011_gems.html'>Are Forcefields Able To Describe Protein Dynamics?</a></h2><i>October 11, 2024</i><p>
<i>
  This post assumes some knowledge of molecular dynamics and forcefields/molecular mechanics. For readers unfamiliar with these topics, Abhishaike Mahajan has <a href=https://www.owlposting.com/p/a-primer-on-molecular-dynamics>a great guide to these topics</a> on his blog.
</i>
</p>

<p>
Although forcefields are commonplace in all sorts of biomolecular simulation today, there’s a growing body of evidence showing that they often give unreliable results. For instance, here’s <a href=https://arxiv.org/pdf/1705.04308>Geoff Hutchison</a> criticizing the use of forcefields for small-molecule geometry optimizations:
</p>

<blockquote>
  The use of classical MM methods for optimizing molecular structures having multiple torsional degrees of freedom is only advised <b>if the precision and accuracy of the final structures and rankings obtained from the conformer searches is of little or no concern</b>... current small molecule force fields should not be trusted to produce accurate potential energy surfaces for large molecules, even in the range of “typical organic compounds.” <i>(emphasis added)</i>
</blockquote>

<p>
Here’s a few other scattered case studies where forcefields have failed:
</p>

<ul>
  <li>
    Plenty of studies have shown that forcefields often produce nonsensical or completely incorrect torsional profiles for organic molecules: <a href=https://pubs.acs.org/doi/10.1021/ci800419j>Gleeson et al showed this in 2009</a>, Hutchison discusses it in the above study, and papers are still demonstrating this today with state-of-the-art forcefields (e.g. the <a href=https://arxiv.org/abs/2312.15211>MACE-OFF23 paper</a>).
  </li>
  <li>
    Similarly, <a href=https://www.nature.com/articles/s41598-018-21070-0>Friederich and co-workers</a> have shown that forcefields which treat each dihedral angle independently (e.g. virtually all commonly-used forcefields) produce very poor results for plenty of common systems, and that explicit dihedral coupling is required to treat these systems accurately.
  </li>
  <li>
    <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00801>MD simulations of a UUCG RNA tetraloop are inaccurate</a>, and the errors cannot be easily fixed owing to the “concerted effect of multiple ff inaccuracies that are coupled and amplifying each other.”
  </li>
  <li>
    The structure and dynamics of a model DNA mini-dumbbell system studied by <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.3c00130>Winkler and co-workers in 2023</a> depends dramatically on the exact forcefield employed, and none of them seem particularly accurate.
  </li>
  <li>
    <a href=https://core.ac.uk/download/pdf/82649571.pdf>A study by D. E. Shaw and coworkers</a> found that the mechanism of protein-folding simulations depends on the forcefield employed, as does the properties of the unfolded state.
  </li>
</ul>

<p>
This list could be a lot longer, but I think the point is clear—even for normal, bio-organic molecules, forcefields often give bad or unreliable answers.
</p>

<p>
Despite all these results, though, it’s tough to know how bad the problem really is because there have been lots of scientific questions that can only be studied with forcefields. Studying protein conformational motion, for instance, is one of the tasks that forcefields have traditionally been developed for, and the scale and size of the systems in question makes it really challenging to study any other way. So although researchers can show that different forcefields give different answers, it’s tough to quantify how close any of these answers is to the truth, and it’s always been possible to hope that a good forcefield really is describing the underlying motion of the system quite well. 
</p>

<p>
It’s for this reason that I’ve been so fascinated by <a href=https://www.science.org/doi/epdf/10.1126/sciadv.adn4397>this April 2024 work from Oliver Unke and co-workers</a>, which studies the dynamics of peptides and proteins using neural network potentials (NNPs). NNPs allow scientists to approach the accuracy of quantum chemical calculations in a tiny fraction of the time by training ML models to reproduce the output of high-level QM-based simulations: although NNPs are still significantly slower than forcefields, they’re typically about 3–6 orders of magnitude faster than the corresponding high-level calculations would be, with only slightly lower accuracy. 
</p>

<figure>
  <img class=centered-img src="../img/20241010_intro.png" style="width:550px;" />
  <figcaption>
    A nice overview of the paper.
  </figcaption>
</figure>

<p>
In this case, Unke and co-workers train a <a href=https://www.nature.com/articles/s41467-021-27504-0>SpookyNet</a>-based NNP to reproduce PBE0/def2-TZVPPD+MBD reference data comprising fragments from the precise systems under study. (MBD refers to <a href=https://manual.q-chem.com/5.2/Ch5.S7.SS5.html>Tkatchenko’s many-body dispersion correction</a>, which can be thought of as a fancier alternative to pairwise dispersion corrections like D3 or D4.) In total, about 60 million atom-labeled data points were used to train the NNPs used in this study—which reportedly took 110,000 hours of CPU time to compute, equivalent to 12 CPU-years! 
</p>

<p>
(This might be a nitpick, but I don’t love the use of PBE0 here. Range-separated hybrids are crucial for producing consistent and accurate results for large zwitterionic biomolecules (see e.g. <a href=https://pubs.acs.org/doi/full/10.1021/acs.jctc.4c00712>this recent work from Valeev</a>), so it’s possible that the underlying training data isn’t as accurate as it seems.)
</p>

<p>
The authors find that the resulting NNPs (“GEMS”) perform much better than existing forcefields in terms of overall error metrics: for instance, GEMS has an MAE of 0.45 meV/atom on snapshots of AceAla15Nme structures taken from MD simulations, while Amber has an MAE of 2.27 meV/atom. <b>What’s much more interesting, however, is that GEMS gives significantly different dynamics than forcefields!</b> While Amber simulations of AceAla15Nme predict that a stable α-helix will form at 300 K, GEMS predicts that a mixture of α- and 3<sub>10</sub> helices exist, which is exactly what’s seen in Ala-rich peptides experimentally. The CHARMM and GROMOS forcefields also get this system wrong, suggesting that GEMS really is significantly more accurate than forcefields at modeling the structure of peptides.
</p>

<figure>
  <img class=centered-img src="../img/20241010_ala15.png" style="width:550px;" />
  <figcaption>
    Amber-based simulations stay in one configuration, while GEMS-based simulations are significantly more flexible. 
  </figcaption>
</figure>

<p>
The authors next study crambin, a small 46-residue protein which is frequently chosen as a model system in papers like this. Similar to what was seen with the Ala<sub>15</sub> helices, crambin is significantly more flexible when modeled by GEMS than when modeled with Amber (see below figure). The authors conduct a variety of other analyses, and argue that there are “qualitative differences between simulations with conventional FFs and GEMS on all timescales.” <b>This is an incredibly significant result, and one that casts doubt on literal decades of forcefield-based MD simulations.</b> Think about what this means for <a href=https://www.dennisgong.com/blog/Relay/>Relay’s MD-based platform</a>, for instance!
</p>

<figure>
  <img class=centered-img src="../img/20241010_crambin.png" style="width:550px;" />
  <figcaption>
A UMAP plot of protein motion through conformational space. (Yes, <a href=https://x.com/lpachter/status/1431325969411821572>we all know UMAP is bad</a>, but this is still a nice plot!)
  </figcaption>
</figure>

<p>
Why do Amber and GEMS differ so much here? Here’s what Unke and coworkers think is going on:
</p>

<blockquote>
AmberFF is a conventional FF, and as such, models bonded interactions with harmonic terms. Consequently, structural fluctuations on small timescales are mostly related to these terms. Intermediate-scale conformational changes as involved in, for example, the “flipping” of the dihedral angle in the disulfide bridges of crambin, on the other hand, can only be mediated by (nonbonded) electrostatic and dispersion terms, because the vast majority of (local) bonded terms stay unchanged for all conformations. On the other hand, GEMS makes no distinction between bonded and non-bonded terms, and individual contributions are not restricted to harmonic potentials or any other fixed functional form. Consequently, it can be expected that large structural fluctuations for AmberFF always correspond to “rare events” associated with large energy barriers, whereas GEMS dynamics arise from a richer interplay between chemical bonds and nonlocal interactions.
</blockquote>

<p>
The overall idea that (1) forcefields impose an unphysical distinction between bonded and non-bonded interactions, and (2) this distinction leads to strange dynamical effects makes sense to me. There’s parts of this discussion that I don’t fully understand—what’s to stop a large structural fluctuation in Amber from having a small barrier? Aren’t all high-barrier processes “rare events” irrespective of where the barrier comes from?
</p>

<p>
There are some obvious caveats here that mean this sort of strategy isn’t ready for widespread adoption yet. These aren’t foundation models; the authors create a new model for each peptide and protein under study by adding system-specific fragments to the training data and retraining the NNP. This takes “between 1 and 2 weeks, depending on the system,” not counting the cost of running all the DFT calculations, so this is far too expensive and slow for routine use. While this might seem like a failure, I think it’s worth reflecting on how tough this problem is. Crambin alone has thousands of degrees of freedom, not counting the surrounding water molecules, and accurately reproducing the results of the Schrodinger equation for this system is an incredible feat. The fact that we can’t automatically also solve this problem in a zero-shot manner for every other protein is hardly a failure, particularly because it seems very likely that scaling these models will dramatically improve their generalizability! 
</p>

<p>
The other big limitation is inference speed: the SpookyNet-based NNPs are about 250x slower than a conventional forcefield, so it’s much tougher to access the long timescales that are needed to simulate processes like protein folding. There are a lot of techniques that can help address these problems: <a href=https://arxiv.org/abs/2409.01931>NNPs can become faster</a> and not require system-specific retraining, <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jpcb.3c05928>coarse graining</a> can reduce the number of particles in the system, and <a href=https://arxiv.org/abs/2208.01893>Boltzmann generators</a> can reduce the number of evaluations needed. So the future is bright, but there’s clearly a lot of ML engineering and applied research that will be needed to help NNP-based simulations scale.
</p>

<p>
But overall, I think this is a very significant piece of work, and one that should make anyone adjacent to forcefield-based MD pause and take note. One day it will be possible to run simulations like this just as quickly as people run regular MD simulations today, and I can’t wait to see what comes of that.
</p>

<p>
<i>
Thanks to Abhishaike Mahajan for helpful feedback on this post.
</i>
</p>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20240917_robotics.html'>Robots Won't Solve Organic Synthesis</a></h2><i>September 17, 2024</i>
<p>
Abhishaike Mahajan recently wrote <a href=https://www.owlposting.com/p/generative-ml-in-chemistry-is-bottlenecked>an excellent piece</a> on how generative ML in chemistry is bottlenecked by synthesis (<i>disclaimer: I gave some comments on the piece, so I may be biased</i>). One of the common reactions to this piece has been that self-driving labs and robotics will soon solve this problem—this is a pretty common sentiment, and one that I’ve heard a lot. 
</p>

<p>
Unfortunately, I think that the strongest version of this take is wrong: organic synthesis won’t be “solved” by just replacing laboratory scientists with robots, because (1) figuring out what reactions to run is hard and (2) running reactions is even harder and (3) we need scientific advances to fix this, not just engineering. 
</p>

<h2>
Predicting What Reactions To Run Is Hard
</h2>

<p>
Organic molecules are typically made through a sequence of reactions, and figuring out how to make a molecule involves both the strategic question of which reactions to run in what order and the tactical question of how to run each reaction. 
</p>

<p>
There’s been a ton of work on both of these problems, and it’s certainly true that computer-assisted retrosynthesis tools have come a long way in the last decade! But retrosynthesis is one of those problems that’s (relatively) easy to be good at and almost impossible to be great at. In part, this is because data in this field tends to be very bad: publications and patents are full of irreproducible or misreported reactions, and negative results are virtually never reported. (<a href=https://www.science.org/content/blog-post/sorta-artificial-intelligence>This post</a> by Derek Lowe is a good overview of some of the problems that the field faces.)
</p>

<p>
But also, the problems are just hard! I got the chance to try out one of the leading retrosynthesis software packages back in my career as an organic chemist, and when we fed it some of the tough synthetic problems we were facing, it gave us all the logical suggestions that we had already tried (unsuccessfully) and then began suggesting insane reactions to us. I can’t really blame the model for not being able to invent new chemistry—but this illustrates the limits of what pure retrosynthesis can accomplish, absent new scientific discoveries.
</p>

<p>
The tactical problem of optimizing reaction conditions is also difficult. In cases where there are a lot of continuous variables (like temperatures or concentrations), conventional optimization methods like design-of-experiments can work well—but where reagents or catalysts are involved, optimization becomes significantly more challenging. Lots of cheminformatics/small-data ML work has been done in this area, but it’s still not straightforward to reliably take a reaction drawn on paper and get it to work in the lab. 
</p>

<h2>
Running Reactions Is Even Harder
</h2>

<p>
All of the above problems are, in principle, solvable. Where I think robotics is likely to struggle even more is in the actual execution of these routes. Synthetic organic chemistry is an arcane and specialized craft that typically requires at least five years of training to be decent at—most published reaction procedures assume that the reader is themselves a trained organic chemist, and omit most of the “obvious” details that are needed to unambiguously specify a sequence of steps. (The incredibly detailed procedures in <a href=https://www.orgsyn.org/><i>Organic Syntheses</i></a> illustrate just how much is missing from the average publication.)
</p>

<p>
My favorite illustration of how irreproducible organic chemistry can be is BlogSyn, a brief project that aimed to anonymously assess how easily published reactions could be reproduced. <a href=https://blog-syn.blogspot.com/2013/02/blog-syn-002-pd-catalyzed-c-3-selective.html>The second BlogSyn post</a> found that a reported olefination of pyridine could not be reproduced—the original author of the paper, Jin-Quan Yu (Scripps) responded, and the shape of reaction tube was ultimately found to be critical to reaction success.
</p>

<p>
<a href=https://blog-syn.blogspot.com/2013/02/blog-syn-003-benzylic-oxidation-of_18.html>The third BlogSyn post</a> found that an IBX-mediated benzylic oxidation reported by Phil Baran (also of Scripps) could not be reproduced at all as written. Phil and his co-authors responded pretty aggressively, and after several weeks of back-and-forth <a href=https://blog-syn.blogspot.com/2013/03/blog-syn-003a-secret-ingredient.html>it was ultimately found</a> that the reaction could be reproduced after modifying virtually every parameter. A comment from Phil’s co-author Tamsyn illustrates some of the complexities at play:
</p>

<blockquote>
There is in [BlogSyn’s] discussion a throw away comment about the 2-methylnaphthalene not being volatile. Have you never showered and then left your hair to dry at room temperature? – water evaporates at RT, just as 2-methylnaphthalene does at 95 ºC. I suggest to you that at the working temperatures of this reaction, the biggest problem may be substrate evaporation (or “hanging out” on the colder parts of the flask as Phil said)... We need fluorobenzene to reflux in these reactions and in so-doing wash substrate back into the reaction from the walls of the vessel, but it clearly slows/inhibits the reaction also – so, we need to tune this balance carefully and with patience. Scale will have a big influence on how well this process works.
</blockquote>

<p>
Tamsyn is, of course, right—volatile substrates can evaporate, and part of setting up a reaction is thinking about the vapor pressure of your substrates and how you can address this. But this sort of thinking requires a trained chemist, and isn’t easily automated. There are a million judgment calls to make in organic synthesis—what concentration to use, how quickly to add the reagent, how to work up the reaction, what extraction solvent to use, and so on—and it’s hard enough to teach first-year graduate students how to do all this, let alone robots. Perhaps at the limit as robots achieve AGI this will be possible, but for now these remain difficult problems. 
</p>

<h2>
We Need Scientific Advances To Fix This
</h2>

<p>
What can be done, then? 
</p>

<p>
From a synthetic point of view, we need more robust reactions. Lots of academics work on reaction development, but the list of truly reliable reactions remains miniscule: amide couplings, Suzuki couplings, addition to Ellman auxiliaries, SuFFEx chemistry, and so on. From a practical point of view, every reaction like this is worth a thousand random papers with a terrible substrate scope (<a href=https://onlinelibrary.wiley.com/doi/full/10.1002/1521-3773%2820010601%2940%3A11%3C2004%3A%3AAID-ANIE2004%3E3.0.CO%3B2-5>Sharpless said it better in 2001 than I ever could</a>; see also <a href=https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409>this 2015 study</a> about how basically no new reactions are used in industry). Approaches like skeletal editing are incredibly exciting, but there’s a limit to how impactful any non-general methodology can be.
</p>

<p>
Perhaps even more important is finding better methods for reaction purification. Purification is one of those topics which doesn’t get a lot of academic attention, but being able to efficiently automate purification unlocks a whole new set of possibilities. Solid-phase synthesis (which makes purification as simple as rinsing off some beads) has always seen some amount of use in organic chemistry, but a lot of commonly-used reactions aren’t compatible with solid support: either new supports or new reactions could address this problem. There are also cool approaches like Marty Burke’s <a href=https://www.nature.com/articles/s44160-024-00558-w?fromPaywallRec=false>“catch-and-release”</a> boronate platform which haven’t yet seen broad adoption.
</p>

<p>
Ultimately, I share the dream of the robotics enthusiasts: if we’re able to make organic synthesis routine, we can stop worrying about how to make molecules and start thinking about what to make! I’m very optimistic about the opportunity of new technologies to address synthetic bottlenecks and enable higher-throughput data generation in chemistry. But getting to this point will take not only laboratory automation but also a ton of scientific progress in organic chemistry, and the first step in solving these problems is actually taking them seriously and recognizing that they’re unsolved.
</p>

<i>
  Thanks to Abhishaike Mahajan and Ari Wagen for helpful comments about this post.
</i>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20240710_dissimilar_clustering.html'>Choosing Maximally Dissimilar Molecules</a></h2><i>July 10, 2024</i>
<p>
I've been playing around with generating non-equilibrium conformations by molecular dynamics recently, and I've been thinking about how to best parse the outputs of a dynamics simulation. A technique I've seen quite often in the literature is "choose a dissimilar subset of conformers by RMSD"—for instance, here's what <a href=https://www.nature.com/articles/s41597-022-01882-6>the SPICE paper</a> says: 
</p>

<blockquote>
  For each of the 677 molecules, the dataset includes 50 conformations of which half are low energy and half are high energy. To generate them, RDKit 2020.09.3 was first used to generate 10 diverse conformations. Each was used as a starting point for 100 ps of molecular dynamics at a temperature of 500 K using OpenMM 7.6 and the Amber14 force field. A conformation was saved every 10 ps to produce 100 candidate high energy conformations. <b>From these, a subset of 25 was selected that were maximally different from each other as measured by all atom RMSD.</b>
</blockquote>

<p>
This makes a good amount of sense: you want to choose conformers which cover as much chemical space as possible so that you get information about the PES as efficiently as possible, and RMSD is a cheap and reasonable way to do this. But how do you actually do this in practice? Nothing super helpful came up after a quick Google search, so I wrote a little script myself:
</p>

<pre class=code-block>
import cctk
import numpy as np
from sklearn.cluster import AgglomerativeClustering
import sys
import tqdm
import copy

e = cctk.XYZFile.read_file(sys.argv[1]).ensemble
molecules = e.molecule_list()

rmsd_matrix = np.zeros((len(molecules), len(molecules)))
comparison_atoms = molecules[0].get_heavy_atoms()

def compute_rmsd(mol1: cctk.Molecule, mol2: cctk.Molecule) -> float:
    geom1 = copy.deepcopy(mol1.geometry[comparison_atoms])
    geom1 -= geom1.mean(axis=0)

    geom2 = copy.deepcopy(mol2.geometry[comparison_atoms])
    geom2 -= geom2.mean(axis=0)

    return cctk.helper_functions.compute_RMSD(geom1, geom2)


for i in tqdm.tqdm(range(len(molecules))):
    for j in range(i + 1, len(molecules)):
        rmsd_matrix[i, j] = compute_rmsd(molecules[i], molecules[j])
        rmsd_matrix[j, i] = rmsd_matrix[i, j]

clustering = AgglomerativeClustering(
    n_clusters=50,
    metric="precomputed",
    linkage="average"
)
clustering.fit(rmsd_matrix)

selected_molecules: list[int] = []
for cluster_id in range(50):
    cluster_indices = np.where(clustering.labels_ == cluster_id)[0]
    selected_molecule = cluster_indices[
        np.argmin(rmsd_matrix[cluster_indices].sum(axis=1))
    ]
    selected_molecules.append(selected_molecule)

e2 = cctk.ConformationalEnsemble()
for idx in selected_molecules:
    e2.add_molecule(molecules[idx])

cctk.XYZFile.write_ensemble_to_file(sys.argv[2], e2)
print("done!")
</pre>

<p> 
This script uses <a href=https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>agglomerative clustering</a> to sort conformations into clusters, but could easily be adapted to work with other algorithms.
To run this script, simply paste into into a file (<span class=code>choose_dissimilar.py</span>) and run:
</p>

<pre class=code-block>
python choose_dissimilar.py input.xyz output.xyz
</pre>

<p>
This will dump 50 output conformers into <span class=code>output.xyz</span>. Hopefully this saves someone some time... happy computing!
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20240613_simple_md.html'>Running Simple MD Simulations</a></h2><i>June 13, 2024</i>
<p>
Scientific software can be confusing, particularly when you're doing something that the software isn't primarily intended for. 
I often find myself wanting to run quick-and-dirty molecular dynamics simulations on small organic molecules, but I've struggled to find an easy way to do this using open-source tools like OpenMM. 
</p>

<p>
This is particularly frustrating since I feel like I should be equipped to succeed at this task: 
</p>

<ul>
<li>I know how to code (well enough for a scientist).</li>
<li>I've read <a href=https://www.amazon.com/Understanding-Molecular-Simulation-Applications-Computational/dp/0122673514>a textbook on MD</a> cover-to-cover.</li>
<li>I've even written <a href=https://github.com/corinwagen/presto>my own MD package</a> from scratch.</li>
</ul>

<p>
Yet despite all this, I've probably tried and failed to use OpenMM for my research a half-dozen times over the past five years (<a href=https://github.com/openmm/openmm/issues/2937>evidence</a>). I always get bogged down somewhere: I don't have PDB files for my compounds, or I can't figure out how to get the forcefield parameters right for a small molecule, or I just get lost in a sea of similar-sounding classes and objects. Part of the problem here is that all the "intro to MD" tutorials seem to assume that you're hoping to run an MD simulation on a protein from the PDB—so if you have an .xyz file, it's not obvious how to proceed.
</p>

<p> Nevertheless, I've finally succeeded. Here's the code, with minor annotations interspersed:</p>

<pre class=code-block>
from openff.toolkit import Molecule, Topology

from openmm import *
from openmm.app import *

import nglview
import mdtraj
import matplotlib.pyplot as plt
import numpy as np
import openmoltools
import tempfile
import cctk
import openmmtools
import math
from random import random, randint

from sys import stdout
import pandas as pd

from rdkit import Chem
from rdkit.Chem import AllChem

from openmmforcefields.generators import SMIRNOFFTemplateGenerator

%config InlineBackend.figure_format='retina'
</pre>

<p> 
Already, we're off to a complex start: we need OpenFF, OpenMM, <span class=code>openmoltools</span>, <span class=code>openmmtools</span>, and <span class=code>openmmforcefields</span>
(not to mention <span class=code>nglview</span> and <span class=code>mdtraj</span>). There's a broader point to be made here about the state of scientific software and how this relates to academic incentive structure, but I digress...
</p>

<pre class=code-block>
smiles = "c1cc(F)ccc1O"

def generate_forcefield(smiles: str) -&gt; ForceField:
    """ Creates an OpenMM ForceField object that knows how to handle a given SMILES string """
    molecule = Molecule.from_smiles(smiles)
    smirnoff = SMIRNOFFTemplateGenerator(molecules=molecule)
    forcefield = ForceField(
      'amber/protein.ff14SB.xml',
      'amber/tip3p_standard.xml',
      'amber/tip3p_HFE_multivalent.xml'
     )
    forcefield.registerTemplateGenerator(smirnoff.generator)
    return forcefield

def generate_initial_pdb(
    smiles: str,
    min_side_length: int = 25, # Å
    solvent_smiles = "O",
) -&gt; PDBFile:
    """ Creates a PDB file for a solvated molecule, starting from two SMILES strings. """

    # do some math to figure how big the box needs to be
    solute = cctk.Molecule.new_from_smiles(smiles)
    solute_volume = solute.volume(qhull=True)
    solvent = cctk.Molecule.new_from_smiles(solvent_smiles)
    solvent_volume = solvent.volume(qhull=False)

    total_volume = 50 * solute_volume # seems safe?
    min_allowed_volume = min_side_length ** 3
    total_volume = max(min_allowed_volume, total_volume)

    total_solvent_volume = total_volume - solute_volume
    n_solvent = int(total_solvent_volume // solvent_volume)
    box_size = total_volume ** (1/3)

    # build pdb
    with tempfile.TemporaryDirectory() as tempdir:
        solute_fname = f"{tempdir}/solute.pdb"
        solvent_fname = f"{tempdir}/solvent.pdb"
        system_fname = f"system.pdb"

        smiles_to_pdb(smiles, solute_fname)
        smiles_to_pdb(solvent_smiles, solvent_fname)
        traj_packmol = openmoltools.packmol.pack_box(
          [solute_fname, solvent_fname],
          [1, n_solvent],
          box_size=box_size
         )
        traj_packmol.save_pdb(system_fname)

        return PDBFile(system_fname)

def smiles_to_pdb(smiles: str, filename: str) -&gt; None:
    """ Turns a SMILES string into a PDB file (written to current working directory). """
    m = Chem.MolFromSmiles(smiles)
    mh = Chem.AddHs(m)
    AllChem.EmbedMolecule(mh)
    Chem.MolToPDBFile(mh, filename)

forcefield = generate_forcefield(smiles)
pdb = generate_initial_pdb(smiles, solvent_smiles="O")

system = forcefield.createSystem(
    pdb.topology,
    nonbondedMethod=PME,
    nonbondedCutoff=1*unit.nanometer,
)
</pre>

<p> This code turns a SMILES string representing our molecule into an OpenMM System, which is a core object in the OpenMM ecosystem. To do this, we have to do a lot of shenanigans involving figuring out how many solvent molecules to add, calling <a href=https://m3g.github.io/packmol/>PACKMOL</a>, etc. One of the key steps here is the <span class=code>SMIRNOFFTemplateGenerator</span> (documented <a href=https://github.com/openmm/openmmforcefields>here</a>), which uses one of the recent OpenFF forcefields to describe our chosen molecule.
</p>


<pre class=code-block>
# initialize Langevin integrator and minimize
integrator = LangevinIntegrator(300 * unit.kelvin, 1 / unit.picosecond, 1 * unit.femtoseconds)
simulation = Simulation(pdb.topology, system, integrator)
simulation.context.setPositions(pdb.positions)
simulation.minimizeEnergy()

# we'll make this an NPT simulation now
system.addForce(MonteCarloBarostat(1*unit.bar, 300*unit.kelvin))
simulation.context.reinitialize(preserveState=True)

checkpoint_interval = 100
printout_interval = 10000

# set the reporters collecting the MD output.
simulation.reporters = []
simulation.reporters.append(DCDReporter("traj_01.dcd", checkpoint_interval))
simulation.reporters.append(
    StateDataReporter(
        stdout,
        printout_interval,
        step=True,
        temperature=True,
        elapsedTime=True,
        volume=True,
        density=True
    )
)

simulation.reporters.append(
    StateDataReporter(
        "scalars_01.csv",
        checkpoint_interval,
        time=True,
        potentialEnergy=True,
        totalEnergy=True,
        temperature=True,
        volume=True,
        density=True,
    )
)

# actually run the MD
simulation.step(500000) # this is the number of steps, you may want fewer to test quickly
</pre>

<p>
Here, we configure the settings that will be familiar to people who read MD textbooks. I've chosen to use a Langevin integrator/thermostat to control temperature, and added a Monte Carlo barostat to make this an NPT simulation (constant pressure &amp; temperature). This isn't appropriate for all uses, but it means we don't have to worry about getting the box size exactly right. We also configure how we're going to save data from the simulation, and then the last line actually runs it (this might take a while if you don't have a GPU). 
<p>

<p> Once we've run this simulation, we can visualize the output and watch things wiggle around! The MDTraj trajectory is <a href=https://www.mdtraj.org/1.9.8.dev0/examples/index.html>a pretty versatile object</a> so you can do much more analysis here if you want to.</p>

<pre class=code-block>
# build MDTraj trajectory
t = mdtraj.load("traj_01.dcd", top="init_01.pdb")

# visualize the trajectory - this works in jupyter at least
view = nglview.show_mdtraj(t)
view.clear_representations()
view.add_licorice()
view.add_unitcell()
view
</pre>

<figure>
  <img class=centered-img src=../img/20240613_nglview.png style="width:550px;" />
  <figcaption>NGLView is actually pretty great, except if you're forming or breaking bonds.</figcaption>
</figure>

<p>It's also good to check that the thermostat actually worked and nothing blew up:</p>

<pre class=code-block>
# check that the thermostat is working
df = pd.read_csv("scalars_01.csv")
df.plot(kind="line", x='#"Time (ps)"', y="Temperature (K)")
</pre>

<p>This is probably laughably simple to an OpenMM expert, but it works well enough for me. Here's my current list of things I still don't know how to do: </p>

<ul>
<li>Start with a specific set of 3D coordinates, not just a SMILES string. This is my biggest issue: it's not easy to specify the geometry of a supramolecular complex using SMILES strings, and while I feel that I ought to be able to modify one of these objects and supply the coordinate information directly, I'm not sure exactly how. (Relatedly, I've never gotten the hang of PDB files for small molecules.)
<li>Fit specific torsions or interactions to high-level data—I see people do this in the literature, but I don't know how.</li>
<li>Frankly, run any more interesting sorts of simulations! Normal MD is way too slow for a lot of things, but enhanced sampling methods get complicated fast: I've done WHAM using my own software, but I don't really know how to do WHAM or metadynamics in other software packages.</li>
<li>Relatedly, I've never gotten the hang of <a href=https://www.plumed.org/>PLUMED</a>.</li>
<li>Build or validate models of non-aqueous solvents. Can I just use this workflow to study, e.g., liquid HF? And how do I figure out if I have the right dipole moment or boiling point for liquid HF? </li>
</ul>

<p>Still, I hope this script is a useful asset to the community, and helps other people not make the same mistakes I did.</p>

<i>Thanks to Dominic Rufa for answering some of my stupid questions.</i>
</div><div class='next-link'><a href='blog_p2.html'>next page</a></div><br>
  </body>
  <br>
  <footer>
    <a href="mailto:corin.wagen+blog@gmail.com">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">x</a>
    <a href="https://scholar.google.com/citations?user=SW0Uhs0AAAAJ">google scholar</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
