<!DOCTYPE html>
<html>
  <head>
    <title>
      
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>
      <!--<li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    
---
title: "Peer Review, Imperfect Feedback"
date: 2023-06-22
blog: True
summary: "Why peer review is the only place that scientists get proper feedback on their work, and why that's good (and bad)."
---

<p>
I’ve been pretty critical of peer review in the past, arguing that it <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>doesn’t accomplish much</a>, <a href=https://corinwagen.github.io/public/blog/20230320_domain_arbitrage.html>contributes to status quo bias</a>, etc. But a few recent experiences remind me of the value that peer review provides: <u>in today’s scientific culture, peer review is essentially the only time that scientists get honest and unbiased feedback on their work.</u> 
</p>

<p>
How can this be true? In experimental science, scientists typically work alongside other students and postdocs under the supervision of a professor. This body of people forms a lab, also known as a research group, and it’s to these people that you present most frequently. Your lab generally knows the techniques and methods that you employ very well: so if you’ve misinterpreted a piece of data or designed an experiment poorly, group meeting is a great place to get feedback. 
</p>

<p>
But a lab is also biased in certain ways. People are attracted to a lab because they think the science is exciting and shows promise, and so they’re likely to be credulous about positive results. Certain labs also develop beliefs or dogmas about how to conduct science: the best ways to perform a mechanistic study, or the most useful reaction conditions. To some extent, every lab is a paradigm unto itself. This means that paradigm-shifting criticism is hard to find among one’s coworkers, even if it’s common in the outside world.
<p>

<p>
Here are some examples of controversial-in-the-field statements that are unlikely to be controversial within given labs:
</p>
<ul>
<li>
Palladium-based catalysis is going to become obsolete due to the scarcity of Pd and must be replaced. <a href=https://www.dal.ca/sites/stradiotto/research.html>Some labs</a> build their research program around this; others think Pd will always be relevant.
<li>
</li>
Water is the greenest possible solvent. Some scientists believe this <a href=https://lipshutz.chem.ucsb.edu/research>wholeheartedly</a>, while others think it’s stupid (for instance, a 2021 <a href=https://link.springer.com/article/10.1007/s10098-021-02188-8>review</a> states that “to meet regulations concerning the discharge of waste water into rivers and other natural waters… [water] often requires very extensive treatment prior to discharge, making the use of water as a reaction medium much less attractive.”)
</li>
<li>
More broadly, the belief that machine learning and generative AI are the future of chemical discovery: while many computational chemists believe this, plenty of experimentalists (even young ones) are pretty skeptical.
</li>
</ul>

<p>
In each of these cases, it’s unlikely that criticism along these lines is available internally: people who’ve chosen to do their PhDs studying ML in chemistry aren’t likely to criticize your paper for overemphasizing the importance of ML in chemistry!
</p>

<p>
More generally, internal criticism works best when a lab serves as a shared repository of expertise, i.e. when everyone in the lab has roughly the same skillset. Some labs focus instead on a single overarching goal and employ many different tools to get to that point: a given chemical biology group might have a synthetic chemist, a MS specialist, a genomics guru, a mechanistic enzymologist, and someone specializing in cell culture. If this is the case, your techniques are opaque to your coworkers: what advice can someone who does cell culture give about improving Q-TOF signal-to-noise?
</p>

<p>
Ideally, one’s professor is well-versed enough in each of the techniques employed that he or she can dispense criticism as needed. But professors are often busy, aren’t always operational experts at each of the techniques they oversee, and suffer from the same viewpoint biases that their students do (perhaps even more so).
</p>

<p>
So, it’s important to solicit feedback from external sources. Unfortunately, at least in my experience most external feedback is too positive: “great talk,” “nice job,” etc. Our scientific culture tries so hard to be supportive that I almost never get any meaningful criticism from people outside my group, either publicly or privately. (Ideally one’s committee would help, but I never really got to present research results to my committee, and this doesn’t help postdocs anyhow.) 
</p>

<p>
Peer review, then, serves as the last bastion against low-quality science: reviewers are outside the lab, have no incentive to be nice, and are tasked specifically with poking holes in your argument or pointing out extra experiments that would improve it. Peer review has improved each one of my papers, and I’m grateful for it.<sup<a href="fn1">1</a></sup>
</p>

<p>
What’s a little sad is that the excellent feedback that reviewers give only comes at the end of a project, which for me has often meant that the results are more than a year old. Much more useful would be critical feedback delivered early on in a project, when my own thinking is more flexible and the barrier to running additional experiments is lower. And more useful still would be high-quality criticism available at every step of the project, given not anonymously but by people whom you can talk to and learn from.
</p>

<p>
What might this practically look like? 
</p>
<ul>
<li>
A culture of “red teaming,” where students are incentivized to find flaws in others’ projects in somewhat adversarial ways. This would need to be done within a supportive and collegial atmosphere, lest it degenerate into bullying: red teaming need not be red in tooth and claw.
</li>
<li>
Similarly, PIs could invite other professors to come to group meetings and (constructively) criticize the projects, particularly professors from adjacent subfields who might have different perspectives.
</li>
<li>
Poster presentations or talks (at conferences), although often used to present finished projects, can also be used to present unfinished work. I presented some unfinished work at <a href=http://jiwu.chem.uh.edu/tpoc.html>TPOC</a> a few years ago, and got really helpful feedback from Ken Houk and Dean Tantillo that fundamentally changed how we approached the rest of the project. Maybe this is something that we should encourage more, although finished projects will probably always look more impressive than unfinished projects.
</li>
<li>
Decentralized peer review solutions like <a href=https://www.theseedsofscience.org/><i>Seeds of Science</i></a> or <a href=https://pubpeer.com/><i>PubPeer</i></a> might also help here, but my sense is that it’s unlikely that qualified experts will just spend their time investigating something that they found online; they need to be solicited somehow.
</li>
</ul>

<p>
I don’t know what the right solution looks like here: the burden of peer review is already substantial, and I don’t mean to suggest that this work ought to be arbitrarily multiplied for free. But I do worry that eliminating peer review, without any other changes, would simply mean that one of the only meaningful chances to get unfiltered feedback on one’s science would be eliminated, and that this would be bad.
<p>

<i>
Thanks to Croix Laconsay and Lucas Karas for helpful feedback on this piece.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
  It's also true that the threat of peer review increases paper quality. I agree with <a href=https://twitter.com/dasingleton/status/1528877848093954048?s=20>Singleton</a> that this is important today, but am less convinced that this is necessary from an institutional design perspective: if peer review didn't exist, I think some other system of norms or regulations would spring forth to protect good-quality science. See the "Anarchic Preprint Lake" discussion in <a href=https://corinwagen.github.io/public/blog/20230427_journals.html>my piece on journals</a>. (h/t Croix Laconsay for raising this point)
  </li
</ol>


  </body>
  <br>
  <footer>
    <a href="mailto:cwagen@g.harvard.edu">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">twitter</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
