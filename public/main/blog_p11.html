<!DOCTYPE html>
<html>
  <head>
    <title>
      Blog
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>
      <!--<li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    <h1 class='blogroll-header'>Blog</h1><div class='previous-link'><a href='blog_p10.html'>previous page</a></div><div class='next-link'><a href='blog_p12.html'>next page</a></div><br><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230103_hdkie_puzzle.html'>Misleading H/D KIE Experiments</a></h2><i>January 3, 2023</i>
<p>
<i>
Note: old versions of this post lacked a discussion of S<sub>N</sub>2. I've added an appendix which remedies this.
</i>
</p>

<p>
In <a href=https://corinwagen.github.io/public/blog/20220815_rate_determining_span.html>“The Rate-Limiting Span,”</a> I discussed how thinking in terms of the span from ground state to transition state, rather than in terms of elementary steps, can help prevent conceptual errors. Today, I want to illustrate why this is important in the context of a little H/D KIE puzzle. 
</p>

<p>
Consider the following reaction, which could conceivably proceed via an S<sub>N</sub>2 mechanism (red), an S<sub>N</sub>1 mechanism (blue), or really anywhere on the continuum:
</p>

<figure>
  <img class="centered-img" src="../img/20230103_structures.png" style="width:550px;" />
  <figcaption> 
    This is a bit of a silly reaction, admittedly. The intermolecular version or the <i>endo</i> version would have been better.
  </figcaption>
</figure>

<p>
What experiment can be used to investigate the mechanism of this reaction? One possibility is an alpha H/D KIE experiment at the iminium position. Reactions with a sp<sup>3</sup> ground state and an sp<sup>2</sup> transition state display secondary normal alpha H/D KIEs, while reactions with an sp<sup>2</sup> ground state and an sp<sup>3</sup> transition state display secondary inverse KIEs.
Thus, one might think “if iminium formation is rate-limiting, the KIE will be normal, but if alkene addition is rate-limiting, the KIE will be inverse.”
</p>

<p>
Unfortunately this is not true. Instead, all mechanistic possibilities give secondary normal KIEs! I investigated this model system computationally at the wB97X-D/6-31G(d)/SMD(CH<sub>2</sub>Cl<sub>2</sub>) level of theory. Here’s a More O’Ferrall–Jencks plot of the H/D KIE at the iminium position, computed with <a href=https://github.com/ekwan/PyQuiver>PyQuiver</a>
(breaking bond on Y axis, forming bond on X axis): 
</p>

<!--
<figure>
  <img class="centered-img" src="../img/20230103_energy.png" style="width:500px;" />
  <figcaption> 
    Some of the grid points didn't finish, which explains the scattered gaps. I've also excluded anything with energy more than 30 kcal/mol above the ground state.
  </figcaption>
</figure>
-->

<figure>
  <img class="centered-img" src="../img/20230103_kie.png" style="width:550px;" />
  <figcaption> 
    The raw data from PyQuiver is a little fuzzy, so I applied a convolution to smooth the data. Details later on. 
  </figcaption>
</figure>

<p>
Rather than telling us which step is rate-limiting, all the KIE shows us is how much the transition state resembles an iminium ion (bottom right). Structures with long C–Cl bond distances and short C–C bond distances have substantial isotope effects (around 20%), while structures with forming or breaking bonds have smaller isotope effects. 
</p>

<p>
Why is this? Both ionization and addition proceed through iminium-like structures that are substantially sp<sup>2</sup>-hybridized at carbon, irrespective of whether sp<sup>2</sup> character is technically increasing or decreasing in the elementary step. Relative to the starting material, both transition states look like iminium ions and thus lead to large isotope effects. 
</p>

<figure>
  <img class="centered-img" src="../img/20230103_structures2.png" style="width:300px;" />
  <figcaption>
  These both look pretty much like iminium ions.
  </figcaption>
</figure>

<p>
We can also conceptualize this in terms of elementary steps. Taken by itself, alkene addition does lead to an inverse kinetic isotope effect, as seen by the decreasing values as one moves left from the iminium—but an inverse isotope effect relative to the normal equilibrium isotope effect of the iminium. In this system, the equilibrium isotope effect of the iminium is larger than the kinetic isotope effect for alkene addition, and so the combination of these two effects leads to a (smaller) overall normal effect. 
</p>

<p>
(This is the opposite of primary H/D KIEs, where central transition states lead to the largest isotope effects and equilibrium effects are typically small. Here, the isotope effect is mainly a function of hybridization, and so the later the TS, the greater the difference in hybridization and the larger the isotope effect.)
</p>

<p>
In summary, although this experiment seems informative, it’s actually not very useful. It tells you something about the structure of the transition state, but not which step is rate-limiting! In this case, a better experiment would be to measure <sup>13</sup>C KIEs, or an H/D KIE on the nucleophile. 
</p>

<h3>Appendix I: What About S<sub>N</sub>2?</h3>

<p>
On Twitter, <a href="https://twitter.com/LevinChem/status/1610287881993728000">Mark Levin</a> asks about the KIE for the concerted path. I originally meant to include a discussion of this, but then totally forgot to! So let’s fix that.
</p>

<p>
As shown in the graph above, extremely concerted pathways (i.e. going right down the middle of the MOJ plot) will have pretty small isotope effects. These sorts of mechanisms are common where the carbocation would be extremely unstable (methyl iodide) but much less common for stabilized carbocations like we’re discussing here. When oxocarbeniums or iminiums are involved, even “concerted” mechanisms shift towards the bottom right corner: this is the <a href=https://pubs.acs.org/doi/pdf/10.1021/ja506092h>“loose”/“exploded” S<sub>N</sub>2</a> often seen in glycosylation. These pathways will have a modest to large H/D KIE, depending on the exact system and how “exploded” they are (see page 53 of <a href=https://pubs.acs.org/doi/10.1021/jacs.6b10621>this SI</a> for examples)
</p>

<p>
Putting this together, then, what experimental results would be conclusive? A very small KIE would be diagnostic for a “classic” synchronous S<sub>N</sub>2 process, which I consider to be very unlikely here. But medium or large H/D KIEs are consistent with any possibility: S<sub>N</sub>1 with rate-limiting ionization, S<sub>N</sub>1 with rate-limiting nucleophilic attack, or a loose S<sub>N</sub>2. There’s an annulus of different mechanistic possibilities that all give about the same isotope effect, as shown below:
</p>

<figure>
  <img class="centered-img" src="../img/20230103_sn2_plot.png" style="width:470px;" />
  <figcaption> 
  Any TS in the red zone is consistent with a moderately large KIE (say, 15%).
  </figcaption>
</figure>

<p>
To make matters worse, H/D KIEs are pretty tough to simulate quantitatively, because of tunneling, so the annulus isn’t even that precise. That’s why I think this isn’t a very good experiment.
</p>

<h3>Appendix II: Smoothing the KIE Grid</h3>

<p>
The raw KIE values from PyQuiver were pretty noisy, probably because there are small or multiple imaginary frequencies for some of these non-stationary points, so I used convolution to smooth things out a bit. 
</p>

<pre class=code-block>
import numpy as np
from scipy.ndimage import convolve

#### this code block takes a 2d np.ndarray of KIE values 
#### and returns a smoothed np.ndarray with the same dimensions

corner_w = 0.05
edge_w = 0.2
kernel = np.array([
    [corner_w, edge_w, corner_w],
    [edge_w, 1 ,edge_w],
    [corner_w, edge_w, corner_w]
])
kernel = kernel / np.sum(kernel)

smooth_grid = convolve(kie_grid, kernel, mode="nearest")
</pre>

<p>
I haven’t seen this technique used before, but it doesn’t seem unreasonable to me.
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221231_books.html'>Books from 2022</a></h2><i>December 31, 2022</i>
<p>
Last January, I aimed to read 50 books in 2022. I got through 32, which is at least more than I read in 2021.
</p>

<p>
There’s been a bit of <a href=https://twitter.com/nabeelqu/status/1608874209375313920>discourse</a> around whether setting numerical reading goals for oneself is worthwhile or counterproductive. I don’t have a strong opinion in the abstract, but I noticed that consciously tracking how many books I read served as a little nudge to read more than I would otherwise, without really compromising the quality of books I was reading.
</p>

<p>
In order, then:
</p>

<b>
#1. Neal Stephenson, <i>Snow Crash</i> (reread)
</b>

<p>
I read this in high school, and wanted to read it again in light of recent Metaverse-related discourse. It didn’t disappoint, although it’s a little more “action movie” than Stephenson’s later works.
</p>

<b>
#2. Aristotle, <i>Nicomachean Ethics</i>
</b>
<br>
<b>
#3. Tim Keller, <i>Prayer</i> (reread)
</b>
<br>
<b>
#4–17. Robert Jordan &amp; Brandon Sanderson, <i>Wheel of Time</i>
</b>

<p>
Beyond the surface-level plot (which is fun), <i>Wheel of Time</i> is a fascinating exploration of a number of societies structured around complementarianism at a deep level.
</p>

<b>
#18. Paul Tripp, <i>Parenting</i>
</b>

<br>
<b>
#19. Peter Thiel, <i>Zero To One</i>
</b>

<p>
I’ve discussed this book <a href=https://corinwagen.github.io/public/blog/20220914_zero_to_one.html>previously</a>.
</p>

<b>
#20. Peter Scazzero, <i>Emotionally Healthy Spirituality</i>
</b>

<br>
<b>
#21. Eric Berger, <i>Liftoff</i>
</b>

<p>
This is a good account of the early days of SpaceX, and works well as a book-length answer to the question “What decisions or mindsets allowed Elon Musk to succeed in starting a rocket company when so many other billionaires failed?” or equivalently “What—besides money—explains SpaceX’s success?”
</p>

<p>
My summary, based on the book, would be (in no particular order): (1) a focus on recruiting top talent, (2) a “can-do” spirit / commitment to moving quickly and recklessly, (3) decisive decision-making at top levels of the organization, (4) a willingness to internalize lots of tasks to increase efficiency, and (5) luck.
</p>

<b>
#22. Roald Dahl, <i>Going Solo</i>
</b>
<br>
<b>
#23. Yiyun Li, <i>Must I Go</i>
</b>
<br>
<b>
#24. Tyler Cowen &amp; Daniel Gross, <i>Talent</i>
</b>

<p>
I’ve also discussed this book <a href=https://corinwagen.github.io/public/blog/20220928_talent.html>previously</a>.
</p>

<b>
#25. Stanley Gundry (Ed.), <i>Five Views on Law and Gospel</i>
</b>

<p>
This book presents five different theological “takes” on the relationship between Old Testament law and the New Testament—there was much less consensus than I expected! It is interesting but scholarly, and not an easy read.
</p>

<p>
There are a whole bunch of books in this series; each author writes an essay explaining their position, and then writes brief responses to the other authors’ positions. This format should be more common!
</p>

<b>
#26. Albert Hirschman, <i>Exit, Voice, and Loyalty</i>
</b>

<p>
I discussed pieces of this book <a href="https://corinwagen.github.io/public/blog/20221018_omelas_hirschman_altom.html">here</a>; the rest is also good.
</p>

<b>
#27. Celeste Ng, <i>Little Fires Everywhere</i>
</b>
<br>
<b>
#28. Fuchsia Dunlop, <i>The Food of Sichuan</i>
</b>

<p>
As discussed on <a href=https://conversationswithtyler.com/episodes/fuchsia-dunlop/> Conversations with Tyler</a>.
</p>

<b>
#29. Geoffrey Moore, <i>Crossing The Chasm</i>
</b>

<p>
This book is about 100 pages longer than it needs to be.
</p>

<b>
#30. John Owen, <i>The Mortification of Sin</i> (abridged)
</b>

<p>
As recommended <a href=https://twitter.com/timkellernyc/status/1477975783641595905>by Tim Keller</a>!
</p>

<b>
#31. Margaret Atwood, <i>Oryx and Crake</i>
</b>
<br>
<b>
#32. Alison Weir, <i>The Wars of the Roses</i>
</b>

<p>
This book is a nice account of the Wars of the Roses in the style of a novel; I didn’t know anything beyond the broad strokes, so I found it quite gripping. My biggest complaint is that the book only goes through 1471, and so doesn’t cover any of the Bosworth Field-adjacent events.
</p>

<p>
My reading this year was about 50% fiction (18 books), with the remainder mostly divided between business (5 books) and Christianity (5 books). My main goal for next year is to read more history; I didn’t end up reading very much history this year, and I miss it.
</p>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221228_boltzmann_error.html'>Evaluating Error in Boltzmann Weighting</a></h2><i>December 28, 2022</i>
<p>
A technique that I’ve seen employed more and more in computational papers over the past few years is to calculate Boltzmann-weighted averages of some property over a conformational ensemble. This is potentially very useful because most complex molecules exist in a plethora of conformations, and so just considering the lowest energy conformer might be totally irrelevant. 
To quote a <a href=https://onlinelibrary.wiley.com/doi/full/10.1002/anie.202205735>recent perspective</a> from Grimme and friends:
</p>

<blockquote>
For highly flexible structures, a molecular property, such as energy, nuclear magnetic resonance spectra, or optical rotation values may not be sufficiently described by a single structure. At finite temperatures, various conformers are populated and the overall property must be described as thermal average over the unique property values of each conformer.
</blockquote>

<p>
What's been bothering me about this, however, is that Boltzmann weights are calculated as <i>e</i> to the power of the relative energy:
</p>

<pre class=code-block>
def boltzmann_weight(conformer_properties, conformer_energies, temp=298):
    """ Some simple Python code to calculate Boltzmann weights. """

    energies = conformer_energies - np.min(conformer_energies)

    R = 3.1668105e-6 # eH/K
    weights = np.exp(-1*energies/(627.509*R*temp))
    weights = weights / np.sum(weights)

    return weights, np.average(conformer_properties, weights=weights)
</pre>

<p>
Since relative energies of conformers are usually calculated with only middling accuracy (±0.2 kcal/mol with <a href=https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j>common methods</a>), we’re taking the exponential of a potentially inaccurate value—which seems bad from the standpoint of error propagation!
</p>

<figure>
  <img class=centered-img src=../img/20221228_xkcd.png style="width:375px;" />
  <figcaption>
  Excerpt from <a href="https://xkcd.com/2295/">xkcd</a>, on error propagation (line #3 is what's relevant here).
  </figcaption>
</figure>

<p>
Grimme and co-authors address this point explicitly in their review:
</p>

<blockquote>
At the same time, however, conformational energies need to be accurate to within about 0.1–0.2 kcal mol<sup>−1</sup> to predict Boltzmann populations at room temperature reasonably well. This is particularly important since properties can vary strongly and even qualitatively between populated conformers…
</blockquote>

<p>
Although the best answer is, of course, to just get more accurate energies, it's not always practical to do that in the real world.
If we take imperfect energies as our starting point, what's the best strategy to pursue?
</p>

<p>
One could imagine a scenario in which error causes relatively unimportant conformers to end up with large weights, making the answer even worse than the naïve approach would have been. If the lowest energy conformer accounts for 60-70% of the answer, might it be best to just stick with that, instead of trying to throw in some messy corrections?
</p>

<p>
To test this, I drew a random flexible-looking molecule with a few functional groups, conducted a conformational search using <i>crest</i>, and then optimized it and calculated <sup>19</sup>F NMR shieldings using wB97X-D/6-31G(d). (There are <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.8b00624>better NMR methods</a> out there, but the absolute accuracy of the shift isn’t really the point here.)
</p>

<figure>
  <img class=centered-img src=../img/20221228_cylview.png style="width:350px;" />
  <figcaption>
  The lowest energy conformer of the molecule I drew (3-fluorohex-5-enal).
  </figcaption>
</figure>

<p>
I then computed more accurate energies using DLPNO-CCSD(T)/cc-pVTZ, and compared the results from Boltzmann weighting with DFT and coupled-cluster energies.
(<sup>19</sup>F values are just the isotropic shielding tensor, and energies are in kcal/mol.)
</p>

<table>
  <tr>
    <th>Conformer</th> 
    <th><sup>19</sup>F shift</th> 
    <th>DFT energy</th>
    <th>DFT weight</th>
    <th>CC energy</th>
    <th>CC weight</th>
  </tr>
  <tr><td>c00003</td><td>401.76</td><td>0.00</td><td>0.624</td><td>0.00</td><td>0.529</td></tr>
  <tr><td>c00001</td><td>403.08</td><td>1.02</td><td>0.112</td><td>0.68</td><td>0.167</td></tr>
  <tr><td>c00010</td><td>396.63</td><td>1.12</td><td>0.093</td><td>1.10</td><td>0.083</td></tr>
  <tr><td>c00007</td><td>391.45</td><td>1.56</td><td>0.045</td><td>1.54</td><td>0.039</td></tr>
  <tr><td>c00004</td><td>396.77</td><td>1.82</td><td>0.029</td><td>1.64</td><td>0.033</td></tr>
  <tr><td>c00006</td><td>400.16</td><td>2.31</td><td>0.013</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00029</td><td>400.37</td><td>2.36</td><td>0.012</td><td>1.75</td><td>0.028</td></tr>
  <tr><td>c00032</td><td>393.96</td><td>2.05</td><td>0.020</td><td>1.76</td><td>0.027</td></tr>
  <tr><td>c00027</td><td>394.60</td><td>2.54</td><td>0.009</td><td>2.21</td><td>0.013</td></tr>
  <tr><td>c00017</td><td>394.69</td><td>3.12</td><td>0.003</td><td>2.27</td><td>0.011</td></tr>
  <tr><td>c00018</td><td>402.24</td><td>2.24</td><td>0.014</td><td>2.35</td><td>0.010</td></tr>
  <tr><td>c00011</td><td>381.31</td><td>2.59</td><td>0.008</td><td>2.49</td><td>0.008</td></tr>
  <tr><td>c00023</td><td>388.77</td><td>2.51</td><td>0.009</td><td>2.54</td><td>0.007</td></tr>
  <tr><td>c00013</td><td>390.32</td><td>3.02</td><td>0.004</td><td>2.61</td><td>0.006</td></tr>
  <tr><td>c00020</td><td>394.97</td><td>3.23</td><td>0.003</td><td>2.62</td><td>0.006</td></tr>
  <tr><td>c00015</td><td>398.24</td><td>3.02</td><td>0.004</td><td>2.97</td><td>0.004</td></tr>
  <tr><td>&nbsp;</td><td></td><td></td><td></td><td></td><td></td><tr>
  <tr><th>Final <sup>19</sup>F Shift</th><td></td><td></td><td>400.20</td><td></td><td>400.13</td></tr>
</table>

<p>
The match is really quite good, much better than just guessing the lowest energy conformer would have been! This is despite having a decent number of low-energy conformers, so I don’t think this is a particularly rigged case.
</p>

<p>
But, what if we just got lucky in this case? The relative energies are off by 0.28 kcal/mol on average. If we simulate adding 0.28 kcal/mol of error to each of the “true” energies a bunch of times, we can see how well Boltzmann weighting does on average, even with potentially unlucky combinations of errors.
</p>

<figure>
  <img class=centered-img src=../img/20221228_randomShifts.png style="width:400px;" />
  <figcaption>
  Shifts from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
The above image shows the predicted shift from 100,000 different randomly generated sets of “wrong” energies. We can see that the Boltzmann-weighted value is almost always closer to the true value than the shift of the major conformer is (99.01% of the time, to be precise). This is despite substantial changes in the weight of the major conformer:
</p>

<figure>
  <img class=centered-img src=../img/20221228_randomWeights.png style="width:400px;" />
  <figcaption>
  Major conformer weights from 100,000 simulations with random error added to CCSD(T) energies.
  </figcaption>
</figure>

<p>
Thus, we can see that Boltzmann weighting is relatively resistant to random errors in this case. Although this is only one molecule, and no doubt scenarios can be devised where inaccurate energies lead to ludicrously incorrect predictions, this little exercise has helped temper my skepticism of Boltzmann weighting.
</p>

<i>
Thanks to Eugene Kwan and Joe Gair for discussions around these topics over the past few years. Data available upon request.
</i>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221219_low_code_csearch.html'>Low-Code Conformational Searching</a></h2><i>December 19, 2022</i>
<p>
Today I want to engage in some shameless self-promotion and highlight how <a href=https://github.com/ekwan/cctk><i>cctk</i></a>, an open-source Python package that I develop and maintain with Eugene Kwan, can make conformational searching easy.
</p>

<p>
Conformational searching is a really crucial task in computational chemistry, because pretty much everything else you do depends on having the correct structure in the computer. In simple cases you can just draw out every conformer manually, but as the system under study gains degrees of freedom it becomes increasingly impractical to think through every possibility.
</p>

<p>
Failure to identify the correct conformer can lead to completely incorrect results, as demonstrated by Neese and coworkeers in <a href="https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC02274E">this recent article</a>. They reexamine a reaction <a href="https://www.science.org/doi/10.1126/science.aaq0445">originally studied by Ben List</a> and demonstrate that the conformers examined in the initial publication are almost 5 kcal/mol above the true lowest-energy conformers.
</p>

<figure>
  <img class=centered-img src=../img/20221219_neese_csearch.gif style="width:450px;" />
  <figcaption>
  Figure 1 from the paper; the previously reported conformers are shown in green.
  </figcaption>
</figure>

<p>
Conformational searching approaches attempt to prevent this sort of error by automating the process of finding conformers. There are lots of different algorithms one can use, like <a href="https://pubs.acs.org/doi/10.1021/ja952478m">low-mode searching</a>, metadynamics, and replica exchange (to name just a few), and decades of literature on this topic.
</p>

<p>
Since conformational searching requires many individual calculations, it’s <a href=https://link.springer.com/protocol/10.1007/978-1-0716-0282-9_14>almost never practical</a> to do a conformational search at a high level of theory (e.g. using DFT or <i>ab initio</i> methods). Instead, <a href="https://pubs.acs.org/doi/10.1021/acs.joc.2c00066">forcefields</a> or <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.5b00671">semiempirical</a> methods are generally used, with the caveat that the conformers generated might have somewhat inaccurate geometries.
</p>

<p>
<i>cctk</i> uses <a href="https://crest-lab.github.io/crest-docs/">crest</a> (from Grimme and coworkers), which uses a <a href=https://crest-lab.github.io/crest-docs/page/overview/workflows.html#imtd-gc-algorithm>metadynamics-based algorithm</a> with the <i>GFN2-xtb</i> semiempirical method to generate and score conformers. Although <i>crest</i> isn’t perfect, it’s simple, easy to use, and often generates very reasonable results.
</p>

<p>
I personally find the <i>crest</i> syntax a little tough to remember, so I’ve created a Python script so that I don’t have to look it up every time. 
</p>

<h3>Installing Packages</h3>

<p>
To run this tutorial, you’ll need to have <i>cctk</i> and <i>crest</i> installed. It’s often easiest to manage dependencies using a <i>conda</i> environment; if you don’t already have one, you can create one for this project with this code:
</p>

<pre class=code-block>
conda create --name=chem python=3.8
pip install cctk
pip install pyyaml
conda install -c conda-forge crest
</pre>

<p>
And in the future, you can activate the environment like this:
</p>

<pre class=code-block>
conda activate chem
</pre>

<h3>Running the Tutorial</h3>

<p>
The files for this tutorial can be found <a href="https://github.com/corinwagen/utilities/tree/master/csearch">here</a>. <span class=code>ex.yaml</span>, which is the only file you should need to modify, contains all the information needed for the python script <span class=code>do_crest.py</span>:
</p>

<pre class=code-block>
# list of atoms to constrain
# atom1, atom2, distance (or "auto" to keep distance from initial geometry)
constraints:
    constraint1: 17 31 auto
    constraint2: 30 31 auto

# location of input geometry, either as Gaussian .gjf or .out file
input_geom: pictet_spengler.gjf

# directory in which crest will run (will be created)
directory: crest

# name of logfile
logfile: crest.log

# whether or not this is a noncovalent complex (true or false).
# this simply gets passed to crest; some settings are changed.
noncovalent: false
</pre>

<p>
To generate conformers, simply run:
</p>

<pre class=code-block>
python do_crest.py ex.yaml
</pre>

<p>
This takes about 30 seconds to run on my laptop, and will generate about a hundred output conformers, which can (if desired) be further refined using DFT.
</p>

<p>
Hopefully this is useful! Please feel free to contact me with questions or bug reports.
</p>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20221216_carbon_nmr_response.html'>Response to Comments on "Against Carbon NMR"</a></h2><i>December 16, 2022</i>
<p>
Since my previous <a href="https://twitter.com/CraftyCarbene/status/1603572282953289729">“based and red pilled”</a> post seems to have struck a nerve, I figured I should address some common objections people are raising. 
</p>

<p>
Although this is obvious, I wanted to preface all of this by saying: this is my opinion, I'm not some expert on systems of science,
and many of the criticisms come from people with much more scientific and institutional expertise than me. 
It's very possible that I'm just totally wrong here! 
But what I'm saying makes sense to me, and (it seems) to a lot of other people, so I think it's at least worth having this discussion.
</p>

<h2>Commenters Who Feel <sup>13</sup>C NMR Is Scientifically Crucial</h2>

<p>
A few people pointed out that there are lots of instances in which carbon NMR <i>is</i> very important
(<a href="https://twitter.com/MuhammadAdilSA/status/1603339486431416320">1</a>,
<a href="https://twitter.com/craigdc1983/status/1603300877208621056">2</a>,
<a href="https://twitter.com/Double_Anne_/status/1603358363580088320">3</a>,
<a href="https://twitter.com/BogdosMichael/status/1603413304885612545">4</a>,
<a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">5</a>).
I don't disagree with this at all; I've also used <sup>13</sup>C NMR to solve problems that <sup>1</sup>H NMR and mass spectrometry alone couldn't solve!
But just because it’s crucial sometimes doesn’t mean it’s crucial all the time.
Does anyone really think that you need carbon NMR to tell if Boc protection of a primary amine worked? 
</p>

<p>
Most of the reactions that people do—especially people for whom synthetic chemistry is a means and not an end—are <a href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.5b01409">pretty straightforward</a>, such that I think it’s fair to assume you could deduce the correct product with high confidence without <sup>13</sup>C NMR.
(Again, if carbon spectra were so crucial, it wouldn’t be the case that many people don’t get them until the very end of the project.)

<h2>Commenters Who Feel That It's Important To Have Non-Crucial Data To Test Your Hypotheses</h2>

<p>
This point was also made by a number of people
(<a href=https://twitter.com/OrthaberLab/status/1603120117444919301>1</a>,
<a href=https://twitter.com/dasingleton/status/1603453351592706067>2</a>,
<a href=https://twitter.com/andrechemist/status/1603465213436633088>3</a>), 
perhaps most succinctly by <a href="https://twitter.com/OscarErlenmeyer/status/1603521689635414019">“Chris Farley”</a>:

<figure>
  <img class=centered-img src=../img/20221216_chris_farley.png style="width:450px;" />
  <figcaption>
  Never meet your heroes.
  </figcaption>
</figure>

<p>
I think this is an important point—part of what we ought to do, as a scientific community, is challenge one another to test our hypotheses and scrutinize our assumptions. Nevertheless, I’m not convinced this is a particularly strong argument for carbon NMR specifically. What makes <sup>13</sup>C{<sup>1</sup>H} spectra uniquely powerful at challenging one’s assumptions, as opposed to other data?
</p>

<p>
<a href=https://twitter.com/kjfritzsc/status/1603224459753959424>Keith Fritzsching</a> points out that HSQC is much faster and gives pretty comparable information (as did other readers, privately), and simply replacing <sup>13</sup>C with HSQC in most cases seems like it would nicely balance hypothesis testing with efficiency.  
</p>

<p>
(Relatedly, <a href=https://twitter.com/XiaoX_chem/status/1603251065293373440>Xiao Xiao</a> recounts how reviewers will request <sup>13</sup>C spectra even when there’s plenty of other data, including HSQC and HMBC. This is a pretty nice illustration of how powerful status quo bias can be.)
</p>

<h2>Commenters Who Say Carbon Spectra Are Easy To Acquire</h2>

<p>
Both <a href="https://twitter.com/VT_Chemist/status/1603153621511806979">@VT_Chemist</a> and <a href="https://twitter.com/spfletcher/status/1603452172510924800">Steve Fletcher</a> made this point:
</p>

<figure>
  <img class=centered-img src=../img/20221216_steve_fletcher.png style="width:450px;" />
  <figcaption>
  (cue flashbacks to undergraduate me trying to dissolve enough of some tetracyclic monster in pyridine-d5 to see my last quat)
  </figcaption>
</figure>

<p>
I've heard this from plenty of people before, and it's true that sometimes it's not hard at all to get a nice carbon spectrum! But sometimes it <i>is</i> really hard, also.
Based on the other responses, it seems like lots of other people agree with this sentiment.
</p>

<p>
(Is it possible that some of this disagreement reflects whether one has access to a helium cryoprobe?)
</p>

<h2>Commenters Who Feel It's Important To Have Consistent Journal Standards</h2>

<p>
A few people pointed out that making carbon NMR necessary on a case-by-case basis would be burdensome for editors and reviewers, since they'd have to think through each case themselves
(<a href="https://twitter.com/Double_Anne_/status/1603358846734548992">1</a>, <a href="https://twitter.com/rapodaca/status/1603471344468840448">2</a>). 
This is a fair point, and one I don't have a good response to. 
</p>

<p>
However, it's worth noting that this is already what we do for pretty much every other claim, including many complex structural problems: give the data, draw a conclusion, and ask reviewers to evaluate the logic.
Arguments about where the burden of proof should lie are tedious and usually unproductive, but I think we should have a pretty high barrier to making specific methods <i>de jure</i> required for publication.
</p>

<h2>Commenters Who Dislike My Claim That Journals Could Permit More Errors</h2>

<p>
I'm going to highlight <a href="https://twitter.com/dasingleton/status/1603441344747388929">Dan Singleton</a> here, someone I respect a ton:
</p>

<figure>
  <img class=centered-img src=../img/20221216_dan_singleton.png style="width:450px;" />
  <figcaption>
  The thread goes on, obviously, and is worth reading.
  </figcaption>
</figure>

<p>
I’m not trying to suggest that journals ought not to care about accuracy at all; <i>ceteris paribus</i>, accuracy should be prioritized. But given that we’re all constrained by finite resources, it’s important to consider the tradeoffs we’re making with every policy decision. It’s possible that trying to increase accuracy by asking for more data could have deleterious consequences:
</p>

<blockquote>
There’s clear extremes on both ends: requiring <sup>1</sup>H NMR spectra for publication is probably good, but requiring a crystal structure of every compound would be ridiculous. 
</blockquote>

<p>
I think it’s easiest to think about these issues in terms of two separate questions: (1) relative to where we are today, should we push for more accuracy in the literature or less, and (2) are we achieving our chosen degree of accuracy in the most efficient manner possible? 
</p>

<p>
The first question is clearly complex, and probably deserves a longer and fuller treatment that I can provide here—although I’ll note that <a href=https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review>others have espoused</a> <a href=https://www.liamkofibright.com/uploads/4/8/9/8/48985425/is_peer_review_a_good_idea_.pdf>more radical positions</a> than mine on peer review (h/t <a href=https://twitter.com/BogdosMichael/status/1603413314868060160>Michael Bogdos</a> for the second link). I hope to write more on this subject later.
</p>

<p>
But the second question seems more straightforward. Is requiring <sup>13</sup>C NMR for every compound a Pareto-optimal way to ensure accuracy, as opposed to HSQC or HMBC? I struggle to see how the answer can be yes.
</p>

</div><div class='previous-link'><a href='blog_p10.html'>previous page</a></div><div class='next-link'><a href='blog_p12.html'>next page</a></div><br>
  </body>
  <br>
  <footer>
    <a href="mailto:corin.wagen+blog@gmail.com">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">twitter</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
