<!DOCTYPE html>
<html>
  <head>
    <title>
      Blog
    </title>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="../../static/style.css">

    <link rel="alternate" type="application/rss+xml" title="RSS feed for the blog" href="/rss.xml">

    <!--google-->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MTNZ0ZSG3W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-MTNZ0ZSG3W');
    </script>

  </head>
  <body>
    <ul class="menu-list">
      <li class="menu-item"><a href="index.html" class="menu-link menu-title">Corin Wagen</a></li>
      <li class="menu-item"><a href="index.html#about" class="menu-link">About</a></li>
      <!--<li class="menu-item"><a href="index.html#projects" class="menu-link">Projects</a></li>-->
      <!--<li class="menu-item"><a href="index.html#past_work" class="menu-link">Past Work</a></li>-->
      <li class="menu-item">
        <a href="blog_p1.html" class="menu-link">Blog</a>
        <a href='archive.html' class="menu-link">(Archive)</a>
      </li>
    </ul>
    <h1 class='blogroll-header'>Blog</h1><div class='previous-link'><a href='blog_p6.html'>previous page</a></div><div class='next-link'><a href='blog_p8.html'>next page</a></div><br><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230804_abundance.html'>How Common Are Different Functional Groups?</a></h2><i>August 4, 2023</i>
<p>
Since the ostensible purpose of organic methodology is to develop reactions that are useful in the real world, the utility of a method is in large part dictated by the accessibility of the starting materials. If a compound is difficult to synthesize or hazardous to work with, then it’s difficult to convince people to use it in a reaction (e.g. most diazoalkanes). Organic chemists are pragmatic, and would usually prefer to run a reaction that starts from a commercial and bench-stable starting material. 
</p>

<p>
For instance, this explains the immense popularity of the Suzuki reaction: although the Neigishi reaction (using organozinc nucleophiles) usually works better for the same substrates, you can buy lots of the organoboron nucleophiles needed to run a Suzuki and leave them lying around without taking any precautions. In contrast, organozinc compounds usually have to be made from the corresponding organolithium/Grignard reagent and used freshly, which is considerably more annoying.
</p>

<p>
The ideal starting material, then, is one which is commercially available and cheap. In recent years, it’s become popular to advertise new synthetic methods by showing that they work on exceptionally cheap and common functional groups, and in particular to compare the abundance of different functional groups to demonstrate that one starting material is more common than another. To pick just one of many examples, Dave MacMillan used this plot to show why cross-coupling reactions of alcohols were important (<a href=https://macmillan.princeton.edu/wp-content/uploads/Zhe.pdf>ref</a>):
</p>

<figure>
  <img class=centered-img src=../img/20230804_macmillan.png style="width:500px;" />
  <figcaption>
  This visual works really well.
  </figcaption>
</figure>

<p>
When I saw MacMillan’s talk at MIT last year, I was curious what it would take to make additional graphics like this. The “number of reactions” plot can be made pretty easily from Reaxys, but I’ve always been uncertain how the “number of commercial sources” plots are made: I haven’t seen references listed for these numbers, nor is anything usually found in the Supporting Information. 
</p>

<p>
I decided to take a swing at getting this data myself by analyzing the <a href=https://mcule.com/database/>Mcule</a> "building blocks" database, which contains about 3.5 million compounds. Although Mcule doesn't define what a building block is (at least, not that I can find), it’s likely that their definition is similar to that of ZINC, which defines building blocks as “those catalogs of compounds available in preparative quantities, typically 250 mg or more” (<a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559>ref</a>). This seems like a reasonable proxy for the sorts of compounds synthetic chemists might use in reactions. I defined patterns to match a bunch of functional groups using SMARTS/SMILES, and then used RDKit to find matches in the Mcule building blocks database. The code can be found on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule>Github</a>, along with the patterns I used.
</p>

<p>
The results are shown below. As expected, ethers, amines, amides, and alcohols are quite common. Surprisingly, aryl chlorides aren't that much more common than aryl bromides—and, except for aliphatic fluorides, all aliphatic halides are quite rare. Allenes, carbodiimides, and SF5 groups are virtually unheard of (&lt;100 examples).
</p>

<table class=left-aligned-table>
  <tr>
    <th>Functional Group</th>
    <th>Number</th>
    <th>Percent</th>
  </tr>
	<tr>
		<td>acid chloride</td>
		<td>6913</td>
		<td>0.19</td>
	</tr>
	<tr>
		<td>alcohol</td>
		<td>1022229</td>
		<td>28.60</td>
	</tr>
	<tr>
		<td>aliphatic bromide</td>
		<td>42018</td>
		<td>1.18</td>
	</tr>
	<tr>
		<td>aliphatic chloride</td>
		<td>70410</td>
		<td>1.97</td>
	</tr>
	<tr>
		<td>aliphatic fluoride</td>
		<td>650576</td>
		<td>18.20</td>
	</tr>
	<tr>
		<td>aliphatic iodide</td>
		<td>3159</td>
		<td>0.09</td>
	</tr>
	<tr>
		<td>alkene</td>
		<td>176484</td>
		<td>4.94</td>
	</tr>
	<tr>
		<td>alkyne</td>
		<td>35577</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>allene</td>
		<td>99</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>amide</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td>anhydride</td>
		<td>1279</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>aryl bromide</td>
		<td>451451</td>
		<td>12.63</td>
	</tr>
	<tr>
		<td>aryl chloride</td>
		<td>661591</td>
		<td>18.51</td>
	</tr>
	<tr>
		<td>aryl fluoride</td>
		<td>618620</td>
		<td>17.31</td>
	</tr>
	<tr>
		<td>aryl iodide</td>
		<td>216723</td>
		<td>6.06</td>
	</tr>
	<tr>
		<td>azide</td>
		<td>5164</td>
		<td>0.14</td>
	</tr>
	<tr>
		<td>aziridine</td>
		<td>748</td>
		<td>0.02</td>
	</tr>
	<tr>
		<td>carbamate</td>
		<td>127103</td>
		<td>3.56</td>
	</tr>
	<tr>
		<td>carbodiimide</td>
		<td>28</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>carbonate</td>
		<td>1231</td>
		<td>0.03</td>
	</tr>
	<tr>
		<td>carboxylic acid</td>
		<td>410860</td>
		<td>11.49</td>
	</tr>
	<tr>
		<td>chloroformate</td>
		<td>250</td>
		<td>0.01</td>
	</tr>
	<tr>
		<td>cyclobutane</td>
		<td>195728</td>
		<td>5.48</td>
	</tr>
	<tr>
		<td>cyclopropane</td>
		<td>349455</td>
		<td>9.78</td>
	</tr>
	<tr>
		<td>diene</td>
		<td>10188</td>
		<td>0.29</td>
	</tr>
	<tr>
		<td>difluoromethyl</td>
		<td>163395</td>
		<td>4.57</td>
	</tr>
	<tr>
		<td>epoxide</td>
		<td>5859</td>
		<td>0.16</td>
	</tr>
	<tr>
		<td>ester</td>
		<td>422715</td>
		<td>11.83</td>
	</tr>
	<tr>
		<td>ether</td>
		<td>1434485</td>
		<td>40.13</td>
	</tr>
	<tr>
		<td>isocyanate</td>
		<td>1440</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>isothiocyanate</td>
		<td>1389</td>
		<td>0.04</td>
	</tr>
	<tr>
		<td>nitrile</td>
		<td>209183</td>
		<td>5.85</td>
	</tr>
	<tr>
		<td>nitro</td>
		<td>126200</td>
		<td>3.53</td>
	</tr>
	<tr>
		<td>pentafluorosulfanyl</td>
		<td>18</td>
		<td>0.00</td>
	</tr>
	<tr>
		<td>primary amine</td>
		<td>904118</td>
		<td>25.29</td>
	</tr>
	<tr>
		<td>secondary amine</td>
		<td>857290</td>
		<td>23.98</td>
	</tr>
	<tr>
		<td>tertiary amine</td>
		<td>609261</td>
		<td>17.04</td>
	</tr>
	<tr>
		<td>trifluoromethoxy</td>
		<td>18567</td>
		<td>0.52</td>
	</tr>
	<tr>
		<td>trifluoromethyl</td>
		<td>455348</td>
		<td>12.74</td>
	</tr>
	<tr>
		<td>urea</td>
		<td>518151</td>
		<td>14.50</td>
	</tr>
	<tr>
		<td><i>Total</i></td>
		<td><i>3574611</i></td>
		<td><i>100.00</i></td>
	</tr>
</table>

<p>
(Fair warning: I’ve spotchecked a number of the SMILES files generated (also on <a href=https://github.com/corinwagen/scratchpad/tree/master/mcule/output>Github</a>), but I haven’t looked through every molecule, so it’s possible that there are some faulty matches. I wouldn’t consider these publication-quality numbers yet.)
</p>

<p>
An obvious caveat: there are lots of commercially “rare” functional groups which are easily accessible from more abundant functional groups. For instance, acid chlorides seem uncommon in the above table, but can usually be made from ubiquitous carboxylic acids with e.g. SOCl2. So these data shouldn’t be taken as a proxy for a more holistic measure of synthetic accessibility—they measure commercial availability, that’s all. 
</p>

<p>
What conclusions can we draw from this?
</p>

<ul>
<li>
The most common functional groups are the milquetoast ones: alcohols, amines, esters, etc. Perhaps this explains <a href=https://pubs.acs.org/doi/pdf/10.1021/acs.jmedchem.5b01409>where all the new reactions have gone</a>: unless your new method works on alcohols or amines, it will struggle to get traction in most of chemical space relative to e.g. Williamson ether synthesis or reductive amination. (Kudos to MacMillan for identifying this; <i>vide supra</i>.)
</li>
<li>
Ureas are much more common than you’d expect from academic methods papers. This I think speaks to the difference between what methodologists want and what medicinal chemists want. Ureas are a bit annoying to work with: they’re pretty polar by the standards of academia, they’re not always soluble in organic solvents, and they have a tendency to stick to transition metal catalysts or get deprotonated by strong bases. But they’re easy to make in libraries, since the isocyanate/amine disconnection is so robust, and they’re excellent hydrogen-bond donors and acceptors.
<b>CORRECTION: There's a SMARTS error, so the match for "ureas" actually matches amides—disregard this section. Thanks to <a href=https://twitter.com/wmdhn>@wmdhn</a> for catching this.</a></b>
</li>
<li>
Uncommon functional groups, like SF<sub>5</sub> and allenes, are very uncommon. If you want to introduce an SF<sub>5</sub> group, you are in for a rough time: there aren’t great ways to add it to molecules (although there have been some steps forward <a href=https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/chem.202201491>in recent years</a>), and there are only 18 commercial examples. So people can write as many <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7728329/>papers</a> as they want about how cool SF<sub>5</sub> groups are: I still doubt we’ll see them used very much in the near future.
</li>
<li>
But also, the abundance of a given functional group is very elastic in the long run. Trifluoromethyl groups used to be extremely rare—they’re not found in nature!—but now 1 in 8 molecules has a CF<sub>3</sub> group. CF<sub>3</sub> just turns out to be a very good handle for a lot of molecular design tasks, and so people found ways to introduce it all over the place, and now it’s not hard to get molecules that have trifluoromethyl groups. Synthetic chemists should feel good about this.
</li>
</ul>

<p>
The functional-group-specific SMILES files are in the previously mentioned Github repo, so anyone who wants to e.g. look through all the commercially available alkenes and perform further cheminformatics analyses can do so. I hope the attached code and data helps other chemists perform similar, and better, studies, and that this sort of thinking can be useful for those who are currently engaged in reaction discovery. 
</p>

<i>
Thanks to Eric Jacobsen for helpful conversations about these data.
</i>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230728_two_cultures.html'>The Two Cultures in Atomistic Simulation</a></h2><i>July 28, 2023</i><br>
<br>
<i>
TW: stereotypes about molecular dynamics.
</i>

<p>
In his fantastic essay “<a href=https://en.wikipedia.org/wiki/The_Two_Cultures>The Two Cultures</a>,” C. P. Snow observed that there was (in 1950s England) a growing divide between the academic cultures of science and the humanities:
</p>

<blockquote>
Literary intellectuals at one pole—at the other scientists, and as the most representative, the physical scientists. Between the two a gulf of mutual incomprehension—sometimes (particularly among the young) hostility and dislike, but most of all lack of understanding. They have a curious distorted image of each other. Their attitudes are so different that, even on the level of emotion, they can't find much common ground.
</blockquote>

<p>
He reflects on the origins of this phenomenon, which he contends is new to the 20th century, and argues that it ought to be opposed:
</p>

<blockquote>
This polarisation is sheer loss to us all. To us as people, and to our society. It is at the same time practical and intellectual and creative loss, and I repeat that it is false to imagine that those three considerations are clearly separable. But for a moment I want to concentrate on the intellectual loss.
</blockquote>

<p>
Snow’s essay is wonderful: his portrait of a vanishing cultural intellectual unity should inspire us all, scientists and otherwise, to improve ourselves, and the elegiac prose reminds the reader that even the best cultural institutions are fragile and fleeting things.
</p>

<p>
I want to make an analogous—but much less powerful—observation about the two cultures present in atomistic simulation. I’ll call these the “QM tribe” and the “MD tribe” for convenience: crudely, “people who use Gaussian/ORCA/Psi4 for their research” and “people who use Schrodinger/AMBER/OpenMM/LAMMPS for their research,” respectively. Although this dichotomy is crude, I contend there are real differences between these two groups, and that their disunity hurts scientific progress.
</p>

<h2>
The Nature of Energy Surfaces
</h2>

<p>
The most fundamental disagreement between these two cultures is in how they think about energy surfaces, I think. Most QM-tribe people think in terms of optimizing to discrete critical points on the potential energy surface: one can perform some sort of gradient-informed optimization to a ground state, or follow negative eigenvalues to a transition state.
</p>

<p>
Implicit to this assumption is that there exist well-defined critical points on the PES, and that finding such critical points is meaningful and productive. Conformers exist, and many people now compute properties as Boltzmann-weighted averages over conformational ensembles, but this is usually done for 10–100 conformers, not thousands or millions. Entropy and solvation, if they’re considered at all, are viewed as corrections, not key factors: since QM is so frequently used to study high-barrier bond-breaking processes where enthalpic factors dominate, one can often get reasonable results with cartoonish treatments of entropy.
</p>

<p>
In contrast, MD-tribe scientists generally don’t think about transition states as specific configurations of atoms—rather, a transition state can emerge from some sort of simulation involving biased sampling, but it’s just a position along some abstract reaction coordinate, rather than a structure which can be visualized in CYLView. Any information gleaned is statistical, rather than concretely visual (e.g. “what is the mean number of hydrogen bonds to this oxygen near this transition state”).
</p>

<p>
Unlike the QM tribe, MD-tribe scientists generally cannot study bond-breaking processes, and so focus on conformational processes (protein folding, aggregation, nucleation, transport) where entropy and solvation are of critical importance: as such, free energy is almost always taken into consideration by MD-tribe scientists, and the underlying PES itself is rarely (to my knowledge) viewed as a worthy topic of study in and of itself.
</p>

<h2>
Molecular Representations
</h2>

<p>
This divide also affects how the two cultures view the task of molecular representation. QM-tribe scientists generally view a list of coordinates and atomic numbers as the most logical representation of a molecule (perhaps with charge and multiplicity information). To the QM tribe, a minimum on the PES represents a structure, and different minima naturally ought to have different representations. Bonding and bond order are not specified, because QM methods can figure that out without assistance (and it’s not uncommon for bonds to change in a QM simulation anyway).
</p>

<p>
In contrast, people in the MD tribe generally want a molecular representation that’s independent of conformation, since many different conformations will intrinsically be considered. (See Connor Coley’s <a href=https://drive.google.com/file/d/1PMZ8GHvvJv_jhy8t6fH73wv4e9ZjHwUN/view>presentation</a> from a recent MolSSI workshop for a discussion of this.) Thus, it’s common to represent molecules through their topology, where connectivity and bond order are explicitly specified. This allows for some pretty wild <a href=https://pubs.acs.org/doi/10.1021/ja00241a001>simulations</a> of species that would be reactive in a QM simulation, but also means that e.g. tautomers can be a massive problem in MD (<a href=https://link.springer.com/article/10.1007/s10822-016-9920-5>ref</a>), since protons can’t equilibrate freely.
</p>

<p>
For property prediction, an uneasy compromise can be reached wherein one takes a SMILES string, performs a conformational search, and then Boltzmann-averages properties over all different conformers, to return a set of values which are associated only with the SMILES string and not any individual conformation. (Matt Sigman does this, as does Bobby Paton <a href=https://nova.chem.colostate.edu/cascade/predict/>for NMR prediction.</a>) This is a lot of work, though.
</p>

<h2>
“A Gulf Of Mutual Incomprehension”
</h2>

<p>
These differences also become apparent when comparing the software packages that different tribes use. Take, for instance, the task of predicting partial charges for a given small molecule. A QM-tribe scientist would expect these charges to be a function of the geometry, whereas an MD-tribe scientist would want the results to be explicitly geometry-independent (<a href=https://www.cell.com/biophysj/fulltext/S0006-3495(22)01326-1>e.g.</a>) so that they can be used for subsequent MD simulations.
</p>

<p>
The assumptions implicit to these worldviews mean that it’s often quite difficult to go from QM-tribe software packages to MD-tribe software packages or vice versa. I’ve been <a href=https://github.com/openforcefield/openff-toolkit/issues/611>stymied</a> before by trying to get OpenMM and openforcefield to work on organic molecules for which I had a list of coordinates and not e.g. a SMILES string—although obviously coordinates will at some point be needed in the MD simulation, most workflows expect you to start from a topology and not an xyz file.
</p>

<p>
Similarly, it’s <a href=https://github.com/nglviewer/nglview/issues/589#issuecomment-468339971>very difficult</a> to get the graphics package NGLView to illustrate the process of bonds breaking and forming—NGLView is typically used for MD, and expects that the system’s topology will be defined at the start of the simulation and never changed. (There are kludgy workarounds, like defining a new object for every frame, but it’s nevertheless true that NGLView is not made for QM-tribe people.)
</p>

<p>
(I’m sure that MD-tribe people are very frustrated by QM software as well, but I don’t have as much experience going in this direction. In general, MD tooling seems quite a bit more advanced than QM-tribe tooling; most MD people I’ve talked to seem to interact with QM software as little as possible, and I can’t say I blame them.)
</p>

<h2>
“A Curious Distorted Image of Each Other”
</h2>

<p>
There are also cultural factors to consider here. The questions that QM-tribe scientists think about are different than those that MD-tribe scientists think about: a somewhat famous QM expert once told me that they were “stuck on an ivory tower where people hold their nose when it comes to DFT, forget anything more approximate,” whereas MD-tribe scientists often seem alarmingly unconcerned about forcefield error but are obsessed with proper sampling and simulation convergence.
</p>

<p>
It seems that most people have only a vague sense of what their congeners in the other tribe actually work on. I don’t think most QM-tribe scientists I know have ever run or analyzed a regular molecular dynamics simulation using e.g. AMBER or OpenMM, nor do I expect that most MD-tribe scientists have tried to find a transition state in Gaussian or ORCA. In theory, coursework could remedy this, but education for QM alone already seems chaotic and ad hoc—trying to cram in MD, statistical mechanics, etc is probably ill-advised at the present.
</p>

<p>
Social considerations also play a role. There’s limited crosstalk between the two fields, especially at the trainee level. How many QM people even know who Prayush Tiwary is, or Michael Shirts, or Jay Ponder? How many MD graduate students have heard of Frank Neese or Troy Van Voorhis? As always, generational talent manages to transcend narrow boundaries—but rank-and-file scientists would benefit immensely from increased contact with the other tribe.
</p>

<h2>
“Unite Them”
</h2>

<p>
I’m not an expert on the history of chemistry, but my understanding is that the two fields were not always so different: Martin Karplus, Arieh Warshel, and Bill Jorgensen, key figures in the development of modern MD, were also formidable quantum chemists. (If any famous chemists who read this blog care to share their thoughts on this history, please email me: you know who you are!)
</p>

<p>
And as the two fields advance, I think they will come closer together once more. As QM becomes capable of tackling larger and larger systems, QM-tribe scientists will be forced to deal with more and more complicated conformational landscapes: modern enantioselective catalysts routinely have hundreds of ground-state complexes to consider (<a href=https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc02274e>ref</a>), and QSimulate and Amgen recently reported full DFT calculations on protein–ligand complexes (<a href=https://core.ac.uk/download/pdf/345075772.pdf>ref</a>).
</p>

<p>
Similarly, the increase in computing power means that many MD use cases (like FEP) are now limited not by insufficient sampling but by the poor energetics of the forcefields they employ. This is difficult to prove unequivocally, but I’ve heard this in interviews with industry folks, and there are certainly plenty of references complaining about poor forcefield accuracy (<a href=https://pubs.acs.org/doi/10.1021/acs.jctc.0c00801>1</a>, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c01081>2</a>): a Psivant <a href=https://pubs.acs.org/doi/10.1021/acs.jcim.7b00564>review</a> dryly notes that “historically solvation energy errors on the order of 2–3 kcal/mol have been considered to be accurate,” which is hardly encouraging.
</p>

<p>
Many QM-tribe professors now work on dynamics: Dean Tantillo and Todd Martinez (who have long been voices “crying out in the wilderness” for dynamics) perhaps most prominently, but also Steven Lopez, Daniel Ess, Fernanda Duarte, Peng Liu, etc. And MD-tribe professors seem more and more interested in using ML mimics of QM to replace forcefields (<a href=https://www.biorxiv.org/content/10.1101/2020.07.29.227959v1>e.g.</a>), which will inevitably lead them down the speed–accuracy rabbit hole that is quantum chemistry. So it seems likely to me that the two fields will increasingly reunite, and that being a good 21st-century computational chemist will require competency in both areas.
</p>

<p>
If this is true, the conclusions for individual computational chemists are obvious: learn techniques outside your specialty, before you get forcibly dragged along by the current of scientific progress! There’s plenty to learn from the other culture of people that deals with more-or-less the same scientific problems you do, and no reason to wait.
</p>

<i>
As a denizen of quantum chemistry myself, I apologize for any misrepresentations or harmful stereotypes about practitioners of molecular dynamics, for whom I have only love and respect. I would be happy to hear any corrections over email.
</i>

</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230720_ml_aimd.html'>Machine Learning for Explicit Solvent Molecular Dynamics</a></h2><i>July 20, 2023</i>
<p>
An important problem with simulating chemical reactions is that reactions generally take place in solvent, but most simulations are run without solvent molecules. This is a big deal, since much of the inaccuracy associated with simulation actually stems from poor treatment of solvation: when gas phase experimental data is compared to computations, the results are often quite good. 
</p>

<p>
Why don’t computational chemists include solvent molecules in their models? It takes a lot of solvent molecules to accurately mimic bulk solvent (enough to cover the system with a few different layers, usually ~10<sup>3</sup>).<sup><a href="#fn1">1</a></sup> Since most quantum chemical methods scale in practice as <i>O</i>(N<sup>2</sup>)–<i>O</i>(N<sup>3</sup>), adding hundreds of additional atoms has a catastrophic effect on the speed of the simulation.
</p>

<p>
To make matters worse, the additional degrees of freedom introduced by the solvent molecules are very “flat”—solvent molecules don’t usually have well-defined positions about the substrate, meaning that the number of energetically accessible conformations goes to infinity (with attendant consequences for entropy). This necessitates a fundamental change in how calculations are performed: instead of finding well-defined extrema on the electronic potential energy surface (ground states or transition states), molecular dynamics (MD) or Monte Carlo simulations must be used to sample from an underlying distribution of structures and reconstruct the free energy surface. Sufficient sampling usually requires consideration of 10<sup>4</sup>–10<sup>6</sup> individual structures,<sup><a href="#fn2">2</sup></a> meaning that each individual computation must be very fast (which is challenging for quantum chemical methods).
</p>

<figure>
  <img class=centered-img src=../img/20230720_jorgensen.png style="width:550px;" />
  <figcaption>
  The title of this paper makes me so sad, because these techniques are still ignored by most organic chemists.
  </figcaption>
</figure>

<p>
Given the complexity this introduces, it’s not surprising that most computational organic chemists try to avoid explicit solvent at all costs. The typical workaround is to use “implicit solvent” models, which “reduce the complexity of individual solvent−solute interactions such as hydrogen-bond, dipole−dipole, and van der Waals interactions into a fictitious surface potential... scaled to reproduce the experimental solvation free energies” (<a href=https://pubs.acs.org/doi/abs/10.1021/acs.organomet.8b00456?src=recsys>Baik</a>). This preserves the well-defined potential energy surfaces that organic chemists are accustomed to, so you can still find transition states by eigenvector following, etc.
</p>

<p>
Implicit solvent models like PCM, COSMO, or SMD are better than nothing, but are known to struggle for charged species. In particular, they don’t really describe explicit inner-sphere solvent–solute interactions (like hydrogen bonding), meaning that they’ll behave poorly when these interactions are important. Dan Singleton’s paper on <a href=https://pubs.acs.org/doi/10.1021/ja5111392>the Baylis–Hillman reaction</a> is a nice case study of how badly implicit solvent can fail: even high-level quantum chemical methods are useless when solvation free energies are 10 kcal/mol off from experiment!
</p>

<p>
This issue is well-known. To quote from <a href=https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201709943>Schreiner and Grimme</a>:
</p>

<blockquote>
An even more important but still open issue is solvation. In the opinion of the authors it is a ‘scandal’ that in 2018 no routine free energy solvation method is available beyond (moderately successful) continuum theories such as COSMO-RS and SMD and classical FF/MD-based explicit treatments.
</blockquote>

<p>
When computational studies have been performed in explicit solvent, the results have often been promising: Singleton has studied <a href=https://pubs.acs.org/doi/full/10.1021/jacs.0c06295>diene hydrochlorination</a> and <a href=https://pubs.acs.org/doi/10.1021/jacs.6b07328>nitration of toluene</a>, and Peng Liu has recently conducted a nice <a href=https://pubs.acs.org/doi/abs/10.1021/jacs.0c12096>study of chemical glycosylation</a>. Nevertheless, these studies all require heroic levels of effort: quantum chemistry is still very slow, and so a single free energy surface might take months and months to compute.<sup><a href="#fn3">3</a></sup>
</p>

<p>
One promising workaround is using machine learning to accelerate quantum chemistry. Since these MD-type studies look at the same exact system over and over again, we could imagine first training some sort of ML model based on high-level quantum chemistry data, and then employing this model over and over again for the actual MD run. As long as (1) the ML model is faster than the QM method used to train it and (2) it takes less data to train the ML model than it would to run the simulation, this will save time: in most cases, a lot of time. 
</p>

<p>
(This is a somewhat different use case than e.g. <a href=https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a>ANI</a>-type models, which aim to achieve decent accuracy for any organic molecule. Here, we already know what system we want to study, and we’re willing to do some training up front.)
</p>

<p>
A lot of people are working in this field right now, but today I want to highlight some work that I liked from Fernanda Duarte and co-workers. Last year, they published a <a href=https://pubs.rsc.org/en/content/articlelanding/2022/cp/d2cp02978b#cit23>paper</a> comparing a few different ML methods for studying quasiclassical dynamics (in the gas phase), and found that atomic cluster expansion (ACE) performed better than Gaussian approximation potentials while training faster than NequIP. They then went on to show that ACE models could be trained automatically through active learning, and used the models to successfully predict product ratios for cycloadditions with post-TS bifurcations.
</p>

<p>
Their new <a href=https://chemrxiv.org/engage/chemrxiv/article-details/64a8085fba3e99daefab8f89>paper</a>, posted on ChemRxiv yesterday, applies the same ACE/active learning approach to studying reactions in explicit solvent, with the reaction of cyclopentadiene and methyl vinyl ketone chosen as a model system. This is more challenging than their previous work, because the ML model now not only has to recapitulate the solute reactivity but also the solute–solvent and solvent–solvent interactions. To try and capture all the different interactions efficiently, the authors ended up using four different sets of training data: substrates only, substrates with 2 solvent molecules, substrates with 33 solvent molecules, and clusters of solvent only. 
</p>

<p>
Previously, the authors used an energy-based selector to determine if a structure should be added to the training set: they predicted the energy with the model, ran a QM calculation, and selected the structure if the difference between the two values was big enough. This approach makes a lot of sense, but has the unfortunate downside that a lot of QM calculations are needed, which is exactly what this ML-based approach is trying to avoid. Here, the authors found that they could use similarity-based descriptors to select data points to add to the training set: these descriptors are both more efficient (needing fewer structures to converge) and faster to compute, making them overall a much better choice. (This approach is reminiscent of the metadynamics-based approach previously <a href=https://arxiv.org/abs/1712.07240>reported</a> by John Parkhill and co-workers.)
</p>

<p>
With a properly trained model in hand, the authors went on to study the reaction with biased sampling MD. They find that the reaction is indeed accelerated in explicit water, and that the free energy surface begins to look stepwise, as opposed to the concerted mechanism predicted in implicit solvent. (Singleton has observed similar behavior <a href=https://pubs.acs.org/doi/10.1021/ja208779k>before</a>, and <a href=https://chemrxiv.org/engage/chemrxiv/article-details/6362b49531107263acfa50e6>I’ve seen this too.</a>) They do some other interesting studies: they look at the difference between methanol and water as solvents, argue that Houk is wrong about the role of water in the TS,<sup><a href="#fn4">4</a></sup> and suggest that the hydrophobic effect drives solvent-induced rate acceleration.<sup><a href="#fn5">5</a></sup>
</p>

<figure>
  <img class=centered-img src=../img/20230720_pes.png style="width:450px;" />
  <figcaption>
  Figure 4B from the paper, showing the change in the PES.
  </figcaption>
</figure>

<p>
The results they find for this particular system are interesting, but more exciting is the promise that these techniques may soon become accessible to “regular” computational chemists. Duarte and co-workers have shown that ML can be used to solve an age-old problem in chemical simulation; if explicit solvent ML/MD simulations of organic reactions become easy enough for non-experts to run, I have no doubt that they will become a valued and essential part of the physical organic chemistry toolbox. Much work is needed to get to that point—new software packages, further validation on new systems, new ways to assess quality and check robustness of simulations, and much more—but the vision behind this paper is powerful, and I can’t wait until it comes to fruition.
</p>

<i>
Thanks to Croix Laconsay for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    <a href=https://www.youtube.com/watch?v=caxpxBT8JsM>This video</a> from Chris Cramer makes the point nicely.
  </li>
  <li id="fn2">
    This obviously depends on the system in question, and what processes are being studied. But in general insufficient sampling is a big issue in molecular dynamics, which I think is underappreciated by organic chemists wading into the area. Jeff Grossman has a <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.0c00833>nice paper on this</a>.
  </li>
  <li id="fn3">
    If you look carefully, many people who claim to be doing big <i>ab initio</i> molecular dynamics studies are actually doing semiempirical molecular dynamics. This isn’t dishonest per se, but it’s a little underwhelming to a computational chemist, especially when it’s only mentioned in the SI. Things get even more confusing when plane wave DFT is employed: in theory, plane wave DFT can be just as accurate as regular DFT, but in practice there are some sneaky approximations that often get introduced.
  </li>
  <li id="fn4">
  This argument hinges on whether uphill dynamics (starting from reactants, going to transition state) or downhill dynamics (starting from transition state, going to reactants) are more appropriate. The authors argue that "uphill dynamics allow the solvent sufficient time to reorganise [<i>sic</i>] before the trajectory passes the free energy barrier, providing a more realistic view of solvent behaviour [<i>sic</i>] during the reaction." I'm not fully convinced by this—isn't the idea that the system reorganizes to minimize the energy of the transition state a basic precept of transition state theory? But I'm not convinced I understand these issues deeply enough to have an opinion; I will leave this to the experts.
  </li>
  <li id="fn5">
    This argument hearkens back to some old-school computational organic chemistry I love from Bill Jorgensen, studying the <a href=https://pubs.aip.org/aip/jcp/article-abstract/77/11/5757/783263/Monte-Carlo-simulation-of-n-butane-in-water?redirectedFrom=fulltext>hydrophobic effect on conformational preferences of butane</a>. We usually think of the hydrophobic effect as associated with macromolecules (ligands binding to proteins, etc), but it can still matter in tiny systems!
  </li>
</ol>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230717_fmufros.html'>For-Profit Micro Focused Research Organizations: A Proposal</a></h2><i>July 17, 2023</i>
<p>
<i>
TW: sarcasm.
</i>
</p>

<p>
Today, most research is done by academic labs funded mainly by the government. Many articles have been written on the shortcomings with academic research: Sam Rodriques recently had a nice <a href=https://www.sam-rodriques.com/post/academia-is-an-educational-institution>post</a> about how academia is ultimately an educational institution, and how this limits the quality of academic research. (It’s worth a read; I’ve written about these issues from a <a href=https://corinwagen.github.io/public/blog/20221026_structural_diversity.html>few</a> <a href=https://corinwagen.github.io/public/blog/20230502_startups.html>angles</a>, and will probably write more at a later date.)
</p>

<p>
The major alternative to academic research that people put forward is <a href=https://www.nature.com/articles/d41586-022-00018-5>focused research organizations</a> (FROs): large, non-profit research organizations capable of tackling big unsolved problems. These organizations, similar in scope and ambition to e.g. CERN or LIGO, are envisioned to operate with a budget of $20–100M over five years, making them substantially larger and more expensive than a single academic lab. This model is still being tested, but it seems likely that some version of FROs will prove effective for appropriately sized problems. 
</p>

<p>
But FROs have some disadvantages, too: they represent a significant investment on the part of funders, and so it’s important to choose projects where there’s a high likelihood of impact in the given area. (In contrast, it’s expected that most new academic labs will focus on high-risk projects, and pivot if things don’t work out in a few years.) In this piece, I propose a new form of scientific organization that combines aspects of both FROs and academic labs: <u>for-profit micro focused research organizations (FPµFROs)</u>.
</p>

<p>
The key insight behind FPµFROs is that existing financial markets could be used to fund scientific research when there is a realistic possibility for profit as a result of the research. This means that FPµFROs need not be funded by the government or philanthropic spending, but could instead raise capital from e.g. venture capitalists or angel investors, who have access to substantially more money and are used to making high-risk, high-reward investments. 
</p>

<p>
FPµFROs would also be smaller and more nimble than full-fledged FROs, able to tackle high-risk problems just like academia. But unlike academic labs, FPµFROs would be able to spend more freely and hire more aggressively, thus circumventing the human capital issues that plague academic research. While most academic labs are staffed entirely with inexperienced trainees (as Rodriques notes above), FPµFROs could hire experienced scientists, engineers, and programmers, thus accelerating the rate of scientific progress.
</p>

<p>
One limitation of the FPµFRO model is that research would need to be profitable within a reasonable time frame. But this limitation might actually be a blessing in disguise: the need for profitability means that FPµFROs would be incentivized to provide real value to firms, thus preventing useless research through the magic of Adam Smith’s invisible hand. 
</p>

<p>
Another disadvantage of FPµFROs is that they must be able to achieve success with relatively little funding (probably around $10M; big for academia, but small compared to a FRO). This means that their projects would have to be modest in scope. I think this is probably a blessing in disguise, though. Consider the following advice <a href=http://www.paulgraham.com/ambitious.html>from Paul Graham</a>:
</p>

<blockquote>
Empirically, the way to do really big things seems to be to start with deceptively small things.… Maybe it's a bad idea to have really big ambitions initially, because the bigger your ambition, the longer it's going to take, and the further you project into the future, the more likely you'll get it wrong.
</blockquote>

<p>
Thus, the need for FPµFROs to focus on getting a single “minimal viable product” right might be very helpful, and could even lead to more impactful firms later on.
</p>

<p>
In conclusion, FPµFROs could combine the best qualities of academic labs and FROs: they would be agile and risk-tolerant, like academic labs, but properly incentivized to produce useful research instead of publishing papers, like FROs. This novel model should be investigated further as a mechanism for generating new scientific discoveries at scale with immediate short-term utility.
</p>

<br>
<div class=dinkus>* * *</div>
<br>

<p>
Hopefully it’s clear by now that this is a joke: an FPµFRO is just a startup. 
</p>

<p>
The point of this piece isn’t to criticize FROs or academia: both have their unique advantages relative to startups, and much has been written about the relative advantages and disadvantages of different sorts of research institutions (<a href=https://www.convergentresearch.org/about-fros>e.g.</a>).
</p>

<p>
Rather, I want to remind people that startups can do really good scientific work, something that many people seem to forget. It’s true that basic research can be a public good, and something that’s difficult to monetize within a reasonable timeframe. But most research today isn’t quite this basic, which leads me to suspect that many activities today confined to academic labs could be profitably conducted in startups. 
</p>

<p>
Academics are generally very skeptical of organizations motivated by profit. But all incentives are imperfect, and the drive to achieve profitability pushes companies to provide value to real customers, which is more than many academics motivated by publication or prestige ever manage to achieve. It seems likely that for organizations focused on applied research, profit is the least bad incentive. 
</p>

<p>
I’ll close with a quote from Eric Gilliam’s recent <a href=https://www.freaktakes.com/p/an-alternative-approach-to-deep-tech>essay</a> on a new model for “deep tech” startups:
</p>

<blockquote>
Our corporate R&amp;D labs in most industries have taken a step back in how “basic” their research is. Meanwhile, what universities call ‘applied’ research has become much less applied than it used to be. This ‘middle’ of the deep tech pipeline has been hollowed out.
</blockquote>

<p>
What Eric proposes in his piece, and what I’m arguing here, is that scientific startups can help fill this void: not by replacing FROs and academic research, but by complementing them. 
</p>

<i>Thanks to Ari Wagen for reading a draft of this piece.</i>
</div><div class='blogroll-container'><h2><a class='blogroll-title' href='../../public/blog/20230710_pka.html'>pKa and Nonpolar Media</a></h2><i>July 10, 2023</i>
<p>
The concept of p<i>K</i><sub>a</sub> is introduced so early in the organic chemistry curriculum that it’s easy to overlook what a remarkable idea it is.
</p>

<p>
<i>
Briefly, for the non-chemists reading this: p</i>K<i><sub>a</sub> is defined as the negative base-10 logarithm of the acidity constant of a given acid H–A:
</i>
</p>

<p>
p<i>K</i><sub>a</sub> := -log<sub>10</sub>([HA]/[A-][H+]) 
</p>

<p>
<i>
Unlike pH, which describes the acidity of a bulk solution, p</i>K<i><sub>a</sub> describes the intrinsic proclivity of a molecule to shed a proton—a given molecule in a given solvent will always have the same p</i>K<i><sub>a</sub>, no matter the pH. This makes p</i>K<i><sub>a</sub> a very useful tool for ranking molecules by their acidity (e.g. <a href=http://ccc.chem.pitt.edu/wipf/MechOMs/evans_pKa_table.pdf>the Evans p</i>K<i><sub>a</sub> table</a>).
</i>
</p>

<p>
The claim implicit in the definition of p<i>K</i><sub>a</sub> is that a single parameter suffices to describe the acidity of each molecule.<sup><a href="#fn1">1</a></sup> In general, this isn’t true in chemistry—there’s no single “reactivity” parameter which describes how reactive a given molecule is. For various regions of chemical space a two-parameter model can <a href=https://www.cup.lmu.de/oc/mayr/reaktionsdatenbank/>work</a>, but in general we don’t expect to be able to evaluate the efficacy of a given reaction by looking up the reactivity values of the reactants and seeing if they’re close enough. 
</p>

<p>
Instead, structure and reactivity interact with each other in complex, high-dimensional ways. A diene will react with an electron-poor alkene and not an alcohol, while acetyl chloride doesn’t react with alkenes but will readily acetylate alcohols, and a free radical might ignore both the alkene and the alcohol and abstract a hydrogen from somewhere else. Making sense of this confusing morass of different behaviors is, on some level, what organic chemistry is all about. The fact that the reactivity of different functional groups depends on reaction conditions is key to most forms of synthesis!
</p>

<p>
But p<i>K</i><sub>a</sub> isn’t so complicated. If I want to know whether acetic acid will protonate pyridine in a given solvent, all I have to do is look up the p<i>K</i><sub>a</sub> values for acetic acid and pyridinium (pyridine’s conjugate acid). If pyridinium has a higher p<i>K</i><sub>a</sub>, protonation will be favored; otherwise, it’ll be disfavored. More generally, one can predict the equilibrium distribution of protons amongst <i>N</i> different sites from a list of the corresponding p<i>K</i><sub>a</sub>s. 
</p>

<p>
Why is p<i>K</i><sub>a</sub> so well-behaved? The key assumption underlying the above definition is that ions are free and do not interact with one another. This allows us to neglect any specific ion–ion interactions, and makes the scale universal: if the pyridinium cation and the acetate anion never interact, then I can learn everything I need to about pyridinium acetate just by measuring the p<i>K</i><sub>a</sub>s of pyridine and acetic acid in isolation. 
</p>

<p>
This assumption is quite good in solvents like water or DMSO, which excel at stabilizing charged species, but progressively breaks down as one travels to the realm of nonpolar solvents. As ions start to pair with themselves, specific molecule–molecule interactions become important. The relative size of the anions can matter: in a nonpolar solvent, a small anion will be better stabilized by a small cation than by a large, diffuse cation, meaning that e.g. acetate will appear more acidic when protonating smaller molecules. Other more quotidian intermolecular interactions, like hydrogen bonding and π-stacking, can also play a role.
</p>

<p>
And the ions aren’t the only thing that can stick together: aggregation of acids is often observed in nonpolar solvents. Benzenesulfonic acid <a href=https://link.springer.com/article/10.2116/analsci.15.303>forms a trimer</a> in benzonitrile solution, which is still pretty polar, and alcohols and carboxylic acids are known to aggregate under a variety of conditions as well.<sup><a href="#fn2">2</a></sup> Even seemingly innocuous species like tetrabutylammonium chloride will aggregate at high concentrations (<a href=https://pubs.acs.org/doi/pdf/10.1021/ja9723139>ref</a>, <a href=https://pubs.rsc.org/en/content/articlelanding/1995/c3/c39950002513>ref</a>).
</p>

<p>
To reliably extend p<i>K</i><sub>a</sub> scales to nonpolar solvents, one must thus deliberately choose compounds which resist aggregation. As the dielectric constant drops, so does the number of such compounds. The clearest demonstration of this I’ve found is a series of papers (<a href=https://pubs.acs.org/doi/10.1021/jo9713013>1</a>, <a href=https://pubs.acs.org/doi/10.1021/jo0343477>2</a>) by p<i>K</i><sub>a</sub> guru Ivo Leito measuring the acidity of a series of fluorinated compounds in heptane:
</p>

<figure>
  <img class=centered-img src=../img/20230710_leito.png style="width:350px;" />
  <figcaption>
  A portion of the scale, showing the compounds employed.
  </figcaption>
</figure>

<p>
This effort, while heroic, demonstrates the futility of measuring p<i>K</i><sub>a</sub> in nonpolar media from the standpoint of the synthetic chemist. If only weird fluoroalkanes engineered not to aggregate can have p<i>K</i><sub>a</sub> values, then the scale may be analytically robust, but it’s hardly useful for designing reactions! 
</p>

<p>
The key point here is that the difficulty of measuring p<i>K</i><sub>a</sub> in nonpolar media is not an analytical barrier which can be surmounted by new and improved technologies, but rather a fundamental breakdown in the idea of p<i>K</i><sub>a</sub> itself. Even the best p<i>K</i><sub>a</sub> measurement tool in the world can’t determine the p<i>K</i><sub>a</sub> of HCl in hexanes, because no such value exists—the concept itself is borderline nonsensical. Chloride will ion-pair with everything in hexanes, hydrogen chloride will aggregate with itself, chloride will stick to hydrogen chloride, and so forth. Asking for a p<i>K</i><sub>a</sub> in this context just doesn't make much sense.<sup><a href="#fn3">3</a></sup>
</p>

<p>
It’s important to remember, however, that just because the p<i>K</i><sub>a</sub> scale no longer functions in nonpolar solvents doesn’t mean that acids don’t have different acidities. Triflic acid in toluene will still protonate just about <a href=https://pubs.acs.org/doi/abs/10.1021/ol061174+>everything</a>, whereas acetic acid will not. Instead, chemists wishing to think about acidity in nonpolar media have to accept that no one-dimensional scale will be forthcoming. The idealized world of p<i>K</i><sub>a</sub> we’re accustomed to may no longer function in nonpolar solvents, but chemistry itself still works just fine.
</p>

<i>
Thanks to Ivo Leito for discussing these topics with me over Zoom, and to Joe Gair for reading a draft of this post.
</i>

<h3>Footnotes</h3>
<ol class=footnotes>
  <li id="fn1">
    Really, each proton.
  </li>
  <li id="fn2">
   Ivo Leito called carboxylic acids "the world champions of aggregation" when I asked him about these issues.
  </li>
  <li id="fn3">
    Even experimentally nonsensical p<i>K</i><sub>a</sub>s can be simulated, though: Jorgensen <a href=https://pubs.acs.org/doi/10.1021/ja00256a053>famously</a> used free energy perturbation to compute the p<i>K</i><sub>a</sub> of ethane in water.
  </li>
</ol>
</div><div class='previous-link'><a href='blog_p6.html'>previous page</a></div><div class='next-link'><a href='blog_p8.html'>next page</a></div><br>
  </body>
  <br>
  <footer>
    <a href="mailto:corin.wagen+blog@gmail.com">email</a>
    <a href="https://github.com/corinwagen">github</a>
    <a href="https://twitter.com/CorinWagen">x</a>
    <a href="https://scholar.google.com/citations?user=SW0Uhs0AAAAJ">google scholar</a>
    <div style="float:right;">
      <a href="/rss.xml">rss</a>
      <a href="https://cwagen.substack.com">substack</a>
    </div>
  </footer>
</html>
